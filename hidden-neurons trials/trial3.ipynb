{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1)\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df=pd.read_excel('Pre-Processed-Data.xlsx')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Dew Point  Wind Speed  Pressure  Pressure Grad\n",
       "0        4.8        2.47    1029.0             -8\n",
       "1        8.2        7.42    1021.4              0\n",
       "2        6.8        6.81    1021.8             11\n",
       "3        3.6        3.94    1033.7             -1\n",
       "4        6.1        3.33    1033.4            -11"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Pressure Grad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.8</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.2</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.6</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dew Point        0\n",
       "Wind Speed       0\n",
       "Pressure         0\n",
       "Pressure Grad    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "X = df.drop(['Wind Speed'], axis=1)\n",
    "#Assign the Target column as the output \n",
    "Y= df['Wind Speed']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "X_norm=(X-X.min())/(X.max()-X.min())\n",
    "X_norm"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Dew Point  Pressure  Pressure Grad\n",
       "0      0.487805  0.752055       0.449275\n",
       "1      0.606272  0.647945       0.565217\n",
       "2      0.557491  0.653425       0.724638\n",
       "3      0.445993  0.816438       0.550725\n",
       "4      0.533101  0.812329       0.405797\n",
       "...         ...       ...            ...\n",
       "1091   0.411150  0.706849       0.623188\n",
       "1092   0.470383  0.767123       0.594203\n",
       "1093   0.595819  0.795890       0.594203\n",
       "1094   0.564460  0.831507       0.565217\n",
       "1095   0.581882  0.836986       0.594203\n",
       "\n",
       "[1096 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Pressure Grad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.752055</td>\n",
       "      <td>0.449275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.606272</td>\n",
       "      <td>0.647945</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.557491</td>\n",
       "      <td>0.653425</td>\n",
       "      <td>0.724638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.445993</td>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.550725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.533101</td>\n",
       "      <td>0.812329</td>\n",
       "      <td>0.405797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.411150</td>\n",
       "      <td>0.706849</td>\n",
       "      <td>0.623188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.470383</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.594203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.595819</td>\n",
       "      <td>0.795890</td>\n",
       "      <td>0.594203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.564460</td>\n",
       "      <td>0.831507</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.581882</td>\n",
       "      <td>0.836986</td>\n",
       "      <td>0.594203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows Ã— 3 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_norm, Y, test_size=0.2, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_train.shape[1], activation=\"sigmoid\", kernel_initializer='normal'))\n",
    "model.add(Dropout(0.2)) #dropping a few neurons for generalizing the model\n",
    "model.add(Dense(1, activation=\"linear\", kernel_initializer='normal'))\n",
    "adam = Adam(learning_rate=1e-3, decay=1e-3)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mse','mae'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print('Fit model...')\n",
    "filepath=\"/home/m-marouni/Documents/CE-901/Heathrow/best_weights\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_mae', patience=100, verbose=1, mode='min')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "log = model.fit(x_train, y_train,\n",
    "          validation_split=0.40, batch_size=30, epochs=1000, shuffle=True, callbacks=callbacks_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fit model...\n",
      "Epoch 1/1000\n",
      "18/18 [==============================] - 11s 35ms/step - loss: 43.4118 - mse: 43.4118 - mae: 6.0505 - val_loss: 36.3289 - val_mse: 36.3289 - val_mae: 5.5361\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 5.53611, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 2/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.1217 - mse: 39.1217 - mae: 5.6812 - val_loss: 32.9844 - val_mse: 32.9844 - val_mae: 5.2252\n",
      "\n",
      "Epoch 00002: val_mae improved from 5.53611 to 5.22516, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 3/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 36.5349 - mse: 36.5349 - mae: 5.4084 - val_loss: 29.7890 - val_mse: 29.7890 - val_mae: 4.9096\n",
      "\n",
      "Epoch 00003: val_mae improved from 5.22516 to 4.90961, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 4/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 32.4876 - mse: 32.4876 - mae: 5.0656 - val_loss: 26.7418 - val_mse: 26.7418 - val_mae: 4.5884\n",
      "\n",
      "Epoch 00004: val_mae improved from 4.90961 to 4.58838, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 5/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 29.5187 - mse: 29.5187 - mae: 4.7467 - val_loss: 23.7930 - val_mse: 23.7930 - val_mae: 4.2560\n",
      "\n",
      "Epoch 00005: val_mae improved from 4.58838 to 4.25605, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 6/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 27.7439 - mse: 27.7439 - mae: 4.4875 - val_loss: 21.0359 - val_mse: 21.0359 - val_mae: 3.9198\n",
      "\n",
      "Epoch 00006: val_mae improved from 4.25605 to 3.91979, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 7/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.6943 - mse: 22.6943 - mae: 4.0584 - val_loss: 18.4532 - val_mse: 18.4532 - val_mae: 3.5801\n",
      "\n",
      "Epoch 00007: val_mae improved from 3.91979 to 3.58009, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 8/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.2273 - mse: 20.2273 - mae: 3.6678 - val_loss: 16.0862 - val_mse: 16.0862 - val_mae: 3.2451\n",
      "\n",
      "Epoch 00008: val_mae improved from 3.58009 to 3.24513, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 9/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 18.6952 - mse: 18.6952 - mae: 3.4642 - val_loss: 13.9686 - val_mse: 13.9686 - val_mae: 2.9339\n",
      "\n",
      "Epoch 00009: val_mae improved from 3.24513 to 2.93386, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 10/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 16.0403 - mse: 16.0403 - mae: 3.0796 - val_loss: 12.1318 - val_mse: 12.1318 - val_mae: 2.6670\n",
      "\n",
      "Epoch 00010: val_mae improved from 2.93386 to 2.66700, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 11/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 13.3927 - mse: 13.3927 - mae: 2.7578 - val_loss: 10.6016 - val_mse: 10.6016 - val_mae: 2.4561\n",
      "\n",
      "Epoch 00011: val_mae improved from 2.66700 to 2.45608, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 12/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 11.3832 - mse: 11.3832 - mae: 2.4775 - val_loss: 9.3431 - val_mse: 9.3431 - val_mae: 2.2904\n",
      "\n",
      "Epoch 00012: val_mae improved from 2.45608 to 2.29041, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 13/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 11.1042 - mse: 11.1042 - mae: 2.4561 - val_loss: 8.3126 - val_mse: 8.3126 - val_mae: 2.1616\n",
      "\n",
      "Epoch 00013: val_mae improved from 2.29041 to 2.16155, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 14/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 10.8365 - mse: 10.8365 - mae: 2.3663 - val_loss: 7.5632 - val_mse: 7.5632 - val_mae: 2.0726\n",
      "\n",
      "Epoch 00014: val_mae improved from 2.16155 to 2.07257, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 15/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 9.5472 - mse: 9.5472 - mae: 2.1975 - val_loss: 6.9977 - val_mse: 6.9977 - val_mae: 2.0097\n",
      "\n",
      "Epoch 00015: val_mae improved from 2.07257 to 2.00970, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 16/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.0709 - mse: 8.0709 - mae: 2.1174 - val_loss: 6.5652 - val_mse: 6.5652 - val_mae: 1.9657\n",
      "\n",
      "Epoch 00016: val_mae improved from 2.00970 to 1.96567, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 17/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 8.7020 - mse: 8.7020 - mae: 2.2154 - val_loss: 6.2651 - val_mse: 6.2651 - val_mae: 1.9413\n",
      "\n",
      "Epoch 00017: val_mae improved from 1.96567 to 1.94132, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 18/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.4230 - mse: 7.4230 - mae: 2.0136 - val_loss: 6.0884 - val_mse: 6.0884 - val_mae: 1.9356\n",
      "\n",
      "Epoch 00018: val_mae improved from 1.94132 to 1.93560, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 19/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0705 - mse: 7.0705 - mae: 2.0589 - val_loss: 5.9613 - val_mse: 5.9613 - val_mae: 1.9356\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 1.93560\n",
      "Epoch 20/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0324 - mse: 7.0324 - mae: 1.9783 - val_loss: 5.8918 - val_mse: 5.8918 - val_mae: 1.9383\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 1.93560\n",
      "Epoch 21/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8542 - mse: 6.8542 - mae: 2.0491 - val_loss: 5.8529 - val_mse: 5.8529 - val_mae: 1.9433\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 1.93560\n",
      "Epoch 22/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1421 - mse: 7.1421 - mae: 2.0840 - val_loss: 5.8337 - val_mse: 5.8337 - val_mae: 1.9485\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 1.93560\n",
      "Epoch 23/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 7.4751 - mse: 7.4751 - mae: 2.1284 - val_loss: 5.8271 - val_mse: 5.8271 - val_mae: 1.9535\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 1.93560\n",
      "Epoch 24/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.9727 - mse: 6.9727 - mae: 2.0731 - val_loss: 5.8257 - val_mse: 5.8257 - val_mae: 1.9566\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 1.93560\n",
      "Epoch 25/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.8995 - mse: 7.8995 - mae: 2.1911 - val_loss: 5.8288 - val_mse: 5.8288 - val_mae: 1.9620\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 1.93560\n",
      "Epoch 26/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.7335 - mse: 7.7335 - mae: 2.2030 - val_loss: 5.8310 - val_mse: 5.8310 - val_mae: 1.9646\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 1.93560\n",
      "Epoch 27/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.3757 - mse: 7.3757 - mae: 2.1540 - val_loss: 5.8324 - val_mse: 5.8324 - val_mae: 1.9662\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 1.93560\n",
      "Epoch 28/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.2288 - mse: 6.2288 - mae: 1.9831 - val_loss: 5.8351 - val_mse: 5.8351 - val_mae: 1.9682\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 1.93560\n",
      "Epoch 29/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.8692 - mse: 6.8692 - mae: 2.0908 - val_loss: 5.8332 - val_mse: 5.8332 - val_mae: 1.9679\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 1.93560\n",
      "Epoch 30/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9666 - mse: 6.9666 - mae: 2.0936 - val_loss: 5.8425 - val_mse: 5.8425 - val_mae: 1.9728\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 1.93560\n",
      "Epoch 31/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4449 - mse: 6.4449 - mae: 2.0460 - val_loss: 5.8441 - val_mse: 5.8441 - val_mae: 1.9742\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 1.93560\n",
      "Epoch 32/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2754 - mse: 6.2754 - mae: 1.9953 - val_loss: 5.8377 - val_mse: 5.8377 - val_mae: 1.9718\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 1.93560\n",
      "Epoch 33/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6099 - mse: 6.6099 - mae: 2.0965 - val_loss: 5.8346 - val_mse: 5.8346 - val_mae: 1.9710\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 1.93560\n",
      "Epoch 34/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3147 - mse: 6.3147 - mae: 2.0517 - val_loss: 5.8336 - val_mse: 5.8336 - val_mae: 1.9713\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 1.93560\n",
      "Epoch 35/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9380 - mse: 6.9380 - mae: 2.0964 - val_loss: 5.8332 - val_mse: 5.8332 - val_mae: 1.9717\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 1.93560\n",
      "Epoch 36/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0050 - mse: 7.0050 - mae: 2.0815 - val_loss: 5.8355 - val_mse: 5.8355 - val_mae: 1.9734\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 1.93560\n",
      "Epoch 37/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.4144 - mse: 7.4144 - mae: 2.1980 - val_loss: 5.8314 - val_mse: 5.8314 - val_mae: 1.9723\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 1.93560\n",
      "Epoch 38/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.2041 - mse: 7.2041 - mae: 2.1464 - val_loss: 5.8296 - val_mse: 5.8296 - val_mae: 1.9721\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 1.93560\n",
      "Epoch 39/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5583 - mse: 6.5583 - mae: 2.0454 - val_loss: 5.8335 - val_mse: 5.8335 - val_mae: 1.9744\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 1.93560\n",
      "Epoch 40/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5924 - mse: 6.5924 - mae: 2.0731 - val_loss: 5.8291 - val_mse: 5.8291 - val_mae: 1.9731\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 1.93560\n",
      "Epoch 41/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.4078 - mse: 7.4078 - mae: 2.1860 - val_loss: 5.8156 - val_mse: 5.8156 - val_mae: 1.9678\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 1.93560\n",
      "Epoch 42/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9858 - mse: 6.9858 - mae: 2.0676 - val_loss: 5.8159 - val_mse: 5.8159 - val_mae: 1.9687\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 1.93560\n",
      "Epoch 43/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6322 - mse: 6.6322 - mae: 2.0816 - val_loss: 5.8060 - val_mse: 5.8060 - val_mae: 1.9647\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 1.93560\n",
      "Epoch 44/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.4606 - mse: 7.4606 - mae: 2.2252 - val_loss: 5.8064 - val_mse: 5.8064 - val_mae: 1.9658\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 1.93560\n",
      "Epoch 45/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7159 - mse: 6.7159 - mae: 2.1285 - val_loss: 5.7997 - val_mse: 5.7997 - val_mae: 1.9634\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 1.93560\n",
      "Epoch 46/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.7790 - mse: 6.7790 - mae: 2.0756 - val_loss: 5.8055 - val_mse: 5.8055 - val_mae: 1.9668\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 1.93560\n",
      "Epoch 47/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8501 - mse: 6.8501 - mae: 2.0726 - val_loss: 5.8036 - val_mse: 5.8036 - val_mae: 1.9667\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 1.93560\n",
      "Epoch 48/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.3236 - mse: 7.3236 - mae: 2.1554 - val_loss: 5.7962 - val_mse: 5.7962 - val_mae: 1.9640\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 1.93560\n",
      "Epoch 49/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.7999 - mse: 7.7999 - mae: 2.1536 - val_loss: 5.8018 - val_mse: 5.8018 - val_mae: 1.9674\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 1.93560\n",
      "Epoch 50/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6761 - mse: 6.6761 - mae: 2.0821 - val_loss: 5.7956 - val_mse: 5.7956 - val_mae: 1.9653\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 1.93560\n",
      "Epoch 51/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1447 - mse: 7.1447 - mae: 2.1171 - val_loss: 5.7934 - val_mse: 5.7934 - val_mae: 1.9650\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 1.93560\n",
      "Epoch 52/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0931 - mse: 7.0931 - mae: 2.1538 - val_loss: 5.7875 - val_mse: 5.7875 - val_mae: 1.9632\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 1.93560\n",
      "Epoch 53/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.1037 - mse: 7.1037 - mae: 2.1225 - val_loss: 5.7907 - val_mse: 5.7907 - val_mae: 1.9654\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 1.93560\n",
      "Epoch 54/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4871 - mse: 6.4871 - mae: 2.0221 - val_loss: 5.7909 - val_mse: 5.7909 - val_mae: 1.9662\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 1.93560\n",
      "Epoch 55/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9110 - mse: 6.9110 - mae: 2.0818 - val_loss: 5.7857 - val_mse: 5.7857 - val_mae: 1.9648\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 1.93560\n",
      "Epoch 56/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8677 - mse: 6.8677 - mae: 2.0916 - val_loss: 5.7869 - val_mse: 5.7869 - val_mae: 1.9660\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 1.93560\n",
      "Epoch 57/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.5237 - mse: 7.5237 - mae: 2.1744 - val_loss: 5.7766 - val_mse: 5.7766 - val_mae: 1.9621\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 1.93560\n",
      "Epoch 58/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.2654 - mse: 7.2654 - mae: 2.1139 - val_loss: 5.7663 - val_mse: 5.7663 - val_mae: 1.9583\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 1.93560\n",
      "Epoch 59/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0555 - mse: 7.0555 - mae: 2.1545 - val_loss: 5.7700 - val_mse: 5.7700 - val_mae: 1.9607\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 1.93560\n",
      "Epoch 60/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3556 - mse: 6.3556 - mae: 1.9940 - val_loss: 5.7741 - val_mse: 5.7741 - val_mae: 1.9633\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 1.93560\n",
      "Epoch 61/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.7349 - mse: 6.7349 - mae: 2.0426 - val_loss: 5.7619 - val_mse: 5.7619 - val_mae: 1.9587\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 1.93560\n",
      "Epoch 62/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.8524 - mse: 6.8524 - mae: 2.0719 - val_loss: 5.7669 - val_mse: 5.7669 - val_mae: 1.9618\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 1.93560\n",
      "Epoch 63/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.4976 - mse: 6.4976 - mae: 2.0344 - val_loss: 5.7770 - val_mse: 5.7770 - val_mae: 1.9668\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 1.93560\n",
      "Epoch 64/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9216 - mse: 6.9216 - mae: 2.0868 - val_loss: 5.7703 - val_mse: 5.7703 - val_mae: 1.9649\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 1.93560\n",
      "Epoch 65/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.7737 - mse: 7.7737 - mae: 2.1495 - val_loss: 5.7593 - val_mse: 5.7593 - val_mae: 1.9610\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 1.93560\n",
      "Epoch 66/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0717 - mse: 7.0717 - mae: 2.0907 - val_loss: 5.7524 - val_mse: 5.7524 - val_mae: 1.9588\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 1.93560\n",
      "Epoch 67/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 6.3420 - mse: 6.3420 - mae: 2.0133 - val_loss: 5.7471 - val_mse: 5.7471 - val_mae: 1.9572\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 1.93560\n",
      "Epoch 68/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.9429 - mse: 6.9429 - mae: 2.0960 - val_loss: 5.7497 - val_mse: 5.7497 - val_mae: 1.9593\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 1.93560\n",
      "Epoch 69/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8674 - mse: 6.8674 - mae: 2.0883 - val_loss: 5.7436 - val_mse: 5.7436 - val_mae: 1.9575\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 1.93560\n",
      "Epoch 70/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.2866 - mse: 7.2866 - mae: 2.1372 - val_loss: 5.7407 - val_mse: 5.7407 - val_mae: 1.9571\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 1.93560\n",
      "Epoch 71/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.6034 - mse: 7.6034 - mae: 2.1742 - val_loss: 5.7316 - val_mse: 5.7316 - val_mae: 1.9538\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 1.93560\n",
      "Epoch 72/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4544 - mse: 6.4544 - mae: 2.0159 - val_loss: 5.7326 - val_mse: 5.7326 - val_mae: 1.9553\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 1.93560\n",
      "Epoch 73/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1441 - mse: 7.1441 - mae: 2.1083 - val_loss: 5.7247 - val_mse: 5.7247 - val_mae: 1.9526\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 1.93560\n",
      "Epoch 74/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8117 - mse: 6.8117 - mae: 2.0722 - val_loss: 5.7202 - val_mse: 5.7202 - val_mae: 1.9515\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 1.93560\n",
      "Epoch 75/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3607 - mse: 6.3607 - mae: 2.0074 - val_loss: 5.7205 - val_mse: 5.7205 - val_mae: 1.9525\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 1.93560\n",
      "Epoch 76/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4541 - mse: 6.4541 - mae: 2.0242 - val_loss: 5.7206 - val_mse: 5.7206 - val_mae: 1.9536\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 1.93560\n",
      "Epoch 77/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1486 - mse: 7.1486 - mae: 2.1138 - val_loss: 5.7162 - val_mse: 5.7162 - val_mae: 1.9526\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 1.93560\n",
      "Epoch 78/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9928 - mse: 6.9928 - mae: 2.1184 - val_loss: 5.7069 - val_mse: 5.7069 - val_mae: 1.9493\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 1.93560\n",
      "Epoch 79/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1549 - mse: 7.1549 - mae: 2.1517 - val_loss: 5.7051 - val_mse: 5.7051 - val_mae: 1.9495\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 1.93560\n",
      "Epoch 80/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2951 - mse: 6.2951 - mae: 1.9733 - val_loss: 5.7036 - val_mse: 5.7036 - val_mae: 1.9497\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 1.93560\n",
      "Epoch 81/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6857 - mse: 6.6857 - mae: 2.0681 - val_loss: 5.6983 - val_mse: 5.6983 - val_mae: 1.9483\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 1.93560\n",
      "Epoch 82/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7566 - mse: 6.7566 - mae: 2.0647 - val_loss: 5.7013 - val_mse: 5.7013 - val_mae: 1.9507\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 1.93560\n",
      "Epoch 83/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.2884 - mse: 7.2884 - mae: 2.1553 - val_loss: 5.6965 - val_mse: 5.6965 - val_mae: 1.9494\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 1.93560\n",
      "Epoch 84/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0923 - mse: 7.0923 - mae: 2.1161 - val_loss: 5.6921 - val_mse: 5.6921 - val_mae: 1.9485\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 1.93560\n",
      "Epoch 85/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5177 - mse: 6.5177 - mae: 2.0254 - val_loss: 5.6747 - val_mse: 5.6747 - val_mae: 1.9412\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 1.93560\n",
      "Epoch 86/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.4906 - mse: 7.4906 - mae: 2.1653 - val_loss: 5.6776 - val_mse: 5.6776 - val_mae: 1.9438\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 1.93560\n",
      "Epoch 87/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9285 - mse: 6.9285 - mae: 2.1276 - val_loss: 5.6686 - val_mse: 5.6686 - val_mae: 1.9407\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 1.93560\n",
      "Epoch 88/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.5911 - mse: 7.5911 - mae: 2.1881 - val_loss: 5.6760 - val_mse: 5.6760 - val_mae: 1.9453\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 1.93560\n",
      "Epoch 89/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4892 - mse: 6.4892 - mae: 2.0231 - val_loss: 5.6735 - val_mse: 5.6735 - val_mae: 1.9452\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 1.93560\n",
      "Epoch 90/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.4698 - mse: 7.4698 - mae: 2.1267 - val_loss: 5.6714 - val_mse: 5.6714 - val_mae: 1.9453\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 1.93560\n",
      "Epoch 91/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5815 - mse: 6.5815 - mae: 2.0651 - val_loss: 5.6675 - val_mse: 5.6675 - val_mae: 1.9446\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 1.93560\n",
      "Epoch 92/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0995 - mse: 7.0995 - mae: 2.1424 - val_loss: 5.6589 - val_mse: 5.6589 - val_mae: 1.9418\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 1.93560\n",
      "Epoch 93/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.6211 - mse: 7.6211 - mae: 2.2111 - val_loss: 5.6499 - val_mse: 5.6499 - val_mae: 1.9387\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 1.93560\n",
      "Epoch 94/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0392 - mse: 7.0392 - mae: 2.1018 - val_loss: 5.6462 - val_mse: 5.6462 - val_mae: 1.9381\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 1.93560\n",
      "Epoch 95/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3547 - mse: 6.3547 - mae: 1.9870 - val_loss: 5.6460 - val_mse: 5.6460 - val_mae: 1.9392\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 1.93560\n",
      "Epoch 96/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2994 - mse: 6.2994 - mae: 2.0327 - val_loss: 5.6412 - val_mse: 5.6412 - val_mae: 1.9382\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 1.93560\n",
      "Epoch 97/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.2169 - mse: 7.2169 - mae: 2.1340 - val_loss: 5.6488 - val_mse: 5.6488 - val_mae: 1.9428\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 1.93560\n",
      "Epoch 98/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5222 - mse: 6.5222 - mae: 2.0304 - val_loss: 5.6440 - val_mse: 5.6440 - val_mae: 1.9417\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 1.93560\n",
      "Epoch 99/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8568 - mse: 6.8568 - mae: 2.1333 - val_loss: 5.6312 - val_mse: 5.6312 - val_mae: 1.9371\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 1.93560\n",
      "Epoch 100/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0501 - mse: 6.0501 - mae: 1.9645 - val_loss: 5.6403 - val_mse: 5.6403 - val_mae: 1.9423\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 1.93560\n",
      "Epoch 101/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3159 - mse: 6.3159 - mae: 2.0283 - val_loss: 5.6272 - val_mse: 5.6272 - val_mae: 1.9376\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 1.93560\n",
      "Epoch 102/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.3512 - mse: 7.3512 - mae: 2.1595 - val_loss: 5.6294 - val_mse: 5.6294 - val_mae: 1.9397\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 1.93560\n",
      "Epoch 103/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7378 - mse: 6.7378 - mae: 2.1071 - val_loss: 5.6177 - val_mse: 5.6177 - val_mae: 1.9355\n",
      "\n",
      "Epoch 00103: val_mae improved from 1.93560 to 1.93550, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 104/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.2736 - mse: 7.2736 - mae: 2.1643 - val_loss: 5.6154 - val_mse: 5.6154 - val_mae: 1.9357\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 1.93550\n",
      "Epoch 105/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.1080 - mse: 7.1080 - mae: 2.1131 - val_loss: 5.6070 - val_mse: 5.6070 - val_mae: 1.9331\n",
      "\n",
      "Epoch 00105: val_mae improved from 1.93550 to 1.93309, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 106/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 7.4608 - mse: 7.4608 - mae: 2.2301 - val_loss: 5.6139 - val_mse: 5.6139 - val_mae: 1.9372\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 1.93309\n",
      "Epoch 107/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 7.1464 - mse: 7.1464 - mae: 2.1666 - val_loss: 5.5999 - val_mse: 5.5999 - val_mae: 1.9322\n",
      "\n",
      "Epoch 00107: val_mae improved from 1.93309 to 1.93218, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 108/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 6.6081 - mse: 6.6081 - mae: 2.0414 - val_loss: 5.5924 - val_mse: 5.5924 - val_mae: 1.9299\n",
      "\n",
      "Epoch 00108: val_mae improved from 1.93218 to 1.92992, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 109/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.0830 - mse: 7.0830 - mae: 2.1176 - val_loss: 5.5920 - val_mse: 5.5920 - val_mae: 1.9309\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 1.92992\n",
      "Epoch 110/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5912 - mse: 6.5912 - mae: 2.0824 - val_loss: 5.5782 - val_mse: 5.5782 - val_mae: 1.9259\n",
      "\n",
      "Epoch 00110: val_mae improved from 1.92992 to 1.92586, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 111/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.6967 - mse: 7.6967 - mae: 2.1558 - val_loss: 5.5871 - val_mse: 5.5871 - val_mae: 1.9312\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 1.92586\n",
      "Epoch 112/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.9403 - mse: 6.9403 - mae: 2.1177 - val_loss: 5.5727 - val_mse: 5.5727 - val_mae: 1.9257\n",
      "\n",
      "Epoch 00112: val_mae improved from 1.92586 to 1.92571, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 113/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7046 - mse: 6.7046 - mae: 2.0528 - val_loss: 5.5714 - val_mse: 5.5714 - val_mae: 1.9263\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 1.92571\n",
      "Epoch 114/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3291 - mse: 6.3291 - mae: 2.0236 - val_loss: 5.5729 - val_mse: 5.5729 - val_mae: 1.9283\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 1.92571\n",
      "Epoch 115/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7455 - mse: 6.7455 - mae: 2.0849 - val_loss: 5.5706 - val_mse: 5.5706 - val_mae: 1.9284\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 1.92571\n",
      "Epoch 116/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0057 - mse: 7.0057 - mae: 2.0840 - val_loss: 5.5572 - val_mse: 5.5572 - val_mae: 1.9235\n",
      "\n",
      "Epoch 00116: val_mae improved from 1.92571 to 1.92352, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 117/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.3267 - mse: 7.3267 - mae: 2.1272 - val_loss: 5.5551 - val_mse: 5.5551 - val_mae: 1.9239\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 1.92352\n",
      "Epoch 118/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.3026 - mse: 7.3026 - mae: 2.1421 - val_loss: 5.5480 - val_mse: 5.5480 - val_mae: 1.9220\n",
      "\n",
      "Epoch 00118: val_mae improved from 1.92352 to 1.92195, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 119/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2650 - mse: 6.2650 - mae: 1.9473 - val_loss: 5.5416 - val_mse: 5.5416 - val_mae: 1.9203\n",
      "\n",
      "Epoch 00119: val_mae improved from 1.92195 to 1.92031, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 120/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.1274 - mse: 6.1274 - mae: 1.9893 - val_loss: 5.5429 - val_mse: 5.5429 - val_mae: 1.9221\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 1.92031\n",
      "Epoch 121/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7910 - mse: 6.7910 - mae: 2.0880 - val_loss: 5.5496 - val_mse: 5.5496 - val_mae: 1.9263\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 1.92031\n",
      "Epoch 122/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 6.7718 - mse: 6.7718 - mae: 2.1112 - val_loss: 5.5316 - val_mse: 5.5316 - val_mae: 1.9198\n",
      "\n",
      "Epoch 00122: val_mae improved from 1.92031 to 1.91985, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 123/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.3077 - mse: 6.3077 - mae: 2.0148 - val_loss: 5.5308 - val_mse: 5.5308 - val_mae: 1.9206\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 1.91985\n",
      "Epoch 124/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.3034 - mse: 7.3034 - mae: 2.1134 - val_loss: 5.5231 - val_mse: 5.5231 - val_mae: 1.9185\n",
      "\n",
      "Epoch 00124: val_mae improved from 1.91985 to 1.91847, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 125/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.7925 - mse: 6.7925 - mae: 2.0623 - val_loss: 5.5188 - val_mse: 5.5188 - val_mae: 1.9178\n",
      "\n",
      "Epoch 00125: val_mae improved from 1.91847 to 1.91779, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 126/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.6879 - mse: 6.6879 - mae: 2.0495 - val_loss: 5.5062 - val_mse: 5.5062 - val_mae: 1.9132\n",
      "\n",
      "Epoch 00126: val_mae improved from 1.91779 to 1.91324, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 127/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6387 - mse: 6.6387 - mae: 2.0310 - val_loss: 5.5009 - val_mse: 5.5009 - val_mae: 1.9122\n",
      "\n",
      "Epoch 00127: val_mae improved from 1.91324 to 1.91224, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 128/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 7.4680 - mse: 7.4680 - mae: 2.1393 - val_loss: 5.5030 - val_mse: 5.5030 - val_mae: 1.9147\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 1.91224\n",
      "Epoch 129/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0426 - mse: 7.0426 - mae: 2.1295 - val_loss: 5.4999 - val_mse: 5.4999 - val_mae: 1.9146\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 1.91224\n",
      "Epoch 130/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0150 - mse: 7.0150 - mae: 2.0929 - val_loss: 5.4938 - val_mse: 5.4938 - val_mae: 1.9132\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 1.91224\n",
      "Epoch 131/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9124 - mse: 6.9124 - mae: 2.1133 - val_loss: 5.5040 - val_mse: 5.5040 - val_mae: 1.9186\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 1.91224\n",
      "Epoch 132/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7493 - mse: 6.7493 - mae: 2.1314 - val_loss: 5.4964 - val_mse: 5.4964 - val_mae: 1.9166\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 1.91224\n",
      "Epoch 133/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.9026 - mse: 6.9026 - mae: 2.1131 - val_loss: 5.4689 - val_mse: 5.4689 - val_mae: 1.9055\n",
      "\n",
      "Epoch 00133: val_mae improved from 1.91224 to 1.90547, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 134/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2901 - mse: 6.2901 - mae: 2.0041 - val_loss: 5.4675 - val_mse: 5.4675 - val_mae: 1.9064\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 1.90547\n",
      "Epoch 135/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.4353 - mse: 7.4353 - mae: 2.1949 - val_loss: 5.4687 - val_mse: 5.4687 - val_mae: 1.9085\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 1.90547\n",
      "Epoch 136/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5257 - mse: 6.5257 - mae: 2.0037 - val_loss: 5.4609 - val_mse: 5.4609 - val_mae: 1.9063\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 1.90547\n",
      "Epoch 137/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0325 - mse: 7.0325 - mae: 2.0718 - val_loss: 5.4586 - val_mse: 5.4586 - val_mae: 1.9068\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 1.90547\n",
      "Epoch 138/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.8456 - mse: 6.8456 - mae: 2.1183 - val_loss: 5.4636 - val_mse: 5.4636 - val_mae: 1.9102\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 1.90547\n",
      "Epoch 139/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.0565 - mse: 7.0565 - mae: 2.1506 - val_loss: 5.4499 - val_mse: 5.4499 - val_mae: 1.9057\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 1.90547\n",
      "Epoch 140/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4226 - mse: 6.4226 - mae: 2.0347 - val_loss: 5.4420 - val_mse: 5.4420 - val_mae: 1.9035\n",
      "\n",
      "Epoch 00140: val_mae improved from 1.90547 to 1.90347, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 141/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.6000 - mse: 6.6000 - mae: 2.0862 - val_loss: 5.4474 - val_mse: 5.4474 - val_mae: 1.9071\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 1.90347\n",
      "Epoch 142/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9158 - mse: 6.9158 - mae: 2.0992 - val_loss: 5.4336 - val_mse: 5.4336 - val_mae: 1.9025\n",
      "\n",
      "Epoch 00142: val_mae improved from 1.90347 to 1.90250, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 143/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.4583 - mse: 6.4583 - mae: 2.0642 - val_loss: 5.4267 - val_mse: 5.4267 - val_mae: 1.9009\n",
      "\n",
      "Epoch 00143: val_mae improved from 1.90250 to 1.90086, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 144/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5293 - mse: 6.5293 - mae: 2.0029 - val_loss: 5.4315 - val_mse: 5.4315 - val_mae: 1.9040\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 1.90086\n",
      "Epoch 145/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5331 - mse: 6.5331 - mae: 2.0604 - val_loss: 5.4262 - val_mse: 5.4262 - val_mae: 1.9030\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 1.90086\n",
      "Epoch 146/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.2417 - mse: 7.2417 - mae: 2.1165 - val_loss: 5.4231 - val_mse: 5.4231 - val_mae: 1.9030\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 1.90086\n",
      "Epoch 147/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.9096 - mse: 6.9096 - mae: 2.0803 - val_loss: 5.4126 - val_mse: 5.4126 - val_mae: 1.8998\n",
      "\n",
      "Epoch 00147: val_mae improved from 1.90086 to 1.89976, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 148/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.2838 - mse: 6.2838 - mae: 1.9808 - val_loss: 5.4009 - val_mse: 5.4009 - val_mae: 1.8960\n",
      "\n",
      "Epoch 00148: val_mae improved from 1.89976 to 1.89600, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 149/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.1145 - mse: 7.1145 - mae: 2.1076 - val_loss: 5.4013 - val_mse: 5.4013 - val_mae: 1.8975\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 1.89600\n",
      "Epoch 150/1000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.5289 - mse: 6.5289 - mae: 2.0566 - val_loss: 5.3869 - val_mse: 5.3869 - val_mae: 1.8924\n",
      "\n",
      "Epoch 00150: val_mae improved from 1.89600 to 1.89245, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 151/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6559 - mse: 6.6559 - mae: 2.0790 - val_loss: 5.4074 - val_mse: 5.4074 - val_mae: 1.9023\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 1.89245\n",
      "Epoch 152/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5299 - mse: 6.5299 - mae: 2.0699 - val_loss: 5.3844 - val_mse: 5.3844 - val_mae: 1.8944\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 1.89245\n",
      "Epoch 153/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5754 - mse: 6.5754 - mae: 2.0722 - val_loss: 5.3823 - val_mse: 5.3823 - val_mae: 1.8947\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 1.89245\n",
      "Epoch 154/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5833 - mse: 6.5833 - mae: 2.0343 - val_loss: 5.3724 - val_mse: 5.3724 - val_mae: 1.8916\n",
      "\n",
      "Epoch 00154: val_mae improved from 1.89245 to 1.89164, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 155/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.1155 - mse: 7.1155 - mae: 2.0487 - val_loss: 5.3789 - val_mse: 5.3789 - val_mae: 1.8954\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 1.89164\n",
      "Epoch 156/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6401 - mse: 6.6401 - mae: 2.0628 - val_loss: 5.3668 - val_mse: 5.3668 - val_mae: 1.8917\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 1.89164\n",
      "Epoch 157/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.3401 - mse: 7.3401 - mae: 2.1241 - val_loss: 5.3589 - val_mse: 5.3589 - val_mae: 1.8898\n",
      "\n",
      "Epoch 00157: val_mae improved from 1.89164 to 1.88975, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 158/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7801 - mse: 6.7801 - mae: 2.1351 - val_loss: 5.3484 - val_mse: 5.3484 - val_mae: 1.8863\n",
      "\n",
      "Epoch 00158: val_mae improved from 1.88975 to 1.88627, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 159/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.5634 - mse: 7.5634 - mae: 2.1667 - val_loss: 5.3405 - val_mse: 5.3405 - val_mae: 1.8842\n",
      "\n",
      "Epoch 00159: val_mae improved from 1.88627 to 1.88418, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 160/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.8979 - mse: 6.8979 - mae: 2.0756 - val_loss: 5.3323 - val_mse: 5.3323 - val_mae: 1.8815\n",
      "\n",
      "Epoch 00160: val_mae improved from 1.88418 to 1.88154, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 161/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1540 - mse: 7.1540 - mae: 2.1077 - val_loss: 5.3467 - val_mse: 5.3467 - val_mae: 1.8894\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 1.88154\n",
      "Epoch 162/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.9937 - mse: 6.9937 - mae: 2.1351 - val_loss: 5.3320 - val_mse: 5.3320 - val_mae: 1.8846\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 1.88154\n",
      "Epoch 163/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1643 - mse: 7.1643 - mae: 2.1346 - val_loss: 5.3268 - val_mse: 5.3268 - val_mae: 1.8835\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 1.88154\n",
      "Epoch 164/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0209 - mse: 7.0209 - mae: 2.1312 - val_loss: 5.3235 - val_mse: 5.3235 - val_mae: 1.8834\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 1.88154\n",
      "Epoch 165/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0743 - mse: 7.0743 - mae: 2.1149 - val_loss: 5.3175 - val_mse: 5.3175 - val_mae: 1.8821\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 1.88154\n",
      "Epoch 166/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1170 - mse: 7.1170 - mae: 2.0802 - val_loss: 5.3130 - val_mse: 5.3130 - val_mae: 1.8814\n",
      "\n",
      "Epoch 00166: val_mae improved from 1.88154 to 1.88143, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 167/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3639 - mse: 6.3639 - mae: 2.0246 - val_loss: 5.3095 - val_mse: 5.3095 - val_mae: 1.8811\n",
      "\n",
      "Epoch 00167: val_mae improved from 1.88143 to 1.88106, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 168/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8866 - mse: 6.8866 - mae: 2.0534 - val_loss: 5.3009 - val_mse: 5.3009 - val_mae: 1.8787\n",
      "\n",
      "Epoch 00168: val_mae improved from 1.88106 to 1.87873, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 169/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9976 - mse: 5.9976 - mae: 1.9573 - val_loss: 5.2918 - val_mse: 5.2918 - val_mae: 1.8759\n",
      "\n",
      "Epoch 00169: val_mae improved from 1.87873 to 1.87592, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 170/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6610 - mse: 6.6610 - mae: 2.0720 - val_loss: 5.3012 - val_mse: 5.3012 - val_mae: 1.8813\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 1.87592\n",
      "Epoch 171/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8499 - mse: 6.8499 - mae: 2.0503 - val_loss: 5.2985 - val_mse: 5.2985 - val_mae: 1.8814\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 1.87592\n",
      "Epoch 172/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.4959 - mse: 6.4959 - mae: 2.0476 - val_loss: 5.2975 - val_mse: 5.2975 - val_mae: 1.8821\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 1.87592\n",
      "Epoch 173/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8131 - mse: 6.8131 - mae: 2.0825 - val_loss: 5.3006 - val_mse: 5.3006 - val_mae: 1.8843\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 1.87592\n",
      "Epoch 174/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2139 - mse: 6.2139 - mae: 1.9838 - val_loss: 5.2805 - val_mse: 5.2805 - val_mae: 1.8775\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 1.87592\n",
      "Epoch 175/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1237 - mse: 6.1237 - mae: 1.9931 - val_loss: 5.2640 - val_mse: 5.2640 - val_mae: 1.8717\n",
      "\n",
      "Epoch 00175: val_mae improved from 1.87592 to 1.87172, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 176/1000\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 6.3789 - mse: 6.3789 - mae: 2.0048 - val_loss: 5.2668 - val_mse: 5.2668 - val_mae: 1.8742\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 1.87172\n",
      "Epoch 177/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.4769 - mse: 6.4769 - mae: 2.0583 - val_loss: 5.2738 - val_mse: 5.2738 - val_mae: 1.8781\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 1.87172\n",
      "Epoch 178/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6775 - mse: 6.6775 - mae: 2.0563 - val_loss: 5.2755 - val_mse: 5.2755 - val_mae: 1.8798\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 1.87172\n",
      "Epoch 179/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1855 - mse: 7.1855 - mae: 2.1223 - val_loss: 5.2462 - val_mse: 5.2462 - val_mae: 1.8694\n",
      "\n",
      "Epoch 00179: val_mae improved from 1.87172 to 1.86937, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 180/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 7.2018 - mse: 7.2018 - mae: 2.0924 - val_loss: 5.2341 - val_mse: 5.2341 - val_mae: 1.8650\n",
      "\n",
      "Epoch 00180: val_mae improved from 1.86937 to 1.86499, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 181/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4954 - mse: 6.4954 - mae: 2.0785 - val_loss: 5.2231 - val_mse: 5.2231 - val_mae: 1.8607\n",
      "\n",
      "Epoch 00181: val_mae improved from 1.86499 to 1.86070, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 182/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4724 - mse: 6.4724 - mae: 2.0938 - val_loss: 5.2243 - val_mse: 5.2243 - val_mae: 1.8634\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 1.86070\n",
      "Epoch 183/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.2539 - mse: 6.2539 - mae: 2.0265 - val_loss: 5.2239 - val_mse: 5.2239 - val_mae: 1.8648\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 1.86070\n",
      "Epoch 184/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.3950 - mse: 7.3950 - mae: 2.1636 - val_loss: 5.2255 - val_mse: 5.2255 - val_mae: 1.8669\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 1.86070\n",
      "Epoch 185/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7688 - mse: 5.7688 - mae: 1.9245 - val_loss: 5.2116 - val_mse: 5.2116 - val_mae: 1.8619\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 1.86070\n",
      "Epoch 186/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4962 - mse: 6.4962 - mae: 2.0465 - val_loss: 5.2196 - val_mse: 5.2196 - val_mae: 1.8667\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 1.86070\n",
      "Epoch 187/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0816 - mse: 6.0816 - mae: 2.0067 - val_loss: 5.2099 - val_mse: 5.2099 - val_mae: 1.8640\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 1.86070\n",
      "Epoch 188/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3459 - mse: 6.3459 - mae: 2.0241 - val_loss: 5.1984 - val_mse: 5.1984 - val_mae: 1.8605\n",
      "\n",
      "Epoch 00188: val_mae improved from 1.86070 to 1.86050, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 189/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 7.1519 - mse: 7.1519 - mae: 2.0674 - val_loss: 5.1925 - val_mse: 5.1925 - val_mae: 1.8592\n",
      "\n",
      "Epoch 00189: val_mae improved from 1.86050 to 1.85920, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 190/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.0765 - mse: 6.0765 - mae: 1.9715 - val_loss: 5.1915 - val_mse: 5.1915 - val_mae: 1.8601\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 1.85920\n",
      "Epoch 191/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1957 - mse: 7.1957 - mae: 2.1698 - val_loss: 5.2070 - val_mse: 5.2070 - val_mae: 1.8672\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 1.85920\n",
      "Epoch 192/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0959 - mse: 6.0959 - mae: 1.9949 - val_loss: 5.1735 - val_mse: 5.1735 - val_mae: 1.8547\n",
      "\n",
      "Epoch 00192: val_mae improved from 1.85920 to 1.85474, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 193/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.3184 - mse: 6.3184 - mae: 2.0204 - val_loss: 5.1816 - val_mse: 5.1816 - val_mae: 1.8595\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 1.85474\n",
      "Epoch 194/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.4352 - mse: 6.4352 - mae: 2.0205 - val_loss: 5.1747 - val_mse: 5.1747 - val_mae: 1.8578\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 1.85474\n",
      "Epoch 195/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6861 - mse: 5.6861 - mae: 1.9326 - val_loss: 5.1525 - val_mse: 5.1525 - val_mae: 1.8490\n",
      "\n",
      "Epoch 00195: val_mae improved from 1.85474 to 1.84902, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 196/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.2936 - mse: 6.2936 - mae: 1.9921 - val_loss: 5.1641 - val_mse: 5.1641 - val_mae: 1.8558\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 1.84902\n",
      "Epoch 197/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.8631 - mse: 5.8631 - mae: 1.9200 - val_loss: 5.1496 - val_mse: 5.1496 - val_mae: 1.8509\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 1.84902\n",
      "Epoch 198/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4928 - mse: 6.4928 - mae: 2.0327 - val_loss: 5.1625 - val_mse: 5.1625 - val_mae: 1.8575\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 1.84902\n",
      "Epoch 199/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8993 - mse: 5.8993 - mae: 1.9655 - val_loss: 5.1443 - val_mse: 5.1443 - val_mae: 1.8513\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 1.84902\n",
      "Epoch 200/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7274 - mse: 6.7274 - mae: 2.0755 - val_loss: 5.1450 - val_mse: 5.1450 - val_mae: 1.8526\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 1.84902\n",
      "Epoch 201/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.2940 - mse: 6.2940 - mae: 2.0130 - val_loss: 5.1329 - val_mse: 5.1329 - val_mae: 1.8487\n",
      "\n",
      "Epoch 00201: val_mae improved from 1.84902 to 1.84867, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 202/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4458 - mse: 6.4458 - mae: 2.0648 - val_loss: 5.1245 - val_mse: 5.1245 - val_mae: 1.8460\n",
      "\n",
      "Epoch 00202: val_mae improved from 1.84867 to 1.84604, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 203/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2293 - mse: 6.2293 - mae: 2.0062 - val_loss: 5.1282 - val_mse: 5.1282 - val_mae: 1.8490\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 1.84604\n",
      "Epoch 204/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.3230 - mse: 6.3230 - mae: 2.0012 - val_loss: 5.1174 - val_mse: 5.1174 - val_mae: 1.8455\n",
      "\n",
      "Epoch 00204: val_mae improved from 1.84604 to 1.84545, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 205/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8944 - mse: 5.8944 - mae: 1.9497 - val_loss: 5.1060 - val_mse: 5.1060 - val_mae: 1.8414\n",
      "\n",
      "Epoch 00205: val_mae improved from 1.84545 to 1.84142, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 206/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1972 - mse: 6.1972 - mae: 1.9715 - val_loss: 5.1131 - val_mse: 5.1131 - val_mae: 1.8459\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 1.84142\n",
      "Epoch 207/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9592 - mse: 5.9592 - mae: 1.9238 - val_loss: 5.1036 - val_mse: 5.1036 - val_mae: 1.8431\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 1.84142\n",
      "Epoch 208/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2168 - mse: 6.2168 - mae: 1.9726 - val_loss: 5.0979 - val_mse: 5.0979 - val_mae: 1.8419\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 1.84142\n",
      "Epoch 209/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.1449 - mse: 7.1449 - mae: 2.0841 - val_loss: 5.0899 - val_mse: 5.0899 - val_mae: 1.8395\n",
      "\n",
      "Epoch 00209: val_mae improved from 1.84142 to 1.83948, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 210/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3475 - mse: 6.3475 - mae: 1.9878 - val_loss: 5.0821 - val_mse: 5.0821 - val_mae: 1.8371\n",
      "\n",
      "Epoch 00210: val_mae improved from 1.83948 to 1.83710, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 211/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5280 - mse: 6.5280 - mae: 2.0656 - val_loss: 5.0653 - val_mse: 5.0653 - val_mae: 1.8288\n",
      "\n",
      "Epoch 00211: val_mae improved from 1.83710 to 1.82877, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 212/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6585 - mse: 6.6585 - mae: 2.0724 - val_loss: 5.0758 - val_mse: 5.0758 - val_mae: 1.8369\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 1.82877\n",
      "Epoch 213/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5597 - mse: 6.5597 - mae: 2.0471 - val_loss: 5.0669 - val_mse: 5.0669 - val_mae: 1.8341\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 1.82877\n",
      "Epoch 214/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8655 - mse: 6.8655 - mae: 2.1113 - val_loss: 5.0660 - val_mse: 5.0660 - val_mae: 1.8351\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 1.82877\n",
      "Epoch 215/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.1420 - mse: 6.1420 - mae: 2.0068 - val_loss: 5.0470 - val_mse: 5.0470 - val_mae: 1.8264\n",
      "\n",
      "Epoch 00215: val_mae improved from 1.82877 to 1.82639, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 216/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.6172 - mse: 6.6172 - mae: 2.0678 - val_loss: 5.0487 - val_mse: 5.0487 - val_mae: 1.8295\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 1.82639\n",
      "Epoch 217/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4297 - mse: 6.4297 - mae: 2.0173 - val_loss: 5.0599 - val_mse: 5.0599 - val_mae: 1.8358\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 1.82639\n",
      "Epoch 218/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0593 - mse: 7.0593 - mae: 2.1065 - val_loss: 5.0469 - val_mse: 5.0469 - val_mae: 1.8314\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 1.82639\n",
      "Epoch 219/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3158 - mse: 6.3158 - mae: 2.0401 - val_loss: 5.0361 - val_mse: 5.0361 - val_mae: 1.8276\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 1.82639\n",
      "Epoch 220/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3593 - mse: 6.3593 - mae: 1.9848 - val_loss: 5.0278 - val_mse: 5.0278 - val_mae: 1.8248\n",
      "\n",
      "Epoch 00220: val_mae improved from 1.82639 to 1.82477, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 221/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5194 - mse: 6.5194 - mae: 2.0020 - val_loss: 5.0227 - val_mse: 5.0227 - val_mae: 1.8235\n",
      "\n",
      "Epoch 00221: val_mae improved from 1.82477 to 1.82348, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 222/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8753 - mse: 5.8753 - mae: 1.9487 - val_loss: 5.0261 - val_mse: 5.0261 - val_mae: 1.8267\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 1.82348\n",
      "Epoch 223/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.3792 - mse: 6.3792 - mae: 2.0289 - val_loss: 5.0301 - val_mse: 5.0301 - val_mae: 1.8294\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 1.82348\n",
      "Epoch 224/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6908 - mse: 5.6908 - mae: 1.9021 - val_loss: 5.0116 - val_mse: 5.0116 - val_mae: 1.8221\n",
      "\n",
      "Epoch 00224: val_mae improved from 1.82348 to 1.82211, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 225/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7647 - mse: 6.7647 - mae: 2.0551 - val_loss: 5.0309 - val_mse: 5.0309 - val_mae: 1.8314\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 1.82211\n",
      "Epoch 226/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.0672 - mse: 7.0672 - mae: 2.1257 - val_loss: 5.0135 - val_mse: 5.0135 - val_mae: 1.8255\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 1.82211\n",
      "Epoch 227/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7188 - mse: 6.7188 - mae: 2.0336 - val_loss: 5.0074 - val_mse: 5.0074 - val_mae: 1.8239\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 1.82211\n",
      "Epoch 228/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0782 - mse: 6.0782 - mae: 1.9563 - val_loss: 5.0052 - val_mse: 5.0052 - val_mae: 1.8238\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 1.82211\n",
      "Epoch 229/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1595 - mse: 6.1595 - mae: 2.0250 - val_loss: 4.9892 - val_mse: 4.9892 - val_mae: 1.8176\n",
      "\n",
      "Epoch 00229: val_mae improved from 1.82211 to 1.81761, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 230/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8515 - mse: 5.8515 - mae: 1.9566 - val_loss: 4.9895 - val_mse: 4.9895 - val_mae: 1.8190\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 1.81761\n",
      "Epoch 231/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.2534 - mse: 6.2534 - mae: 2.0186 - val_loss: 4.9980 - val_mse: 4.9980 - val_mae: 1.8233\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 1.81761\n",
      "Epoch 232/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6691 - mse: 6.6691 - mae: 2.0719 - val_loss: 4.9846 - val_mse: 4.9846 - val_mae: 1.8189\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 1.81761\n",
      "Epoch 233/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1935 - mse: 6.1935 - mae: 1.9996 - val_loss: 4.9734 - val_mse: 4.9734 - val_mae: 1.8148\n",
      "\n",
      "Epoch 00233: val_mae improved from 1.81761 to 1.81477, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 234/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.9548 - mse: 6.9548 - mae: 2.0486 - val_loss: 4.9724 - val_mse: 4.9724 - val_mae: 1.8155\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 1.81477\n",
      "Epoch 235/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 6.8937 - mse: 6.8937 - mae: 2.0455 - val_loss: 4.9621 - val_mse: 4.9621 - val_mae: 1.8114\n",
      "\n",
      "Epoch 00235: val_mae improved from 1.81477 to 1.81136, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 236/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2651 - mse: 6.2651 - mae: 1.9724 - val_loss: 4.9650 - val_mse: 4.9650 - val_mae: 1.8142\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 1.81136\n",
      "Epoch 237/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1109 - mse: 6.1109 - mae: 1.9914 - val_loss: 4.9589 - val_mse: 4.9589 - val_mae: 1.8123\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 1.81136\n",
      "Epoch 238/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9771 - mse: 5.9771 - mae: 1.9733 - val_loss: 4.9548 - val_mse: 4.9548 - val_mae: 1.8115\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 1.81136\n",
      "Epoch 239/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8548 - mse: 5.8548 - mae: 1.8993 - val_loss: 4.9503 - val_mse: 4.9503 - val_mae: 1.8105\n",
      "\n",
      "Epoch 00239: val_mae improved from 1.81136 to 1.81051, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 240/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9821 - mse: 6.9821 - mae: 2.0928 - val_loss: 4.9468 - val_mse: 4.9468 - val_mae: 1.8099\n",
      "\n",
      "Epoch 00240: val_mae improved from 1.81051 to 1.80989, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 241/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.0340 - mse: 6.0340 - mae: 1.9586 - val_loss: 4.9464 - val_mse: 4.9464 - val_mae: 1.8108\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 1.80989\n",
      "Epoch 242/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3664 - mse: 6.3664 - mae: 1.9907 - val_loss: 4.9402 - val_mse: 4.9402 - val_mae: 1.8089\n",
      "\n",
      "Epoch 00242: val_mae improved from 1.80989 to 1.80888, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 243/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6564 - mse: 5.6564 - mae: 1.9156 - val_loss: 4.9364 - val_mse: 4.9364 - val_mae: 1.8081\n",
      "\n",
      "Epoch 00243: val_mae improved from 1.80888 to 1.80806, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 244/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1833 - mse: 6.1833 - mae: 1.9815 - val_loss: 4.9381 - val_mse: 4.9381 - val_mae: 1.8099\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 1.80806\n",
      "Epoch 245/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 7.2109 - mse: 7.2109 - mae: 2.1455 - val_loss: 4.9355 - val_mse: 4.9355 - val_mae: 1.8097\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 1.80806\n",
      "Epoch 246/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4182 - mse: 6.4182 - mae: 2.0558 - val_loss: 4.9203 - val_mse: 4.9203 - val_mae: 1.8035\n",
      "\n",
      "Epoch 00246: val_mae improved from 1.80806 to 1.80350, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 247/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1979 - mse: 6.1979 - mae: 1.9981 - val_loss: 4.9240 - val_mse: 4.9240 - val_mae: 1.8065\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 1.80350\n",
      "Epoch 248/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.3507 - mse: 6.3507 - mae: 1.9923 - val_loss: 4.9218 - val_mse: 4.9218 - val_mae: 1.8064\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 1.80350\n",
      "Epoch 249/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.3500 - mse: 6.3500 - mae: 2.0247 - val_loss: 4.9162 - val_mse: 4.9162 - val_mae: 1.8048\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 1.80350\n",
      "Epoch 250/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 7.1255 - mse: 7.1255 - mae: 2.1393 - val_loss: 4.9023 - val_mse: 4.9023 - val_mae: 1.7987\n",
      "\n",
      "Epoch 00250: val_mae improved from 1.80350 to 1.79868, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 251/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2817 - mse: 6.2817 - mae: 2.0116 - val_loss: 4.9007 - val_mse: 4.9007 - val_mae: 1.7992\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 1.79868\n",
      "Epoch 252/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3128 - mse: 6.3128 - mae: 2.0035 - val_loss: 4.9044 - val_mse: 4.9044 - val_mae: 1.8022\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 1.79868\n",
      "Epoch 253/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1353 - mse: 6.1353 - mae: 1.9949 - val_loss: 4.9002 - val_mse: 4.9002 - val_mae: 1.8013\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 1.79868\n",
      "Epoch 254/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0985 - mse: 6.0985 - mae: 1.9359 - val_loss: 4.8938 - val_mse: 4.8938 - val_mae: 1.7994\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 1.79868\n",
      "Epoch 255/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3781 - mse: 6.3781 - mae: 1.9962 - val_loss: 4.8867 - val_mse: 4.8867 - val_mae: 1.7968\n",
      "\n",
      "Epoch 00255: val_mae improved from 1.79868 to 1.79677, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 256/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.6478 - mse: 5.6478 - mae: 1.9238 - val_loss: 4.8843 - val_mse: 4.8843 - val_mae: 1.7966\n",
      "\n",
      "Epoch 00256: val_mae improved from 1.79677 to 1.79661, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 257/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4100 - mse: 6.4100 - mae: 2.0196 - val_loss: 4.8879 - val_mse: 4.8879 - val_mae: 1.7992\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 1.79661\n",
      "Epoch 258/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8845 - mse: 5.8845 - mae: 1.9598 - val_loss: 4.8716 - val_mse: 4.8716 - val_mae: 1.7917\n",
      "\n",
      "Epoch 00258: val_mae improved from 1.79661 to 1.79173, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 259/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.7357 - mse: 6.7357 - mae: 2.0625 - val_loss: 4.8680 - val_mse: 4.8680 - val_mae: 1.7910\n",
      "\n",
      "Epoch 00259: val_mae improved from 1.79173 to 1.79095, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 260/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9412 - mse: 5.9412 - mae: 1.9779 - val_loss: 4.8686 - val_mse: 4.8686 - val_mae: 1.7927\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 1.79095\n",
      "Epoch 261/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.2986 - mse: 6.2986 - mae: 1.9985 - val_loss: 4.8697 - val_mse: 4.8697 - val_mae: 1.7944\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 1.79095\n",
      "Epoch 262/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0329 - mse: 6.0329 - mae: 1.9666 - val_loss: 4.8774 - val_mse: 4.8774 - val_mae: 1.7987\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 1.79095\n",
      "Epoch 263/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.8538 - mse: 5.8538 - mae: 1.9297 - val_loss: 4.8711 - val_mse: 4.8711 - val_mae: 1.7967\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 1.79095\n",
      "Epoch 264/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8831 - mse: 5.8831 - mae: 1.9577 - val_loss: 4.8638 - val_mse: 4.8638 - val_mae: 1.7945\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 1.79095\n",
      "Epoch 265/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7475 - mse: 5.7475 - mae: 1.9236 - val_loss: 4.8628 - val_mse: 4.8628 - val_mae: 1.7948\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 1.79095\n",
      "Epoch 266/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1418 - mse: 6.1418 - mae: 1.9860 - val_loss: 4.8575 - val_mse: 4.8575 - val_mae: 1.7934\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 1.79095\n",
      "Epoch 267/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1983 - mse: 6.1983 - mae: 1.9394 - val_loss: 4.8473 - val_mse: 4.8473 - val_mae: 1.7894\n",
      "\n",
      "Epoch 00267: val_mae improved from 1.79095 to 1.78938, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 268/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.4537 - mse: 6.4537 - mae: 2.0300 - val_loss: 4.8377 - val_mse: 4.8377 - val_mae: 1.7850\n",
      "\n",
      "Epoch 00268: val_mae improved from 1.78938 to 1.78497, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 269/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.8895 - mse: 5.8895 - mae: 1.9515 - val_loss: 4.8428 - val_mse: 4.8428 - val_mae: 1.7891\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 1.78497\n",
      "Epoch 270/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1355 - mse: 6.1355 - mae: 2.0009 - val_loss: 4.8345 - val_mse: 4.8345 - val_mae: 1.7859\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 1.78497\n",
      "Epoch 271/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8860 - mse: 5.8860 - mae: 1.9273 - val_loss: 4.8434 - val_mse: 4.8434 - val_mae: 1.7911\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 1.78497\n",
      "Epoch 272/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1059 - mse: 6.1059 - mae: 1.9606 - val_loss: 4.8405 - val_mse: 4.8405 - val_mae: 1.7905\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 1.78497\n",
      "Epoch 273/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5476 - mse: 6.5476 - mae: 1.9892 - val_loss: 4.8319 - val_mse: 4.8319 - val_mae: 1.7875\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 1.78497\n",
      "Epoch 274/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9270 - mse: 5.9270 - mae: 1.9496 - val_loss: 4.8246 - val_mse: 4.8246 - val_mae: 1.7846\n",
      "\n",
      "Epoch 00274: val_mae improved from 1.78497 to 1.78464, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 275/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0731 - mse: 6.0731 - mae: 1.9511 - val_loss: 4.8193 - val_mse: 4.8193 - val_mae: 1.7829\n",
      "\n",
      "Epoch 00275: val_mae improved from 1.78464 to 1.78287, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 276/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1443 - mse: 6.1443 - mae: 1.9723 - val_loss: 4.8146 - val_mse: 4.8146 - val_mae: 1.7813\n",
      "\n",
      "Epoch 00276: val_mae improved from 1.78287 to 1.78126, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 277/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.5833 - mse: 6.5833 - mae: 2.0026 - val_loss: 4.8187 - val_mse: 4.8187 - val_mae: 1.7846\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 1.78126\n",
      "Epoch 278/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0013 - mse: 6.0013 - mae: 1.9354 - val_loss: 4.8133 - val_mse: 4.8133 - val_mae: 1.7830\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 1.78126\n",
      "Epoch 279/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2217 - mse: 6.2217 - mae: 1.9225 - val_loss: 4.8243 - val_mse: 4.8243 - val_mae: 1.7888\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 1.78126\n",
      "Epoch 280/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 7.0218 - mse: 7.0218 - mae: 2.0933 - val_loss: 4.8166 - val_mse: 4.8166 - val_mae: 1.7861\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 1.78126\n",
      "Epoch 281/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2503 - mse: 6.2503 - mae: 2.0262 - val_loss: 4.7982 - val_mse: 4.7982 - val_mae: 1.7773\n",
      "\n",
      "Epoch 00281: val_mae improved from 1.78126 to 1.77729, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 282/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3729 - mse: 6.3729 - mae: 1.9918 - val_loss: 4.8070 - val_mse: 4.8070 - val_mae: 1.7830\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 1.77729\n",
      "Epoch 283/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1618 - mse: 6.1618 - mae: 1.9700 - val_loss: 4.8019 - val_mse: 4.8019 - val_mae: 1.7813\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 1.77729\n",
      "Epoch 284/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1456 - mse: 6.1456 - mae: 2.0171 - val_loss: 4.8037 - val_mse: 4.8037 - val_mae: 1.7830\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 1.77729\n",
      "Epoch 285/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6567 - mse: 5.6567 - mae: 1.8989 - val_loss: 4.7905 - val_mse: 4.7905 - val_mae: 1.7770\n",
      "\n",
      "Epoch 00285: val_mae improved from 1.77729 to 1.77696, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 286/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6244 - mse: 5.6244 - mae: 1.9120 - val_loss: 4.7944 - val_mse: 4.7944 - val_mae: 1.7799\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 1.77696\n",
      "Epoch 287/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.6291 - mse: 5.6291 - mae: 1.9173 - val_loss: 4.7986 - val_mse: 4.7986 - val_mae: 1.7829\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 1.77696\n",
      "Epoch 288/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5765 - mse: 6.5765 - mae: 2.0436 - val_loss: 4.7934 - val_mse: 4.7934 - val_mae: 1.7812\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 1.77696\n",
      "Epoch 289/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1498 - mse: 6.1498 - mae: 2.0347 - val_loss: 4.7837 - val_mse: 4.7837 - val_mae: 1.7771\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 1.77696\n",
      "Epoch 290/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0745 - mse: 6.0745 - mae: 1.9576 - val_loss: 4.7751 - val_mse: 4.7751 - val_mae: 1.7734\n",
      "\n",
      "Epoch 00290: val_mae improved from 1.77696 to 1.77337, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 291/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7870 - mse: 6.7870 - mae: 2.0708 - val_loss: 4.7890 - val_mse: 4.7890 - val_mae: 1.7817\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 1.77337\n",
      "Epoch 292/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.4341 - mse: 5.4341 - mae: 1.8565 - val_loss: 4.7679 - val_mse: 4.7679 - val_mae: 1.7711\n",
      "\n",
      "Epoch 00292: val_mae improved from 1.77337 to 1.77113, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 293/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6726 - mse: 6.6726 - mae: 2.0794 - val_loss: 4.7654 - val_mse: 4.7654 - val_mae: 1.7710\n",
      "\n",
      "Epoch 00293: val_mae improved from 1.77113 to 1.77097, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 294/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4515 - mse: 6.4515 - mae: 2.0073 - val_loss: 4.7643 - val_mse: 4.7643 - val_mae: 1.7714\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 1.77097\n",
      "Epoch 295/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0566 - mse: 6.0566 - mae: 1.9698 - val_loss: 4.7662 - val_mse: 4.7662 - val_mae: 1.7735\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 1.77097\n",
      "Epoch 296/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7211 - mse: 5.7211 - mae: 1.9177 - val_loss: 4.7533 - val_mse: 4.7533 - val_mae: 1.7665\n",
      "\n",
      "Epoch 00296: val_mae improved from 1.77097 to 1.76650, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 297/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2121 - mse: 6.2121 - mae: 1.9788 - val_loss: 4.7660 - val_mse: 4.7660 - val_mae: 1.7752\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 1.76650\n",
      "Epoch 298/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.9527 - mse: 5.9527 - mae: 1.9478 - val_loss: 4.7558 - val_mse: 4.7558 - val_mae: 1.7702\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 1.76650\n",
      "Epoch 299/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.7904 - mse: 6.7904 - mae: 2.0560 - val_loss: 4.7511 - val_mse: 4.7511 - val_mae: 1.7685\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 1.76650\n",
      "Epoch 300/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1394 - mse: 6.1394 - mae: 1.9860 - val_loss: 4.7508 - val_mse: 4.7508 - val_mae: 1.7692\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 1.76650\n",
      "Epoch 301/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3302 - mse: 6.3302 - mae: 1.9844 - val_loss: 4.7585 - val_mse: 4.7585 - val_mae: 1.7745\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 1.76650\n",
      "Epoch 302/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4482 - mse: 6.4482 - mae: 2.0617 - val_loss: 4.7440 - val_mse: 4.7440 - val_mae: 1.7674\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 1.76650\n",
      "Epoch 303/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5341 - mse: 6.5341 - mae: 1.9547 - val_loss: 4.7393 - val_mse: 4.7393 - val_mae: 1.7654\n",
      "\n",
      "Epoch 00303: val_mae improved from 1.76650 to 1.76535, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 304/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0663 - mse: 6.0663 - mae: 1.9797 - val_loss: 4.7330 - val_mse: 4.7330 - val_mae: 1.7619\n",
      "\n",
      "Epoch 00304: val_mae improved from 1.76535 to 1.76185, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 305/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4797 - mse: 6.4797 - mae: 2.0007 - val_loss: 4.7387 - val_mse: 4.7387 - val_mae: 1.7671\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 1.76185\n",
      "Epoch 306/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 6.0924 - mse: 6.0924 - mae: 1.9588 - val_loss: 4.7409 - val_mse: 4.7409 - val_mae: 1.7690\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 1.76185\n",
      "Epoch 307/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1528 - mse: 6.1528 - mae: 1.9815 - val_loss: 4.7279 - val_mse: 4.7279 - val_mae: 1.7617\n",
      "\n",
      "Epoch 00307: val_mae improved from 1.76185 to 1.76166, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 308/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9540 - mse: 5.9540 - mae: 1.9479 - val_loss: 4.7248 - val_mse: 4.7248 - val_mae: 1.7601\n",
      "\n",
      "Epoch 00308: val_mae improved from 1.76166 to 1.76009, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 309/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.9751 - mse: 5.9751 - mae: 1.9381 - val_loss: 4.7333 - val_mse: 4.7333 - val_mae: 1.7668\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 1.76009\n",
      "Epoch 310/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6340 - mse: 6.6340 - mae: 2.0782 - val_loss: 4.7336 - val_mse: 4.7336 - val_mae: 1.7679\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 1.76009\n",
      "Epoch 311/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1321 - mse: 6.1321 - mae: 1.9452 - val_loss: 4.7264 - val_mse: 4.7264 - val_mae: 1.7646\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 1.76009\n",
      "Epoch 312/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9094 - mse: 5.9094 - mae: 1.9576 - val_loss: 4.7198 - val_mse: 4.7198 - val_mae: 1.7613\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 1.76009\n",
      "Epoch 313/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6200 - mse: 5.6200 - mae: 1.9266 - val_loss: 4.7140 - val_mse: 4.7140 - val_mae: 1.7575\n",
      "\n",
      "Epoch 00313: val_mae improved from 1.76009 to 1.75749, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 314/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3653 - mse: 6.3653 - mae: 2.0053 - val_loss: 4.7159 - val_mse: 4.7159 - val_mae: 1.7604\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 1.75749\n",
      "Epoch 315/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7275 - mse: 5.7275 - mae: 1.8524 - val_loss: 4.7192 - val_mse: 4.7192 - val_mae: 1.7633\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 1.75749\n",
      "Epoch 316/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4964 - mse: 6.4964 - mae: 1.9905 - val_loss: 4.7152 - val_mse: 4.7152 - val_mae: 1.7615\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 1.75749\n",
      "Epoch 317/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6064 - mse: 5.6064 - mae: 1.8830 - val_loss: 4.7098 - val_mse: 4.7098 - val_mae: 1.7592\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 1.75749\n",
      "Epoch 318/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1983 - mse: 6.1983 - mae: 1.9915 - val_loss: 4.7194 - val_mse: 4.7194 - val_mae: 1.7655\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 1.75749\n",
      "Epoch 319/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6008 - mse: 6.6008 - mae: 2.0113 - val_loss: 4.7138 - val_mse: 4.7138 - val_mae: 1.7633\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 1.75749\n",
      "Epoch 320/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9037 - mse: 5.9037 - mae: 1.9424 - val_loss: 4.7131 - val_mse: 4.7131 - val_mae: 1.7637\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 1.75749\n",
      "Epoch 321/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1615 - mse: 6.1615 - mae: 1.9677 - val_loss: 4.7089 - val_mse: 4.7089 - val_mae: 1.7620\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 1.75749\n",
      "Epoch 322/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.5518 - mse: 6.5518 - mae: 2.0102 - val_loss: 4.7019 - val_mse: 4.7019 - val_mae: 1.7584\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 1.75749\n",
      "Epoch 323/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2034 - mse: 6.2034 - mae: 1.9482 - val_loss: 4.6947 - val_mse: 4.6947 - val_mae: 1.7542\n",
      "\n",
      "Epoch 00323: val_mae improved from 1.75749 to 1.75417, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 324/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.6319 - mse: 6.6319 - mae: 2.0352 - val_loss: 4.7042 - val_mse: 4.7042 - val_mae: 1.7612\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 1.75417\n",
      "Epoch 325/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8298 - mse: 5.8298 - mae: 1.9655 - val_loss: 4.7016 - val_mse: 4.7016 - val_mae: 1.7603\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 1.75417\n",
      "Epoch 326/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6074 - mse: 5.6074 - mae: 1.9483 - val_loss: 4.6900 - val_mse: 4.6900 - val_mae: 1.7531\n",
      "\n",
      "Epoch 00326: val_mae improved from 1.75417 to 1.75311, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 327/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.3769 - mse: 6.3769 - mae: 1.9885 - val_loss: 4.6967 - val_mse: 4.6967 - val_mae: 1.7589\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 1.75311\n",
      "Epoch 328/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9734 - mse: 5.9734 - mae: 1.9594 - val_loss: 4.6988 - val_mse: 4.6988 - val_mae: 1.7607\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 1.75311\n",
      "Epoch 329/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8150 - mse: 5.8150 - mae: 1.9259 - val_loss: 4.6915 - val_mse: 4.6915 - val_mae: 1.7571\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 1.75311\n",
      "Epoch 330/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1780 - mse: 6.1780 - mae: 1.9665 - val_loss: 4.6867 - val_mse: 4.6867 - val_mae: 1.7546\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 1.75311\n",
      "Epoch 331/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2181 - mse: 6.2181 - mae: 1.9583 - val_loss: 4.6871 - val_mse: 4.6871 - val_mae: 1.7558\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 1.75311\n",
      "Epoch 332/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.6517 - mse: 6.6517 - mae: 2.0476 - val_loss: 4.6845 - val_mse: 4.6845 - val_mae: 1.7548\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 1.75311\n",
      "Epoch 333/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0740 - mse: 6.0740 - mae: 2.0221 - val_loss: 4.6862 - val_mse: 4.6862 - val_mae: 1.7567\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 1.75311\n",
      "Epoch 334/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1666 - mse: 6.1666 - mae: 1.9932 - val_loss: 4.6833 - val_mse: 4.6833 - val_mae: 1.7557\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 1.75311\n",
      "Epoch 335/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4171 - mse: 6.4171 - mae: 1.9664 - val_loss: 4.6799 - val_mse: 4.6799 - val_mae: 1.7543\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 1.75311\n",
      "Epoch 336/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8689 - mse: 5.8689 - mae: 1.9308 - val_loss: 4.6857 - val_mse: 4.6857 - val_mae: 1.7581\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 1.75311\n",
      "Epoch 337/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2690 - mse: 6.2690 - mae: 1.9951 - val_loss: 4.6860 - val_mse: 4.6860 - val_mae: 1.7588\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 1.75311\n",
      "Epoch 338/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2750 - mse: 6.2750 - mae: 1.9892 - val_loss: 4.6748 - val_mse: 4.6748 - val_mae: 1.7527\n",
      "\n",
      "Epoch 00338: val_mae improved from 1.75311 to 1.75274, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 339/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.3621 - mse: 5.3621 - mae: 1.8517 - val_loss: 4.6796 - val_mse: 4.6796 - val_mae: 1.7563\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 1.75274\n",
      "Epoch 340/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1297 - mse: 6.1297 - mae: 1.9776 - val_loss: 4.6836 - val_mse: 4.6836 - val_mae: 1.7588\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 1.75274\n",
      "Epoch 341/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0247 - mse: 6.0247 - mae: 1.9963 - val_loss: 4.6720 - val_mse: 4.6720 - val_mae: 1.7532\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 1.75274\n",
      "Epoch 342/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7158 - mse: 5.7158 - mae: 1.8903 - val_loss: 4.6691 - val_mse: 4.6691 - val_mae: 1.7524\n",
      "\n",
      "Epoch 00342: val_mae improved from 1.75274 to 1.75237, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 343/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.5043 - mse: 6.5043 - mae: 2.0169 - val_loss: 4.6798 - val_mse: 4.6798 - val_mae: 1.7586\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 1.75237\n",
      "Epoch 344/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8360 - mse: 5.8360 - mae: 1.9698 - val_loss: 4.6677 - val_mse: 4.6677 - val_mae: 1.7525\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 1.75237\n",
      "Epoch 345/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2088 - mse: 6.2088 - mae: 1.9335 - val_loss: 4.6665 - val_mse: 4.6665 - val_mae: 1.7523\n",
      "\n",
      "Epoch 00345: val_mae improved from 1.75237 to 1.75232, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 346/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.4710 - mse: 5.4710 - mae: 1.8514 - val_loss: 4.6653 - val_mse: 4.6653 - val_mae: 1.7522\n",
      "\n",
      "Epoch 00346: val_mae improved from 1.75232 to 1.75220, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 347/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0321 - mse: 6.0321 - mae: 1.9712 - val_loss: 4.6681 - val_mse: 4.6681 - val_mae: 1.7543\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 1.75220\n",
      "Epoch 348/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6239 - mse: 5.6239 - mae: 1.9338 - val_loss: 4.6755 - val_mse: 4.6755 - val_mae: 1.7583\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 1.75220\n",
      "Epoch 349/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8748 - mse: 5.8748 - mae: 1.9072 - val_loss: 4.6656 - val_mse: 4.6656 - val_mae: 1.7540\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 1.75220\n",
      "Epoch 350/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6289 - mse: 6.6289 - mae: 1.9963 - val_loss: 4.6569 - val_mse: 4.6569 - val_mae: 1.7490\n",
      "\n",
      "Epoch 00350: val_mae improved from 1.75220 to 1.74901, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 351/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.0257 - mse: 6.0257 - mae: 2.0090 - val_loss: 4.6513 - val_mse: 4.6513 - val_mae: 1.7456\n",
      "\n",
      "Epoch 00351: val_mae improved from 1.74901 to 1.74559, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 352/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2218 - mse: 6.2218 - mae: 1.9688 - val_loss: 4.6586 - val_mse: 4.6586 - val_mae: 1.7513\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 1.74559\n",
      "Epoch 353/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8958 - mse: 5.8958 - mae: 1.9223 - val_loss: 4.6618 - val_mse: 4.6618 - val_mae: 1.7536\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 1.74559\n",
      "Epoch 354/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8950 - mse: 5.8950 - mae: 1.9963 - val_loss: 4.6482 - val_mse: 4.6482 - val_mae: 1.7454\n",
      "\n",
      "Epoch 00354: val_mae improved from 1.74559 to 1.74545, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 355/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1718 - mse: 6.1718 - mae: 2.0083 - val_loss: 4.6507 - val_mse: 4.6507 - val_mae: 1.7479\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 1.74545\n",
      "Epoch 356/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9670 - mse: 5.9670 - mae: 1.9575 - val_loss: 4.6505 - val_mse: 4.6505 - val_mae: 1.7486\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 1.74545\n",
      "Epoch 357/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3577 - mse: 6.3577 - mae: 1.9992 - val_loss: 4.6591 - val_mse: 4.6591 - val_mae: 1.7540\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 1.74545\n",
      "Epoch 358/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5552 - mse: 5.5552 - mae: 1.8681 - val_loss: 4.6416 - val_mse: 4.6416 - val_mae: 1.7435\n",
      "\n",
      "Epoch 00358: val_mae improved from 1.74545 to 1.74347, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 359/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6582 - mse: 5.6582 - mae: 1.9052 - val_loss: 4.6463 - val_mse: 4.6463 - val_mae: 1.7478\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 1.74347\n",
      "Epoch 360/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0998 - mse: 6.0998 - mae: 1.9710 - val_loss: 4.6461 - val_mse: 4.6461 - val_mae: 1.7483\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 1.74347\n",
      "Epoch 361/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6844 - mse: 5.6844 - mae: 1.8807 - val_loss: 4.6463 - val_mse: 4.6463 - val_mae: 1.7488\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 1.74347\n",
      "Epoch 362/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8831 - mse: 5.8831 - mae: 1.9381 - val_loss: 4.6551 - val_mse: 4.6551 - val_mae: 1.7538\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 1.74347\n",
      "Epoch 363/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6505 - mse: 5.6505 - mae: 1.9108 - val_loss: 4.6409 - val_mse: 4.6409 - val_mae: 1.7467\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 1.74347\n",
      "Epoch 364/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0610 - mse: 6.0610 - mae: 1.9484 - val_loss: 4.6400 - val_mse: 4.6400 - val_mae: 1.7464\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 1.74347\n",
      "Epoch 365/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8692 - mse: 5.8692 - mae: 1.9132 - val_loss: 4.6411 - val_mse: 4.6411 - val_mae: 1.7476\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 1.74347\n",
      "Epoch 366/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.6389 - mse: 6.6389 - mae: 2.0032 - val_loss: 4.6389 - val_mse: 4.6389 - val_mae: 1.7466\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 1.74347\n",
      "Epoch 367/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5750 - mse: 6.5750 - mae: 2.0426 - val_loss: 4.6369 - val_mse: 4.6369 - val_mae: 1.7455\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 1.74347\n",
      "Epoch 368/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7649 - mse: 5.7649 - mae: 1.8872 - val_loss: 4.6320 - val_mse: 4.6320 - val_mae: 1.7429\n",
      "\n",
      "Epoch 00368: val_mae improved from 1.74347 to 1.74286, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 369/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.4043 - mse: 6.4043 - mae: 1.9713 - val_loss: 4.6417 - val_mse: 4.6417 - val_mae: 1.7493\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 1.74286\n",
      "Epoch 370/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2870 - mse: 6.2870 - mae: 2.0321 - val_loss: 4.6402 - val_mse: 4.6402 - val_mae: 1.7490\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 1.74286\n",
      "Epoch 371/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7184 - mse: 5.7184 - mae: 1.8744 - val_loss: 4.6473 - val_mse: 4.6473 - val_mae: 1.7528\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 1.74286\n",
      "Epoch 372/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4948 - mse: 5.4948 - mae: 1.8722 - val_loss: 4.6343 - val_mse: 4.6343 - val_mae: 1.7466\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 1.74286\n",
      "Epoch 373/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7168 - mse: 5.7168 - mae: 1.8936 - val_loss: 4.6331 - val_mse: 4.6331 - val_mae: 1.7461\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 1.74286\n",
      "Epoch 374/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1370 - mse: 6.1370 - mae: 1.9440 - val_loss: 4.6339 - val_mse: 4.6339 - val_mae: 1.7470\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 1.74286\n",
      "Epoch 375/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0913 - mse: 6.0913 - mae: 1.9896 - val_loss: 4.6353 - val_mse: 4.6353 - val_mae: 1.7481\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 1.74286\n",
      "Epoch 376/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2821 - mse: 6.2821 - mae: 2.0052 - val_loss: 4.6336 - val_mse: 4.6336 - val_mae: 1.7475\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 1.74286\n",
      "Epoch 377/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7033 - mse: 5.7033 - mae: 1.9193 - val_loss: 4.6290 - val_mse: 4.6290 - val_mae: 1.7451\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 1.74286\n",
      "Epoch 378/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2978 - mse: 6.2978 - mae: 2.0310 - val_loss: 4.6309 - val_mse: 4.6309 - val_mae: 1.7466\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 1.74286\n",
      "Epoch 379/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0537 - mse: 6.0537 - mae: 1.9855 - val_loss: 4.6246 - val_mse: 4.6246 - val_mae: 1.7434\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 1.74286\n",
      "Epoch 380/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3836 - mse: 6.3836 - mae: 2.0304 - val_loss: 4.6289 - val_mse: 4.6289 - val_mae: 1.7463\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 1.74286\n",
      "Epoch 381/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2794 - mse: 6.2794 - mae: 1.9758 - val_loss: 4.6262 - val_mse: 4.6262 - val_mae: 1.7450\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 1.74286\n",
      "Epoch 382/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9762 - mse: 5.9762 - mae: 1.9680 - val_loss: 4.6162 - val_mse: 4.6162 - val_mae: 1.7385\n",
      "\n",
      "Epoch 00382: val_mae improved from 1.74286 to 1.73852, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 383/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.7307 - mse: 5.7307 - mae: 1.9190 - val_loss: 4.6223 - val_mse: 4.6223 - val_mae: 1.7433\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 1.73852\n",
      "Epoch 384/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8855 - mse: 5.8855 - mae: 1.9421 - val_loss: 4.6220 - val_mse: 4.6220 - val_mae: 1.7435\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 1.73852\n",
      "Epoch 385/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2727 - mse: 6.2727 - mae: 2.0079 - val_loss: 4.6214 - val_mse: 4.6214 - val_mae: 1.7435\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 1.73852\n",
      "Epoch 386/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8639 - mse: 5.8639 - mae: 1.9267 - val_loss: 4.6119 - val_mse: 4.6119 - val_mae: 1.7370\n",
      "\n",
      "Epoch 00386: val_mae improved from 1.73852 to 1.73699, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 387/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.6422 - mse: 5.6422 - mae: 1.8894 - val_loss: 4.6167 - val_mse: 4.6167 - val_mae: 1.7412\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 1.73699\n",
      "Epoch 388/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6228 - mse: 5.6228 - mae: 1.9031 - val_loss: 4.6093 - val_mse: 4.6093 - val_mae: 1.7356\n",
      "\n",
      "Epoch 00388: val_mae improved from 1.73699 to 1.73558, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 389/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7975 - mse: 5.7975 - mae: 1.9066 - val_loss: 4.6228 - val_mse: 4.6228 - val_mae: 1.7453\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 1.73558\n",
      "Epoch 390/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.7869 - mse: 6.7869 - mae: 2.0664 - val_loss: 4.6091 - val_mse: 4.6091 - val_mae: 1.7369\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 1.73558\n",
      "Epoch 391/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4544 - mse: 5.4544 - mae: 1.8692 - val_loss: 4.6094 - val_mse: 4.6094 - val_mae: 1.7376\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 1.73558\n",
      "Epoch 392/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7586 - mse: 5.7586 - mae: 1.9276 - val_loss: 4.6169 - val_mse: 4.6169 - val_mae: 1.7428\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 1.73558\n",
      "Epoch 393/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7694 - mse: 5.7694 - mae: 1.9087 - val_loss: 4.6133 - val_mse: 4.6133 - val_mae: 1.7408\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 1.73558\n",
      "Epoch 394/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3932 - mse: 5.3932 - mae: 1.8707 - val_loss: 4.6095 - val_mse: 4.6095 - val_mae: 1.7389\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 1.73558\n",
      "Epoch 395/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6839 - mse: 5.6839 - mae: 1.8828 - val_loss: 4.6113 - val_mse: 4.6113 - val_mae: 1.7404\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 1.73558\n",
      "Epoch 396/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8766 - mse: 5.8766 - mae: 1.9149 - val_loss: 4.6096 - val_mse: 4.6096 - val_mae: 1.7396\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 1.73558\n",
      "Epoch 397/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7072 - mse: 5.7072 - mae: 1.8936 - val_loss: 4.6029 - val_mse: 4.6029 - val_mae: 1.7349\n",
      "\n",
      "Epoch 00397: val_mae improved from 1.73558 to 1.73494, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 398/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5959 - mse: 5.5959 - mae: 1.8996 - val_loss: 4.6127 - val_mse: 4.6127 - val_mae: 1.7420\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 1.73494\n",
      "Epoch 399/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.5149 - mse: 6.5149 - mae: 2.0400 - val_loss: 4.6177 - val_mse: 4.6177 - val_mae: 1.7452\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 1.73494\n",
      "Epoch 400/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1005 - mse: 6.1005 - mae: 1.9814 - val_loss: 4.6062 - val_mse: 4.6062 - val_mae: 1.7388\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 1.73494\n",
      "Epoch 401/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2193 - mse: 6.2193 - mae: 1.9909 - val_loss: 4.6007 - val_mse: 4.6007 - val_mae: 1.7354\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 1.73494\n",
      "Epoch 402/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0975 - mse: 6.0975 - mae: 1.9606 - val_loss: 4.6008 - val_mse: 4.6008 - val_mae: 1.7363\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 1.73494\n",
      "Epoch 403/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8493 - mse: 5.8493 - mae: 1.9453 - val_loss: 4.5968 - val_mse: 4.5968 - val_mae: 1.7331\n",
      "\n",
      "Epoch 00403: val_mae improved from 1.73494 to 1.73308, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 404/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7236 - mse: 6.7236 - mae: 2.0163 - val_loss: 4.5996 - val_mse: 4.5996 - val_mae: 1.7361\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 1.73308\n",
      "Epoch 405/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3009 - mse: 6.3009 - mae: 2.0360 - val_loss: 4.6007 - val_mse: 4.6007 - val_mae: 1.7371\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 1.73308\n",
      "Epoch 406/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1788 - mse: 6.1788 - mae: 1.9769 - val_loss: 4.5999 - val_mse: 4.5999 - val_mae: 1.7369\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 1.73308\n",
      "Epoch 407/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2997 - mse: 6.2997 - mae: 1.9546 - val_loss: 4.6019 - val_mse: 4.6019 - val_mae: 1.7387\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 1.73308\n",
      "Epoch 408/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0215 - mse: 6.0215 - mae: 1.9765 - val_loss: 4.5978 - val_mse: 4.5978 - val_mae: 1.7364\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 1.73308\n",
      "Epoch 409/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5916 - mse: 6.5916 - mae: 1.9955 - val_loss: 4.6056 - val_mse: 4.6056 - val_mae: 1.7414\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 1.73308\n",
      "Epoch 410/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4163 - mse: 5.4163 - mae: 1.9143 - val_loss: 4.5951 - val_mse: 4.5951 - val_mae: 1.7351\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 1.73308\n",
      "Epoch 411/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1867 - mse: 6.1867 - mae: 1.9300 - val_loss: 4.6027 - val_mse: 4.6027 - val_mae: 1.7402\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 1.73308\n",
      "Epoch 412/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1462 - mse: 6.1462 - mae: 1.9453 - val_loss: 4.5963 - val_mse: 4.5963 - val_mae: 1.7364\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 1.73308\n",
      "Epoch 413/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9123 - mse: 6.9123 - mae: 2.1110 - val_loss: 4.5946 - val_mse: 4.5946 - val_mae: 1.7358\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 1.73308\n",
      "Epoch 414/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.6290 - mse: 6.6290 - mae: 1.9943 - val_loss: 4.5922 - val_mse: 4.5922 - val_mae: 1.7344\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 1.73308\n",
      "Epoch 415/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8425 - mse: 5.8425 - mae: 1.9568 - val_loss: 4.5897 - val_mse: 4.5897 - val_mae: 1.7324\n",
      "\n",
      "Epoch 00415: val_mae improved from 1.73308 to 1.73239, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 416/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8158 - mse: 6.8158 - mae: 2.0682 - val_loss: 4.5959 - val_mse: 4.5959 - val_mae: 1.7370\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 1.73239\n",
      "Epoch 417/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.3576 - mse: 6.3576 - mae: 2.0157 - val_loss: 4.5885 - val_mse: 4.5885 - val_mae: 1.7320\n",
      "\n",
      "Epoch 00417: val_mae improved from 1.73239 to 1.73202, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 418/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8668 - mse: 6.8668 - mae: 2.0887 - val_loss: 4.5891 - val_mse: 4.5891 - val_mae: 1.7328\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 1.73202\n",
      "Epoch 419/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7788 - mse: 5.7788 - mae: 1.9123 - val_loss: 4.5868 - val_mse: 4.5868 - val_mae: 1.7312\n",
      "\n",
      "Epoch 00419: val_mae improved from 1.73202 to 1.73117, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 420/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 6.5187 - mse: 6.5187 - mae: 2.0115 - val_loss: 4.5967 - val_mse: 4.5967 - val_mae: 1.7386\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 1.73117\n",
      "Epoch 421/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.3217 - mse: 6.3217 - mae: 2.0589 - val_loss: 4.5872 - val_mse: 4.5872 - val_mae: 1.7325\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 1.73117\n",
      "Epoch 422/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8558 - mse: 5.8558 - mae: 1.9201 - val_loss: 4.5849 - val_mse: 4.5849 - val_mae: 1.7305\n",
      "\n",
      "Epoch 00422: val_mae improved from 1.73117 to 1.73050, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 423/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0317 - mse: 6.0317 - mae: 1.9452 - val_loss: 4.5876 - val_mse: 4.5876 - val_mae: 1.7335\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 1.73050\n",
      "Epoch 424/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1214 - mse: 6.1214 - mae: 1.9750 - val_loss: 4.5951 - val_mse: 4.5951 - val_mae: 1.7385\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 1.73050\n",
      "Epoch 425/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.5687 - mse: 6.5687 - mae: 1.9934 - val_loss: 4.5871 - val_mse: 4.5871 - val_mae: 1.7337\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 1.73050\n",
      "Epoch 426/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8089 - mse: 5.8089 - mae: 1.9632 - val_loss: 4.5885 - val_mse: 4.5885 - val_mae: 1.7351\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 1.73050\n",
      "Epoch 427/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6550 - mse: 6.6550 - mae: 2.0294 - val_loss: 4.5961 - val_mse: 4.5961 - val_mae: 1.7397\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 1.73050\n",
      "Epoch 428/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0441 - mse: 6.0441 - mae: 1.9585 - val_loss: 4.5866 - val_mse: 4.5866 - val_mae: 1.7342\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 1.73050\n",
      "Epoch 429/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0510 - mse: 6.0510 - mae: 1.9744 - val_loss: 4.5806 - val_mse: 4.5806 - val_mae: 1.7296\n",
      "\n",
      "Epoch 00429: val_mae improved from 1.73050 to 1.72963, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 430/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7366 - mse: 5.7366 - mae: 1.9011 - val_loss: 4.5871 - val_mse: 4.5871 - val_mae: 1.7351\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 1.72963\n",
      "Epoch 431/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3333 - mse: 6.3333 - mae: 1.9662 - val_loss: 4.5939 - val_mse: 4.5939 - val_mae: 1.7392\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 1.72963\n",
      "Epoch 432/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8961 - mse: 5.8961 - mae: 1.8960 - val_loss: 4.5857 - val_mse: 4.5857 - val_mae: 1.7346\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 1.72963\n",
      "Epoch 433/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1485 - mse: 6.1485 - mae: 1.9658 - val_loss: 4.5825 - val_mse: 4.5825 - val_mae: 1.7327\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 1.72963\n",
      "Epoch 434/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6647 - mse: 6.6647 - mae: 2.0078 - val_loss: 4.5831 - val_mse: 4.5831 - val_mae: 1.7334\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 1.72963\n",
      "Epoch 435/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2698 - mse: 6.2698 - mae: 1.9848 - val_loss: 4.5806 - val_mse: 4.5806 - val_mae: 1.7317\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 1.72963\n",
      "Epoch 436/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.4425 - mse: 5.4425 - mae: 1.8478 - val_loss: 4.5859 - val_mse: 4.5859 - val_mae: 1.7356\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 1.72963\n",
      "Epoch 437/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8435 - mse: 5.8435 - mae: 1.9077 - val_loss: 4.5853 - val_mse: 4.5853 - val_mae: 1.7354\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 1.72963\n",
      "Epoch 438/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4624 - mse: 6.4624 - mae: 2.0337 - val_loss: 4.5800 - val_mse: 4.5800 - val_mae: 1.7322\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 1.72963\n",
      "Epoch 439/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1974 - mse: 6.1974 - mae: 1.9675 - val_loss: 4.5794 - val_mse: 4.5794 - val_mae: 1.7321\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 1.72963\n",
      "Epoch 440/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2264 - mse: 6.2264 - mae: 1.9887 - val_loss: 4.5811 - val_mse: 4.5811 - val_mae: 1.7333\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 1.72963\n",
      "Epoch 441/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.6975 - mse: 6.6975 - mae: 1.9898 - val_loss: 4.5793 - val_mse: 4.5793 - val_mae: 1.7323\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 1.72963\n",
      "Epoch 442/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4082 - mse: 6.4082 - mae: 1.9860 - val_loss: 4.5742 - val_mse: 4.5742 - val_mae: 1.7285\n",
      "\n",
      "Epoch 00442: val_mae improved from 1.72963 to 1.72854, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 443/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1614 - mse: 6.1614 - mae: 1.9362 - val_loss: 4.5808 - val_mse: 4.5808 - val_mae: 1.7338\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 1.72854\n",
      "Epoch 444/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9185 - mse: 5.9185 - mae: 1.9355 - val_loss: 4.5767 - val_mse: 4.5767 - val_mae: 1.7311\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 1.72854\n",
      "Epoch 445/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5341 - mse: 6.5341 - mae: 2.0188 - val_loss: 4.5785 - val_mse: 4.5785 - val_mae: 1.7325\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 1.72854\n",
      "Epoch 446/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3508 - mse: 6.3508 - mae: 1.9531 - val_loss: 4.5769 - val_mse: 4.5769 - val_mae: 1.7315\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 1.72854\n",
      "Epoch 447/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5741 - mse: 5.5741 - mae: 1.8598 - val_loss: 4.5729 - val_mse: 4.5729 - val_mae: 1.7281\n",
      "\n",
      "Epoch 00447: val_mae improved from 1.72854 to 1.72815, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 448/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.1106 - mse: 6.1106 - mae: 1.9772 - val_loss: 4.5780 - val_mse: 4.5780 - val_mae: 1.7326\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 1.72815\n",
      "Epoch 449/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.4186 - mse: 5.4186 - mae: 1.9016 - val_loss: 4.5756 - val_mse: 4.5756 - val_mae: 1.7310\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 1.72815\n",
      "Epoch 450/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 5.8201 - mse: 5.8201 - mae: 1.9175 - val_loss: 4.5836 - val_mse: 4.5836 - val_mae: 1.7364\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 1.72815\n",
      "Epoch 451/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8527 - mse: 5.8527 - mae: 1.9045 - val_loss: 4.5795 - val_mse: 4.5795 - val_mae: 1.7341\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 1.72815\n",
      "Epoch 452/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7340 - mse: 5.7340 - mae: 1.9087 - val_loss: 4.5803 - val_mse: 4.5803 - val_mae: 1.7346\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 1.72815\n",
      "Epoch 453/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4459 - mse: 5.4459 - mae: 1.8770 - val_loss: 4.5789 - val_mse: 4.5789 - val_mae: 1.7341\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 1.72815\n",
      "Epoch 454/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6152 - mse: 6.6152 - mae: 2.0073 - val_loss: 4.5852 - val_mse: 4.5852 - val_mae: 1.7379\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 1.72815\n",
      "Epoch 455/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1202 - mse: 6.1202 - mae: 2.0216 - val_loss: 4.5724 - val_mse: 4.5724 - val_mae: 1.7302\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 1.72815\n",
      "Epoch 456/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1061 - mse: 6.1061 - mae: 1.9662 - val_loss: 4.5812 - val_mse: 4.5812 - val_mae: 1.7361\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 1.72815\n",
      "Epoch 457/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0674 - mse: 6.0674 - mae: 1.9377 - val_loss: 4.5728 - val_mse: 4.5728 - val_mae: 1.7308\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 1.72815\n",
      "Epoch 458/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2661 - mse: 6.2661 - mae: 1.9841 - val_loss: 4.5798 - val_mse: 4.5798 - val_mae: 1.7354\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 1.72815\n",
      "Epoch 459/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 5.3963 - mse: 5.3963 - mae: 1.8575 - val_loss: 4.5755 - val_mse: 4.5755 - val_mae: 1.7329\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 1.72815\n",
      "Epoch 460/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5944 - mse: 6.5944 - mae: 1.9968 - val_loss: 4.5713 - val_mse: 4.5713 - val_mae: 1.7302\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 1.72815\n",
      "Epoch 461/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.4861 - mse: 5.4861 - mae: 1.8643 - val_loss: 4.5702 - val_mse: 4.5702 - val_mae: 1.7294\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 1.72815\n",
      "Epoch 462/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9380 - mse: 5.9380 - mae: 1.9589 - val_loss: 4.5719 - val_mse: 4.5719 - val_mae: 1.7310\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 1.72815\n",
      "Epoch 463/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.8804 - mse: 5.8804 - mae: 1.9389 - val_loss: 4.5686 - val_mse: 4.5686 - val_mae: 1.7285\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 1.72815\n",
      "Epoch 464/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8535 - mse: 5.8535 - mae: 1.9019 - val_loss: 4.5783 - val_mse: 4.5783 - val_mae: 1.7353\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 1.72815\n",
      "Epoch 465/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.2903 - mse: 5.2903 - mae: 1.8240 - val_loss: 4.5695 - val_mse: 4.5695 - val_mae: 1.7296\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 1.72815\n",
      "Epoch 466/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.4591 - mse: 5.4591 - mae: 1.8238 - val_loss: 4.5703 - val_mse: 4.5703 - val_mae: 1.7304\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 1.72815\n",
      "Epoch 467/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9320 - mse: 5.9320 - mae: 1.9174 - val_loss: 4.5712 - val_mse: 4.5712 - val_mae: 1.7315\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 1.72815\n",
      "Epoch 468/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.4829 - mse: 6.4829 - mae: 2.0313 - val_loss: 4.5731 - val_mse: 4.5731 - val_mae: 1.7328\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 1.72815\n",
      "Epoch 469/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9399 - mse: 5.9399 - mae: 1.9032 - val_loss: 4.5673 - val_mse: 4.5673 - val_mae: 1.7290\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 1.72815\n",
      "Epoch 470/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6092 - mse: 5.6092 - mae: 1.8723 - val_loss: 4.5645 - val_mse: 4.5645 - val_mae: 1.7271\n",
      "\n",
      "Epoch 00470: val_mae improved from 1.72815 to 1.72713, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 471/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.1818 - mse: 6.1818 - mae: 1.9400 - val_loss: 4.5727 - val_mse: 4.5727 - val_mae: 1.7333\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 1.72713\n",
      "Epoch 472/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9072 - mse: 5.9072 - mae: 1.9551 - val_loss: 4.5697 - val_mse: 4.5697 - val_mae: 1.7313\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 1.72713\n",
      "Epoch 473/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5193 - mse: 6.5193 - mae: 2.0104 - val_loss: 4.5677 - val_mse: 4.5677 - val_mae: 1.7302\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 1.72713\n",
      "Epoch 474/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9585 - mse: 5.9585 - mae: 1.9103 - val_loss: 4.5648 - val_mse: 4.5648 - val_mae: 1.7285\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 1.72713\n",
      "Epoch 475/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2061 - mse: 6.2061 - mae: 1.9323 - val_loss: 4.5655 - val_mse: 4.5655 - val_mae: 1.7290\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 1.72713\n",
      "Epoch 476/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0508 - mse: 6.0508 - mae: 1.9258 - val_loss: 4.5716 - val_mse: 4.5716 - val_mae: 1.7332\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 1.72713\n",
      "Epoch 477/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0548 - mse: 6.0548 - mae: 1.9608 - val_loss: 4.5683 - val_mse: 4.5683 - val_mae: 1.7312\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 1.72713\n",
      "Epoch 478/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6348 - mse: 5.6348 - mae: 1.8940 - val_loss: 4.5687 - val_mse: 4.5687 - val_mae: 1.7317\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 1.72713\n",
      "Epoch 479/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4858 - mse: 6.4858 - mae: 2.0268 - val_loss: 4.5665 - val_mse: 4.5665 - val_mae: 1.7304\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 1.72713\n",
      "Epoch 480/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1935 - mse: 6.1935 - mae: 2.0040 - val_loss: 4.5586 - val_mse: 4.5586 - val_mae: 1.7233\n",
      "\n",
      "Epoch 00480: val_mae improved from 1.72713 to 1.72328, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 481/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7723 - mse: 5.7723 - mae: 1.9060 - val_loss: 4.5628 - val_mse: 4.5628 - val_mae: 1.7283\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 1.72328\n",
      "Epoch 482/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2675 - mse: 6.2675 - mae: 1.9917 - val_loss: 4.5644 - val_mse: 4.5644 - val_mae: 1.7294\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 1.72328\n",
      "Epoch 483/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7887 - mse: 5.7887 - mae: 1.9020 - val_loss: 4.5600 - val_mse: 4.5600 - val_mae: 1.7261\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 1.72328\n",
      "Epoch 484/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9894 - mse: 5.9894 - mae: 1.9829 - val_loss: 4.5630 - val_mse: 4.5630 - val_mae: 1.7287\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 1.72328\n",
      "Epoch 485/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2871 - mse: 6.2871 - mae: 1.9905 - val_loss: 4.5637 - val_mse: 4.5637 - val_mae: 1.7291\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 1.72328\n",
      "Epoch 486/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2354 - mse: 6.2354 - mae: 1.9363 - val_loss: 4.5601 - val_mse: 4.5601 - val_mae: 1.7267\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 1.72328\n",
      "Epoch 487/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7101 - mse: 5.7101 - mae: 1.8891 - val_loss: 4.5581 - val_mse: 4.5581 - val_mae: 1.7247\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 1.72328\n",
      "Epoch 488/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5765 - mse: 5.5765 - mae: 1.8274 - val_loss: 4.5600 - val_mse: 4.5600 - val_mae: 1.7268\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 1.72328\n",
      "Epoch 489/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1885 - mse: 6.1885 - mae: 1.9511 - val_loss: 4.5616 - val_mse: 4.5616 - val_mae: 1.7282\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 1.72328\n",
      "Epoch 490/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.3270 - mse: 5.3270 - mae: 1.8322 - val_loss: 4.5582 - val_mse: 4.5582 - val_mae: 1.7254\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 1.72328\n",
      "Epoch 491/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1675 - mse: 6.1675 - mae: 1.9301 - val_loss: 4.5631 - val_mse: 4.5631 - val_mae: 1.7293\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 1.72328\n",
      "Epoch 492/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.7666 - mse: 6.7666 - mae: 2.0999 - val_loss: 4.5645 - val_mse: 4.5645 - val_mae: 1.7304\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 1.72328\n",
      "Epoch 493/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7926 - mse: 5.7926 - mae: 1.9092 - val_loss: 4.5652 - val_mse: 4.5652 - val_mae: 1.7310\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 1.72328\n",
      "Epoch 494/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8241 - mse: 5.8241 - mae: 1.9607 - val_loss: 4.5597 - val_mse: 4.5597 - val_mae: 1.7274\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 1.72328\n",
      "Epoch 495/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4053 - mse: 6.4053 - mae: 1.9987 - val_loss: 4.5646 - val_mse: 4.5646 - val_mae: 1.7309\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 1.72328\n",
      "Epoch 496/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 5.6110 - mse: 5.6110 - mae: 1.9162 - val_loss: 4.5608 - val_mse: 4.5608 - val_mae: 1.7285\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 1.72328\n",
      "Epoch 497/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1423 - mse: 6.1423 - mae: 1.9651 - val_loss: 4.5578 - val_mse: 4.5579 - val_mae: 1.7264\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 1.72328\n",
      "Epoch 498/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8889 - mse: 5.8889 - mae: 1.9462 - val_loss: 4.5597 - val_mse: 4.5597 - val_mae: 1.7280\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 1.72328\n",
      "Epoch 499/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3083 - mse: 6.3083 - mae: 1.9789 - val_loss: 4.5599 - val_mse: 4.5599 - val_mae: 1.7283\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 1.72328\n",
      "Epoch 500/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8797 - mse: 5.8797 - mae: 1.9082 - val_loss: 4.5702 - val_mse: 4.5702 - val_mae: 1.7347\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 1.72328\n",
      "Epoch 501/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9076 - mse: 5.9076 - mae: 1.9710 - val_loss: 4.5644 - val_mse: 4.5644 - val_mae: 1.7315\n",
      "\n",
      "Epoch 00501: val_mae did not improve from 1.72328\n",
      "Epoch 502/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6254 - mse: 5.6254 - mae: 1.8888 - val_loss: 4.5623 - val_mse: 4.5623 - val_mae: 1.7304\n",
      "\n",
      "Epoch 00502: val_mae did not improve from 1.72328\n",
      "Epoch 503/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1949 - mse: 6.1949 - mae: 1.9715 - val_loss: 4.5645 - val_mse: 4.5645 - val_mae: 1.7318\n",
      "\n",
      "Epoch 00503: val_mae did not improve from 1.72328\n",
      "Epoch 504/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8150 - mse: 5.8150 - mae: 1.9529 - val_loss: 4.5572 - val_mse: 4.5572 - val_mae: 1.7272\n",
      "\n",
      "Epoch 00504: val_mae did not improve from 1.72328\n",
      "Epoch 505/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7981 - mse: 5.7981 - mae: 1.9351 - val_loss: 4.5542 - val_mse: 4.5542 - val_mae: 1.7245\n",
      "\n",
      "Epoch 00505: val_mae did not improve from 1.72328\n",
      "Epoch 506/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2777 - mse: 6.2777 - mae: 1.9987 - val_loss: 4.5613 - val_mse: 4.5613 - val_mae: 1.7299\n",
      "\n",
      "Epoch 00506: val_mae did not improve from 1.72328\n",
      "Epoch 507/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.2985 - mse: 5.2985 - mae: 1.8363 - val_loss: 4.5576 - val_mse: 4.5576 - val_mae: 1.7276\n",
      "\n",
      "Epoch 00507: val_mae did not improve from 1.72328\n",
      "Epoch 508/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4794 - mse: 6.4794 - mae: 2.0240 - val_loss: 4.5635 - val_mse: 4.5635 - val_mae: 1.7317\n",
      "\n",
      "Epoch 00508: val_mae did not improve from 1.72328\n",
      "Epoch 509/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4263 - mse: 6.4263 - mae: 2.0148 - val_loss: 4.5540 - val_mse: 4.5540 - val_mae: 1.7255\n",
      "\n",
      "Epoch 00509: val_mae did not improve from 1.72328\n",
      "Epoch 510/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8800 - mse: 5.8800 - mae: 1.9277 - val_loss: 4.5528 - val_mse: 4.5528 - val_mae: 1.7248\n",
      "\n",
      "Epoch 00510: val_mae did not improve from 1.72328\n",
      "Epoch 511/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7983 - mse: 6.7983 - mae: 2.0807 - val_loss: 4.5508 - val_mse: 4.5508 - val_mae: 1.7229\n",
      "\n",
      "Epoch 00511: val_mae improved from 1.72328 to 1.72288, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 512/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.0063 - mse: 6.0063 - mae: 1.9299 - val_loss: 4.5531 - val_mse: 4.5531 - val_mae: 1.7251\n",
      "\n",
      "Epoch 00512: val_mae did not improve from 1.72288\n",
      "Epoch 513/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4282 - mse: 6.4282 - mae: 2.0059 - val_loss: 4.5545 - val_mse: 4.5545 - val_mae: 1.7265\n",
      "\n",
      "Epoch 00513: val_mae did not improve from 1.72288\n",
      "Epoch 514/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8272 - mse: 5.8272 - mae: 1.9085 - val_loss: 4.5567 - val_mse: 4.5567 - val_mae: 1.7281\n",
      "\n",
      "Epoch 00514: val_mae did not improve from 1.72288\n",
      "Epoch 515/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9430 - mse: 5.9430 - mae: 1.9492 - val_loss: 4.5532 - val_mse: 4.5532 - val_mae: 1.7257\n",
      "\n",
      "Epoch 00515: val_mae did not improve from 1.72288\n",
      "Epoch 516/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9910 - mse: 5.9910 - mae: 1.9531 - val_loss: 4.5564 - val_mse: 4.5564 - val_mae: 1.7281\n",
      "\n",
      "Epoch 00516: val_mae did not improve from 1.72288\n",
      "Epoch 517/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3846 - mse: 6.3846 - mae: 1.9779 - val_loss: 4.5526 - val_mse: 4.5526 - val_mae: 1.7255\n",
      "\n",
      "Epoch 00517: val_mae did not improve from 1.72288\n",
      "Epoch 518/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1475 - mse: 6.1475 - mae: 1.9900 - val_loss: 4.5492 - val_mse: 4.5492 - val_mae: 1.7228\n",
      "\n",
      "Epoch 00518: val_mae improved from 1.72288 to 1.72281, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 519/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5543 - mse: 6.5543 - mae: 2.0623 - val_loss: 4.5566 - val_mse: 4.5566 - val_mae: 1.7286\n",
      "\n",
      "Epoch 00519: val_mae did not improve from 1.72281\n",
      "Epoch 520/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8243 - mse: 5.8243 - mae: 1.8943 - val_loss: 4.5494 - val_mse: 4.5494 - val_mae: 1.7235\n",
      "\n",
      "Epoch 00520: val_mae did not improve from 1.72281\n",
      "Epoch 521/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9570 - mse: 5.9570 - mae: 1.9181 - val_loss: 4.5554 - val_mse: 4.5554 - val_mae: 1.7280\n",
      "\n",
      "Epoch 00521: val_mae did not improve from 1.72281\n",
      "Epoch 522/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8784 - mse: 5.8784 - mae: 1.8948 - val_loss: 4.5553 - val_mse: 4.5553 - val_mae: 1.7282\n",
      "\n",
      "Epoch 00522: val_mae did not improve from 1.72281\n",
      "Epoch 523/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 7.0209 - mse: 7.0209 - mae: 2.0700 - val_loss: 4.5569 - val_mse: 4.5569 - val_mae: 1.7293\n",
      "\n",
      "Epoch 00523: val_mae did not improve from 1.72281\n",
      "Epoch 524/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1524 - mse: 6.1524 - mae: 1.9439 - val_loss: 4.5569 - val_mse: 4.5569 - val_mae: 1.7294\n",
      "\n",
      "Epoch 00524: val_mae did not improve from 1.72281\n",
      "Epoch 525/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6220 - mse: 6.6220 - mae: 1.9990 - val_loss: 4.5528 - val_mse: 4.5528 - val_mae: 1.7269\n",
      "\n",
      "Epoch 00525: val_mae did not improve from 1.72281\n",
      "Epoch 526/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.4763 - mse: 6.4763 - mae: 2.0451 - val_loss: 4.5507 - val_mse: 4.5507 - val_mae: 1.7255\n",
      "\n",
      "Epoch 00526: val_mae did not improve from 1.72281\n",
      "Epoch 527/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2462 - mse: 6.2462 - mae: 1.9898 - val_loss: 4.5475 - val_mse: 4.5475 - val_mae: 1.7230\n",
      "\n",
      "Epoch 00527: val_mae did not improve from 1.72281\n",
      "Epoch 528/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.3145 - mse: 5.3145 - mae: 1.8044 - val_loss: 4.5476 - val_mse: 4.5476 - val_mae: 1.7231\n",
      "\n",
      "Epoch 00528: val_mae did not improve from 1.72281\n",
      "Epoch 529/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9395 - mse: 5.9395 - mae: 1.8994 - val_loss: 4.5635 - val_mse: 4.5635 - val_mae: 1.7336\n",
      "\n",
      "Epoch 00529: val_mae did not improve from 1.72281\n",
      "Epoch 530/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4852 - mse: 6.4852 - mae: 1.9894 - val_loss: 4.5586 - val_mse: 4.5586 - val_mae: 1.7311\n",
      "\n",
      "Epoch 00530: val_mae did not improve from 1.72281\n",
      "Epoch 531/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5542 - mse: 6.5542 - mae: 2.0537 - val_loss: 4.5528 - val_mse: 4.5528 - val_mae: 1.7273\n",
      "\n",
      "Epoch 00531: val_mae did not improve from 1.72281\n",
      "Epoch 532/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5994 - mse: 5.5994 - mae: 1.8754 - val_loss: 4.5487 - val_mse: 4.5487 - val_mae: 1.7245\n",
      "\n",
      "Epoch 00532: val_mae did not improve from 1.72281\n",
      "Epoch 533/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8066 - mse: 5.8066 - mae: 1.9458 - val_loss: 4.5466 - val_mse: 4.5466 - val_mae: 1.7227\n",
      "\n",
      "Epoch 00533: val_mae improved from 1.72281 to 1.72267, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 534/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4227 - mse: 6.4227 - mae: 1.9638 - val_loss: 4.5452 - val_mse: 4.5452 - val_mae: 1.7215\n",
      "\n",
      "Epoch 00534: val_mae improved from 1.72267 to 1.72147, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 535/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.4597 - mse: 5.4597 - mae: 1.8406 - val_loss: 4.5463 - val_mse: 4.5463 - val_mae: 1.7225\n",
      "\n",
      "Epoch 00535: val_mae did not improve from 1.72147\n",
      "Epoch 536/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.6022 - mse: 5.6022 - mae: 1.8332 - val_loss: 4.5462 - val_mse: 4.5462 - val_mae: 1.7222\n",
      "\n",
      "Epoch 00536: val_mae did not improve from 1.72147\n",
      "Epoch 537/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1791 - mse: 6.1791 - mae: 1.9733 - val_loss: 4.5474 - val_mse: 4.5474 - val_mae: 1.7233\n",
      "\n",
      "Epoch 00537: val_mae did not improve from 1.72147\n",
      "Epoch 538/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0196 - mse: 6.0196 - mae: 1.9589 - val_loss: 4.5553 - val_mse: 4.5553 - val_mae: 1.7292\n",
      "\n",
      "Epoch 00538: val_mae did not improve from 1.72147\n",
      "Epoch 539/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1900 - mse: 6.1900 - mae: 1.9537 - val_loss: 4.5520 - val_mse: 4.5520 - val_mae: 1.7272\n",
      "\n",
      "Epoch 00539: val_mae did not improve from 1.72147\n",
      "Epoch 540/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3443 - mse: 6.3443 - mae: 1.9364 - val_loss: 4.5432 - val_mse: 4.5432 - val_mae: 1.7191\n",
      "\n",
      "Epoch 00540: val_mae improved from 1.72147 to 1.71912, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 541/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 5.5711 - mse: 5.5711 - mae: 1.8780 - val_loss: 4.5454 - val_mse: 4.5454 - val_mae: 1.7219\n",
      "\n",
      "Epoch 00541: val_mae did not improve from 1.71912\n",
      "Epoch 542/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3427 - mse: 6.3427 - mae: 2.0168 - val_loss: 4.5567 - val_mse: 4.5567 - val_mae: 1.7303\n",
      "\n",
      "Epoch 00542: val_mae did not improve from 1.71912\n",
      "Epoch 543/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7873 - mse: 6.7873 - mae: 2.0696 - val_loss: 4.5519 - val_mse: 4.5519 - val_mae: 1.7275\n",
      "\n",
      "Epoch 00543: val_mae did not improve from 1.71912\n",
      "Epoch 544/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1172 - mse: 6.1172 - mae: 1.9151 - val_loss: 4.5469 - val_mse: 4.5469 - val_mae: 1.7241\n",
      "\n",
      "Epoch 00544: val_mae did not improve from 1.71912\n",
      "Epoch 545/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6433 - mse: 5.6433 - mae: 1.8908 - val_loss: 4.5440 - val_mse: 4.5440 - val_mae: 1.7215\n",
      "\n",
      "Epoch 00545: val_mae did not improve from 1.71912\n",
      "Epoch 546/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7143 - mse: 5.7143 - mae: 1.9057 - val_loss: 4.5512 - val_mse: 4.5512 - val_mae: 1.7273\n",
      "\n",
      "Epoch 00546: val_mae did not improve from 1.71912\n",
      "Epoch 547/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6849 - mse: 5.6849 - mae: 1.9034 - val_loss: 4.5506 - val_mse: 4.5506 - val_mae: 1.7269\n",
      "\n",
      "Epoch 00547: val_mae did not improve from 1.71912\n",
      "Epoch 548/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6113 - mse: 5.6113 - mae: 1.9002 - val_loss: 4.5493 - val_mse: 4.5493 - val_mae: 1.7263\n",
      "\n",
      "Epoch 00548: val_mae did not improve from 1.71912\n",
      "Epoch 549/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1852 - mse: 6.1852 - mae: 1.9839 - val_loss: 4.5576 - val_mse: 4.5576 - val_mae: 1.7314\n",
      "\n",
      "Epoch 00549: val_mae did not improve from 1.71912\n",
      "Epoch 550/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0004 - mse: 6.0004 - mae: 1.9980 - val_loss: 4.5519 - val_mse: 4.5519 - val_mae: 1.7281\n",
      "\n",
      "Epoch 00550: val_mae did not improve from 1.71912\n",
      "Epoch 551/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6439 - mse: 5.6439 - mae: 1.8918 - val_loss: 4.5537 - val_mse: 4.5537 - val_mae: 1.7293\n",
      "\n",
      "Epoch 00551: val_mae did not improve from 1.71912\n",
      "Epoch 552/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7045 - mse: 5.7045 - mae: 1.9031 - val_loss: 4.5505 - val_mse: 4.5505 - val_mae: 1.7273\n",
      "\n",
      "Epoch 00552: val_mae did not improve from 1.71912\n",
      "Epoch 553/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7378 - mse: 5.7378 - mae: 1.9006 - val_loss: 4.5461 - val_mse: 4.5461 - val_mae: 1.7243\n",
      "\n",
      "Epoch 00553: val_mae did not improve from 1.71912\n",
      "Epoch 554/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0714 - mse: 6.0714 - mae: 1.9459 - val_loss: 4.5454 - val_mse: 4.5454 - val_mae: 1.7240\n",
      "\n",
      "Epoch 00554: val_mae did not improve from 1.71912\n",
      "Epoch 555/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9490 - mse: 5.9490 - mae: 1.9219 - val_loss: 4.5420 - val_mse: 4.5420 - val_mae: 1.7215\n",
      "\n",
      "Epoch 00555: val_mae did not improve from 1.71912\n",
      "Epoch 556/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5268 - mse: 5.5268 - mae: 1.8548 - val_loss: 4.5428 - val_mse: 4.5428 - val_mae: 1.7223\n",
      "\n",
      "Epoch 00556: val_mae did not improve from 1.71912\n",
      "Epoch 557/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9749 - mse: 5.9749 - mae: 1.9491 - val_loss: 4.5517 - val_mse: 4.5517 - val_mae: 1.7287\n",
      "\n",
      "Epoch 00557: val_mae did not improve from 1.71912\n",
      "Epoch 558/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9403 - mse: 5.9403 - mae: 1.9012 - val_loss: 4.5461 - val_mse: 4.5461 - val_mae: 1.7249\n",
      "\n",
      "Epoch 00558: val_mae did not improve from 1.71912\n",
      "Epoch 559/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.3672 - mse: 5.3672 - mae: 1.8645 - val_loss: 4.5438 - val_mse: 4.5438 - val_mae: 1.7235\n",
      "\n",
      "Epoch 00559: val_mae did not improve from 1.71912\n",
      "Epoch 560/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3204 - mse: 6.3204 - mae: 1.9812 - val_loss: 4.5516 - val_mse: 4.5516 - val_mae: 1.7287\n",
      "\n",
      "Epoch 00560: val_mae did not improve from 1.71912\n",
      "Epoch 561/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5243 - mse: 6.5243 - mae: 2.0022 - val_loss: 4.5499 - val_mse: 4.5499 - val_mae: 1.7277\n",
      "\n",
      "Epoch 00561: val_mae did not improve from 1.71912\n",
      "Epoch 562/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0655 - mse: 6.0655 - mae: 1.9755 - val_loss: 4.5391 - val_mse: 4.5391 - val_mae: 1.7195\n",
      "\n",
      "Epoch 00562: val_mae did not improve from 1.71912\n",
      "Epoch 563/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1461 - mse: 6.1461 - mae: 1.9629 - val_loss: 4.5436 - val_mse: 4.5436 - val_mae: 1.7238\n",
      "\n",
      "Epoch 00563: val_mae did not improve from 1.71912\n",
      "Epoch 564/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8905 - mse: 5.8905 - mae: 1.9228 - val_loss: 4.5415 - val_mse: 4.5415 - val_mae: 1.7221\n",
      "\n",
      "Epoch 00564: val_mae did not improve from 1.71912\n",
      "Epoch 565/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1485 - mse: 6.1485 - mae: 1.9434 - val_loss: 4.5445 - val_mse: 4.5445 - val_mae: 1.7245\n",
      "\n",
      "Epoch 00565: val_mae did not improve from 1.71912\n",
      "Epoch 566/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1772 - mse: 6.1772 - mae: 1.9364 - val_loss: 4.5490 - val_mse: 4.5490 - val_mae: 1.7275\n",
      "\n",
      "Epoch 00566: val_mae did not improve from 1.71912\n",
      "Epoch 567/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4746 - mse: 6.4746 - mae: 2.0229 - val_loss: 4.5440 - val_mse: 4.5440 - val_mae: 1.7242\n",
      "\n",
      "Epoch 00567: val_mae did not improve from 1.71912\n",
      "Epoch 568/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9617 - mse: 5.9617 - mae: 1.9439 - val_loss: 4.5426 - val_mse: 4.5426 - val_mae: 1.7233\n",
      "\n",
      "Epoch 00568: val_mae did not improve from 1.71912\n",
      "Epoch 569/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1452 - mse: 6.1452 - mae: 1.9191 - val_loss: 4.5441 - val_mse: 4.5441 - val_mae: 1.7243\n",
      "\n",
      "Epoch 00569: val_mae did not improve from 1.71912\n",
      "Epoch 570/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8848 - mse: 5.8848 - mae: 1.9384 - val_loss: 4.5450 - val_mse: 4.5450 - val_mae: 1.7249\n",
      "\n",
      "Epoch 00570: val_mae did not improve from 1.71912\n",
      "Epoch 571/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9679 - mse: 5.9679 - mae: 1.8852 - val_loss: 4.5446 - val_mse: 4.5446 - val_mae: 1.7247\n",
      "\n",
      "Epoch 00571: val_mae did not improve from 1.71912\n",
      "Epoch 572/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8980 - mse: 5.8980 - mae: 1.9271 - val_loss: 4.5409 - val_mse: 4.5409 - val_mae: 1.7221\n",
      "\n",
      "Epoch 00572: val_mae did not improve from 1.71912\n",
      "Epoch 573/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3783 - mse: 6.3783 - mae: 1.9770 - val_loss: 4.5450 - val_mse: 4.5450 - val_mae: 1.7250\n",
      "\n",
      "Epoch 00573: val_mae did not improve from 1.71912\n",
      "Epoch 574/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6480 - mse: 5.6480 - mae: 1.9079 - val_loss: 4.5439 - val_mse: 4.5439 - val_mae: 1.7242\n",
      "\n",
      "Epoch 00574: val_mae did not improve from 1.71912\n",
      "Epoch 575/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3804 - mse: 5.3804 - mae: 1.8138 - val_loss: 4.5641 - val_mse: 4.5641 - val_mae: 1.7359\n",
      "\n",
      "Epoch 00575: val_mae did not improve from 1.71912\n",
      "Epoch 576/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8662 - mse: 6.8662 - mae: 2.0699 - val_loss: 4.5431 - val_mse: 4.5431 - val_mae: 1.7237\n",
      "\n",
      "Epoch 00576: val_mae did not improve from 1.71912\n",
      "Epoch 577/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3624 - mse: 6.3624 - mae: 1.9672 - val_loss: 4.5477 - val_mse: 4.5477 - val_mae: 1.7270\n",
      "\n",
      "Epoch 00577: val_mae did not improve from 1.71912\n",
      "Epoch 578/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8882 - mse: 5.8882 - mae: 1.9590 - val_loss: 4.5449 - val_mse: 4.5449 - val_mae: 1.7249\n",
      "\n",
      "Epoch 00578: val_mae did not improve from 1.71912\n",
      "Epoch 579/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1376 - mse: 6.1376 - mae: 1.9284 - val_loss: 4.5448 - val_mse: 4.5448 - val_mae: 1.7249\n",
      "\n",
      "Epoch 00579: val_mae did not improve from 1.71912\n",
      "Epoch 580/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1315 - mse: 6.1315 - mae: 1.9515 - val_loss: 4.5457 - val_mse: 4.5457 - val_mae: 1.7254\n",
      "\n",
      "Epoch 00580: val_mae did not improve from 1.71912\n",
      "Epoch 581/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7743 - mse: 5.7743 - mae: 1.9268 - val_loss: 4.5375 - val_mse: 4.5375 - val_mae: 1.7184\n",
      "\n",
      "Epoch 00581: val_mae improved from 1.71912 to 1.71838, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 582/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9302 - mse: 5.9302 - mae: 1.9226 - val_loss: 4.5410 - val_mse: 4.5410 - val_mae: 1.7222\n",
      "\n",
      "Epoch 00582: val_mae did not improve from 1.71838\n",
      "Epoch 583/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5090 - mse: 5.5090 - mae: 1.8876 - val_loss: 4.5424 - val_mse: 4.5424 - val_mae: 1.7234\n",
      "\n",
      "Epoch 00583: val_mae did not improve from 1.71838\n",
      "Epoch 584/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6987 - mse: 6.6987 - mae: 2.0359 - val_loss: 4.5485 - val_mse: 4.5485 - val_mae: 1.7278\n",
      "\n",
      "Epoch 00584: val_mae did not improve from 1.71838\n",
      "Epoch 585/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 7.0501 - mse: 7.0501 - mae: 2.1040 - val_loss: 4.5446 - val_mse: 4.5446 - val_mae: 1.7252\n",
      "\n",
      "Epoch 00585: val_mae did not improve from 1.71838\n",
      "Epoch 586/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8235 - mse: 5.8235 - mae: 1.8983 - val_loss: 4.5392 - val_mse: 4.5392 - val_mae: 1.7215\n",
      "\n",
      "Epoch 00586: val_mae did not improve from 1.71838\n",
      "Epoch 587/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0361 - mse: 6.0361 - mae: 1.9746 - val_loss: 4.5416 - val_mse: 4.5416 - val_mae: 1.7234\n",
      "\n",
      "Epoch 00587: val_mae did not improve from 1.71838\n",
      "Epoch 588/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8710 - mse: 6.8710 - mae: 2.0530 - val_loss: 4.5474 - val_mse: 4.5474 - val_mae: 1.7273\n",
      "\n",
      "Epoch 00588: val_mae did not improve from 1.71838\n",
      "Epoch 589/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6296 - mse: 5.6296 - mae: 1.8817 - val_loss: 4.5420 - val_mse: 4.5420 - val_mae: 1.7233\n",
      "\n",
      "Epoch 00589: val_mae did not improve from 1.71838\n",
      "Epoch 590/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0609 - mse: 6.0609 - mae: 1.9730 - val_loss: 4.5437 - val_mse: 4.5437 - val_mae: 1.7246\n",
      "\n",
      "Epoch 00590: val_mae did not improve from 1.71838\n",
      "Epoch 591/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1554 - mse: 6.1554 - mae: 1.9740 - val_loss: 4.5412 - val_mse: 4.5412 - val_mae: 1.7228\n",
      "\n",
      "Epoch 00591: val_mae did not improve from 1.71838\n",
      "Epoch 592/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5969 - mse: 5.5969 - mae: 1.8930 - val_loss: 4.5451 - val_mse: 4.5451 - val_mae: 1.7256\n",
      "\n",
      "Epoch 00592: val_mae did not improve from 1.71838\n",
      "Epoch 593/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9233 - mse: 5.9233 - mae: 1.9132 - val_loss: 4.5462 - val_mse: 4.5462 - val_mae: 1.7263\n",
      "\n",
      "Epoch 00593: val_mae did not improve from 1.71838\n",
      "Epoch 594/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4294 - mse: 6.4294 - mae: 1.9572 - val_loss: 4.5522 - val_mse: 4.5522 - val_mae: 1.7303\n",
      "\n",
      "Epoch 00594: val_mae did not improve from 1.71838\n",
      "Epoch 595/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.5134 - mse: 6.5134 - mae: 1.9967 - val_loss: 4.5476 - val_mse: 4.5476 - val_mae: 1.7275\n",
      "\n",
      "Epoch 00595: val_mae did not improve from 1.71838\n",
      "Epoch 596/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7886 - mse: 5.7886 - mae: 1.8858 - val_loss: 4.5419 - val_mse: 4.5419 - val_mae: 1.7236\n",
      "\n",
      "Epoch 00596: val_mae did not improve from 1.71838\n",
      "Epoch 597/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1932 - mse: 6.1932 - mae: 2.0123 - val_loss: 4.5418 - val_mse: 4.5418 - val_mae: 1.7235\n",
      "\n",
      "Epoch 00597: val_mae did not improve from 1.71838\n",
      "Epoch 598/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0941 - mse: 6.0941 - mae: 1.9295 - val_loss: 4.5458 - val_mse: 4.5458 - val_mae: 1.7263\n",
      "\n",
      "Epoch 00598: val_mae did not improve from 1.71838\n",
      "Epoch 599/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 5.8475 - mse: 5.8475 - mae: 1.9091 - val_loss: 4.5465 - val_mse: 4.5465 - val_mae: 1.7269\n",
      "\n",
      "Epoch 00599: val_mae did not improve from 1.71838\n",
      "Epoch 600/1000\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 6.6657 - mse: 6.6657 - mae: 1.9976 - val_loss: 4.5409 - val_mse: 4.5409 - val_mae: 1.7231\n",
      "\n",
      "Epoch 00600: val_mae did not improve from 1.71838\n",
      "Epoch 601/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5408 - mse: 6.5408 - mae: 1.9742 - val_loss: 4.5378 - val_mse: 4.5378 - val_mae: 1.7207\n",
      "\n",
      "Epoch 00601: val_mae did not improve from 1.71838\n",
      "Epoch 602/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6417 - mse: 6.6417 - mae: 1.9901 - val_loss: 4.5376 - val_mse: 4.5376 - val_mae: 1.7204\n",
      "\n",
      "Epoch 00602: val_mae did not improve from 1.71838\n",
      "Epoch 603/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9880 - mse: 5.9880 - mae: 1.9483 - val_loss: 4.5367 - val_mse: 4.5367 - val_mae: 1.7197\n",
      "\n",
      "Epoch 00603: val_mae did not improve from 1.71838\n",
      "Epoch 604/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3369 - mse: 6.3369 - mae: 1.9792 - val_loss: 4.5410 - val_mse: 4.5410 - val_mae: 1.7232\n",
      "\n",
      "Epoch 00604: val_mae did not improve from 1.71838\n",
      "Epoch 605/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5137 - mse: 5.5137 - mae: 1.9051 - val_loss: 4.5410 - val_mse: 4.5410 - val_mae: 1.7233\n",
      "\n",
      "Epoch 00605: val_mae did not improve from 1.71838\n",
      "Epoch 606/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2771 - mse: 6.2771 - mae: 2.0182 - val_loss: 4.5531 - val_mse: 4.5531 - val_mae: 1.7312\n",
      "\n",
      "Epoch 00606: val_mae did not improve from 1.71838\n",
      "Epoch 607/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1572 - mse: 6.1572 - mae: 1.9599 - val_loss: 4.5487 - val_mse: 4.5487 - val_mae: 1.7287\n",
      "\n",
      "Epoch 00607: val_mae did not improve from 1.71838\n",
      "Epoch 608/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7059 - mse: 5.7059 - mae: 1.9285 - val_loss: 4.5367 - val_mse: 4.5367 - val_mae: 1.7201\n",
      "\n",
      "Epoch 00608: val_mae did not improve from 1.71838\n",
      "Epoch 609/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2125 - mse: 6.2125 - mae: 1.9891 - val_loss: 4.5353 - val_mse: 4.5353 - val_mae: 1.7188\n",
      "\n",
      "Epoch 00609: val_mae did not improve from 1.71838\n",
      "Epoch 610/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4506 - mse: 5.4506 - mae: 1.8717 - val_loss: 4.5360 - val_mse: 4.5360 - val_mae: 1.7199\n",
      "\n",
      "Epoch 00610: val_mae did not improve from 1.71838\n",
      "Epoch 611/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0191 - mse: 6.0191 - mae: 1.8942 - val_loss: 4.5365 - val_mse: 4.5365 - val_mae: 1.7202\n",
      "\n",
      "Epoch 00611: val_mae did not improve from 1.71838\n",
      "Epoch 612/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6086 - mse: 5.6086 - mae: 1.8502 - val_loss: 4.5385 - val_mse: 4.5385 - val_mae: 1.7220\n",
      "\n",
      "Epoch 00612: val_mae did not improve from 1.71838\n",
      "Epoch 613/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.7100 - mse: 6.7100 - mae: 2.0614 - val_loss: 4.5398 - val_mse: 4.5398 - val_mae: 1.7229\n",
      "\n",
      "Epoch 00613: val_mae did not improve from 1.71838\n",
      "Epoch 614/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6468 - mse: 5.6468 - mae: 1.9246 - val_loss: 4.5378 - val_mse: 4.5378 - val_mae: 1.7215\n",
      "\n",
      "Epoch 00614: val_mae did not improve from 1.71838\n",
      "Epoch 615/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8480 - mse: 5.8480 - mae: 1.9125 - val_loss: 4.5415 - val_mse: 4.5415 - val_mae: 1.7242\n",
      "\n",
      "Epoch 00615: val_mae did not improve from 1.71838\n",
      "Epoch 616/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0789 - mse: 6.0789 - mae: 1.9729 - val_loss: 4.5428 - val_mse: 4.5428 - val_mae: 1.7251\n",
      "\n",
      "Epoch 00616: val_mae did not improve from 1.71838\n",
      "Epoch 617/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7164 - mse: 5.7164 - mae: 1.9273 - val_loss: 4.5414 - val_mse: 4.5414 - val_mae: 1.7244\n",
      "\n",
      "Epoch 00617: val_mae did not improve from 1.71838\n",
      "Epoch 618/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9824 - mse: 5.9824 - mae: 1.9386 - val_loss: 4.5370 - val_mse: 4.5370 - val_mae: 1.7213\n",
      "\n",
      "Epoch 00618: val_mae did not improve from 1.71838\n",
      "Epoch 619/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3404 - mse: 6.3404 - mae: 1.9968 - val_loss: 4.5395 - val_mse: 4.5395 - val_mae: 1.7231\n",
      "\n",
      "Epoch 00619: val_mae did not improve from 1.71838\n",
      "Epoch 620/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.4269 - mse: 6.4269 - mae: 2.0015 - val_loss: 4.5393 - val_mse: 4.5393 - val_mae: 1.7229\n",
      "\n",
      "Epoch 00620: val_mae did not improve from 1.71838\n",
      "Epoch 621/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9432 - mse: 5.9432 - mae: 1.9293 - val_loss: 4.5331 - val_mse: 4.5331 - val_mae: 1.7164\n",
      "\n",
      "Epoch 00621: val_mae improved from 1.71838 to 1.71644, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 622/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2440 - mse: 6.2440 - mae: 1.9584 - val_loss: 4.5355 - val_mse: 4.5355 - val_mae: 1.7197\n",
      "\n",
      "Epoch 00622: val_mae did not improve from 1.71644\n",
      "Epoch 623/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7828 - mse: 5.7828 - mae: 1.9186 - val_loss: 4.5360 - val_mse: 4.5360 - val_mae: 1.7201\n",
      "\n",
      "Epoch 00623: val_mae did not improve from 1.71644\n",
      "Epoch 624/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0678 - mse: 6.0678 - mae: 1.9535 - val_loss: 4.5374 - val_mse: 4.5374 - val_mae: 1.7215\n",
      "\n",
      "Epoch 00624: val_mae did not improve from 1.71644\n",
      "Epoch 625/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8626 - mse: 5.8626 - mae: 1.9316 - val_loss: 4.5419 - val_mse: 4.5419 - val_mae: 1.7247\n",
      "\n",
      "Epoch 00625: val_mae did not improve from 1.71644\n",
      "Epoch 626/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3131 - mse: 5.3131 - mae: 1.8606 - val_loss: 4.5429 - val_mse: 4.5429 - val_mae: 1.7254\n",
      "\n",
      "Epoch 00626: val_mae did not improve from 1.71644\n",
      "Epoch 627/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6077 - mse: 6.6077 - mae: 2.0311 - val_loss: 4.5399 - val_mse: 4.5399 - val_mae: 1.7233\n",
      "\n",
      "Epoch 00627: val_mae did not improve from 1.71644\n",
      "Epoch 628/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9473 - mse: 6.9473 - mae: 2.0699 - val_loss: 4.5393 - val_mse: 4.5393 - val_mae: 1.7229\n",
      "\n",
      "Epoch 00628: val_mae did not improve from 1.71644\n",
      "Epoch 629/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9364 - mse: 5.9364 - mae: 1.8853 - val_loss: 4.5377 - val_mse: 4.5377 - val_mae: 1.7217\n",
      "\n",
      "Epoch 00629: val_mae did not improve from 1.71644\n",
      "Epoch 630/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6722 - mse: 6.6722 - mae: 2.0804 - val_loss: 4.5398 - val_mse: 4.5398 - val_mae: 1.7232\n",
      "\n",
      "Epoch 00630: val_mae did not improve from 1.71644\n",
      "Epoch 631/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9773 - mse: 5.9773 - mae: 1.9724 - val_loss: 4.5431 - val_mse: 4.5431 - val_mae: 1.7258\n",
      "\n",
      "Epoch 00631: val_mae did not improve from 1.71644\n",
      "Epoch 632/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8501 - mse: 5.8501 - mae: 1.9460 - val_loss: 4.5367 - val_mse: 4.5367 - val_mae: 1.7212\n",
      "\n",
      "Epoch 00632: val_mae did not improve from 1.71644\n",
      "Epoch 633/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4900 - mse: 5.4900 - mae: 1.8588 - val_loss: 4.5353 - val_mse: 4.5353 - val_mae: 1.7198\n",
      "\n",
      "Epoch 00633: val_mae did not improve from 1.71644\n",
      "Epoch 634/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1308 - mse: 6.1308 - mae: 1.9858 - val_loss: 4.5355 - val_mse: 4.5355 - val_mae: 1.7199\n",
      "\n",
      "Epoch 00634: val_mae did not improve from 1.71644\n",
      "Epoch 635/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9364 - mse: 5.9364 - mae: 1.9430 - val_loss: 4.5336 - val_mse: 4.5336 - val_mae: 1.7176\n",
      "\n",
      "Epoch 00635: val_mae did not improve from 1.71644\n",
      "Epoch 636/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0475 - mse: 6.0475 - mae: 1.9326 - val_loss: 4.5340 - val_mse: 4.5340 - val_mae: 1.7184\n",
      "\n",
      "Epoch 00636: val_mae did not improve from 1.71644\n",
      "Epoch 637/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1310 - mse: 6.1310 - mae: 1.9032 - val_loss: 4.5382 - val_mse: 4.5382 - val_mae: 1.7222\n",
      "\n",
      "Epoch 00637: val_mae did not improve from 1.71644\n",
      "Epoch 638/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3620 - mse: 6.3620 - mae: 1.9839 - val_loss: 4.5443 - val_mse: 4.5443 - val_mae: 1.7264\n",
      "\n",
      "Epoch 00638: val_mae did not improve from 1.71644\n",
      "Epoch 639/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.8436 - mse: 6.8436 - mae: 2.0723 - val_loss: 4.5429 - val_mse: 4.5429 - val_mae: 1.7255\n",
      "\n",
      "Epoch 00639: val_mae did not improve from 1.71644\n",
      "Epoch 640/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9022 - mse: 5.9022 - mae: 1.9577 - val_loss: 4.5342 - val_mse: 4.5342 - val_mae: 1.7187\n",
      "\n",
      "Epoch 00640: val_mae did not improve from 1.71644\n",
      "Epoch 641/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8578 - mse: 5.8578 - mae: 1.9286 - val_loss: 4.5361 - val_mse: 4.5361 - val_mae: 1.7205\n",
      "\n",
      "Epoch 00641: val_mae did not improve from 1.71644\n",
      "Epoch 642/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2687 - mse: 6.2687 - mae: 1.9656 - val_loss: 4.5395 - val_mse: 4.5395 - val_mae: 1.7232\n",
      "\n",
      "Epoch 00642: val_mae did not improve from 1.71644\n",
      "Epoch 643/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1251 - mse: 6.1251 - mae: 1.9859 - val_loss: 4.5467 - val_mse: 4.5467 - val_mae: 1.7281\n",
      "\n",
      "Epoch 00643: val_mae did not improve from 1.71644\n",
      "Epoch 644/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3244 - mse: 6.3244 - mae: 1.9447 - val_loss: 4.5408 - val_mse: 4.5408 - val_mae: 1.7241\n",
      "\n",
      "Epoch 00644: val_mae did not improve from 1.71644\n",
      "Epoch 645/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1802 - mse: 6.1802 - mae: 1.9543 - val_loss: 4.5436 - val_mse: 4.5436 - val_mae: 1.7261\n",
      "\n",
      "Epoch 00645: val_mae did not improve from 1.71644\n",
      "Epoch 646/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2570 - mse: 6.2570 - mae: 1.9893 - val_loss: 4.5379 - val_mse: 4.5379 - val_mae: 1.7223\n",
      "\n",
      "Epoch 00646: val_mae did not improve from 1.71644\n",
      "Epoch 647/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8694 - mse: 5.8694 - mae: 1.9234 - val_loss: 4.5396 - val_mse: 4.5396 - val_mae: 1.7235\n",
      "\n",
      "Epoch 00647: val_mae did not improve from 1.71644\n",
      "Epoch 648/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.7668 - mse: 6.7668 - mae: 2.0359 - val_loss: 4.5426 - val_mse: 4.5426 - val_mae: 1.7256\n",
      "\n",
      "Epoch 00648: val_mae did not improve from 1.71644\n",
      "Epoch 649/1000\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 6.3588 - mse: 6.3588 - mae: 1.9968 - val_loss: 4.5501 - val_mse: 4.5501 - val_mae: 1.7303\n",
      "\n",
      "Epoch 00649: val_mae did not improve from 1.71644\n",
      "Epoch 650/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0176 - mse: 6.0176 - mae: 1.9504 - val_loss: 4.5441 - val_mse: 4.5441 - val_mae: 1.7268\n",
      "\n",
      "Epoch 00650: val_mae did not improve from 1.71644\n",
      "Epoch 651/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.8496 - mse: 5.8496 - mae: 1.9300 - val_loss: 4.5380 - val_mse: 4.5380 - val_mae: 1.7228\n",
      "\n",
      "Epoch 00651: val_mae did not improve from 1.71644\n",
      "Epoch 652/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4913 - mse: 6.4913 - mae: 2.0520 - val_loss: 4.5473 - val_mse: 4.5473 - val_mae: 1.7289\n",
      "\n",
      "Epoch 00652: val_mae did not improve from 1.71644\n",
      "Epoch 653/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9452 - mse: 5.9452 - mae: 1.8731 - val_loss: 4.5484 - val_mse: 4.5484 - val_mae: 1.7296\n",
      "\n",
      "Epoch 00653: val_mae did not improve from 1.71644\n",
      "Epoch 654/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5870 - mse: 6.5870 - mae: 1.9704 - val_loss: 4.5387 - val_mse: 4.5387 - val_mae: 1.7232\n",
      "\n",
      "Epoch 00654: val_mae did not improve from 1.71644\n",
      "Epoch 655/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2513 - mse: 6.2513 - mae: 1.9492 - val_loss: 4.5359 - val_mse: 4.5359 - val_mae: 1.7212\n",
      "\n",
      "Epoch 00655: val_mae did not improve from 1.71644\n",
      "Epoch 656/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.5890 - mse: 6.5890 - mae: 2.0051 - val_loss: 4.5334 - val_mse: 4.5334 - val_mae: 1.7187\n",
      "\n",
      "Epoch 00656: val_mae did not improve from 1.71644\n",
      "Epoch 657/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.7621 - mse: 5.7621 - mae: 1.9267 - val_loss: 4.5367 - val_mse: 4.5367 - val_mae: 1.7218\n",
      "\n",
      "Epoch 00657: val_mae did not improve from 1.71644\n",
      "Epoch 658/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1091 - mse: 6.1091 - mae: 1.9669 - val_loss: 4.5355 - val_mse: 4.5355 - val_mae: 1.7207\n",
      "\n",
      "Epoch 00658: val_mae did not improve from 1.71644\n",
      "Epoch 659/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8988 - mse: 5.8988 - mae: 1.9624 - val_loss: 4.5406 - val_mse: 4.5406 - val_mae: 1.7245\n",
      "\n",
      "Epoch 00659: val_mae did not improve from 1.71644\n",
      "Epoch 660/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7261 - mse: 5.7261 - mae: 1.8998 - val_loss: 4.5427 - val_mse: 4.5427 - val_mae: 1.7260\n",
      "\n",
      "Epoch 00660: val_mae did not improve from 1.71644\n",
      "Epoch 661/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.6142 - mse: 6.6142 - mae: 2.0007 - val_loss: 4.5443 - val_mse: 4.5443 - val_mae: 1.7269\n",
      "\n",
      "Epoch 00661: val_mae did not improve from 1.71644\n",
      "Epoch 662/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3603 - mse: 5.3603 - mae: 1.8654 - val_loss: 4.5402 - val_mse: 4.5402 - val_mae: 1.7243\n",
      "\n",
      "Epoch 00662: val_mae did not improve from 1.71644\n",
      "Epoch 663/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3192 - mse: 6.3192 - mae: 2.0070 - val_loss: 4.5383 - val_mse: 4.5383 - val_mae: 1.7229\n",
      "\n",
      "Epoch 00663: val_mae did not improve from 1.71644\n",
      "Epoch 664/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7264 - mse: 5.7264 - mae: 1.9023 - val_loss: 4.5458 - val_mse: 4.5458 - val_mae: 1.7282\n",
      "\n",
      "Epoch 00664: val_mae did not improve from 1.71644\n",
      "Epoch 665/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6298 - mse: 6.6298 - mae: 2.0453 - val_loss: 4.5509 - val_mse: 4.5509 - val_mae: 1.7311\n",
      "\n",
      "Epoch 00665: val_mae did not improve from 1.71644\n",
      "Epoch 666/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.5441 - mse: 6.5441 - mae: 2.0025 - val_loss: 4.5339 - val_mse: 4.5339 - val_mae: 1.7198\n",
      "\n",
      "Epoch 00666: val_mae did not improve from 1.71644\n",
      "Epoch 667/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9686 - mse: 5.9686 - mae: 1.9286 - val_loss: 4.5332 - val_mse: 4.5332 - val_mae: 1.7189\n",
      "\n",
      "Epoch 00667: val_mae did not improve from 1.71644\n",
      "Epoch 668/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.3305 - mse: 5.3305 - mae: 1.8242 - val_loss: 4.5364 - val_mse: 4.5364 - val_mae: 1.7219\n",
      "\n",
      "Epoch 00668: val_mae did not improve from 1.71644\n",
      "Epoch 669/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.8603 - mse: 6.8603 - mae: 2.0593 - val_loss: 4.5418 - val_mse: 4.5418 - val_mae: 1.7257\n",
      "\n",
      "Epoch 00669: val_mae did not improve from 1.71644\n",
      "Epoch 670/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5195 - mse: 6.5195 - mae: 2.0097 - val_loss: 4.5425 - val_mse: 4.5425 - val_mae: 1.7261\n",
      "\n",
      "Epoch 00670: val_mae did not improve from 1.71644\n",
      "Epoch 671/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5461 - mse: 6.5461 - mae: 1.9379 - val_loss: 4.5371 - val_mse: 4.5371 - val_mae: 1.7225\n",
      "\n",
      "Epoch 00671: val_mae did not improve from 1.71644\n",
      "Epoch 672/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8699 - mse: 5.8699 - mae: 1.8904 - val_loss: 4.5345 - val_mse: 4.5345 - val_mae: 1.7202\n",
      "\n",
      "Epoch 00672: val_mae did not improve from 1.71644\n",
      "Epoch 673/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3215 - mse: 6.3215 - mae: 2.0025 - val_loss: 4.5366 - val_mse: 4.5366 - val_mae: 1.7219\n",
      "\n",
      "Epoch 00673: val_mae did not improve from 1.71644\n",
      "Epoch 674/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5799 - mse: 5.5799 - mae: 1.8510 - val_loss: 4.5364 - val_mse: 4.5364 - val_mae: 1.7217\n",
      "\n",
      "Epoch 00674: val_mae did not improve from 1.71644\n",
      "Epoch 675/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5437 - mse: 5.5437 - mae: 1.8861 - val_loss: 4.5452 - val_mse: 4.5452 - val_mae: 1.7277\n",
      "\n",
      "Epoch 00675: val_mae did not improve from 1.71644\n",
      "Epoch 676/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9016 - mse: 5.9016 - mae: 1.9146 - val_loss: 4.5422 - val_mse: 4.5422 - val_mae: 1.7258\n",
      "\n",
      "Epoch 00676: val_mae did not improve from 1.71644\n",
      "Epoch 677/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0109 - mse: 6.0109 - mae: 1.9727 - val_loss: 4.5464 - val_mse: 4.5464 - val_mae: 1.7285\n",
      "\n",
      "Epoch 00677: val_mae did not improve from 1.71644\n",
      "Epoch 678/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7634 - mse: 5.7634 - mae: 1.8719 - val_loss: 4.5421 - val_mse: 4.5421 - val_mae: 1.7259\n",
      "\n",
      "Epoch 00678: val_mae did not improve from 1.71644\n",
      "Epoch 679/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0403 - mse: 6.0403 - mae: 1.9599 - val_loss: 4.5433 - val_mse: 4.5433 - val_mae: 1.7269\n",
      "\n",
      "Epoch 00679: val_mae did not improve from 1.71644\n",
      "Epoch 680/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9115 - mse: 5.9115 - mae: 1.9678 - val_loss: 4.5351 - val_mse: 4.5351 - val_mae: 1.7212\n",
      "\n",
      "Epoch 00680: val_mae did not improve from 1.71644\n",
      "Epoch 681/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1834 - mse: 6.1834 - mae: 1.9764 - val_loss: 4.5351 - val_mse: 4.5351 - val_mae: 1.7210\n",
      "\n",
      "Epoch 00681: val_mae did not improve from 1.71644\n",
      "Epoch 682/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5096 - mse: 6.5096 - mae: 2.0118 - val_loss: 4.5345 - val_mse: 4.5345 - val_mae: 1.7206\n",
      "\n",
      "Epoch 00682: val_mae did not improve from 1.71644\n",
      "Epoch 683/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1951 - mse: 6.1951 - mae: 1.9704 - val_loss: 4.5334 - val_mse: 4.5334 - val_mae: 1.7196\n",
      "\n",
      "Epoch 00683: val_mae did not improve from 1.71644\n",
      "Epoch 684/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3107 - mse: 6.3107 - mae: 1.9938 - val_loss: 4.5370 - val_mse: 4.5370 - val_mae: 1.7223\n",
      "\n",
      "Epoch 00684: val_mae did not improve from 1.71644\n",
      "Epoch 685/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9018 - mse: 5.9018 - mae: 1.9349 - val_loss: 4.5359 - val_mse: 4.5359 - val_mae: 1.7213\n",
      "\n",
      "Epoch 00685: val_mae did not improve from 1.71644\n",
      "Epoch 686/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9346 - mse: 5.9346 - mae: 1.9374 - val_loss: 4.5354 - val_mse: 4.5354 - val_mae: 1.7210\n",
      "\n",
      "Epoch 00686: val_mae did not improve from 1.71644\n",
      "Epoch 687/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6959 - mse: 5.6959 - mae: 1.9021 - val_loss: 4.5342 - val_mse: 4.5342 - val_mae: 1.7198\n",
      "\n",
      "Epoch 00687: val_mae did not improve from 1.71644\n",
      "Epoch 688/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6014 - mse: 5.6014 - mae: 1.8807 - val_loss: 4.5352 - val_mse: 4.5352 - val_mae: 1.7208\n",
      "\n",
      "Epoch 00688: val_mae did not improve from 1.71644\n",
      "Epoch 689/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6360 - mse: 5.6360 - mae: 1.8394 - val_loss: 4.5388 - val_mse: 4.5388 - val_mae: 1.7235\n",
      "\n",
      "Epoch 00689: val_mae did not improve from 1.71644\n",
      "Epoch 690/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9066 - mse: 5.9066 - mae: 1.9143 - val_loss: 4.5497 - val_mse: 4.5497 - val_mae: 1.7303\n",
      "\n",
      "Epoch 00690: val_mae did not improve from 1.71644\n",
      "Epoch 691/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2822 - mse: 6.2822 - mae: 1.9376 - val_loss: 4.5458 - val_mse: 4.5458 - val_mae: 1.7281\n",
      "\n",
      "Epoch 00691: val_mae did not improve from 1.71644\n",
      "Epoch 692/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8265 - mse: 5.8265 - mae: 1.9051 - val_loss: 4.5367 - val_mse: 4.5367 - val_mae: 1.7221\n",
      "\n",
      "Epoch 00692: val_mae did not improve from 1.71644\n",
      "Epoch 693/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0682 - mse: 6.0682 - mae: 1.9690 - val_loss: 4.5336 - val_mse: 4.5336 - val_mae: 1.7193\n",
      "\n",
      "Epoch 00693: val_mae did not improve from 1.71644\n",
      "Epoch 694/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7645 - mse: 5.7645 - mae: 1.9120 - val_loss: 4.5367 - val_mse: 4.5367 - val_mae: 1.7220\n",
      "\n",
      "Epoch 00694: val_mae did not improve from 1.71644\n",
      "Epoch 695/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0489 - mse: 6.0489 - mae: 1.9007 - val_loss: 4.5391 - val_mse: 4.5391 - val_mae: 1.7237\n",
      "\n",
      "Epoch 00695: val_mae did not improve from 1.71644\n",
      "Epoch 696/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9676 - mse: 5.9676 - mae: 1.8706 - val_loss: 4.5338 - val_mse: 4.5338 - val_mae: 1.7196\n",
      "\n",
      "Epoch 00696: val_mae did not improve from 1.71644\n",
      "Epoch 697/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9255 - mse: 5.9255 - mae: 1.9242 - val_loss: 4.5419 - val_mse: 4.5419 - val_mae: 1.7255\n",
      "\n",
      "Epoch 00697: val_mae did not improve from 1.71644\n",
      "Epoch 698/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.4315 - mse: 6.4315 - mae: 1.9682 - val_loss: 4.5355 - val_mse: 4.5355 - val_mae: 1.7209\n",
      "\n",
      "Epoch 00698: val_mae did not improve from 1.71644\n",
      "Epoch 699/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2301 - mse: 6.2301 - mae: 1.9815 - val_loss: 4.5359 - val_mse: 4.5359 - val_mae: 1.7211\n",
      "\n",
      "Epoch 00699: val_mae did not improve from 1.71644\n",
      "Epoch 700/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1530 - mse: 6.1530 - mae: 1.9804 - val_loss: 4.5357 - val_mse: 4.5357 - val_mae: 1.7209\n",
      "\n",
      "Epoch 00700: val_mae did not improve from 1.71644\n",
      "Epoch 701/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.9280 - mse: 6.9280 - mae: 2.0982 - val_loss: 4.5361 - val_mse: 4.5361 - val_mae: 1.7213\n",
      "\n",
      "Epoch 00701: val_mae did not improve from 1.71644\n",
      "Epoch 702/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3384 - mse: 6.3384 - mae: 1.9906 - val_loss: 4.5382 - val_mse: 4.5382 - val_mae: 1.7227\n",
      "\n",
      "Epoch 00702: val_mae did not improve from 1.71644\n",
      "Epoch 703/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8931 - mse: 5.8931 - mae: 1.9312 - val_loss: 4.5402 - val_mse: 4.5402 - val_mae: 1.7242\n",
      "\n",
      "Epoch 00703: val_mae did not improve from 1.71644\n",
      "Epoch 704/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0761 - mse: 6.0761 - mae: 1.9557 - val_loss: 4.5374 - val_mse: 4.5374 - val_mae: 1.7223\n",
      "\n",
      "Epoch 00704: val_mae did not improve from 1.71644\n",
      "Epoch 705/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2271 - mse: 6.2271 - mae: 1.9488 - val_loss: 4.5447 - val_mse: 4.5447 - val_mae: 1.7273\n",
      "\n",
      "Epoch 00705: val_mae did not improve from 1.71644\n",
      "Epoch 706/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0872 - mse: 6.0872 - mae: 1.9697 - val_loss: 4.5356 - val_mse: 4.5356 - val_mae: 1.7209\n",
      "\n",
      "Epoch 00706: val_mae did not improve from 1.71644\n",
      "Epoch 707/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3734 - mse: 5.3734 - mae: 1.8161 - val_loss: 4.5322 - val_mse: 4.5322 - val_mae: 1.7171\n",
      "\n",
      "Epoch 00707: val_mae did not improve from 1.71644\n",
      "Epoch 708/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7132 - mse: 5.7132 - mae: 1.8850 - val_loss: 4.5349 - val_mse: 4.5349 - val_mae: 1.7199\n",
      "\n",
      "Epoch 00708: val_mae did not improve from 1.71644\n",
      "Epoch 709/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8883 - mse: 5.8883 - mae: 1.9117 - val_loss: 4.5365 - val_mse: 4.5365 - val_mae: 1.7215\n",
      "\n",
      "Epoch 00709: val_mae did not improve from 1.71644\n",
      "Epoch 710/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.4417 - mse: 6.4417 - mae: 2.0036 - val_loss: 4.5414 - val_mse: 4.5414 - val_mae: 1.7251\n",
      "\n",
      "Epoch 00710: val_mae did not improve from 1.71644\n",
      "Epoch 711/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0562 - mse: 6.0562 - mae: 1.9855 - val_loss: 4.5352 - val_mse: 4.5352 - val_mae: 1.7205\n",
      "\n",
      "Epoch 00711: val_mae did not improve from 1.71644\n",
      "Epoch 712/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4389 - mse: 6.4389 - mae: 2.0033 - val_loss: 4.5370 - val_mse: 4.5370 - val_mae: 1.7222\n",
      "\n",
      "Epoch 00712: val_mae did not improve from 1.71644\n",
      "Epoch 713/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2991 - mse: 6.2991 - mae: 2.0097 - val_loss: 4.5371 - val_mse: 4.5371 - val_mae: 1.7223\n",
      "\n",
      "Epoch 00713: val_mae did not improve from 1.71644\n",
      "Epoch 714/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.2882 - mse: 5.2882 - mae: 1.8351 - val_loss: 4.5311 - val_mse: 4.5311 - val_mae: 1.7159\n",
      "\n",
      "Epoch 00714: val_mae improved from 1.71644 to 1.71586, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 715/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.2116 - mse: 6.2116 - mae: 1.9469 - val_loss: 4.5378 - val_mse: 4.5378 - val_mae: 1.7229\n",
      "\n",
      "Epoch 00715: val_mae did not improve from 1.71586\n",
      "Epoch 716/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2117 - mse: 6.2117 - mae: 1.9930 - val_loss: 4.5380 - val_mse: 4.5380 - val_mae: 1.7230\n",
      "\n",
      "Epoch 00716: val_mae did not improve from 1.71586\n",
      "Epoch 717/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9880 - mse: 5.9880 - mae: 1.9099 - val_loss: 4.5323 - val_mse: 4.5323 - val_mae: 1.7180\n",
      "\n",
      "Epoch 00717: val_mae did not improve from 1.71586\n",
      "Epoch 718/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3384 - mse: 5.3384 - mae: 1.8086 - val_loss: 4.5395 - val_mse: 4.5395 - val_mae: 1.7239\n",
      "\n",
      "Epoch 00718: val_mae did not improve from 1.71586\n",
      "Epoch 719/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6681 - mse: 5.6681 - mae: 1.8683 - val_loss: 4.5334 - val_mse: 4.5334 - val_mae: 1.7187\n",
      "\n",
      "Epoch 00719: val_mae did not improve from 1.71586\n",
      "Epoch 720/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6036 - mse: 5.6036 - mae: 1.9028 - val_loss: 4.5439 - val_mse: 4.5439 - val_mae: 1.7269\n",
      "\n",
      "Epoch 00720: val_mae did not improve from 1.71586\n",
      "Epoch 721/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3813 - mse: 6.3813 - mae: 2.0077 - val_loss: 4.5451 - val_mse: 4.5451 - val_mae: 1.7278\n",
      "\n",
      "Epoch 00721: val_mae did not improve from 1.71586\n",
      "Epoch 722/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7444 - mse: 5.7444 - mae: 1.8807 - val_loss: 4.5436 - val_mse: 4.5436 - val_mae: 1.7269\n",
      "\n",
      "Epoch 00722: val_mae did not improve from 1.71586\n",
      "Epoch 723/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0666 - mse: 6.0666 - mae: 1.9406 - val_loss: 4.5445 - val_mse: 4.5445 - val_mae: 1.7276\n",
      "\n",
      "Epoch 00723: val_mae did not improve from 1.71586\n",
      "Epoch 724/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0445 - mse: 6.0445 - mae: 1.9534 - val_loss: 4.5350 - val_mse: 4.5350 - val_mae: 1.7207\n",
      "\n",
      "Epoch 00724: val_mae did not improve from 1.71586\n",
      "Epoch 725/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9378 - mse: 5.9378 - mae: 1.9243 - val_loss: 4.5342 - val_mse: 4.5342 - val_mae: 1.7199\n",
      "\n",
      "Epoch 00725: val_mae did not improve from 1.71586\n",
      "Epoch 726/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8842 - mse: 5.8842 - mae: 1.8956 - val_loss: 4.5353 - val_mse: 4.5353 - val_mae: 1.7212\n",
      "\n",
      "Epoch 00726: val_mae did not improve from 1.71586\n",
      "Epoch 727/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.3440 - mse: 5.3440 - mae: 1.8257 - val_loss: 4.5383 - val_mse: 4.5383 - val_mae: 1.7233\n",
      "\n",
      "Epoch 00727: val_mae did not improve from 1.71586\n",
      "Epoch 728/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7707 - mse: 5.7707 - mae: 1.9266 - val_loss: 4.5406 - val_mse: 4.5406 - val_mae: 1.7249\n",
      "\n",
      "Epoch 00728: val_mae did not improve from 1.71586\n",
      "Epoch 729/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.8430 - mse: 6.8430 - mae: 2.0216 - val_loss: 4.5379 - val_mse: 4.5379 - val_mae: 1.7230\n",
      "\n",
      "Epoch 00729: val_mae did not improve from 1.71586\n",
      "Epoch 730/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5060 - mse: 5.5060 - mae: 1.9082 - val_loss: 4.5393 - val_mse: 4.5393 - val_mae: 1.7240\n",
      "\n",
      "Epoch 00730: val_mae did not improve from 1.71586\n",
      "Epoch 731/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2663 - mse: 5.2663 - mae: 1.8375 - val_loss: 4.5352 - val_mse: 4.5352 - val_mae: 1.7212\n",
      "\n",
      "Epoch 00731: val_mae did not improve from 1.71586\n",
      "Epoch 732/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.4832 - mse: 6.4832 - mae: 2.0024 - val_loss: 4.5388 - val_mse: 4.5388 - val_mae: 1.7237\n",
      "\n",
      "Epoch 00732: val_mae did not improve from 1.71586\n",
      "Epoch 733/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1826 - mse: 6.1826 - mae: 1.9318 - val_loss: 4.5340 - val_mse: 4.5340 - val_mae: 1.7200\n",
      "\n",
      "Epoch 00733: val_mae did not improve from 1.71586\n",
      "Epoch 734/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9964 - mse: 5.9964 - mae: 1.9238 - val_loss: 4.5443 - val_mse: 4.5443 - val_mae: 1.7274\n",
      "\n",
      "Epoch 00734: val_mae did not improve from 1.71586\n",
      "Epoch 735/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3049 - mse: 6.3049 - mae: 2.0378 - val_loss: 4.5525 - val_mse: 4.5525 - val_mae: 1.7319\n",
      "\n",
      "Epoch 00735: val_mae did not improve from 1.71586\n",
      "Epoch 736/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8607 - mse: 5.8607 - mae: 1.9178 - val_loss: 4.5364 - val_mse: 4.5364 - val_mae: 1.7217\n",
      "\n",
      "Epoch 00736: val_mae did not improve from 1.71586\n",
      "Epoch 737/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.7291 - mse: 6.7291 - mae: 2.0262 - val_loss: 4.5374 - val_mse: 4.5374 - val_mae: 1.7227\n",
      "\n",
      "Epoch 00737: val_mae did not improve from 1.71586\n",
      "Epoch 738/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3410 - mse: 6.3410 - mae: 2.0034 - val_loss: 4.5335 - val_mse: 4.5335 - val_mae: 1.7192\n",
      "\n",
      "Epoch 00738: val_mae did not improve from 1.71586\n",
      "Epoch 739/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2305 - mse: 6.2305 - mae: 1.9771 - val_loss: 4.5348 - val_mse: 4.5348 - val_mae: 1.7206\n",
      "\n",
      "Epoch 00739: val_mae did not improve from 1.71586\n",
      "Epoch 740/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4501 - mse: 6.4501 - mae: 1.9639 - val_loss: 4.5384 - val_mse: 4.5384 - val_mae: 1.7232\n",
      "\n",
      "Epoch 00740: val_mae did not improve from 1.71586\n",
      "Epoch 741/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0299 - mse: 6.0299 - mae: 1.9425 - val_loss: 4.5364 - val_mse: 4.5364 - val_mae: 1.7217\n",
      "\n",
      "Epoch 00741: val_mae did not improve from 1.71586\n",
      "Epoch 742/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2996 - mse: 6.2996 - mae: 1.9985 - val_loss: 4.5347 - val_mse: 4.5347 - val_mae: 1.7203\n",
      "\n",
      "Epoch 00742: val_mae did not improve from 1.71586\n",
      "Epoch 743/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0295 - mse: 6.0295 - mae: 1.9495 - val_loss: 4.5365 - val_mse: 4.5365 - val_mae: 1.7219\n",
      "\n",
      "Epoch 00743: val_mae did not improve from 1.71586\n",
      "Epoch 744/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.4413 - mse: 6.4413 - mae: 2.0083 - val_loss: 4.5351 - val_mse: 4.5351 - val_mae: 1.7206\n",
      "\n",
      "Epoch 00744: val_mae did not improve from 1.71586\n",
      "Epoch 745/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2769 - mse: 6.2769 - mae: 1.9954 - val_loss: 4.5342 - val_mse: 4.5342 - val_mae: 1.7199\n",
      "\n",
      "Epoch 00745: val_mae did not improve from 1.71586\n",
      "Epoch 746/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5122 - mse: 5.5122 - mae: 1.8526 - val_loss: 4.5334 - val_mse: 4.5334 - val_mae: 1.7195\n",
      "\n",
      "Epoch 00746: val_mae did not improve from 1.71586\n",
      "Epoch 747/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3114 - mse: 6.3114 - mae: 1.9571 - val_loss: 4.5363 - val_mse: 4.5363 - val_mae: 1.7220\n",
      "\n",
      "Epoch 00747: val_mae did not improve from 1.71586\n",
      "Epoch 748/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8794 - mse: 5.8794 - mae: 1.9138 - val_loss: 4.5402 - val_mse: 4.5402 - val_mae: 1.7247\n",
      "\n",
      "Epoch 00748: val_mae did not improve from 1.71586\n",
      "Epoch 749/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1558 - mse: 6.1558 - mae: 1.9536 - val_loss: 4.5369 - val_mse: 4.5369 - val_mae: 1.7225\n",
      "\n",
      "Epoch 00749: val_mae did not improve from 1.71586\n",
      "Epoch 750/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0678 - mse: 6.0678 - mae: 1.9786 - val_loss: 4.5399 - val_mse: 4.5399 - val_mae: 1.7247\n",
      "\n",
      "Epoch 00750: val_mae did not improve from 1.71586\n",
      "Epoch 751/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6371 - mse: 5.6371 - mae: 1.8726 - val_loss: 4.5337 - val_mse: 4.5337 - val_mae: 1.7199\n",
      "\n",
      "Epoch 00751: val_mae did not improve from 1.71586\n",
      "Epoch 752/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6599 - mse: 5.6599 - mae: 1.9084 - val_loss: 4.5374 - val_mse: 4.5374 - val_mae: 1.7229\n",
      "\n",
      "Epoch 00752: val_mae did not improve from 1.71586\n",
      "Epoch 753/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8729 - mse: 5.8729 - mae: 1.9170 - val_loss: 4.5407 - val_mse: 4.5407 - val_mae: 1.7251\n",
      "\n",
      "Epoch 00753: val_mae did not improve from 1.71586\n",
      "Epoch 754/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7888 - mse: 5.7888 - mae: 1.9287 - val_loss: 4.5382 - val_mse: 4.5382 - val_mae: 1.7235\n",
      "\n",
      "Epoch 00754: val_mae did not improve from 1.71586\n",
      "Epoch 755/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0064 - mse: 6.0064 - mae: 1.9152 - val_loss: 4.5343 - val_mse: 4.5343 - val_mae: 1.7205\n",
      "\n",
      "Epoch 00755: val_mae did not improve from 1.71586\n",
      "Epoch 756/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1995 - mse: 6.1995 - mae: 1.9985 - val_loss: 4.5395 - val_mse: 4.5395 - val_mae: 1.7243\n",
      "\n",
      "Epoch 00756: val_mae did not improve from 1.71586\n",
      "Epoch 757/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2442 - mse: 6.2442 - mae: 1.9911 - val_loss: 4.5358 - val_mse: 4.5358 - val_mae: 1.7216\n",
      "\n",
      "Epoch 00757: val_mae did not improve from 1.71586\n",
      "Epoch 758/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8408 - mse: 5.8408 - mae: 1.8919 - val_loss: 4.5316 - val_mse: 4.5316 - val_mae: 1.7168\n",
      "\n",
      "Epoch 00758: val_mae did not improve from 1.71586\n",
      "Epoch 759/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4260 - mse: 5.4260 - mae: 1.8593 - val_loss: 4.5400 - val_mse: 4.5400 - val_mae: 1.7243\n",
      "\n",
      "Epoch 00759: val_mae did not improve from 1.71586\n",
      "Epoch 760/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7396 - mse: 5.7396 - mae: 1.8706 - val_loss: 4.5369 - val_mse: 4.5369 - val_mae: 1.7223\n",
      "\n",
      "Epoch 00760: val_mae did not improve from 1.71586\n",
      "Epoch 761/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9186 - mse: 5.9186 - mae: 1.9029 - val_loss: 4.5456 - val_mse: 4.5456 - val_mae: 1.7280\n",
      "\n",
      "Epoch 00761: val_mae did not improve from 1.71586\n",
      "Epoch 762/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3771 - mse: 6.3771 - mae: 1.9611 - val_loss: 4.5402 - val_mse: 4.5402 - val_mae: 1.7246\n",
      "\n",
      "Epoch 00762: val_mae did not improve from 1.71586\n",
      "Epoch 763/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9927 - mse: 5.9927 - mae: 1.9753 - val_loss: 4.5469 - val_mse: 4.5469 - val_mae: 1.7288\n",
      "\n",
      "Epoch 00763: val_mae did not improve from 1.71586\n",
      "Epoch 764/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3888 - mse: 5.3888 - mae: 1.8692 - val_loss: 4.5415 - val_mse: 4.5415 - val_mae: 1.7254\n",
      "\n",
      "Epoch 00764: val_mae did not improve from 1.71586\n",
      "Epoch 765/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.5915 - mse: 6.5915 - mae: 2.0660 - val_loss: 4.5465 - val_mse: 4.5465 - val_mae: 1.7287\n",
      "\n",
      "Epoch 00765: val_mae did not improve from 1.71586\n",
      "Epoch 766/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9624 - mse: 5.9624 - mae: 1.9800 - val_loss: 4.5382 - val_mse: 4.5382 - val_mae: 1.7233\n",
      "\n",
      "Epoch 00766: val_mae did not improve from 1.71586\n",
      "Epoch 767/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4534 - mse: 5.4534 - mae: 1.8582 - val_loss: 4.5427 - val_mse: 4.5427 - val_mae: 1.7264\n",
      "\n",
      "Epoch 00767: val_mae did not improve from 1.71586\n",
      "Epoch 768/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.6613 - mse: 6.6613 - mae: 2.0342 - val_loss: 4.5409 - val_mse: 4.5409 - val_mae: 1.7251\n",
      "\n",
      "Epoch 00768: val_mae did not improve from 1.71586\n",
      "Epoch 769/1000\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 6.1935 - mse: 6.1935 - mae: 1.9770 - val_loss: 4.5432 - val_mse: 4.5432 - val_mae: 1.7266\n",
      "\n",
      "Epoch 00769: val_mae did not improve from 1.71586\n",
      "Epoch 770/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8296 - mse: 5.8296 - mae: 1.8882 - val_loss: 4.5413 - val_mse: 4.5413 - val_mae: 1.7252\n",
      "\n",
      "Epoch 00770: val_mae did not improve from 1.71586\n",
      "Epoch 771/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.4707 - mse: 6.4707 - mae: 2.0288 - val_loss: 4.5404 - val_mse: 4.5404 - val_mae: 1.7248\n",
      "\n",
      "Epoch 00771: val_mae did not improve from 1.71586\n",
      "Epoch 772/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9662 - mse: 5.9662 - mae: 1.9034 - val_loss: 4.5416 - val_mse: 4.5416 - val_mae: 1.7255\n",
      "\n",
      "Epoch 00772: val_mae did not improve from 1.71586\n",
      "Epoch 773/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7335 - mse: 5.7335 - mae: 1.8763 - val_loss: 4.5377 - val_mse: 4.5377 - val_mae: 1.7230\n",
      "\n",
      "Epoch 00773: val_mae did not improve from 1.71586\n",
      "Epoch 774/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.1548 - mse: 6.1548 - mae: 1.9918 - val_loss: 4.5363 - val_mse: 4.5363 - val_mae: 1.7221\n",
      "\n",
      "Epoch 00774: val_mae did not improve from 1.71586\n",
      "Epoch 775/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9659 - mse: 5.9659 - mae: 1.9578 - val_loss: 4.5368 - val_mse: 4.5368 - val_mae: 1.7226\n",
      "\n",
      "Epoch 00775: val_mae did not improve from 1.71586\n",
      "Epoch 776/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8453 - mse: 5.8453 - mae: 1.8854 - val_loss: 4.5355 - val_mse: 4.5355 - val_mae: 1.7216\n",
      "\n",
      "Epoch 00776: val_mae did not improve from 1.71586\n",
      "Epoch 777/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8866 - mse: 5.8866 - mae: 1.9082 - val_loss: 4.5434 - val_mse: 4.5434 - val_mae: 1.7270\n",
      "\n",
      "Epoch 00777: val_mae did not improve from 1.71586\n",
      "Epoch 778/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3333 - mse: 6.3333 - mae: 2.0171 - val_loss: 4.5417 - val_mse: 4.5417 - val_mae: 1.7257\n",
      "\n",
      "Epoch 00778: val_mae did not improve from 1.71586\n",
      "Epoch 779/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7551 - mse: 5.7551 - mae: 1.9147 - val_loss: 4.5379 - val_mse: 4.5379 - val_mae: 1.7231\n",
      "\n",
      "Epoch 00779: val_mae did not improve from 1.71586\n",
      "Epoch 780/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.7891 - mse: 6.7891 - mae: 2.0524 - val_loss: 4.5427 - val_mse: 4.5427 - val_mae: 1.7265\n",
      "\n",
      "Epoch 00780: val_mae did not improve from 1.71586\n",
      "Epoch 781/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0748 - mse: 6.0748 - mae: 1.9495 - val_loss: 4.5370 - val_mse: 4.5370 - val_mae: 1.7228\n",
      "\n",
      "Epoch 00781: val_mae did not improve from 1.71586\n",
      "Epoch 782/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3287 - mse: 6.3287 - mae: 2.0100 - val_loss: 4.5420 - val_mse: 4.5420 - val_mae: 1.7262\n",
      "\n",
      "Epoch 00782: val_mae did not improve from 1.71586\n",
      "Epoch 783/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4946 - mse: 5.4946 - mae: 1.8820 - val_loss: 4.5324 - val_mse: 4.5324 - val_mae: 1.7190\n",
      "\n",
      "Epoch 00783: val_mae did not improve from 1.71586\n",
      "Epoch 784/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5384 - mse: 6.5384 - mae: 1.9760 - val_loss: 4.5395 - val_mse: 4.5395 - val_mae: 1.7246\n",
      "\n",
      "Epoch 00784: val_mae did not improve from 1.71586\n",
      "Epoch 785/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.5488 - mse: 6.5488 - mae: 2.0471 - val_loss: 4.5434 - val_mse: 4.5434 - val_mae: 1.7271\n",
      "\n",
      "Epoch 00785: val_mae did not improve from 1.71586\n",
      "Epoch 786/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.5319 - mse: 6.5319 - mae: 2.0173 - val_loss: 4.5342 - val_mse: 4.5342 - val_mae: 1.7211\n",
      "\n",
      "Epoch 00786: val_mae did not improve from 1.71586\n",
      "Epoch 787/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7765 - mse: 5.7765 - mae: 1.8975 - val_loss: 4.5377 - val_mse: 4.5377 - val_mae: 1.7235\n",
      "\n",
      "Epoch 00787: val_mae did not improve from 1.71586\n",
      "Epoch 788/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3994 - mse: 5.3994 - mae: 1.8208 - val_loss: 4.5371 - val_mse: 4.5371 - val_mae: 1.7231\n",
      "\n",
      "Epoch 00788: val_mae did not improve from 1.71586\n",
      "Epoch 789/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9190 - mse: 5.9190 - mae: 1.9335 - val_loss: 4.5397 - val_mse: 4.5397 - val_mae: 1.7248\n",
      "\n",
      "Epoch 00789: val_mae did not improve from 1.71586\n",
      "Epoch 790/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2462 - mse: 6.2462 - mae: 1.9573 - val_loss: 4.5448 - val_mse: 4.5448 - val_mae: 1.7282\n",
      "\n",
      "Epoch 00790: val_mae did not improve from 1.71586\n",
      "Epoch 791/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7235 - mse: 5.7235 - mae: 1.9055 - val_loss: 4.5402 - val_mse: 4.5402 - val_mae: 1.7253\n",
      "\n",
      "Epoch 00791: val_mae did not improve from 1.71586\n",
      "Epoch 792/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8750 - mse: 5.8750 - mae: 1.9408 - val_loss: 4.5410 - val_mse: 4.5410 - val_mae: 1.7259\n",
      "\n",
      "Epoch 00792: val_mae did not improve from 1.71586\n",
      "Epoch 793/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5763 - mse: 5.5763 - mae: 1.8655 - val_loss: 4.5341 - val_mse: 4.5341 - val_mae: 1.7210\n",
      "\n",
      "Epoch 00793: val_mae did not improve from 1.71586\n",
      "Epoch 794/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7501 - mse: 5.7501 - mae: 1.9082 - val_loss: 4.5352 - val_mse: 4.5352 - val_mae: 1.7218\n",
      "\n",
      "Epoch 00794: val_mae did not improve from 1.71586\n",
      "Epoch 795/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2973 - mse: 6.2973 - mae: 1.9190 - val_loss: 4.5410 - val_mse: 4.5410 - val_mae: 1.7257\n",
      "\n",
      "Epoch 00795: val_mae did not improve from 1.71586\n",
      "Epoch 796/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3005 - mse: 6.3005 - mae: 2.0380 - val_loss: 4.5384 - val_mse: 4.5384 - val_mae: 1.7239\n",
      "\n",
      "Epoch 00796: val_mae did not improve from 1.71586\n",
      "Epoch 797/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.3736 - mse: 6.3736 - mae: 1.9861 - val_loss: 4.5406 - val_mse: 4.5406 - val_mae: 1.7254\n",
      "\n",
      "Epoch 00797: val_mae did not improve from 1.71586\n",
      "Epoch 798/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.5317 - mse: 5.5317 - mae: 1.9186 - val_loss: 4.5412 - val_mse: 4.5412 - val_mae: 1.7257\n",
      "\n",
      "Epoch 00798: val_mae did not improve from 1.71586\n",
      "Epoch 799/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.3083 - mse: 6.3083 - mae: 1.9753 - val_loss: 4.5409 - val_mse: 4.5409 - val_mae: 1.7255\n",
      "\n",
      "Epoch 00799: val_mae did not improve from 1.71586\n",
      "Epoch 800/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5587 - mse: 5.5587 - mae: 1.8615 - val_loss: 4.5415 - val_mse: 4.5415 - val_mae: 1.7260\n",
      "\n",
      "Epoch 00800: val_mae did not improve from 1.71586\n",
      "Epoch 801/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6778 - mse: 5.6778 - mae: 1.8718 - val_loss: 4.5404 - val_mse: 4.5404 - val_mae: 1.7254\n",
      "\n",
      "Epoch 00801: val_mae did not improve from 1.71586\n",
      "Epoch 802/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7885 - mse: 5.7885 - mae: 1.9307 - val_loss: 4.5350 - val_mse: 4.5350 - val_mae: 1.7216\n",
      "\n",
      "Epoch 00802: val_mae did not improve from 1.71586\n",
      "Epoch 803/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.0772 - mse: 6.0772 - mae: 1.9768 - val_loss: 4.5397 - val_mse: 4.5397 - val_mae: 1.7249\n",
      "\n",
      "Epoch 00803: val_mae did not improve from 1.71586\n",
      "Epoch 804/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.9067 - mse: 5.9067 - mae: 1.9053 - val_loss: 4.5360 - val_mse: 4.5360 - val_mae: 1.7223\n",
      "\n",
      "Epoch 00804: val_mae did not improve from 1.71586\n",
      "Epoch 805/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.2366 - mse: 6.2366 - mae: 1.9392 - val_loss: 4.5352 - val_mse: 4.5352 - val_mae: 1.7220\n",
      "\n",
      "Epoch 00805: val_mae did not improve from 1.71586\n",
      "Epoch 806/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.0151 - mse: 6.0151 - mae: 1.9072 - val_loss: 4.5359 - val_mse: 4.5359 - val_mae: 1.7226\n",
      "\n",
      "Epoch 00806: val_mae did not improve from 1.71586\n",
      "Epoch 807/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.2868 - mse: 6.2868 - mae: 1.9627 - val_loss: 4.5385 - val_mse: 4.5385 - val_mae: 1.7241\n",
      "\n",
      "Epoch 00807: val_mae did not improve from 1.71586\n",
      "Epoch 808/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.8942 - mse: 6.8942 - mae: 2.0771 - val_loss: 4.5430 - val_mse: 4.5430 - val_mae: 1.7271\n",
      "\n",
      "Epoch 00808: val_mae did not improve from 1.71586\n",
      "Epoch 809/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.7095 - mse: 5.7095 - mae: 1.8838 - val_loss: 4.5377 - val_mse: 4.5377 - val_mae: 1.7236\n",
      "\n",
      "Epoch 00809: val_mae did not improve from 1.71586\n",
      "Epoch 810/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.8162 - mse: 6.8162 - mae: 2.0097 - val_loss: 4.5323 - val_mse: 4.5323 - val_mae: 1.7193\n",
      "\n",
      "Epoch 00810: val_mae did not improve from 1.71586\n",
      "Epoch 811/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1386 - mse: 6.1386 - mae: 1.9928 - val_loss: 4.5302 - val_mse: 4.5302 - val_mae: 1.7162\n",
      "\n",
      "Epoch 00811: val_mae did not improve from 1.71586\n",
      "Epoch 812/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 6.1086 - mse: 6.1086 - mae: 1.9542 - val_loss: 4.5329 - val_mse: 4.5329 - val_mae: 1.7195\n",
      "\n",
      "Epoch 00812: val_mae did not improve from 1.71586\n",
      "Epoch 813/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.9208 - mse: 5.9208 - mae: 1.9262 - val_loss: 4.5355 - val_mse: 4.5355 - val_mae: 1.7217\n",
      "\n",
      "Epoch 00813: val_mae did not improve from 1.71586\n",
      "Epoch 814/1000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.8101 - mse: 5.8101 - mae: 1.9029 - val_loss: 4.5384 - val_mse: 4.5384 - val_mae: 1.7238\n",
      "\n",
      "Epoch 00814: val_mae did not improve from 1.71586\n",
      "Epoch 00814: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def show_info(model, X, y, log, weights = None):\n",
    "    '''\n",
    "    Show metrics about the evaluation model and plots about loss, rmse and rmspe\n",
    "    '''\n",
    "    if (log != None):\n",
    "        # summarize history for loss\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['loss'])\n",
    "        plt.plot(log.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MAE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mae'])\n",
    "        plt.plot(log.history['val_mae'])\n",
    "        plt.title('Model MAE')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MSE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mse'])\n",
    "        plt.plot(log.history['val_mse'])\n",
    "        plt.title('Model MSE')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    if (weights != None):\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    predictions = model.predict(X, verbose=1)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    mae= mean_absolute_error(y, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "show_info(model, x_test, y_test, log, weights='/home/m-marouni/Documents/CE-901/Heathrow/best_weights')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-14T14:29:23.649096</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m6568eeaf3b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.465453\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(153.921703 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.81869\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(241.27494 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"338.171927\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(328.628177 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"425.525165\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(415.981415 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"512.878402\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(503.334652 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"600.231639\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(590.687889 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"687.584876\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(678.041126 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"774.938113\" xlink:href=\"#m6568eeaf3b\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(765.394363 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mee9454ebda\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mee9454ebda\" y=\"534.97186\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 538.771079)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mee9454ebda\" y=\"468.58369\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 472.382909)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mee9454ebda\" y=\"402.195519\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 405.994738)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mee9454ebda\" y=\"335.807349\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 339.606568)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mee9454ebda\" y=\"269.419179\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 273.218397)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mee9454ebda\" y=\"203.031008\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 206.830227)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mee9454ebda\" y=\"136.642838\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 140.442057)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mee9454ebda\" y=\"70.254668\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 74.053886)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 303.775937)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p2afc437be3)\" d=\"M 76.112216 47.027216 \nL 77.859281 138.792279 \nL 79.606345 224.755163 \nL 81.35341 302.207354 \nL 83.100475 369.931194 \nL 84.84754 423.030005 \nL 86.594604 458.292731 \nL 88.341669 483.232436 \nL 90.088734 495.290491 \nL 90.962266 499.71437 \nL 91.835799 502.058307 \nL 92.709331 504.985449 \nL 93.582863 506.606982 \nL 94.456396 505.673171 \nL 95.329928 505.904547 \nL 96.20346 506.549114 \nL 97.076993 508.581047 \nL 98.824058 508.595039 \nL 99.69759 508.343301 \nL 100.571122 505.978585 \nL 101.444655 507.694618 \nL 102.318187 507.762907 \nL 103.191719 509.293631 \nL 104.065252 508.288194 \nL 104.938784 511.315516 \nL 105.812317 508.352621 \nL 106.685849 509.710469 \nL 107.559381 509.745304 \nL 108.432914 505.901964 \nL 109.306446 508.636857 \nL 110.179978 506.322822 \nL 111.053511 505.199953 \nL 111.927043 509.950247 \nL 112.800576 507.622967 \nL 113.674108 508.283781 \nL 114.54764 507.508903 \nL 115.421173 508.847258 \nL 116.294705 507.445837 \nL 117.168237 507.427103 \nL 118.04177 508.540761 \nL 118.915302 511.689997 \nL 119.788834 507.373287 \nL 120.662367 510.562835 \nL 121.535899 508.603865 \nL 122.409432 510.814731 \nL 123.282964 508.455352 \nL 124.156496 507.942095 \nL 125.030029 506.300821 \nL 125.903561 509.099793 \nL 126.777093 510.523164 \nL 127.650626 510.504277 \nL 128.524158 508.626423 \nL 130.271223 509.650746 \nL 131.144755 507.985192 \nL 132.018288 510.282322 \nL 132.89182 509.361686 \nL 133.765352 510.614606 \nL 134.638885 509.110277 \nL 135.512417 509.316841 \nL 136.38595 506.972955 \nL 137.259482 506.131586 \nL 138.133014 509.675267 \nL 139.006547 510.064482 \nL 139.880079 511.362867 \nL 140.753611 511.536293 \nL 141.627144 510.33063 \nL 142.500676 511.836338 \nL 143.374209 507.37192 \nL 145.121273 510.917849 \nL 145.994806 507.32888 \nL 146.868338 510.234996 \nL 147.74187 509.301855 \nL 148.615403 509.579374 \nL 149.488935 511.828722 \nL 150.362468 511.128382 \nL 151.236 507.569474 \nL 152.109532 507.119207 \nL 152.983065 513.12767 \nL 153.856597 511.109851 \nL 154.730129 507.521958 \nL 155.603662 507.108805 \nL 156.477194 508.397902 \nL 157.350726 510.182434 \nL 158.224259 510.183883 \nL 159.097791 509.398293 \nL 159.971324 510.739845 \nL 160.844856 510.658672 \nL 161.718388 509.392443 \nL 162.591921 513.471452 \nL 163.465453 508.070183 \nL 164.338985 511.183312 \nL 165.212518 509.108283 \nL 166.08605 511.351104 \nL 166.959583 513.060825 \nL 167.833115 508.314747 \nL 168.706647 510.222935 \nL 169.58018 511.46482 \nL 170.453712 509.129809 \nL 171.327244 510.249172 \nL 172.200777 510.09039 \nL 173.074309 510.359627 \nL 173.947842 513.609252 \nL 174.821374 512.48637 \nL 175.694906 509.863179 \nL 176.568439 506.878549 \nL 177.441971 510.82212 \nL 178.315503 510.575966 \nL 179.189036 512.254652 \nL 180.062568 510.918412 \nL 180.936101 509.983436 \nL 181.809633 511.38214 \nL 182.683165 511.616802 \nL 183.556698 509.630113 \nL 184.43023 511.040023 \nL 185.303762 515.030154 \nL 186.177295 510.848578 \nL 187.050827 513.013663 \nL 187.924359 511.965021 \nL 188.797892 514.255339 \nL 189.671424 511.671953 \nL 190.544957 512.257273 \nL 191.418489 510.857448 \nL 192.292021 511.703654 \nL 193.165554 510.501194 \nL 194.039086 510.900469 \nL 194.912618 510.60414 \nL 195.786151 511.880334 \nL 196.659683 511.163109 \nL 197.533216 513.472319 \nL 198.406748 512.726167 \nL 199.28028 511.559219 \nL 200.153813 510.604577 \nL 201.027345 513.110532 \nL 201.900877 512.024149 \nL 202.77441 512.748522 \nL 203.647942 511.63986 \nL 204.521475 513.355299 \nL 205.395007 511.903589 \nL 206.268539 510.763524 \nL 207.142072 515.196552 \nL 208.015604 512.437309 \nL 208.889136 511.795939 \nL 209.762669 515.201921 \nL 210.636201 512.515032 \nL 211.509734 513.357622 \nL 213.256798 512.909451 \nL 214.130331 514.059526 \nL 215.003863 512.611425 \nL 215.877395 513.235099 \nL 216.750928 514.187082 \nL 217.62446 513.493295 \nL 218.497992 514.595322 \nL 219.371525 516.264067 \nL 220.245057 512.83849 \nL 221.11859 513.696332 \nL 221.992122 513.663897 \nL 222.865654 513.862199 \nL 223.739187 515.334143 \nL 224.612719 514.126871 \nL 225.486251 516.73274 \nL 226.359784 513.948437 \nL 227.233316 514.919179 \nL 228.106849 513.6855 \nL 228.980381 514.420003 \nL 229.853913 514.573087 \nL 230.727446 513.276335 \nL 231.600978 513.355172 \nL 232.47451 512.002015 \nL 233.348043 515.515661 \nL 234.221575 514.59819 \nL 235.095108 516.669686 \nL 235.96864 512.452409 \nL 236.842172 515.817884 \nL 237.715705 514.242525 \nL 238.589237 513.374185 \nL 239.462769 515.129371 \nL 240.336302 512.115896 \nL 241.209834 515.69469 \nL 242.083367 515.231 \nL 242.956899 513.031999 \nL 243.830431 515.317321 \nL 244.703964 512.625006 \nL 245.577496 514.193134 \nL 246.451028 515.018396 \nL 247.324561 513.487306 \nL 249.071626 518.872753 \nL 249.945158 516.75752 \nL 250.81869 515.642514 \nL 251.692223 515.621399 \nL 252.565755 516.449257 \nL 253.439287 516.077225 \nL 254.31282 516.077896 \nL 255.186352 516.730448 \nL 256.059884 516.749802 \nL 256.933417 516.045208 \nL 257.806949 514.471058 \nL 258.680482 514.291478 \nL 259.554014 517.480614 \nL 260.427546 514.026211 \nL 261.301079 516.041422 \nL 262.174611 516.022738 \nL 263.048143 516.893459 \nL 263.921676 515.404629 \nL 264.795208 517.287618 \nL 265.668741 515.135601 \nL 266.542273 518.048688 \nL 267.415805 515.690594 \nL 268.289338 518.831169 \nL 269.16287 517.377573 \nL 270.036402 517.591348 \nL 270.909935 516.310153 \nL 271.783467 519.152278 \nL 272.657 517.660733 \nL 274.404064 520.263074 \nL 275.277597 519.43012 \nL 276.151129 516.412801 \nL 277.024661 519.965739 \nL 277.898194 515.464745 \nL 278.771726 517.658586 \nL 279.645259 516.500319 \nL 280.518791 515.803797 \nL 281.392323 515.97129 \nL 282.265856 515.704738 \nL 283.139388 518.88092 \nL 284.886453 515.266145 \nL 285.759985 517.636186 \nL 286.633517 519.268122 \nL 287.50705 518.680529 \nL 288.380582 519.46769 \nL 289.254115 516.573825 \nL 291.001179 519.914259 \nL 291.874712 516.733759 \nL 292.748244 517.630874 \nL 293.621776 516.387837 \nL 294.495309 519.51349 \nL 295.368841 517.597401 \nL 296.242374 517.222216 \nL 297.115906 519.502429 \nL 297.989438 517.153085 \nL 298.862971 518.751927 \nL 299.736503 515.990867 \nL 300.610035 519.745461 \nL 301.483568 515.219104 \nL 302.3571 518.390234 \nL 303.230633 518.290263 \nL 304.104165 518.979011 \nL 304.977697 521.725097 \nL 305.85123 518.350322 \nL 306.724762 520.277161 \nL 307.598294 520.201281 \nL 309.345359 515.523879 \nL 310.218892 519.22764 \nL 311.092424 520.307678 \nL 312.839489 522.662632 \nL 313.713021 517.903505 \nL 314.586553 516.149725 \nL 315.460086 521.824865 \nL 316.333618 517.078572 \nL 317.20715 519.124402 \nL 318.080683 519.721086 \nL 318.954215 521.726978 \nL 319.827748 521.789049 \nL 320.70128 520.084589 \nL 321.574812 519.639001 \nL 322.448345 520.648858 \nL 323.321877 520.136848 \nL 324.195409 518.277436 \nL 325.068942 520.995007 \nL 325.942474 520.267918 \nL 326.816007 518.288345 \nL 327.689539 520.76843 \nL 328.563071 519.536719 \nL 329.436604 518.534391 \nL 330.310136 520.039454 \nL 331.183668 517.650039 \nL 332.057201 522.204672 \nL 332.930733 520.71783 \nL 333.804266 518.966956 \nL 334.677798 518.618717 \nL 335.55133 521.219938 \nL 336.424863 517.960119 \nL 337.298395 521.330305 \nL 338.171927 519.265817 \nL 339.04546 520.69203 \nL 339.918992 520.101108 \nL 340.792525 520.264068 \nL 341.666057 518.71996 \nL 342.539589 519.103629 \nL 343.413122 520.871047 \nL 344.286654 517.394813 \nL 345.160186 519.377285 \nL 346.033719 520.208214 \nL 346.907251 519.000714 \nL 347.780784 518.694395 \nL 349.527848 519.44748 \nL 350.401381 517.862713 \nL 351.274913 519.692393 \nL 352.148445 518.196111 \nL 353.021978 518.67958 \nL 353.89551 520.670403 \nL 354.769042 519.886155 \nL 356.516107 519.979598 \nL 357.38964 520.789868 \nL 358.263172 517.79153 \nL 359.136704 520.68977 \nL 360.010237 520.766543 \nL 360.883769 518.749724 \nL 361.757301 519.36701 \nL 362.630834 520.838555 \nL 363.504366 519.925884 \nL 364.377899 519.85037 \nL 365.251431 522.130754 \nL 366.124963 520.899367 \nL 366.998496 520.02436 \nL 367.872028 518.900978 \nL 368.74556 520.13203 \nL 369.619093 518.344662 \nL 371.366158 523.30109 \nL 372.23969 518.015106 \nL 373.113222 519.838537 \nL 373.986755 520.468891 \nL 374.860287 517.561123 \nL 375.733819 522.019413 \nL 376.607352 520.477407 \nL 377.480884 519.164415 \nL 378.354417 520.435177 \nL 379.227949 520.65315 \nL 380.101481 517.573849 \nL 380.975014 518.894039 \nL 381.848546 523.205646 \nL 382.722078 521.289899 \nL 383.595611 520.670213 \nL 384.469143 518.661649 \nL 385.342675 522.125018 \nL 386.216208 520.106926 \nL 387.08974 519.982523 \nL 387.963273 518.923536 \nL 388.836805 520.400431 \nL 389.710337 520.361715 \nL 390.58387 520.721876 \nL 391.457402 520.957785 \nL 392.330934 521.56648 \nL 393.204467 523.71927 \nL 394.077999 518.099996 \nL 394.951532 517.491162 \nL 395.825064 519.850744 \nL 396.698596 521.852729 \nL 397.572129 521.608501 \nL 398.445661 522.318483 \nL 399.319193 523.246501 \nL 400.192726 521.560984 \nL 401.066258 521.169611 \nL 401.939791 518.705145 \nL 402.813323 519.340159 \nL 403.686855 519.438749 \nL 404.560388 520.337802 \nL 405.43392 518.809997 \nL 406.307452 518.115444 \nL 407.180985 519.763664 \nL 408.054517 518.447994 \nL 408.92805 519.953013 \nL 410.675114 521.227858 \nL 411.548647 519.972608 \nL 412.422179 517.051886 \nL 413.295711 521.308835 \nL 414.169244 520.341398 \nL 415.042776 519.679439 \nL 415.916308 519.916602 \nL 416.789841 519.247304 \nL 417.663373 520.835548 \nL 418.536906 519.709031 \nL 419.410438 521.154973 \nL 420.28397 520.619924 \nL 421.157503 520.370237 \nL 422.031035 518.423834 \nL 422.904567 521.709896 \nL 423.7781 522.202614 \nL 424.651632 518.219442 \nL 425.525165 521.008087 \nL 426.398697 520.211797 \nL 427.272229 519.713659 \nL 428.145762 518.6929 \nL 429.019294 521.393453 \nL 429.892826 523.146809 \nL 430.766359 520.426389 \nL 431.639891 518.861515 \nL 433.386956 523.418845 \nL 434.260488 519.805881 \nL 435.134021 519.574144 \nL 436.007553 517.736011 \nL 436.881085 521.415859 \nL 437.754618 519.680927 \nL 438.62815 519.902673 \nL 439.501683 522.534651 \nL 440.375215 518.984981 \nL 441.248747 520.148624 \nL 442.12228 519.50921 \nL 442.995812 520.834737 \nL 443.869344 520.454304 \nL 444.742877 518.672026 \nL 445.616409 521.935587 \nL 446.489942 520.508702 \nL 447.363474 520.422907 \nL 448.237006 518.930614 \nL 449.110539 520.02531 \nL 449.984071 521.53364 \nL 451.731136 518.045174 \nL 452.604668 520.803505 \nL 453.4782 521.914561 \nL 454.351733 520.141267 \nL 455.225265 520.19854 \nL 456.098798 523.060774 \nL 457.845862 519.582146 \nL 458.719395 518.663663 \nL 459.592927 522.220981 \nL 460.466459 520.563417 \nL 461.339992 518.214605 \nL 462.213524 520.743757 \nL 463.960589 519.578867 \nL 464.834121 520.11075 \nL 465.707654 519.703396 \nL 466.581186 518.695389 \nL 467.454718 517.960689 \nL 468.328251 518.84299 \nL 469.201783 521.547303 \nL 470.075316 521.29535 \nL 471.82238 521.139512 \nL 474.442977 519.688676 \nL 475.31651 520.014046 \nL 476.190042 521.8383 \nL 478.810639 520.563202 \nL 479.684172 520.676696 \nL 480.557704 521.440127 \nL 481.431236 523.086301 \nL 482.304769 522.831012 \nL 483.178301 519.8843 \nL 484.051833 522.71941 \nL 484.925366 519.931322 \nL 485.798898 520.632795 \nL 486.672431 522.843592 \nL 487.545963 520.767993 \nL 488.419495 520.395297 \nL 490.16656 520.168662 \nL 491.040092 522.987046 \nL 491.913625 523.055113 \nL 492.787157 521.105291 \nL 493.66069 521.420715 \nL 494.534222 519.124579 \nL 495.407754 524.716673 \nL 496.281287 515.799074 \nL 497.154819 522.158973 \nL 498.028351 518.334836 \nL 498.901884 519.138274 \nL 499.775416 521.618485 \nL 500.648949 522.336312 \nL 501.522481 520.440166 \nL 502.396013 520.0367 \nL 503.269546 520.25404 \nL 504.143078 518.709298 \nL 505.01661 521.498887 \nL 505.890143 521.67006 \nL 506.763675 521.718171 \nL 507.637208 521.156448 \nL 508.51074 525.357264 \nL 509.384272 521.407407 \nL 510.257805 521.866158 \nL 511.131337 520.063171 \nL 512.004869 520.974424 \nL 512.878402 523.110303 \nL 513.751934 523.478599 \nL 514.625466 520.423642 \nL 516.372531 519.586192 \nL 517.246064 519.530724 \nL 518.119596 522.987261 \nL 518.993128 519.818214 \nL 519.866661 521.296667 \nL 520.740193 522.233384 \nL 521.613725 520.506398 \nL 522.487258 520.77818 \nL 523.36079 518.656217 \nL 524.234323 522.234916 \nL 525.107855 519.551573 \nL 525.981387 522.186261 \nL 526.85492 519.780182 \nL 527.728452 519.885338 \nL 528.601984 520.697779 \nL 529.475517 523.265185 \nL 530.349049 520.224333 \nL 531.222582 522.992858 \nL 532.096114 519.959281 \nL 532.969646 521.772329 \nL 533.843179 520.393144 \nL 534.716711 518.528997 \nL 535.590243 519.96893 \nL 536.463776 522.261805 \nL 537.337308 524.03565 \nL 538.210841 519.217263 \nL 539.084373 520.505454 \nL 539.957905 523.323015 \nL 540.831438 521.354851 \nL 541.70497 518.539829 \nL 542.578502 519.591548 \nL 543.452035 521.792753 \nL 544.325567 519.680667 \nL 545.1991 522.893489 \nL 546.072632 521.404697 \nL 546.946164 520.469898 \nL 547.819697 520.559118 \nL 548.693229 519.140812 \nL 549.566761 519.24538 \nL 550.440294 520.953797 \nL 551.313826 521.976708 \nL 552.187358 521.16899 \nL 553.060891 525.341328 \nL 554.807956 517.836134 \nL 555.681488 519.084027 \nL 556.55502 521.379062 \nL 557.428553 519.473153 \nL 558.302085 521.105646 \nL 559.175617 519.923053 \nL 560.04915 521.406571 \nL 560.922682 523.464405 \nL 561.796215 520.529488 \nL 562.669747 520.60426 \nL 563.543279 520.315364 \nL 564.416812 521.319586 \nL 565.290344 521.046195 \nL 566.163876 521.531911 \nL 567.037409 519.702801 \nL 567.910941 523.035341 \nL 568.784474 519.734375 \nL 569.658006 522.213422 \nL 570.531538 518.76811 \nL 571.405071 522.890064 \nL 574.025668 520.946535 \nL 574.8992 519.747177 \nL 575.772733 520.369838 \nL 576.646265 522.443184 \nL 577.519797 521.7886 \nL 578.39333 519.565736 \nL 579.266862 521.13287 \nL 580.140394 520.656981 \nL 581.013927 520.887173 \nL 581.887459 523.007705 \nL 582.760991 519.54369 \nL 583.634524 518.183999 \nL 584.508056 521.268809 \nL 585.381589 521.684723 \nL 586.255121 520.909383 \nL 587.128653 520.488968 \nL 588.002186 519.275928 \nL 588.875718 520.877492 \nL 589.74925 520.432125 \nL 590.622783 520.400811 \nL 591.496315 517.454954 \nL 592.369848 521.194252 \nL 593.24338 520.137968 \nL 594.116912 522.037292 \nL 595.863977 521.004725 \nL 596.737509 521.910141 \nL 597.611042 521.390984 \nL 598.484574 524.582513 \nL 599.358107 519.579215 \nL 600.231639 519.638425 \nL 601.105171 520.277396 \nL 601.978704 520.322386 \nL 602.852236 520.74615 \nL 603.725768 517.801426 \nL 604.599301 523.203791 \nL 605.472833 522.567371 \nL 606.346366 521.091027 \nL 607.219898 517.829923 \nL 608.09343 524.469557 \nL 608.966963 519.683807 \nL 609.840495 520.205441 \nL 610.714027 521.22059 \nL 611.58756 521.275647 \nL 612.461092 524.138621 \nL 613.334624 522.359706 \nL 614.208157 522.036178 \nL 615.081689 520.142369 \nL 615.955222 521.875465 \nL 616.828754 520.933967 \nL 617.702286 520.773818 \nL 618.575819 520.98632 \nL 619.449351 521.044062 \nL 620.322883 518.876824 \nL 621.196416 523.620546 \nL 622.069948 524.363584 \nL 622.943481 518.530377 \nL 623.817013 520.559416 \nL 624.690545 520.871136 \nL 625.564078 521.496279 \nL 626.43761 521.624455 \nL 628.184675 520.476349 \nL 629.058207 520.898088 \nL 629.93174 522.408735 \nL 630.805272 523.294917 \nL 631.678804 519.978978 \nL 632.552337 521.031906 \nL 633.425869 520.225707 \nL 634.299401 520.768056 \nL 635.172934 522.741279 \nL 636.046466 522.105195 \nL 636.919999 524.506867 \nL 637.793531 520.768987 \nL 638.667063 523.18895 \nL 640.414128 521.171991 \nL 641.28766 522.379909 \nL 642.161193 522.480266 \nL 643.908258 521.088912 \nL 644.78179 519.697964 \nL 645.655322 522.022623 \nL 646.528855 517.78297 \nL 647.402387 518.348783 \nL 648.275919 517.661587 \nL 649.149452 520.716665 \nL 650.022984 520.461876 \nL 650.896516 521.491119 \nL 651.770049 521.640727 \nL 652.643581 517.68816 \nL 653.517114 523.975642 \nL 654.390646 519.598798 \nL 655.264178 519.825109 \nL 656.137711 520.980388 \nL 657.884775 521.673896 \nL 658.758308 521.089989 \nL 659.63184 520.920589 \nL 660.505373 521.311665 \nL 661.378905 519.24745 \nL 662.252437 518.786825 \nL 663.999502 519.942807 \nL 664.873034 519.874201 \nL 665.746567 521.33292 \nL 666.620099 522.082289 \nL 667.493632 522.319135 \nL 668.367164 522.199227 \nL 669.240696 521.061815 \nL 670.114229 520.713322 \nL 670.987761 520.180192 \nL 671.861293 519.328554 \nL 672.734826 520.627629 \nL 673.608358 522.277298 \nL 674.481891 518.173319 \nL 675.355423 520.702743 \nL 676.228955 521.227713 \nL 677.97602 521.162304 \nL 678.849552 520.211164 \nL 679.723085 520.228461 \nL 680.596617 520.562252 \nL 681.470149 520.444965 \nL 682.343682 520.127572 \nL 683.217214 519.523759 \nL 684.090747 519.86618 \nL 684.964279 521.335066 \nL 685.837811 519.492559 \nL 686.711344 521.414365 \nL 687.584876 520.128092 \nL 688.458408 520.619221 \nL 689.331941 521.411617 \nL 690.205473 520.594339 \nL 691.079006 521.477247 \nL 691.952538 520.035959 \nL 692.82607 521.18523 \nL 693.699603 519.715907 \nL 694.573135 520.871332 \nL 695.446667 521.610926 \nL 696.3202 520.533318 \nL 697.193732 519.976578 \nL 698.067265 517.434909 \nL 698.940797 522.843738 \nL 699.814329 521.320175 \nL 700.687862 521.004365 \nL 701.561394 520.377468 \nL 702.434926 521.499894 \nL 703.308459 519.144016 \nL 704.181991 520.03815 \nL 705.055524 521.499451 \nL 705.929056 522.629475 \nL 706.802588 520.850205 \nL 707.676121 519.35341 \nL 708.549653 520.93386 \nL 709.423185 522.824851 \nL 710.296718 519.297556 \nL 711.17025 521.101638 \nL 712.043782 519.657286 \nL 712.917315 523.426525 \nL 713.790847 521.960836 \nL 714.66438 518.760962 \nL 715.537912 520.100272 \nL 716.411444 520.951518 \nL 717.284977 519.584381 \nL 718.158509 521.17479 \nL 719.032041 521.827753 \nL 719.905574 520.735672 \nL 720.779106 518.281887 \nL 721.652639 521.733341 \nL 723.399703 519.618146 \nL 724.273236 521.787182 \nL 725.146768 522.293601 \nL 726.893833 520.259408 \nL 727.767365 520.553553 \nL 728.640898 521.907242 \nL 729.51443 519.006273 \nL 730.387962 521.942285 \nL 731.261495 522.19106 \nL 732.135027 519.893088 \nL 733.008559 521.77626 \nL 733.882092 520.262251 \nL 734.755624 521.570298 \nL 735.629157 518.976934 \nL 736.502689 520.399235 \nL 737.376221 521.348944 \nL 738.249754 521.772379 \nL 739.123286 522.569948 \nL 739.996818 520.348648 \nL 740.870351 521.207459 \nL 741.743883 517.545219 \nL 742.617416 522.684158 \nL 743.490948 518.452243 \nL 744.36448 520.644445 \nL 745.238013 521.031665 \nL 746.111545 518.387474 \nL 746.985077 520.208119 \nL 747.85861 521.181203 \nL 748.732142 520.165376 \nL 749.605674 521.611476 \nL 750.479207 519.980067 \nL 751.352739 520.084482 \nL 752.226272 522.175694 \nL 753.099804 520.165775 \nL 753.973336 521.062625 \nL 754.846869 521.791487 \nL 755.720401 522.371438 \nL 756.593933 520.173822 \nL 757.467466 522.47767 \nL 758.340998 520.816085 \nL 760.088063 521.280516 \nL 760.961595 519.673418 \nL 762.70866 523.882123 \nL 763.582192 521.642974 \nL 764.455725 522.431958 \nL 765.329257 522.25001 \nL 766.20279 519.928036 \nL 767.076322 522.256937 \nL 767.949854 522.027333 \nL 768.823387 524.108776 \nL 769.696919 521.195968 \nL 770.570451 521.415315 \nL 771.443984 519.861051 \nL 772.317516 521.661519 \nL 773.191049 522.14638 \nL 774.064581 523.340692 \nL 774.938113 522.327967 \nL 775.811646 520.955532 \nL 776.685178 523.076481 \nL 777.55871 521.909401 \nL 778.432243 518.952995 \nL 779.305775 521.203869 \nL 780.179307 521.178513 \nL 781.05284 519.487538 \nL 781.926372 522.138004 \nL 783.673437 520.333788 \nL 785.420502 522.497683 \nL 786.294034 520.234444 \nL 786.294034 520.234444 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p2afc437be3)\" d=\"M 76.112216 118.998046 \nL 77.859281 205.83295 \nL 79.606345 285.445799 \nL 81.35341 356.345835 \nL 83.100475 415.88946 \nL 83.974007 440.277952 \nL 84.84754 460.596376 \nL 85.721072 477.305824 \nL 86.594604 490.988418 \nL 87.468137 500.93911 \nL 88.341669 508.447115 \nL 89.215201 514.189975 \nL 90.088734 518.173743 \nL 90.962266 520.519934 \nL 91.835799 522.207933 \nL 92.709331 523.130424 \nL 93.582863 523.648005 \nL 94.456396 523.902174 \nL 96.20346 524.008641 \nL 102.318187 523.764792 \nL 104.065252 523.890018 \nL 110.179978 523.963435 \nL 111.053511 524.143015 \nL 111.927043 524.138735 \nL 112.800576 524.270578 \nL 116.294705 524.302215 \nL 117.168237 524.400299 \nL 118.04177 524.325805 \nL 120.662367 524.515775 \nL 122.409432 524.470697 \nL 126.777093 524.74812 \nL 127.650626 524.693773 \nL 128.524158 524.856087 \nL 131.144755 524.744321 \nL 132.89182 524.981282 \nL 135.512417 525.098866 \nL 139.006547 525.348996 \nL 140.753611 525.405129 \nL 142.500676 525.463022 \nL 144.247741 525.609451 \nL 148.615403 525.782896 \nL 149.488935 526.013937 \nL 150.362468 525.975183 \nL 151.236 526.093996 \nL 152.109532 525.995697 \nL 155.603662 526.22266 \nL 157.350726 526.392382 \nL 160.844856 526.420848 \nL 161.718388 526.590754 \nL 162.591921 526.470681 \nL 163.465453 526.643936 \nL 164.338985 526.614604 \nL 165.212518 526.769859 \nL 167.833115 526.821009 \nL 169.58018 527.106714 \nL 170.453712 527.112108 \nL 171.327244 527.294189 \nL 172.200777 527.175985 \nL 173.074309 527.367917 \nL 175.694906 527.396053 \nL 176.568439 527.573386 \nL 178.315503 527.69582 \nL 180.062568 527.763754 \nL 180.936101 527.674439 \nL 181.809633 527.913394 \nL 183.556698 528.026382 \nL 185.303762 528.25073 \nL 187.050827 528.292548 \nL 188.797892 528.415653 \nL 189.671424 528.27936 \nL 190.544957 528.380319 \nL 191.418489 528.745867 \nL 195.786151 528.816948 \nL 197.533216 529.103464 \nL 198.406748 529.031832 \nL 200.153813 529.305893 \nL 201.027345 529.243074 \nL 203.647942 529.4939 \nL 204.521475 529.649377 \nL 205.395007 529.643185 \nL 206.268539 529.834428 \nL 207.142072 529.561923 \nL 208.015604 529.86792 \nL 210.636201 529.941616 \nL 213.256798 530.346532 \nL 215.003863 530.559219 \nL 215.877395 530.369091 \nL 216.750928 530.563594 \nL 222.865654 531.098079 \nL 223.739187 530.972606 \nL 226.359784 530.98078 \nL 228.106849 531.467205 \nL 230.727446 531.313254 \nL 231.600978 531.702393 \nL 233.348043 532.009991 \nL 235.96864 531.978005 \nL 236.842172 532.162765 \nL 237.715705 532.056311 \nL 240.336302 532.41649 \nL 241.209834 532.429596 \nL 242.083367 532.222766 \nL 242.956899 532.668025 \nL 243.830431 532.560754 \nL 244.703964 532.651646 \nL 245.577496 532.947329 \nL 246.451028 532.793365 \nL 247.324561 532.984905 \nL 248.198093 532.814031 \nL 249.071626 533.05586 \nL 249.945158 533.046496 \nL 251.692223 533.318842 \nL 252.565755 533.269477 \nL 254.31282 533.564552 \nL 255.186352 533.469811 \nL 259.554014 534.104672 \nL 260.427546 533.964789 \nL 263.048143 534.347724 \nL 263.921676 534.324671 \nL 264.795208 534.177153 \nL 267.415805 534.602082 \nL 268.289338 534.671106 \nL 270.036402 534.57192 \nL 270.909935 534.818055 \nL 271.783467 534.561201 \nL 272.657 534.79285 \nL 276.151129 535.11085 \nL 277.024661 534.999066 \nL 278.771726 535.324575 \nL 279.645259 535.338776 \nL 280.518791 535.475221 \nL 281.392323 535.437069 \nL 284.886453 535.677581 \nL 286.633517 535.766422 \nL 303.230633 536.701576 \nL 304.104165 536.599414 \nL 309.345359 537.126537 \nL 310.218892 537.059096 \nL 311.092424 537.169311 \nL 311.965956 537.051195 \nL 313.713021 537.203683 \nL 316.333618 537.434129 \nL 317.20715 537.378863 \nL 318.080683 537.450736 \nL 318.954215 537.304445 \nL 319.827748 537.406607 \nL 320.70128 537.651443 \nL 321.574812 537.533948 \nL 325.068942 537.702055 \nL 325.942474 537.645568 \nL 328.563071 537.958276 \nL 329.436604 537.772871 \nL 330.310136 538.053093 \nL 333.804266 538.247621 \nL 334.677798 538.07938 \nL 336.424863 538.276713 \nL 339.04546 538.370834 \nL 340.792525 538.517016 \nL 342.539589 538.41212 \nL 343.413122 538.58528 \nL 344.286654 538.626433 \nL 346.033719 538.508551 \nL 348.654316 538.768969 \nL 351.274913 538.753838 \nL 352.148445 538.825153 \nL 353.021978 538.698141 \nL 357.38964 539.025063 \nL 358.263172 538.898931 \nL 359.136704 538.93412 \nL 360.010237 539.087571 \nL 361.757301 538.970842 \nL 363.504366 539.132099 \nL 366.998496 539.176456 \nL 367.872028 539.222662 \nL 369.619093 539.141577 \nL 370.492625 539.289577 \nL 372.23969 539.173556 \nL 373.113222 539.326501 \nL 373.986755 539.365084 \nL 374.860287 539.222801 \nL 375.733819 539.383362 \nL 378.354417 539.378335 \nL 379.227949 539.280909 \nL 381.848546 539.602183 \nL 383.595611 539.461882 \nL 384.469143 539.642583 \nL 387.08974 539.49835 \nL 387.963273 539.731031 \nL 389.710337 539.670282 \nL 390.58387 539.66875 \nL 391.457402 539.551774 \nL 392.330934 539.739363 \nL 398.445661 539.748771 \nL 399.319193 539.654359 \nL 400.192726 539.827171 \nL 414.169244 540.159246 \nL 415.042776 539.979692 \nL 415.916308 540.161684 \nL 424.651632 540.200096 \nL 427.272229 540.325936 \nL 430.766359 540.257875 \nL 431.639891 540.311545 \nL 432.513424 540.208808 \nL 433.386956 540.348608 \nL 434.260488 540.247631 \nL 436.007553 540.354186 \nL 437.754618 540.419012 \nL 438.62815 540.337123 \nL 440.375215 540.427483 \nL 441.248747 540.457854 \nL 442.12228 540.326512 \nL 443.869344 540.483477 \nL 448.237006 540.335218 \nL 449.984071 540.539977 \nL 451.731136 540.363677 \nL 453.4782 540.514988 \nL 460.466459 540.558262 \nL 461.339992 540.624937 \nL 462.213524 540.53804 \nL 465.707654 540.642803 \nL 468.328251 540.500692 \nL 470.948848 540.562928 \nL 471.82238 540.479191 \nL 472.695913 540.649723 \nL 473.569445 540.531918 \nL 474.442977 540.644563 \nL 475.31651 540.550474 \nL 477.937107 540.678835 \nL 482.304769 540.677853 \nL 484.925366 540.717487 \nL 485.798898 540.75398 \nL 486.672431 540.644855 \nL 490.16656 540.740659 \nL 491.040092 540.660487 \nL 495.407754 540.777033 \nL 498.901884 540.764845 \nL 501.522481 540.814203 \nL 505.890143 540.745389 \nL 506.763675 540.818395 \nL 507.637208 540.753132 \nL 510.257805 540.818439 \nL 511.131337 540.815628 \nL 512.004869 540.678974 \nL 513.751934 540.782952 \nL 514.625466 540.754816 \nL 516.372531 540.89097 \nL 517.246064 540.79702 \nL 518.119596 540.84648 \nL 518.993128 540.767757 \nL 520.740193 540.910072 \nL 523.36079 540.887019 \nL 525.981387 540.86165 \nL 527.728452 540.957259 \nL 528.601984 540.858598 \nL 529.475517 540.954207 \nL 531.222582 540.87613 \nL 533.843179 540.910255 \nL 536.463776 540.978506 \nL 537.337308 540.766928 \nL 541.70497 541.01003 \nL 546.072632 540.920157 \nL 546.946164 541.036786 \nL 547.819697 541.008168 \nL 548.693229 540.858231 \nL 551.313826 541.025953 \nL 553.060891 540.93855 \nL 553.934423 540.956562 \nL 554.807956 540.846436 \nL 556.55502 540.897947 \nL 560.922682 541.042465 \nL 561.796215 540.924 \nL 563.543279 541.029074 \nL 564.416812 540.926109 \nL 565.290344 540.948749 \nL 566.163876 541.091994 \nL 569.658006 540.960627 \nL 571.405071 541.045238 \nL 576.646265 541.028207 \nL 577.519797 540.759932 \nL 578.39333 541.038767 \nL 580.140394 541.014632 \nL 593.24338 540.997133 \nL 594.116912 540.916966 \nL 596.737509 541.055969 \nL 598.484574 540.993499 \nL 601.105171 541.111324 \nL 603.725768 541.065681 \nL 604.599301 540.905513 \nL 608.966963 541.125607 \nL 614.208157 541.060927 \nL 615.955222 541.086226 \nL 631.678804 541.103156 \nL 633.425869 541.041274 \nL 634.299401 541.156592 \nL 642.161193 540.945615 \nL 643.908258 541.106195 \nL 645.655322 540.968484 \nL 647.402387 541.134477 \nL 650.022984 541.139017 \nL 652.643581 541.02216 \nL 654.390646 541.10251 \nL 656.137711 540.934795 \nL 657.011243 541.160499 \nL 658.758308 541.127551 \nL 660.505373 541.046732 \nL 662.252437 541.152521 \nL 668.367164 541.035925 \nL 670.114229 541.144423 \nL 676.228955 541.143467 \nL 678.849552 541.002932 \nL 680.596617 541.164975 \nL 682.343682 541.09131 \nL 683.217214 541.161657 \nL 684.090747 541.054969 \nL 685.837811 541.133629 \nL 690.205473 541.114173 \nL 691.079006 541.016874 \nL 692.82607 541.182924 \nL 696.3202 541.14386 \nL 698.067265 541.117978 \nL 698.940797 541.197802 \nL 700.687862 541.105771 \nL 701.561394 541.181373 \nL 702.434926 541.085676 \nL 703.308459 541.167818 \nL 704.181991 541.027409 \nL 706.802588 541.019235 \nL 707.676121 541.145633 \nL 710.296718 541.10265 \nL 712.917315 541.088974 \nL 715.537912 541.159562 \nL 717.284977 540.912965 \nL 718.158509 541.12776 \nL 724.273236 541.12586 \nL 727.767365 541.129273 \nL 730.387962 541.080269 \nL 731.261495 541.163841 \nL 733.882092 541.103644 \nL 734.755624 541.155168 \nL 735.629157 541.086195 \nL 737.376221 541.191357 \nL 738.249754 541.080136 \nL 739.123286 541.120396 \nL 739.996818 541.004889 \nL 740.870351 541.0764 \nL 741.743883 540.988193 \nL 742.617416 541.059958 \nL 743.490948 540.993917 \nL 744.36448 541.104081 \nL 746.111545 541.068018 \nL 748.732142 541.074248 \nL 752.226272 541.121992 \nL 753.099804 541.138947 \nL 753.973336 541.034633 \nL 760.088063 541.085549 \nL 760.961595 541.034589 \nL 761.835128 541.156092 \nL 765.329257 541.01619 \nL 767.949854 541.15758 \nL 774.938113 541.073899 \nL 775.811646 541.146487 \nL 776.685178 541.083365 \nL 779.305775 541.133496 \nL 781.926372 541.109785 \nL 783.673437 541.209034 \nL 786.294034 541.101428 \nL 786.294034 541.101428 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Model Loss -->\n    <g transform=\"translate(398.119687 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"385.994141\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"447.175781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"499.275391\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_21\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_22\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p2afc437be3\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 822.640625 603.474375\" width=\"822.640625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-14T14:29:23.951694</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 822.640625 603.474375 \nL 822.640625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \nL 815.440625 22.318125 \nL 34.240625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m8a444e7798\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.749716\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(66.568466 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"157.102953\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(147.559203 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"244.45619\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(234.91244 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"331.809427\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(322.265677 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"419.162665\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(409.618915 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"506.515902\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(496.972152 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"593.869139\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(584.325389 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"681.222376\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(671.678626 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"768.575613\" xlink:href=\"#m8a444e7798\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(759.031863 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(409.6125 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m454ea02bf2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m454ea02bf2\" y=\"507.820237\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2 -->\n      <g transform=\"translate(20.878125 511.619456)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m454ea02bf2\" y=\"390.310494\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 3 -->\n      <g transform=\"translate(20.878125 394.109713)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m454ea02bf2\" y=\"272.800751\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 4 -->\n      <g transform=\"translate(20.878125 276.59997)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m454ea02bf2\" y=\"155.291008\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 5 -->\n      <g transform=\"translate(20.878125 159.090226)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m454ea02bf2\" y=\"37.781265\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 6 -->\n      <g transform=\"translate(20.878125 41.580483)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- MAE -->\n     <g transform=\"translate(14.798438 305.011875)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"154.6875\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p6bbaebe7e9)\" d=\"M 69.749716 47.027216 \nL 73.243845 196.382033 \nL 76.737975 354.237421 \nL 78.48504 421.450168 \nL 80.232104 464.844374 \nL 81.979169 488.241602 \nL 82.852701 493.015531 \nL 83.726234 495.80886 \nL 84.599766 496.462289 \nL 85.473299 498.693804 \nL 86.346831 499.831582 \nL 87.220363 498.581598 \nL 88.093896 495.96693 \nL 88.967428 494.858401 \nL 89.84096 494.976602 \nL 90.714493 497.576282 \nL 91.588025 496.036214 \nL 92.461558 496.175541 \nL 93.33509 495.947962 \nL 94.208622 491.033363 \nL 95.082155 494.235257 \nL 95.955687 495.353284 \nL 96.829219 495.722093 \nL 97.702752 493.349403 \nL 98.576284 495.618096 \nL 99.449817 495.427808 \nL 100.323349 497.453654 \nL 101.196881 497.466401 \nL 102.070414 492.965521 \nL 102.943946 494.75967 \nL 104.691011 488.816641 \nL 105.564543 497.496715 \nL 106.438076 493.88592 \nL 107.311608 494.13395 \nL 108.18514 491.802864 \nL 109.058673 496.633666 \nL 109.932205 492.654034 \nL 110.805737 493.235684 \nL 111.67927 496.335851 \nL 112.552802 497.621584 \nL 113.426334 493.130959 \nL 114.299867 496.324224 \nL 115.173399 496.578614 \nL 116.046932 497.971371 \nL 116.920464 495.714025 \nL 117.793996 495.324763 \nL 118.667529 492.81073 \nL 119.541061 496.750047 \nL 120.414593 497.218623 \nL 121.288126 499.875793 \nL 122.161658 497.086497 \nL 123.035191 496.367622 \nL 123.908723 497.09776 \nL 124.782255 494.078561 \nL 125.655788 497.233724 \nL 126.52932 496.880156 \nL 127.402852 500.612318 \nL 128.276385 494.591823 \nL 129.149917 495.553826 \nL 130.02345 493.877459 \nL 130.896982 493.305698 \nL 131.770514 497.974088 \nL 132.644047 498.863108 \nL 133.517579 495.978108 \nL 134.391111 498.888967 \nL 135.264644 498.496316 \nL 136.138176 499.283552 \nL 137.011709 494.987865 \nL 137.885241 494.171632 \nL 138.758773 498.707028 \nL 139.632306 493.148721 \nL 140.505838 499.00179 \nL 141.37937 497.078513 \nL 142.252903 495.808832 \nL 143.126435 499.769134 \nL 143.999968 500.767726 \nL 144.8735 493.657837 \nL 145.747032 492.727129 \nL 146.620565 499.64729 \nL 147.494097 498.624884 \nL 148.367629 491.867078 \nL 149.241162 493.151047 \nL 150.114694 492.566538 \nL 150.988226 496.702531 \nL 151.861759 497.08504 \nL 152.735291 494.696885 \nL 153.608824 497.554793 \nL 154.482356 498.431261 \nL 155.355888 494.831757 \nL 156.229421 502.917797 \nL 157.102953 494.261229 \nL 157.976485 498.189927 \nL 158.850018 493.916514 \nL 159.72355 499.350791 \nL 160.597083 501.078093 \nL 161.470615 491.493142 \nL 162.344147 495.776642 \nL 163.21768 498.42902 \nL 164.091212 497.105745 \nL 164.964744 499.296159 \nL 166.711809 493.740486 \nL 167.585342 502.300313 \nL 168.458874 501.516775 \nL 169.332406 494.914013 \nL 170.205939 492.864298 \nL 171.079471 497.733566 \nL 171.953003 496.181648 \nL 172.826536 500.528521 \nL 173.700068 499.037651 \nL 174.573601 496.419929 \nL 175.447133 500.078548 \nL 176.320665 497.812657 \nL 177.194198 497.435331 \nL 178.06773 496.182685 \nL 178.941262 504.115418 \nL 179.814795 499.370655 \nL 180.688327 500.097375 \nL 181.561859 499.141732 \nL 182.435392 502.916424 \nL 183.308924 500.482742 \nL 184.182457 496.61526 \nL 185.929521 499.242479 \nL 186.803054 498.144316 \nL 187.676586 500.491511 \nL 188.550118 498.197996 \nL 189.423651 498.2728 \nL 190.297183 495.180814 \nL 191.170716 502.790742 \nL 192.044248 498.462472 \nL 192.91778 499.826119 \nL 193.791313 496.036523 \nL 194.664845 501.188646 \nL 195.538377 500.491147 \nL 196.41191 500.152203 \nL 197.285442 496.865727 \nL 198.158975 502.824418 \nL 199.032507 498.541366 \nL 199.906039 497.460265 \nL 200.779572 503.277249 \nL 201.653104 497.770492 \nL 202.526636 498.529936 \nL 203.400169 503.29686 \nL 204.273701 499.666061 \nL 205.147234 499.907115 \nL 206.020766 501.675993 \nL 206.894298 498.679488 \nL 207.767831 504.038205 \nL 208.641363 499.49729 \nL 209.514895 503.03457 \nL 210.388428 501.902983 \nL 211.26196 500.203053 \nL 212.135492 503.418536 \nL 213.009025 503.306106 \nL 213.882557 500.463607 \nL 214.75609 500.882033 \nL 215.629622 503.176333 \nL 216.503154 504.157163 \nL 217.376687 503.18429 \nL 218.250219 504.483303 \nL 219.123751 503.568396 \nL 219.997284 500.952243 \nL 220.870816 503.48656 \nL 221.744349 502.428769 \nL 222.617881 503.569909 \nL 223.491413 502.272521 \nL 224.364946 501.643466 \nL 225.238478 500.5267 \nL 226.11201 498.991452 \nL 226.985543 503.829958 \nL 227.859075 501.64764 \nL 228.732608 505.620885 \nL 229.60614 498.506738 \nL 230.479672 507.358637 \nL 231.353205 500.903298 \nL 232.226737 501.090532 \nL 233.100269 503.044936 \nL 233.973802 500.957034 \nL 234.847334 504.518296 \nL 235.720867 504.811152 \nL 236.594399 500.523002 \nL 237.467931 503.180564 \nL 238.341464 500.863542 \nL 239.214996 501.727768 \nL 240.088528 506.548232 \nL 240.962061 504.721247 \nL 241.835593 502.239685 \nL 242.709126 506.477126 \nL 243.582658 505.424462 \nL 244.45619 506.404731 \nL 245.329723 505.032903 \nL 246.203255 502.776061 \nL 247.95032 505.278271 \nL 248.823852 505.865609 \nL 249.697384 507.547945 \nL 250.570917 504.733659 \nL 252.317982 504.066782 \nL 253.191514 506.787689 \nL 254.065046 503.398672 \nL 254.938579 502.297511 \nL 255.812111 504.4856 \nL 256.685643 503.696628 \nL 257.559176 501.393671 \nL 258.432708 508.823886 \nL 259.306241 503.108393 \nL 260.179773 505.123256 \nL 261.053305 505.936631 \nL 261.926838 510.406118 \nL 262.80037 508.881782 \nL 263.673902 506.327518 \nL 264.547435 507.115398 \nL 265.420967 510.182505 \nL 266.2945 505.846418 \nL 268.041564 510.439808 \nL 268.915097 508.451015 \nL 269.788629 504.308704 \nL 270.662161 510.320892 \nL 271.535694 503.859347 \nL 272.409226 506.575548 \nL 273.282759 506.640518 \nL 274.156291 503.451959 \nL 275.029823 505.264627 \nL 275.903356 505.308781 \nL 276.776888 508.179941 \nL 277.65042 507.629809 \nL 278.523953 506.050574 \nL 279.397485 505.325675 \nL 280.271017 509.802013 \nL 282.018082 511.619766 \nL 282.891615 505.623211 \nL 283.765147 506.215984 \nL 284.638679 511.069521 \nL 285.512212 505.91094 \nL 286.385744 506.405992 \nL 287.259276 504.574945 \nL 288.132809 509.02279 \nL 289.006341 508.468413 \nL 289.879874 505.892757 \nL 290.753406 511.8481 \nL 291.626938 507.057852 \nL 292.500471 508.256286 \nL 293.374003 504.106593 \nL 294.247535 509.285094 \nL 295.121068 503.750783 \nL 295.9946 507.27809 \nL 296.868133 507.876354 \nL 297.741665 509.025115 \nL 298.615197 513.444901 \nL 299.48873 509.512504 \nL 300.362262 510.932759 \nL 301.235794 510.23928 \nL 302.109327 505.469288 \nL 302.982859 502.507775 \nL 303.856392 509.256433 \nL 304.729924 511.211425 \nL 305.603456 513.742268 \nL 306.476989 514.954248 \nL 307.350521 508.610274 \nL 308.224053 505.837145 \nL 309.097586 513.849095 \nL 309.971118 507.318629 \nL 310.84465 510.838203 \nL 311.718183 512.599951 \nL 312.591715 515.769136 \nL 313.465248 511.383292 \nL 314.33878 510.933011 \nL 315.212312 512.165023 \nL 316.085845 513.798791 \nL 316.959377 509.58354 \nL 317.832909 509.918673 \nL 318.706442 513.930595 \nL 319.579974 510.054203 \nL 320.453507 509.222631 \nL 322.200571 513.007185 \nL 323.074104 506.618133 \nL 323.947636 510.321579 \nL 324.821168 507.598038 \nL 325.694701 515.773142 \nL 326.568233 511.74196 \nL 327.441766 511.978979 \nL 328.315298 509.768407 \nL 329.18883 512.199371 \nL 330.062363 506.480572 \nL 330.935895 513.101699 \nL 331.809427 510.200379 \nL 332.68296 511.839961 \nL 333.556492 514.586111 \nL 334.430025 511.492066 \nL 335.303557 511.468757 \nL 336.177089 511.739326 \nL 337.050622 513.750953 \nL 337.924154 509.058693 \nL 338.797686 511.809059 \nL 339.671219 509.207768 \nL 340.544751 511.215333 \nL 341.418284 510.364094 \nL 342.291816 509.955949 \nL 343.165348 511.544079 \nL 344.038881 510.189887 \nL 344.912413 513.019736 \nL 345.785945 509.159678 \nL 346.659478 509.581929 \nL 347.53301 511.944309 \nL 348.406542 510.376911 \nL 349.280075 510.346233 \nL 350.153607 510.930195 \nL 351.02714 512.598494 \nL 351.900672 508.6061 \nL 352.774204 510.767924 \nL 353.647737 511.071454 \nL 354.521269 510.152317 \nL 355.394801 509.399541 \nL 356.268334 512.064177 \nL 357.141866 511.077002 \nL 358.015399 512.060269 \nL 358.888931 513.760129 \nL 359.762463 511.226764 \nL 360.635996 511.97975 \nL 361.509528 512.200562 \nL 362.38306 509.372478 \nL 363.256593 507.310056 \nL 364.130125 512.638039 \nL 365.003658 516.324689 \nL 365.87719 508.606016 \nL 366.750722 510.403443 \nL 367.624255 511.342878 \nL 368.497787 507.238334 \nL 369.371319 511.764765 \nL 370.244852 514.279849 \nL 371.118384 511.525756 \nL 371.991917 512.31945 \nL 372.865449 511.245549 \nL 373.738981 508.940603 \nL 374.612514 509.9065 \nL 375.486046 514.543806 \nL 376.359578 516.325138 \nL 377.233111 509.131424 \nL 378.106643 509.517197 \nL 378.980175 512.300903 \nL 379.853708 512.354554 \nL 380.72724 511.231891 \nL 382.474305 511.292981 \nL 383.347837 511.858298 \nL 384.22137 513.921293 \nL 385.968434 512.038192 \nL 386.841967 518.91609 \nL 387.715499 509.665642 \nL 388.589032 509.577166 \nL 390.336096 516.817444 \nL 391.209629 513.868749 \nL 392.083161 513.055878 \nL 392.956693 517.024724 \nL 393.830226 510.762811 \nL 394.703758 513.975463 \nL 395.577291 511.210851 \nL 396.450823 509.744551 \nL 397.324355 511.999095 \nL 398.197888 511.894888 \nL 399.07142 507.838518 \nL 399.944952 509.182273 \nL 401.692017 510.121471 \nL 402.56555 512.647019 \nL 403.439082 510.779621 \nL 404.312614 512.709538 \nL 405.186147 509.853437 \nL 406.059679 508.19133 \nL 406.933211 513.469163 \nL 407.806744 512.954108 \nL 408.680276 510.14754 \nL 409.553808 512.671309 \nL 410.427341 512.405082 \nL 411.300873 511.784054 \nL 412.174406 513.039726 \nL 413.047938 511.13536 \nL 413.92147 514.009237 \nL 414.795003 513.450014 \nL 415.668535 509.99803 \nL 416.542067 514.712396 \nL 417.4156 513.104458 \nL 418.289132 506.088901 \nL 419.162665 513.027413 \nL 420.036197 511.552358 \nL 420.909729 511.030578 \nL 421.783262 514.600526 \nL 422.656794 513.304566 \nL 423.530326 518.042872 \nL 424.403859 512.802987 \nL 425.277391 510.615626 \nL 426.150924 513.712921 \nL 427.024456 514.917014 \nL 427.897988 511.727769 \nL 428.771521 512.102672 \nL 429.645053 508.789972 \nL 430.518585 514.596071 \nL 431.392118 512.156688 \nL 432.26565 510.66691 \nL 433.139183 515.614037 \nL 434.012715 511.338186 \nL 434.886247 513.353567 \nL 435.75978 510.622896 \nL 436.633312 510.285872 \nL 437.506844 512.21048 \nL 438.380377 510.472784 \nL 439.253909 513.802602 \nL 440.127442 513.561379 \nL 441.874506 511.108997 \nL 442.748039 512.036469 \nL 443.621571 513.420961 \nL 444.495103 512.157122 \nL 445.368636 510.003815 \nL 447.1157 515.065824 \nL 447.989233 512.495323 \nL 448.862765 512.329256 \nL 449.736298 519.401714 \nL 450.60983 513.475579 \nL 451.483362 511.10457 \nL 452.356895 510.977823 \nL 453.230427 515.105495 \nL 454.103959 514.877553 \nL 454.977492 511.985997 \nL 455.851024 514.026692 \nL 456.724557 512.319744 \nL 457.598089 509.009384 \nL 458.471621 512.962457 \nL 459.345154 512.100024 \nL 460.218686 509.457312 \nL 461.092218 509.961118 \nL 461.965751 510.654779 \nL 462.839283 513.09807 \nL 463.712816 512.656558 \nL 464.586348 513.358022 \nL 465.45988 514.922225 \nL 466.333413 513.096558 \nL 467.206945 512.729023 \nL 468.080477 513.757761 \nL 468.95401 512.464155 \nL 469.827542 514.839703 \nL 470.701075 515.020857 \nL 471.574607 514.912335 \nL 472.448139 512.747374 \nL 473.321672 514.144991 \nL 474.195204 513.147828 \nL 475.068736 516.775489 \nL 475.942269 517.189433 \nL 476.815801 510.923149 \nL 477.689333 516.381689 \nL 478.562866 511.621811 \nL 479.436398 514.68861 \nL 480.309931 517.09936 \nL 481.183463 511.198719 \nL 482.056995 513.566955 \nL 482.930528 511.601975 \nL 483.80406 515.700047 \nL 484.677592 515.174192 \nL 485.551125 515.518164 \nL 486.424657 513.975113 \nL 487.29819 513.431817 \nL 488.171722 510.44205 \nL 489.045254 517.958906 \nL 489.918787 507.171991 \nL 490.792319 516.287596 \nL 491.665851 507.788214 \nL 492.539384 511.968263 \nL 493.412916 517.525127 \nL 494.286449 517.841181 \nL 495.159981 513.478843 \nL 496.033513 512.437861 \nL 496.907046 514.239393 \nL 497.780578 511.456191 \nL 498.65411 513.854404 \nL 499.527643 515.453096 \nL 500.401175 515.837763 \nL 501.274708 513.585614 \nL 502.14824 518.933727 \nL 503.021772 512.813633 \nL 503.895305 514.673228 \nL 504.768837 512.584458 \nL 505.642369 513.201325 \nL 506.515902 517.527284 \nL 507.389434 514.950284 \nL 508.262966 511.755618 \nL 509.136499 512.627001 \nL 510.883564 508.557211 \nL 511.757096 516.296071 \nL 512.630628 510.681016 \nL 513.504161 514.226673 \nL 514.377693 516.831536 \nL 515.251225 511.12837 \nL 516.124758 515.621363 \nL 516.99829 511.40387 \nL 517.871823 515.949717 \nL 518.745355 512.232487 \nL 519.618887 515.74437 \nL 520.49242 513.612664 \nL 521.365952 510.88041 \nL 522.239484 512.168413 \nL 523.113017 517.287645 \nL 523.986549 514.438268 \nL 524.860082 517.335231 \nL 525.733614 513.542384 \nL 526.607146 514.227808 \nL 527.480679 513.210277 \nL 528.354211 510.616396 \nL 529.227743 511.736188 \nL 530.101276 516.165458 \nL 530.974808 519.039181 \nL 531.848341 511.081148 \nL 532.721873 512.5542 \nL 533.595405 518.007179 \nL 535.34247 512.245458 \nL 537.089535 516.745679 \nL 537.963067 513.354183 \nL 538.8366 514.15935 \nL 539.710132 513.481238 \nL 541.457197 515.774515 \nL 542.330729 511.013502 \nL 543.204261 511.514634 \nL 544.077794 514.593746 \nL 544.951326 516.33577 \nL 545.824858 515.803106 \nL 546.698391 518.920727 \nL 547.571923 512.681283 \nL 548.445456 509.935105 \nL 549.318988 508.225622 \nL 550.19252 515.439718 \nL 551.066053 511.959115 \nL 551.939585 513.471755 \nL 552.813117 513.439662 \nL 553.68665 513.108254 \nL 554.560182 517.295209 \nL 555.433715 514.652804 \nL 556.307247 514.064766 \nL 557.180779 513.15091 \nL 558.054312 514.931695 \nL 558.927844 513.000279 \nL 559.801376 513.992637 \nL 560.674909 512.304531 \nL 561.548441 517.648399 \nL 562.421974 512.13611 \nL 563.295506 514.592891 \nL 564.169038 508.426472 \nL 565.042571 515.304314 \nL 565.916103 517.53447 \nL 566.789635 513.548366 \nL 567.663168 512.718937 \nL 568.5367 513.715288 \nL 569.410233 514.36974 \nL 570.283765 515.971836 \nL 571.157297 514.545473 \nL 572.03083 512.236829 \nL 572.904362 515.782682 \nL 573.777894 514.849831 \nL 575.524959 516.309771 \nL 576.398491 511.777526 \nL 577.272024 508.85724 \nL 578.145556 514.017082 \nL 579.019089 512.39155 \nL 579.892621 514.226015 \nL 581.639686 510.9444 \nL 582.513218 515.68744 \nL 583.38675 513.122081 \nL 584.260283 511.824328 \nL 585.133815 509.635048 \nL 586.007348 514.096957 \nL 586.88088 511.094764 \nL 587.754412 516.649457 \nL 589.501477 514.063659 \nL 590.375009 514.003872 \nL 591.248542 515.512981 \nL 592.122074 517.661063 \nL 592.995607 511.896246 \nL 593.869139 511.273566 \nL 594.742671 512.524783 \nL 595.616204 515.854278 \nL 596.489736 514.128574 \nL 597.363268 509.127963 \nL 598.236801 514.142302 \nL 599.110333 516.762601 \nL 599.983866 512.1907 \nL 600.857398 508.885439 \nL 601.73093 520.394577 \nL 602.604463 513.756472 \nL 603.477995 513.482065 \nL 604.351527 513.724954 \nL 605.22506 513.086262 \nL 606.098592 517.843758 \nL 606.972124 514.493306 \nL 607.845657 516.313539 \nL 608.719189 512.478527 \nL 609.592722 515.550033 \nL 610.466254 513.257442 \nL 611.339786 515.063022 \nL 612.213319 513.864616 \nL 613.086851 514.630952 \nL 613.960383 513.12711 \nL 614.833916 517.314667 \nL 615.707448 516.99067 \nL 616.580981 510.924984 \nL 617.454513 513.331924 \nL 618.328045 514.530793 \nL 620.07511 513.853255 \nL 620.948642 514.98896 \nL 621.822175 512.700138 \nL 622.695707 514.020052 \nL 623.56924 518.243862 \nL 625.316304 514.010722 \nL 626.189837 512.439094 \nL 627.063369 512.392755 \nL 627.936901 513.626546 \nL 628.810434 517.388308 \nL 629.683966 515.831627 \nL 630.557499 516.677459 \nL 631.431031 513.099457 \nL 632.304563 514.787256 \nL 633.178096 513.215558 \nL 634.051628 516.431362 \nL 635.798693 515.553059 \nL 636.672225 514.950774 \nL 637.545758 512.990739 \nL 638.41929 511.324822 \nL 639.292822 516.019211 \nL 640.166355 508.792578 \nL 641.039887 512.407239 \nL 641.913419 511.555608 \nL 642.786952 514.217694 \nL 643.660484 513.378474 \nL 644.534016 514.847673 \nL 645.407549 514.260153 \nL 646.281081 510.637927 \nL 647.154614 517.996056 \nL 648.028146 510.864342 \nL 648.901678 512.697575 \nL 649.775211 511.584801 \nL 650.648743 512.504289 \nL 651.522275 514.44502 \nL 652.395808 515.785428 \nL 653.26934 513.789784 \nL 654.142873 513.083068 \nL 655.016405 512.974546 \nL 655.889937 514.388483 \nL 656.76347 511.890811 \nL 657.637002 516.189132 \nL 658.510534 513.298865 \nL 659.384067 512.929299 \nL 660.257599 514.359976 \nL 661.131132 516.440678 \nL 662.004664 514.950718 \nL 662.878196 512.260181 \nL 663.751729 514.964432 \nL 664.625261 511.353805 \nL 665.498793 511.304678 \nL 666.372326 515.008502 \nL 667.245858 515.800893 \nL 668.119391 509.323042 \nL 668.992923 516.443031 \nL 669.866455 514.014028 \nL 670.739988 517.076554 \nL 671.61352 513.806412 \nL 672.487052 512.357076 \nL 673.360585 512.165681 \nL 674.234117 513.816063 \nL 675.107649 513.958373 \nL 675.981182 513.135879 \nL 676.854714 512.754434 \nL 677.728247 512.677921 \nL 678.601779 514.388161 \nL 679.475311 514.24148 \nL 680.348844 516.537265 \nL 681.222376 513.090114 \nL 682.095908 514.104451 \nL 682.969441 514.924368 \nL 683.842973 513.019652 \nL 684.716506 515.40523 \nL 685.590038 512.338025 \nL 686.46357 515.591708 \nL 687.337103 511.208805 \nL 688.210635 512.838119 \nL 689.084167 511.452367 \nL 689.9577 511.18925 \nL 690.831232 512.363408 \nL 691.704765 508.608257 \nL 692.578297 515.009567 \nL 693.451829 516.426053 \nL 694.325362 511.141286 \nL 695.198894 514.098386 \nL 696.072426 514.613581 \nL 696.945959 511.747717 \nL 697.819491 511.363667 \nL 698.693024 515.616726 \nL 699.566556 516.659459 \nL 700.440088 513.154734 \nL 701.313621 511.166837 \nL 702.187153 513.649113 \nL 703.060685 517.929433 \nL 703.934218 511.097958 \nL 704.80775 512.450245 \nL 705.681282 509.510347 \nL 706.554815 515.677368 \nL 707.428347 513.875514 \nL 708.30188 512.663114 \nL 709.175412 513.499001 \nL 710.048944 513.030228 \nL 710.922477 509.975673 \nL 711.796009 513.781057 \nL 712.669541 515.495863 \nL 713.543074 514.320277 \nL 714.416606 508.295705 \nL 715.290139 516.708768 \nL 716.163671 513.611571 \nL 717.037203 511.669089 \nL 717.910736 514.909814 \nL 718.784268 515.612664 \nL 719.6578 514.706218 \nL 720.531333 514.751689 \nL 721.404865 513.968697 \nL 722.278398 514.23823 \nL 723.15193 511.797936 \nL 724.025462 514.460009 \nL 724.898995 515.136692 \nL 725.772527 512.963997 \nL 726.646059 516.666099 \nL 727.519592 513.625453 \nL 728.393124 517.435292 \nL 729.266657 510.133574 \nL 731.013721 515.794267 \nL 731.887254 514.962247 \nL 732.760786 517.457985 \nL 733.634318 513.362308 \nL 734.507851 512.266723 \nL 735.381383 510.146938 \nL 736.254916 515.286972 \nL 737.128448 510.029198 \nL 738.00198 511.268775 \nL 738.875513 514.567802 \nL 739.749045 510.087921 \nL 740.622577 511.999361 \nL 741.49611 515.322133 \nL 742.369642 512.34409 \nL 743.243174 516.673411 \nL 744.116707 513.243448 \nL 744.990239 510.457319 \nL 745.863772 516.377823 \nL 746.737304 515.391866 \nL 747.610836 515.05232 \nL 748.484369 516.552884 \nL 749.357901 515.660138 \nL 750.231433 512.446687 \nL 751.104966 518.403892 \nL 751.978498 512.297779 \nL 752.852031 513.161724 \nL 753.725563 516.079153 \nL 754.599095 510.688959 \nL 756.34616 519.237818 \nL 757.219692 514.65576 \nL 758.093225 517.258297 \nL 758.966757 514.006828 \nL 759.84029 512.150972 \nL 760.713822 514.248932 \nL 761.587354 516.711289 \nL 762.460887 518.282287 \nL 763.334419 514.703612 \nL 764.207951 512.417914 \nL 765.081484 513.444158 \nL 766.828549 514.549872 \nL 767.702081 518.607348 \nL 769.449146 512.561779 \nL 770.322678 514.725535 \nL 771.19621 515.489433 \nL 772.069743 511.910101 \nL 772.943275 511.784573 \nL 773.816807 514.230091 \nL 774.69034 510.640224 \nL 775.563872 515.484629 \nL 776.437405 514.412031 \nL 777.310937 510.788796 \nL 778.184469 514.924649 \nL 779.058002 517.002927 \nL 779.931534 513.215586 \nL 779.931534 513.215586 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p6bbaebe7e9)\" d=\"M 69.749716 92.292644 \nL 73.243845 242.712872 \nL 76.737975 398.082413 \nL 77.611507 429.441736 \nL 78.48504 454.225922 \nL 79.358572 473.694676 \nL 80.232104 488.836365 \nL 81.105637 499.292797 \nL 81.979169 506.679881 \nL 82.852701 511.854782 \nL 83.726234 514.715645 \nL 84.599766 515.388084 \nL 85.473299 515.384161 \nL 86.346831 515.072814 \nL 88.967428 513.283063 \nL 89.84096 512.925307 \nL 90.714493 512.286026 \nL 92.461558 511.796844 \nL 93.33509 511.560525 \nL 94.208622 511.590432 \nL 95.082155 511.015744 \nL 95.955687 510.85738 \nL 96.829219 511.128692 \nL 97.702752 511.223892 \nL 99.449817 511.141244 \nL 100.323349 510.941304 \nL 101.196881 511.080406 \nL 102.070414 511.103814 \nL 102.943946 510.831857 \nL 103.817478 510.977081 \nL 104.691011 511.600434 \nL 105.564543 511.499533 \nL 106.438076 511.962786 \nL 107.311608 511.8438 \nL 108.18514 512.123698 \nL 109.058673 511.720863 \nL 109.932205 511.733737 \nL 110.805737 512.047718 \nL 111.67927 511.653133 \nL 112.552802 511.901457 \nL 113.426334 511.928409 \nL 114.299867 512.146742 \nL 115.173399 511.889606 \nL 116.046932 511.791437 \nL 116.920464 511.962001 \nL 117.793996 511.820392 \nL 119.541061 512.724078 \nL 121.288126 512.127172 \nL 122.161658 512.671687 \nL 123.035191 512.303999 \nL 123.908723 511.716619 \nL 124.782255 511.948511 \nL 125.655788 512.405236 \nL 127.402852 512.850503 \nL 128.276385 512.600287 \nL 129.149917 512.811882 \nL 130.02345 512.862676 \nL 130.896982 513.248673 \nL 131.770514 513.075951 \nL 132.644047 513.388111 \nL 133.517579 513.520293 \nL 135.264644 513.275765 \nL 136.138176 513.393645 \nL 137.011709 513.781337 \nL 138.758773 513.730011 \nL 139.632306 513.900505 \nL 140.505838 513.61258 \nL 142.252903 513.872713 \nL 143.126435 514.726726 \nL 143.999968 514.421304 \nL 144.8735 514.791906 \nL 145.747032 514.246565 \nL 148.367629 514.33217 \nL 150.114694 515.019947 \nL 150.988226 515.095185 \nL 151.861759 514.958969 \nL 152.735291 515.076624 \nL 153.608824 514.542069 \nL 154.482356 514.665482 \nL 155.355888 515.207055 \nL 156.229421 514.60428 \nL 157.102953 515.149621 \nL 157.976485 514.909702 \nL 158.850018 515.400075 \nL 159.72355 515.374734 \nL 160.597083 515.682635 \nL 161.470615 515.196675 \nL 162.344147 515.790064 \nL 163.21768 516.055815 \nL 164.091212 515.941073 \nL 164.964744 516.53218 \nL 165.838277 515.902046 \nL 166.711809 516.549494 \nL 167.585342 516.47483 \nL 168.458874 516.244142 \nL 169.332406 516.231885 \nL 170.205939 516.806994 \nL 171.079471 516.767518 \nL 172.826536 517.18495 \nL 173.700068 516.971156 \nL 174.573601 516.478528 \nL 175.447133 517.238938 \nL 176.320665 517.148949 \nL 177.194198 517.400355 \nL 178.06773 517.480244 \nL 178.941262 518.014813 \nL 179.814795 518.132777 \nL 180.688327 517.84607 \nL 181.561859 517.85722 \nL 182.435392 518.021047 \nL 183.308924 517.384554 \nL 184.182457 517.620971 \nL 185.055989 518.928403 \nL 185.929521 518.813704 \nL 186.803054 518.572594 \nL 187.676586 518.827908 \nL 188.550118 518.77654 \nL 189.423651 518.374321 \nL 190.297183 518.905962 \nL 191.170716 519.163448 \nL 192.044248 518.735482 \nL 192.91778 519.277965 \nL 193.791313 519.470257 \nL 194.664845 519.097119 \nL 195.538377 519.217478 \nL 196.41191 519.222465 \nL 198.158975 520.041667 \nL 199.032507 519.868091 \nL 199.906039 520.458847 \nL 200.779572 519.300687 \nL 201.653104 520.231073 \nL 202.526636 520.196641 \nL 203.400169 520.553837 \nL 204.273701 520.108501 \nL 205.147234 520.540907 \nL 206.020766 520.775251 \nL 206.894298 521.185175 \nL 208.641363 521.739916 \nL 209.514895 520.81278 \nL 210.388428 521.382649 \nL 213.009025 521.674427 \nL 214.75609 521.796495 \nL 216.503154 522.401189 \nL 217.376687 521.766125 \nL 219.123751 521.677859 \nL 219.997284 521.421971 \nL 221.744349 522.894728 \nL 222.617881 522.600891 \nL 223.491413 522.145259 \nL 224.364946 521.945235 \nL 225.238478 523.170775 \nL 226.985543 524.189357 \nL 227.859075 523.877645 \nL 229.60614 523.464696 \nL 230.479672 524.049302 \nL 231.353205 523.488426 \nL 233.973802 524.366155 \nL 234.847334 524.254481 \nL 235.720867 523.431244 \nL 236.594399 524.889237 \nL 237.467931 524.335295 \nL 238.341464 524.524602 \nL 239.214996 525.562082 \nL 240.088528 524.759521 \nL 240.962061 525.337123 \nL 241.835593 524.569653 \nL 242.709126 525.293908 \nL 243.582658 525.135965 \nL 244.45619 525.60335 \nL 245.329723 525.91149 \nL 246.203255 525.564911 \nL 247.95032 526.455094 \nL 248.823852 525.92285 \nL 249.697384 526.257717 \nL 250.570917 526.393373 \nL 252.317982 526.962066 \nL 253.191514 527.94193 \nL 254.065046 526.986665 \nL 254.938579 527.319655 \nL 255.812111 527.202266 \nL 256.685643 528.221114 \nL 257.559176 527.859743 \nL 258.432708 527.118286 \nL 260.179773 528.077852 \nL 261.053305 528.411598 \nL 261.926838 528.56286 \nL 263.673902 527.869591 \nL 264.547435 528.723618 \nL 265.420967 527.628775 \nL 266.2945 528.326778 \nL 267.168032 528.518958 \nL 268.041564 528.52267 \nL 268.915097 529.252906 \nL 269.788629 529.086964 \nL 270.662161 528.579319 \nL 272.409226 529.586919 \nL 273.282759 529.505671 \nL 274.156291 529.987527 \nL 275.029823 529.655069 \nL 275.903356 529.874326 \nL 278.523953 530.159772 \nL 279.397485 530.057736 \nL 280.271017 530.278198 \nL 281.14455 530.374939 \nL 282.018082 530.162952 \nL 282.891615 530.181135 \nL 283.765147 530.910895 \nL 284.638679 530.563995 \nL 285.512212 530.568631 \nL 286.385744 530.756398 \nL 287.259276 531.477879 \nL 288.132809 531.41445 \nL 289.006341 531.064944 \nL 289.879874 531.170328 \nL 291.626938 531.701997 \nL 292.500471 531.720404 \nL 293.374003 531.41099 \nL 294.247535 532.293425 \nL 295.121068 532.385432 \nL 296.868133 531.98275 \nL 297.741665 531.480078 \nL 299.48873 531.973127 \nL 300.362262 531.931648 \nL 301.235794 532.098991 \nL 302.982859 533.087988 \nL 303.856392 532.606748 \nL 304.729924 532.980502 \nL 305.603456 532.363943 \nL 306.476989 532.441002 \nL 308.224053 533.127099 \nL 309.971118 533.524723 \nL 310.84465 533.133598 \nL 311.718183 533.320006 \nL 312.591715 532.643492 \nL 313.465248 532.954517 \nL 314.33878 533.990791 \nL 315.212312 533.315019 \nL 316.085845 533.517481 \nL 316.959377 533.324881 \nL 317.832909 534.030057 \nL 319.579974 533.33648 \nL 320.453507 533.530214 \nL 322.200571 534.451425 \nL 323.074104 533.476885 \nL 323.947636 534.714332 \nL 325.694701 534.680222 \nL 326.568233 534.440779 \nL 327.441766 535.258861 \nL 328.315298 534.241021 \nL 329.18883 534.824381 \nL 330.062363 535.028845 \nL 330.935895 534.941223 \nL 331.809427 534.322409 \nL 332.68296 535.157637 \nL 333.556492 535.393522 \nL 334.430025 535.804608 \nL 335.303557 535.19029 \nL 336.177089 534.966032 \nL 337.050622 535.827652 \nL 337.924154 536.01242 \nL 338.797686 535.218012 \nL 339.671219 535.089347 \nL 342.291816 536.31766 \nL 344.038881 535.632783 \nL 345.785945 536.118141 \nL 346.659478 535.371025 \nL 347.53301 535.631158 \nL 348.406542 535.586149 \nL 349.280075 535.782559 \nL 351.02714 536.708014 \nL 351.900672 535.878348 \nL 352.774204 535.988508 \nL 353.647737 536.832296 \nL 354.521269 536.153554 \nL 355.394801 535.943332 \nL 357.141866 536.652794 \nL 358.015399 536.518132 \nL 358.888931 536.633392 \nL 359.762463 536.408028 \nL 361.509528 536.698054 \nL 362.38306 536.246078 \nL 363.256593 536.169047 \nL 364.130125 536.875231 \nL 365.87719 536.160586 \nL 366.750722 536.816284 \nL 367.624255 536.919119 \nL 368.497787 536.187328 \nL 369.371319 536.90378 \nL 371.118384 536.938576 \nL 371.991917 536.69325 \nL 372.865449 536.217123 \nL 375.486046 537.71616 \nL 376.359578 537.050082 \nL 377.233111 536.773797 \nL 378.106643 537.732732 \nL 378.980175 537.441206 \nL 379.853708 537.357787 \nL 380.72724 536.726183 \nL 381.600773 537.965185 \nL 382.474305 537.462051 \nL 384.22137 537.344101 \nL 385.094902 536.7477 \nL 385.968434 537.588769 \nL 386.841967 537.615973 \nL 387.715499 537.484029 \nL 389.462564 537.724369 \nL 390.336096 538.036781 \nL 391.209629 537.281778 \nL 392.083161 537.314306 \nL 392.956693 536.868647 \nL 393.830226 537.59772 \nL 394.703758 537.656205 \nL 396.450823 537.417883 \nL 397.324355 537.489185 \nL 398.197888 537.77026 \nL 399.07142 537.592159 \nL 399.944952 537.972287 \nL 400.818485 537.637658 \nL 401.692017 537.779506 \nL 402.56555 538.546373 \nL 403.439082 537.984797 \nL 405.186147 537.959666 \nL 406.059679 538.726323 \nL 406.933211 538.234087 \nL 407.806744 538.892447 \nL 408.680276 537.747987 \nL 409.553808 538.741382 \nL 410.427341 538.651309 \nL 411.300873 538.04796 \nL 413.047938 538.498773 \nL 413.92147 538.324146 \nL 414.795003 538.421728 \nL 415.668535 538.967377 \nL 416.542067 538.142277 \nL 417.4156 537.765035 \nL 418.289132 538.508817 \nL 419.162665 538.916331 \nL 420.036197 538.807319 \nL 420.909729 539.186032 \nL 421.783262 538.834355 \nL 422.656794 538.712875 \nL 423.530326 538.738328 \nL 424.403859 538.528331 \nL 425.277391 538.800875 \nL 426.150924 538.206603 \nL 427.024456 538.946855 \nL 427.897988 538.349473 \nL 428.771521 538.792568 \nL 430.518585 539.033328 \nL 431.392118 539.267182 \nL 432.26565 538.72037 \nL 433.139183 539.310019 \nL 434.012715 539.214441 \nL 434.886247 539.41029 \nL 435.75978 538.538655 \nL 436.633312 539.259225 \nL 437.506844 539.488737 \nL 438.380377 539.142326 \nL 439.253909 538.545435 \nL 440.127442 539.113946 \nL 441.000974 538.948999 \nL 441.874506 538.404568 \nL 443.621571 539.591809 \nL 444.495103 538.944222 \nL 445.368636 538.468572 \nL 446.242168 539.010915 \nL 447.1157 539.229416 \nL 447.989233 539.153295 \nL 448.862765 539.347281 \nL 449.736298 538.890304 \nL 450.60983 538.909691 \nL 451.483362 539.293434 \nL 452.356895 539.302161 \nL 453.230427 539.162638 \nL 454.103959 539.273528 \nL 454.977492 539.719523 \nL 455.851024 539.10089 \nL 456.724557 539.415081 \nL 457.598089 539.258315 \nL 458.471621 539.373168 \nL 459.345154 539.765344 \nL 460.218686 539.246828 \nL 461.092218 539.426596 \nL 461.965751 538.796099 \nL 462.839283 539.070548 \nL 463.712816 539.003056 \nL 464.586348 539.069147 \nL 465.45988 538.615504 \nL 466.333413 539.522062 \nL 467.206945 538.833879 \nL 468.080477 539.458983 \nL 468.95401 538.909747 \nL 470.701075 539.528478 \nL 471.574607 539.613466 \nL 472.448139 539.433614 \nL 473.321672 539.723375 \nL 474.195204 538.927524 \nL 475.068736 539.598099 \nL 477.689333 539.213474 \nL 478.562866 539.661262 \nL 479.436398 539.885296 \nL 480.309931 539.161224 \nL 482.056995 539.519989 \nL 482.930528 539.723291 \nL 483.80406 539.665773 \nL 484.677592 539.177221 \nL 485.551125 539.406466 \nL 486.424657 539.350951 \nL 487.29819 539.503389 \nL 488.171722 540.337132 \nL 489.045254 539.750901 \nL 489.918787 539.618313 \nL 490.792319 540.012029 \nL 491.665851 539.702306 \nL 492.539384 539.65472 \nL 494.286449 540.17014 \nL 496.033513 539.757975 \nL 496.907046 540.089074 \nL 497.780578 539.629407 \nL 499.527643 539.427296 \nL 500.401175 539.851606 \nL 501.274708 539.441739 \nL 503.021772 539.97006 \nL 503.895305 539.78319 \nL 504.768837 539.746741 \nL 505.642369 538.994694 \nL 506.515902 539.376601 \nL 507.389434 539.50549 \nL 508.262966 539.337798 \nL 509.136499 539.882761 \nL 510.010031 540.195383 \nL 510.883564 539.559296 \nL 511.757096 539.828199 \nL 512.630628 539.353487 \nL 513.504161 540.075752 \nL 514.377693 540.157449 \nL 515.251225 540.385083 \nL 516.99829 539.963308 \nL 517.871823 539.770541 \nL 518.745355 540.055833 \nL 519.618887 539.775317 \nL 521.365952 540.392353 \nL 522.239484 539.712687 \nL 523.113017 540.313809 \nL 523.986549 539.781453 \nL 524.860082 539.763578 \nL 525.733614 539.628161 \nL 526.607146 539.617108 \nL 527.480679 539.916927 \nL 528.354211 540.073567 \nL 529.227743 540.367096 \nL 530.101276 540.357767 \nL 530.974808 539.127674 \nL 531.848341 539.421231 \nL 533.595405 540.197526 \nL 535.34247 540.550576 \nL 536.216002 540.430399 \nL 537.089535 540.461876 \nL 537.963067 540.330114 \nL 538.8366 539.643178 \nL 539.710132 539.873305 \nL 540.583664 540.826203 \nL 541.457197 540.499936 \nL 542.330729 539.513755 \nL 544.951326 540.548839 \nL 545.824858 539.870952 \nL 547.571923 539.986814 \nL 548.445456 539.381153 \nL 549.318988 539.767879 \nL 550.19252 539.634759 \nL 551.066053 539.862099 \nL 551.939585 540.218791 \nL 552.813117 540.250365 \nL 553.68665 540.552411 \nL 554.560182 540.451356 \nL 555.433715 539.700107 \nL 556.307247 540.152854 \nL 557.180779 540.311974 \nL 558.054312 539.698496 \nL 558.927844 539.812509 \nL 559.801376 540.787386 \nL 560.674909 540.280301 \nL 561.548441 540.47248 \nL 563.295506 539.843117 \nL 564.169038 540.225052 \nL 565.042571 540.337679 \nL 566.789635 540.148119 \nL 567.663168 540.17269 \nL 568.5367 540.481361 \nL 569.410233 540.133691 \nL 570.283765 540.233794 \nL 571.157297 538.849232 \nL 572.03083 540.289392 \nL 572.904362 539.9046 \nL 573.777894 540.144379 \nL 575.524959 540.082658 \nL 576.398491 540.91381 \nL 577.272024 540.463963 \nL 578.145556 540.321499 \nL 579.019089 539.810338 \nL 579.892621 540.111922 \nL 580.766153 540.551543 \nL 581.639686 540.329036 \nL 582.513218 539.867071 \nL 583.38675 540.332748 \nL 584.260283 540.184793 \nL 585.133815 540.398657 \nL 586.007348 540.069631 \nL 586.88088 539.988439 \nL 587.754412 539.516473 \nL 588.627945 539.835861 \nL 589.501477 540.297405 \nL 590.375009 540.308401 \nL 591.248542 539.982177 \nL 592.122074 539.907976 \nL 592.995607 540.355861 \nL 593.869139 540.640117 \nL 595.616204 540.761779 \nL 596.489736 540.342904 \nL 597.363268 540.330254 \nL 598.236801 539.40896 \nL 599.110333 539.704786 \nL 599.983866 540.71034 \nL 600.857398 540.867359 \nL 604.351527 540.384774 \nL 605.22506 540.54671 \nL 606.098592 540.233962 \nL 606.972124 540.119304 \nL 607.845657 540.205511 \nL 608.719189 540.567918 \nL 609.592722 540.353508 \nL 610.466254 540.382687 \nL 611.339786 541.140758 \nL 612.213319 540.757492 \nL 613.960383 540.552369 \nL 614.833916 540.165377 \nL 615.707448 540.091862 \nL 616.580981 540.336208 \nL 618.328045 540.517433 \nL 619.201578 540.344193 \nL 620.07511 540.03814 \nL 620.948642 540.585541 \nL 621.822175 540.751987 \nL 622.695707 540.739828 \nL 623.56924 541.006083 \nL 624.442772 540.91664 \nL 626.189837 539.970873 \nL 627.063369 540.071606 \nL 627.936901 540.875105 \nL 629.683966 540.351169 \nL 630.557499 539.776592 \nL 631.431031 540.235699 \nL 632.304563 540.000528 \nL 633.178096 540.448512 \nL 634.051628 540.312254 \nL 634.92516 540.059531 \nL 635.798693 539.512649 \nL 637.545758 540.394118 \nL 638.41929 539.67726 \nL 639.292822 539.595381 \nL 640.166355 540.344529 \nL 641.913419 540.877193 \nL 642.786952 540.51239 \nL 643.660484 540.638198 \nL 644.534016 540.192679 \nL 646.281081 539.909054 \nL 647.154614 540.217586 \nL 648.028146 540.385517 \nL 648.901678 539.761057 \nL 649.775211 539.42067 \nL 650.648743 540.750208 \nL 651.522275 540.850157 \nL 653.26934 540.056449 \nL 654.142873 540.008443 \nL 655.016405 540.429727 \nL 655.889937 540.697285 \nL 656.76347 540.502052 \nL 657.637002 540.52319 \nL 658.510534 539.814821 \nL 659.384067 540.035773 \nL 660.257599 539.720153 \nL 661.131132 540.024594 \nL 662.004664 539.910945 \nL 662.878196 540.587012 \nL 664.625261 540.657585 \nL 665.498793 540.775142 \nL 666.372326 540.453107 \nL 668.992923 540.748289 \nL 669.866455 540.624204 \nL 670.739988 540.310895 \nL 671.61352 539.508026 \nL 672.487052 539.768257 \nL 673.360585 540.476192 \nL 674.234117 540.80407 \nL 675.107649 540.483785 \nL 675.981182 540.291606 \nL 676.854714 540.77052 \nL 677.728247 540.080067 \nL 678.601779 540.61458 \nL 681.222376 540.567624 \nL 682.969441 540.232028 \nL 683.842973 540.449395 \nL 684.716506 539.864816 \nL 685.590038 540.61224 \nL 686.46357 541.058726 \nL 687.337103 540.734084 \nL 688.210635 540.548881 \nL 689.084167 540.128676 \nL 689.9577 540.664884 \nL 690.831232 540.463053 \nL 691.704765 540.455096 \nL 692.578297 541.209034 \nL 693.451829 540.384536 \nL 694.325362 540.374086 \nL 695.198894 540.954994 \nL 696.072426 540.258882 \nL 696.945959 540.878285 \nL 697.819491 539.90869 \nL 698.693024 539.808321 \nL 699.566556 539.917991 \nL 700.440088 539.833171 \nL 701.313621 540.637539 \nL 702.187153 540.735639 \nL 703.934218 540.339766 \nL 704.80775 540.149114 \nL 705.681282 540.364757 \nL 706.554815 540.248754 \nL 707.428347 540.583173 \nL 708.30188 540.285428 \nL 709.175412 540.719866 \nL 710.048944 539.859101 \nL 710.922477 539.32547 \nL 711.796009 540.528737 \nL 712.669541 540.406249 \nL 713.543074 540.813581 \nL 714.416606 540.656437 \nL 715.290139 540.345958 \nL 717.037203 540.692031 \nL 717.910736 540.500945 \nL 719.6578 540.73019 \nL 720.531333 540.779681 \nL 722.278398 540.168543 \nL 723.15193 540.430035 \nL 724.025462 540.173838 \nL 724.898995 540.731521 \nL 726.646059 540.121181 \nL 727.519592 540.314201 \nL 728.393124 540.664659 \nL 729.266657 540.219141 \nL 730.140189 540.533472 \nL 731.013721 541.095945 \nL 731.887254 540.218483 \nL 732.760786 540.450249 \nL 733.634318 539.777195 \nL 734.507851 540.182117 \nL 735.381383 539.685931 \nL 736.254916 540.091988 \nL 737.128448 539.703399 \nL 738.00198 540.339724 \nL 738.875513 539.976364 \nL 739.749045 540.119038 \nL 740.622577 539.944187 \nL 742.369642 540.163178 \nL 743.243174 540.076229 \nL 744.116707 540.369449 \nL 744.990239 540.471948 \nL 745.863772 540.423157 \nL 746.737304 540.534775 \nL 747.610836 539.905804 \nL 748.484369 540.04768 \nL 749.357901 540.353956 \nL 750.231433 539.960675 \nL 751.104966 540.39035 \nL 751.978498 539.994785 \nL 752.852031 540.836625 \nL 753.725563 540.184611 \nL 754.599095 539.886753 \nL 755.472628 540.598022 \nL 756.34616 540.317059 \nL 757.219692 540.358803 \nL 758.093225 540.159494 \nL 758.966757 539.765203 \nL 759.84029 540.096246 \nL 760.713822 540.027816 \nL 761.587354 540.606623 \nL 762.460887 540.516634 \nL 763.334419 540.054754 \nL 764.207951 540.268912 \nL 765.081484 540.08836 \nL 767.702081 540.015783 \nL 768.575613 540.087982 \nL 769.449146 540.533458 \nL 770.322678 540.141773 \nL 771.19621 540.446915 \nL 772.943275 540.421476 \nL 773.816807 540.236861 \nL 774.69034 539.885618 \nL 777.310937 541.170595 \nL 778.184469 540.783253 \nL 779.931534 540.280889 \nL 779.931534 540.280889 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 565.918125 \nL 34.240625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 815.440625 565.918125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 22.318125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Model MAE -->\n    <g transform=\"translate(391.845313 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"486.71875\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 41.240625 59.674375 \nL 96.515625 59.674375 \nQ 98.515625 59.674375 98.515625 57.674375 \nL 98.515625 29.318125 \nQ 98.515625 27.318125 96.515625 27.318125 \nL 41.240625 27.318125 \nQ 39.240625 27.318125 39.240625 29.318125 \nL 39.240625 57.674375 \nQ 39.240625 59.674375 41.240625 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 43.240625 35.416562 \nL 63.240625 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_18\">\n     <!-- train -->\n     <g transform=\"translate(71.240625 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 43.240625 50.094687 \nL 63.240625 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_19\">\n     <!-- test -->\n     <g transform=\"translate(71.240625 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6bbaebe7e9\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"34.240625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-14T14:29:24.343535</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m16885375ce\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.465453\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(153.921703 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.81869\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(241.27494 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"338.171927\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(328.628177 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"425.525165\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(415.981415 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"512.878402\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(503.334652 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"600.231639\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(590.687889 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"687.584876\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(678.041126 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"774.938113\" xlink:href=\"#m16885375ce\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(765.394363 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m751d3e3bd9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m751d3e3bd9\" y=\"534.97186\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 538.771079)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m751d3e3bd9\" y=\"468.58369\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 472.382909)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m751d3e3bd9\" y=\"402.195519\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 405.994738)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m751d3e3bd9\" y=\"335.807349\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 339.606568)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m751d3e3bd9\" y=\"269.419179\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 273.218397)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m751d3e3bd9\" y=\"203.031008\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 206.830227)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m751d3e3bd9\" y=\"136.642838\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 140.442057)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m751d3e3bd9\" y=\"70.254668\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 74.053886)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- MSE -->\n     <g transform=\"translate(14.798438 304.765781)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p15a2e0b5ee)\" d=\"M 76.112216 47.027216 \nL 77.859281 138.792279 \nL 79.606345 224.755163 \nL 81.35341 302.207354 \nL 83.100475 369.931194 \nL 84.84754 423.030005 \nL 86.594604 458.292731 \nL 88.341669 483.232436 \nL 90.088734 495.290491 \nL 90.962266 499.71437 \nL 91.835799 502.058307 \nL 92.709331 504.985449 \nL 93.582863 506.606982 \nL 94.456396 505.673171 \nL 95.329928 505.904547 \nL 96.20346 506.549114 \nL 97.076993 508.581059 \nL 98.824058 508.595039 \nL 99.69759 508.343301 \nL 100.571122 505.978585 \nL 101.444655 507.694618 \nL 102.318187 507.762907 \nL 103.191719 509.293631 \nL 104.065252 508.288194 \nL 104.938784 511.315516 \nL 105.812317 508.352621 \nL 106.685849 509.710469 \nL 107.559381 509.745304 \nL 108.432914 505.901964 \nL 109.306446 508.636857 \nL 110.179978 506.322822 \nL 111.053511 505.199953 \nL 111.927043 509.950247 \nL 112.800576 507.622967 \nL 113.674108 508.283781 \nL 114.54764 507.508903 \nL 115.421173 508.847258 \nL 116.294705 507.445837 \nL 117.168237 507.427103 \nL 118.04177 508.540761 \nL 118.915302 511.689997 \nL 119.788834 507.373287 \nL 120.662367 510.562835 \nL 121.535899 508.603865 \nL 122.409432 510.814731 \nL 123.282964 508.455352 \nL 124.156496 507.942095 \nL 125.030029 506.300821 \nL 125.903561 509.099793 \nL 126.777093 510.523164 \nL 127.650626 510.504277 \nL 128.524158 508.626423 \nL 130.271223 509.650746 \nL 131.144755 507.985192 \nL 132.018288 510.282322 \nL 132.89182 509.361686 \nL 133.765352 510.614606 \nL 134.638885 509.110277 \nL 135.512417 509.316841 \nL 136.38595 506.972955 \nL 137.259482 506.131586 \nL 138.133014 509.675267 \nL 139.006547 510.064482 \nL 139.880079 511.362867 \nL 140.753611 511.536293 \nL 141.627144 510.33063 \nL 142.500676 511.836338 \nL 143.374209 507.37192 \nL 145.121273 510.917849 \nL 145.994806 507.328886 \nL 146.868338 510.234996 \nL 147.74187 509.301855 \nL 148.615403 509.579374 \nL 149.488935 511.828722 \nL 150.362468 511.128382 \nL 151.236 507.569474 \nL 152.109532 507.119207 \nL 152.983065 513.12767 \nL 153.856597 511.109851 \nL 154.730129 507.521958 \nL 155.603662 507.108805 \nL 156.477194 508.397902 \nL 157.350726 510.182434 \nL 158.224259 510.183883 \nL 159.097791 509.398293 \nL 159.971324 510.739839 \nL 160.844856 510.658672 \nL 161.718388 509.392443 \nL 162.591921 513.471452 \nL 163.465453 508.070183 \nL 164.338985 511.183312 \nL 165.212518 509.108283 \nL 166.08605 511.351104 \nL 166.959583 513.060825 \nL 167.833115 508.314747 \nL 168.706647 510.222935 \nL 169.58018 511.464807 \nL 170.453712 509.129809 \nL 171.327244 510.249172 \nL 172.200777 510.09039 \nL 173.074309 510.359627 \nL 173.947842 513.609252 \nL 174.821374 512.48637 \nL 175.694906 509.863179 \nL 176.568439 506.878549 \nL 177.441971 510.82212 \nL 178.315503 510.575966 \nL 179.189036 512.254652 \nL 180.062568 510.918412 \nL 180.936101 509.983436 \nL 181.809633 511.382146 \nL 182.683165 511.616802 \nL 183.556698 509.630113 \nL 184.43023 511.040023 \nL 185.303762 515.030154 \nL 186.177295 510.848578 \nL 187.050827 513.013663 \nL 187.924359 511.965021 \nL 188.797892 514.255339 \nL 189.671424 511.671953 \nL 190.544957 512.257273 \nL 191.418489 510.857448 \nL 192.292021 511.703654 \nL 193.165554 510.501194 \nL 194.039086 510.900469 \nL 194.912618 510.60414 \nL 195.786151 511.880334 \nL 196.659683 511.163109 \nL 197.533216 513.472319 \nL 198.406748 512.726167 \nL 199.28028 511.559219 \nL 200.153813 510.604577 \nL 201.027345 513.110532 \nL 201.900877 512.024149 \nL 202.77441 512.748522 \nL 203.647942 511.63986 \nL 204.521475 513.355299 \nL 205.395007 511.903589 \nL 206.268539 510.763524 \nL 207.142072 515.196546 \nL 208.015604 512.437309 \nL 208.889136 511.795939 \nL 209.762669 515.201921 \nL 210.636201 512.515032 \nL 211.509734 513.357622 \nL 213.256798 512.909451 \nL 214.130331 514.059526 \nL 215.003863 512.611431 \nL 215.877395 513.235099 \nL 216.750928 514.187082 \nL 217.62446 513.493295 \nL 218.497992 514.595322 \nL 219.371525 516.264067 \nL 220.245057 512.83849 \nL 221.11859 513.696332 \nL 221.992122 513.663897 \nL 222.865654 513.862199 \nL 223.739187 515.334143 \nL 224.612719 514.126871 \nL 225.486251 516.73274 \nL 226.359784 513.948437 \nL 227.233316 514.919179 \nL 228.106849 513.6855 \nL 228.980381 514.420003 \nL 229.853913 514.573087 \nL 230.727446 513.276335 \nL 231.600978 513.355172 \nL 232.47451 512.002021 \nL 233.348043 515.515661 \nL 234.221575 514.59819 \nL 235.095108 516.669686 \nL 235.96864 512.452409 \nL 236.842172 515.817884 \nL 237.715705 514.242525 \nL 238.589237 513.374185 \nL 239.462769 515.129371 \nL 240.336302 512.115896 \nL 241.209834 515.69469 \nL 242.083367 515.231 \nL 242.956899 513.031999 \nL 243.830431 515.317321 \nL 244.703964 512.624993 \nL 245.577496 514.193134 \nL 246.451028 515.018396 \nL 247.324561 513.487306 \nL 249.071626 518.872753 \nL 249.945158 516.75752 \nL 250.81869 515.642514 \nL 251.692223 515.621399 \nL 252.565755 516.449257 \nL 253.439287 516.077225 \nL 254.31282 516.077896 \nL 255.186352 516.730448 \nL 256.059884 516.749802 \nL 256.933417 516.045208 \nL 257.806949 514.471058 \nL 258.680482 514.291478 \nL 259.554014 517.480602 \nL 260.427546 514.026211 \nL 261.301079 516.041422 \nL 262.174611 516.022738 \nL 263.048143 516.893459 \nL 263.921676 515.404629 \nL 264.795208 517.287618 \nL 265.668741 515.135601 \nL 266.542273 518.048688 \nL 267.415805 515.690594 \nL 268.289338 518.831169 \nL 269.16287 517.377573 \nL 270.036402 517.591348 \nL 270.909935 516.310153 \nL 271.783467 519.152278 \nL 272.657 517.660733 \nL 274.404064 520.263074 \nL 275.277597 519.43012 \nL 276.151129 516.412801 \nL 277.024661 519.965739 \nL 277.898194 515.464745 \nL 278.771726 517.658586 \nL 279.645259 516.500319 \nL 280.518791 515.803797 \nL 281.392323 515.97129 \nL 282.265856 515.704738 \nL 283.139388 518.88092 \nL 284.886453 515.266145 \nL 285.759985 517.636186 \nL 286.633517 519.268122 \nL 287.50705 518.680529 \nL 288.380582 519.46769 \nL 289.254115 516.573825 \nL 291.001179 519.914259 \nL 291.874712 516.733759 \nL 292.748244 517.630874 \nL 293.621776 516.387837 \nL 294.495309 519.51349 \nL 295.368841 517.597401 \nL 296.242374 517.222216 \nL 297.115906 519.502429 \nL 297.989438 517.153085 \nL 298.862971 518.751927 \nL 299.736503 515.990867 \nL 300.610035 519.745461 \nL 301.483568 515.219104 \nL 302.3571 518.390234 \nL 303.230633 518.290263 \nL 304.104165 518.979011 \nL 304.977697 521.725097 \nL 305.85123 518.350322 \nL 306.724762 520.277161 \nL 307.598294 520.201281 \nL 309.345359 515.523879 \nL 310.218892 519.22764 \nL 311.092424 520.307678 \nL 312.839489 522.662632 \nL 313.713021 517.903505 \nL 314.586553 516.149725 \nL 315.460086 521.824865 \nL 316.333618 517.078572 \nL 317.20715 519.124402 \nL 318.080683 519.721086 \nL 318.954215 521.726978 \nL 319.827748 521.789049 \nL 320.70128 520.084589 \nL 321.574812 519.639001 \nL 322.448345 520.648858 \nL 323.321877 520.136848 \nL 324.195409 518.277436 \nL 325.068942 520.995007 \nL 325.942474 520.267918 \nL 326.816007 518.288345 \nL 327.689539 520.76843 \nL 328.563071 519.536719 \nL 329.436604 518.534391 \nL 330.310136 520.039454 \nL 331.183668 517.650039 \nL 332.057201 522.204672 \nL 332.930733 520.71783 \nL 333.804266 518.966956 \nL 334.677798 518.618717 \nL 335.55133 521.219938 \nL 336.424863 517.960119 \nL 337.298395 521.330305 \nL 338.171927 519.265817 \nL 339.04546 520.69203 \nL 339.918992 520.101108 \nL 340.792525 520.264068 \nL 341.666057 518.71996 \nL 342.539589 519.103629 \nL 343.413122 520.871047 \nL 344.286654 517.394813 \nL 345.160186 519.377285 \nL 346.033719 520.208214 \nL 346.907251 519.000714 \nL 347.780784 518.694395 \nL 349.527848 519.44748 \nL 350.401381 517.862713 \nL 351.274913 519.692393 \nL 352.148445 518.196111 \nL 353.021978 518.67958 \nL 353.89551 520.670403 \nL 354.769042 519.886155 \nL 356.516107 519.979598 \nL 357.38964 520.789868 \nL 358.263172 517.79153 \nL 359.136704 520.68977 \nL 360.010237 520.766543 \nL 360.883769 518.749724 \nL 361.757301 519.36701 \nL 362.630834 520.838555 \nL 363.504366 519.925884 \nL 364.377899 519.85037 \nL 365.251431 522.130754 \nL 366.124963 520.899367 \nL 366.998496 520.02436 \nL 367.872028 518.900978 \nL 368.74556 520.13203 \nL 369.619093 518.344662 \nL 371.366158 523.30109 \nL 372.23969 518.015106 \nL 373.113222 519.838537 \nL 373.986755 520.468891 \nL 374.860287 517.561123 \nL 375.733819 522.019413 \nL 376.607352 520.477407 \nL 377.480884 519.164415 \nL 378.354417 520.435177 \nL 379.227949 520.65315 \nL 380.101481 517.573849 \nL 380.975014 518.894039 \nL 381.848546 523.205646 \nL 382.722078 521.289899 \nL 383.595611 520.670213 \nL 384.469143 518.661649 \nL 385.342675 522.125018 \nL 386.216208 520.106926 \nL 387.08974 519.982523 \nL 387.963273 518.923536 \nL 388.836805 520.400431 \nL 389.710337 520.361715 \nL 390.58387 520.721876 \nL 391.457402 520.957785 \nL 392.330934 521.56648 \nL 393.204467 523.71927 \nL 394.077999 518.099996 \nL 394.951532 517.491162 \nL 395.825064 519.850744 \nL 396.698596 521.852729 \nL 397.572129 521.608501 \nL 398.445661 522.318483 \nL 399.319193 523.246501 \nL 400.192726 521.560984 \nL 401.066258 521.169611 \nL 401.939791 518.705145 \nL 402.813323 519.340159 \nL 403.686855 519.438749 \nL 404.560388 520.337802 \nL 405.43392 518.809997 \nL 406.307452 518.115444 \nL 407.180985 519.763664 \nL 408.054517 518.447994 \nL 408.92805 519.953013 \nL 410.675114 521.227858 \nL 411.548647 519.972608 \nL 412.422179 517.051886 \nL 413.295711 521.308835 \nL 414.169244 520.341398 \nL 415.042776 519.679439 \nL 415.916308 519.916602 \nL 416.789841 519.247304 \nL 417.663373 520.835548 \nL 418.536906 519.709031 \nL 419.410438 521.154973 \nL 420.28397 520.619924 \nL 421.157503 520.370237 \nL 422.031035 518.423834 \nL 422.904567 521.709896 \nL 423.7781 522.202614 \nL 424.651632 518.219442 \nL 425.525165 521.008087 \nL 426.398697 520.211797 \nL 427.272229 519.713659 \nL 428.145762 518.6929 \nL 429.019294 521.393453 \nL 429.892826 523.146815 \nL 430.766359 520.426389 \nL 431.639891 518.861515 \nL 433.386956 523.418845 \nL 434.260488 519.805881 \nL 435.134021 519.574144 \nL 436.007553 517.736011 \nL 436.881085 521.415859 \nL 437.754618 519.680927 \nL 438.62815 519.902673 \nL 439.501683 522.534651 \nL 440.375215 518.984981 \nL 441.248747 520.148624 \nL 442.12228 519.50921 \nL 442.995812 520.834744 \nL 443.869344 520.454304 \nL 444.742877 518.672026 \nL 445.616409 521.935587 \nL 446.489942 520.508702 \nL 447.363474 520.422907 \nL 448.237006 518.930614 \nL 449.110539 520.02531 \nL 449.984071 521.53364 \nL 451.731136 518.045174 \nL 452.604668 520.803505 \nL 453.4782 521.914561 \nL 454.351733 520.141267 \nL 455.225265 520.19854 \nL 456.098798 523.060774 \nL 457.845862 519.582146 \nL 458.719395 518.663663 \nL 459.592927 522.220981 \nL 460.466459 520.563417 \nL 461.339992 518.214605 \nL 462.213524 520.743757 \nL 463.960589 519.578867 \nL 464.834121 520.11075 \nL 465.707654 519.703396 \nL 466.581186 518.695389 \nL 467.454718 517.960689 \nL 468.328251 518.84299 \nL 469.201783 521.547303 \nL 470.075316 521.29535 \nL 471.82238 521.139512 \nL 474.442977 519.688676 \nL 475.31651 520.014046 \nL 476.190042 521.8383 \nL 478.810639 520.563202 \nL 479.684172 520.676696 \nL 480.557704 521.440127 \nL 481.431236 523.086301 \nL 482.304769 522.831012 \nL 483.178301 519.8843 \nL 484.051833 522.71941 \nL 484.925366 519.931322 \nL 485.798898 520.632795 \nL 486.672431 522.843592 \nL 487.545963 520.767993 \nL 488.419495 520.395297 \nL 490.16656 520.168662 \nL 491.040092 522.987046 \nL 491.913625 523.055113 \nL 492.787157 521.105291 \nL 493.66069 521.420715 \nL 494.534222 519.124579 \nL 495.407754 524.716673 \nL 496.281287 515.799074 \nL 497.154819 522.158973 \nL 498.028351 518.334836 \nL 498.901884 519.138274 \nL 499.775416 521.618485 \nL 500.648949 522.336312 \nL 501.522481 520.440166 \nL 502.396013 520.0367 \nL 503.269546 520.25404 \nL 504.143078 518.709298 \nL 505.01661 521.498887 \nL 505.890143 521.67006 \nL 506.763675 521.718171 \nL 507.637208 521.156454 \nL 508.51074 525.357264 \nL 509.384272 521.407407 \nL 510.257805 521.866158 \nL 511.131337 520.063171 \nL 512.004869 520.974424 \nL 512.878402 523.110303 \nL 513.751934 523.478599 \nL 514.625466 520.423642 \nL 516.372531 519.586192 \nL 517.246064 519.530724 \nL 518.119596 522.987261 \nL 518.993128 519.818214 \nL 519.866661 521.296667 \nL 520.740193 522.233384 \nL 521.613725 520.506398 \nL 522.487258 520.77818 \nL 523.36079 518.656217 \nL 524.234323 522.234916 \nL 525.107855 519.551573 \nL 525.981387 522.186261 \nL 526.85492 519.780182 \nL 527.728452 519.885338 \nL 528.601984 520.697779 \nL 529.475517 523.265185 \nL 530.349049 520.224333 \nL 531.222582 522.992858 \nL 532.096114 519.959281 \nL 532.969646 521.772329 \nL 533.843179 520.393144 \nL 534.716711 518.528997 \nL 535.590243 519.96893 \nL 536.463776 522.261805 \nL 537.337308 524.03565 \nL 538.210841 519.217263 \nL 539.084373 520.505454 \nL 539.957905 523.323015 \nL 540.831438 521.354851 \nL 541.70497 518.539829 \nL 542.578502 519.591548 \nL 543.452035 521.792753 \nL 544.325567 519.680667 \nL 545.1991 522.893489 \nL 546.072632 521.404697 \nL 546.946164 520.469898 \nL 547.819697 520.559118 \nL 548.693229 519.140812 \nL 549.566761 519.24538 \nL 550.440294 520.953797 \nL 551.313826 521.976708 \nL 552.187358 521.16899 \nL 553.060891 525.341328 \nL 554.807956 517.836134 \nL 555.681488 519.084027 \nL 556.55502 521.379062 \nL 557.428553 519.473153 \nL 558.302085 521.105646 \nL 559.175617 519.923053 \nL 560.04915 521.406571 \nL 560.922682 523.464405 \nL 561.796215 520.529488 \nL 562.669747 520.60426 \nL 563.543279 520.315364 \nL 564.416812 521.319586 \nL 565.290344 521.046195 \nL 566.163876 521.531911 \nL 567.037409 519.702801 \nL 567.910941 523.035341 \nL 568.784474 519.734375 \nL 569.658006 522.213422 \nL 570.531538 518.76811 \nL 571.405071 522.890064 \nL 574.025668 520.946535 \nL 574.8992 519.747177 \nL 575.772733 520.369838 \nL 576.646265 522.443184 \nL 577.519797 521.7886 \nL 578.39333 519.565736 \nL 579.266862 521.13287 \nL 580.140394 520.656981 \nL 581.013927 520.887173 \nL 581.887459 523.007705 \nL 582.760991 519.54369 \nL 583.634524 518.183999 \nL 584.508056 521.268809 \nL 585.381589 521.684723 \nL 586.255121 520.909383 \nL 587.128653 520.488968 \nL 588.002186 519.275928 \nL 588.875718 520.877492 \nL 589.74925 520.432125 \nL 590.622783 520.400811 \nL 591.496315 517.454954 \nL 592.369848 521.194252 \nL 593.24338 520.137968 \nL 594.116912 522.037292 \nL 595.863977 521.004725 \nL 596.737509 521.910141 \nL 597.611042 521.390984 \nL 598.484574 524.582513 \nL 599.358107 519.579215 \nL 600.231639 519.638425 \nL 601.105171 520.277396 \nL 601.978704 520.322386 \nL 602.852236 520.74615 \nL 603.725768 517.801426 \nL 604.599301 523.203791 \nL 605.472833 522.567371 \nL 606.346366 521.091027 \nL 607.219898 517.829923 \nL 608.09343 524.469557 \nL 608.966963 519.683807 \nL 609.840495 520.205453 \nL 610.714027 521.22059 \nL 611.58756 521.275647 \nL 612.461092 524.138621 \nL 613.334624 522.359706 \nL 614.208157 522.036178 \nL 615.081689 520.142369 \nL 615.955222 521.875465 \nL 616.828754 520.933967 \nL 617.702286 520.773818 \nL 618.575819 520.98632 \nL 619.449351 521.044062 \nL 620.322883 518.876824 \nL 621.196416 523.620546 \nL 622.069948 524.363584 \nL 622.943481 518.530377 \nL 623.817013 520.559416 \nL 624.690545 520.871136 \nL 625.564078 521.496279 \nL 626.43761 521.624455 \nL 628.184675 520.476349 \nL 629.058207 520.898088 \nL 629.93174 522.408735 \nL 630.805272 523.294917 \nL 631.678804 519.978978 \nL 632.552337 521.031906 \nL 633.425869 520.225707 \nL 634.299401 520.768056 \nL 635.172934 522.741279 \nL 636.046466 522.105195 \nL 636.919999 524.506867 \nL 637.793531 520.768987 \nL 638.667063 523.18895 \nL 640.414128 521.171991 \nL 641.28766 522.379909 \nL 642.161193 522.480266 \nL 643.908258 521.088912 \nL 644.78179 519.697964 \nL 645.655322 522.022623 \nL 646.528855 517.78297 \nL 647.402387 518.348783 \nL 648.275919 517.661587 \nL 649.149452 520.716665 \nL 650.022984 520.461876 \nL 650.896516 521.491113 \nL 651.770049 521.640727 \nL 652.643581 517.68816 \nL 653.517114 523.975642 \nL 654.390646 519.598798 \nL 655.264178 519.825109 \nL 656.137711 520.980388 \nL 657.884775 521.673896 \nL 658.758308 521.089989 \nL 659.63184 520.920589 \nL 660.505373 521.311665 \nL 661.378905 519.24745 \nL 662.252437 518.786825 \nL 663.999502 519.942807 \nL 664.873034 519.874201 \nL 665.746567 521.33292 \nL 666.620099 522.082289 \nL 667.493632 522.319135 \nL 668.367164 522.199227 \nL 669.240696 521.061815 \nL 670.114229 520.713322 \nL 670.987761 520.180192 \nL 671.861293 519.328554 \nL 672.734826 520.627629 \nL 673.608358 522.277298 \nL 674.481891 518.173319 \nL 675.355423 520.702743 \nL 676.228955 521.227713 \nL 677.97602 521.162304 \nL 678.849552 520.211164 \nL 679.723085 520.228461 \nL 680.596617 520.562252 \nL 681.470149 520.444965 \nL 682.343682 520.127572 \nL 683.217214 519.523759 \nL 684.090747 519.86618 \nL 684.964279 521.335066 \nL 685.837811 519.492559 \nL 686.711344 521.414365 \nL 687.584876 520.128092 \nL 688.458408 520.619221 \nL 689.331941 521.411617 \nL 690.205473 520.594339 \nL 691.079006 521.477247 \nL 691.952538 520.035959 \nL 692.82607 521.18523 \nL 693.699603 519.715907 \nL 694.573135 520.871332 \nL 695.446667 521.610926 \nL 696.3202 520.533318 \nL 697.193732 519.976578 \nL 698.067265 517.434909 \nL 698.940797 522.843738 \nL 699.814329 521.320175 \nL 700.687862 521.004365 \nL 701.561394 520.377468 \nL 702.434926 521.499894 \nL 703.308459 519.144016 \nL 704.181991 520.03815 \nL 705.055524 521.499451 \nL 705.929056 522.629475 \nL 706.802588 520.850205 \nL 707.676121 519.35341 \nL 708.549653 520.93386 \nL 709.423185 522.824851 \nL 710.296718 519.297556 \nL 711.17025 521.101638 \nL 712.043782 519.657286 \nL 712.917315 523.426525 \nL 713.790847 521.960836 \nL 714.66438 518.760962 \nL 715.537912 520.100272 \nL 716.411444 520.951518 \nL 717.284977 519.584381 \nL 718.158509 521.17479 \nL 719.032041 521.827753 \nL 719.905574 520.735672 \nL 720.779106 518.281887 \nL 721.652639 521.733341 \nL 723.399703 519.618146 \nL 724.273236 521.787182 \nL 725.146768 522.293601 \nL 726.893833 520.259408 \nL 727.767365 520.553553 \nL 728.640898 521.907242 \nL 729.51443 519.006273 \nL 730.387962 521.942285 \nL 731.261495 522.19106 \nL 732.135027 519.893088 \nL 733.008559 521.77626 \nL 733.882092 520.262251 \nL 734.755624 521.570298 \nL 735.629157 518.976934 \nL 736.502689 520.399235 \nL 737.376221 521.348944 \nL 738.249754 521.772379 \nL 739.123286 522.569948 \nL 739.996818 520.348648 \nL 740.870351 521.207459 \nL 741.743883 517.545219 \nL 742.617416 522.684158 \nL 743.490948 518.452243 \nL 744.36448 520.644445 \nL 745.238013 521.031665 \nL 746.111545 518.387474 \nL 746.985077 520.208119 \nL 747.85861 521.181203 \nL 748.732142 520.165383 \nL 749.605674 521.611476 \nL 750.479207 519.980067 \nL 751.352739 520.084482 \nL 752.226272 522.175694 \nL 753.099804 520.165775 \nL 753.973336 521.062625 \nL 754.846869 521.791487 \nL 755.720401 522.371438 \nL 756.593933 520.173822 \nL 757.467466 522.47767 \nL 758.340998 520.816085 \nL 760.088063 521.280516 \nL 760.961595 519.673418 \nL 762.70866 523.882123 \nL 763.582192 521.642974 \nL 764.455725 522.431958 \nL 765.329257 522.25001 \nL 766.20279 519.928036 \nL 767.076322 522.256937 \nL 767.949854 522.027333 \nL 768.823387 524.108776 \nL 769.696919 521.195968 \nL 770.570451 521.415315 \nL 771.443984 519.861051 \nL 772.317516 521.661519 \nL 773.191049 522.14638 \nL 774.064581 523.340692 \nL 774.938113 522.327967 \nL 775.811646 520.955532 \nL 776.685178 523.076481 \nL 777.55871 521.909401 \nL 778.432243 518.952995 \nL 779.305775 521.203869 \nL 780.179307 521.178513 \nL 781.05284 519.487538 \nL 781.926372 522.138004 \nL 783.673437 520.333788 \nL 785.420502 522.497683 \nL 786.294034 520.234444 \nL 786.294034 520.234444 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p15a2e0b5ee)\" d=\"M 76.112216 118.998046 \nL 77.859281 205.83295 \nL 79.606345 285.445799 \nL 81.35341 356.345835 \nL 83.100475 415.88946 \nL 83.974007 440.277952 \nL 84.84754 460.596376 \nL 85.721072 477.305824 \nL 86.594604 490.988418 \nL 87.468137 500.93911 \nL 88.341669 508.447115 \nL 89.215201 514.189975 \nL 90.088734 518.173743 \nL 90.962266 520.519934 \nL 91.835799 522.207933 \nL 92.709331 523.130424 \nL 93.582863 523.648005 \nL 94.456396 523.902174 \nL 96.20346 524.008641 \nL 102.318187 523.764792 \nL 104.065252 523.890018 \nL 110.179978 523.963435 \nL 111.053511 524.143015 \nL 111.927043 524.138735 \nL 112.800576 524.270578 \nL 116.294705 524.302215 \nL 117.168237 524.400299 \nL 118.04177 524.325805 \nL 120.662367 524.515775 \nL 122.409432 524.470697 \nL 126.777093 524.74812 \nL 127.650626 524.693773 \nL 128.524158 524.856087 \nL 131.144755 524.744321 \nL 132.89182 524.981282 \nL 135.512417 525.098866 \nL 139.006547 525.348996 \nL 140.753611 525.405129 \nL 142.500676 525.463022 \nL 144.247741 525.609451 \nL 148.615403 525.782896 \nL 149.488935 526.013937 \nL 150.362468 525.975183 \nL 151.236 526.093996 \nL 152.109532 525.995697 \nL 155.603662 526.22266 \nL 157.350726 526.392382 \nL 160.844856 526.420848 \nL 161.718388 526.590754 \nL 162.591921 526.470681 \nL 163.465453 526.643936 \nL 164.338985 526.614604 \nL 165.212518 526.769859 \nL 167.833115 526.821009 \nL 169.58018 527.106714 \nL 170.453712 527.112108 \nL 171.327244 527.294189 \nL 172.200777 527.175985 \nL 173.074309 527.367917 \nL 175.694906 527.396053 \nL 176.568439 527.573386 \nL 178.315503 527.69582 \nL 180.062568 527.763754 \nL 180.936101 527.674439 \nL 181.809633 527.913394 \nL 183.556698 528.026382 \nL 185.303762 528.25073 \nL 187.050827 528.292548 \nL 188.797892 528.415653 \nL 189.671424 528.27936 \nL 190.544957 528.380319 \nL 191.418489 528.745867 \nL 195.786151 528.816948 \nL 197.533216 529.103464 \nL 198.406748 529.031832 \nL 200.153813 529.305893 \nL 201.027345 529.243074 \nL 203.647942 529.4939 \nL 204.521475 529.649377 \nL 205.395007 529.643185 \nL 206.268539 529.834428 \nL 207.142072 529.561923 \nL 208.015604 529.86792 \nL 210.636201 529.941616 \nL 213.256798 530.346532 \nL 215.003863 530.559219 \nL 215.877395 530.369091 \nL 216.750928 530.563594 \nL 222.865654 531.098079 \nL 223.739187 530.972606 \nL 226.359784 530.98078 \nL 228.106849 531.467205 \nL 230.727446 531.313254 \nL 231.600978 531.702393 \nL 233.348043 532.009991 \nL 235.96864 531.978005 \nL 236.842172 532.162765 \nL 237.715705 532.056311 \nL 240.336302 532.41649 \nL 241.209834 532.429596 \nL 242.083367 532.222766 \nL 242.956899 532.668025 \nL 243.830431 532.560754 \nL 244.703964 532.651646 \nL 245.577496 532.947329 \nL 246.451028 532.793365 \nL 247.324561 532.984905 \nL 248.198093 532.814031 \nL 249.071626 533.05586 \nL 249.945158 533.046496 \nL 251.692223 533.318842 \nL 252.565755 533.269477 \nL 254.31282 533.564552 \nL 255.186352 533.469811 \nL 259.554014 534.104672 \nL 260.427546 533.964789 \nL 263.048143 534.347724 \nL 263.921676 534.324671 \nL 264.795208 534.177153 \nL 267.415805 534.602082 \nL 268.289338 534.671106 \nL 270.036402 534.57192 \nL 270.909935 534.818055 \nL 271.783467 534.561201 \nL 272.657 534.79285 \nL 276.151129 535.11085 \nL 277.024661 534.999066 \nL 278.771726 535.324575 \nL 279.645259 535.338776 \nL 280.518791 535.475221 \nL 281.392323 535.437069 \nL 284.886453 535.677581 \nL 286.633517 535.766422 \nL 303.230633 536.701576 \nL 304.104165 536.599414 \nL 309.345359 537.126537 \nL 310.218892 537.059096 \nL 311.092424 537.169311 \nL 311.965956 537.051195 \nL 313.713021 537.203683 \nL 316.333618 537.434129 \nL 317.20715 537.378869 \nL 318.080683 537.450736 \nL 318.954215 537.304445 \nL 319.827748 537.406607 \nL 320.70128 537.651443 \nL 321.574812 537.533948 \nL 325.068942 537.702055 \nL 325.942474 537.645568 \nL 328.563071 537.958276 \nL 329.436604 537.772871 \nL 330.310136 538.053093 \nL 333.804266 538.247621 \nL 334.677798 538.07938 \nL 336.424863 538.276713 \nL 339.04546 538.370834 \nL 340.792525 538.517016 \nL 342.539589 538.41212 \nL 343.413122 538.58528 \nL 344.286654 538.626427 \nL 346.033719 538.508551 \nL 348.654316 538.768969 \nL 351.274913 538.753838 \nL 352.148445 538.825153 \nL 353.021978 538.698141 \nL 357.38964 539.025063 \nL 358.263172 538.898931 \nL 359.136704 538.93412 \nL 360.010237 539.087571 \nL 361.757301 538.970842 \nL 363.504366 539.132099 \nL 366.998496 539.176456 \nL 367.872028 539.222662 \nL 369.619093 539.141577 \nL 370.492625 539.289583 \nL 372.23969 539.173556 \nL 373.113222 539.326501 \nL 373.986755 539.365084 \nL 374.860287 539.222801 \nL 375.733819 539.383362 \nL 378.354417 539.378335 \nL 379.227949 539.280909 \nL 381.848546 539.602183 \nL 383.595611 539.461882 \nL 384.469143 539.642583 \nL 387.08974 539.49835 \nL 387.963273 539.731031 \nL 389.710337 539.670282 \nL 390.58387 539.66875 \nL 391.457402 539.551774 \nL 392.330934 539.739363 \nL 398.445661 539.748771 \nL 399.319193 539.654359 \nL 400.192726 539.827171 \nL 414.169244 540.159246 \nL 415.042776 539.979692 \nL 415.916308 540.161684 \nL 424.651632 540.200096 \nL 427.272229 540.325936 \nL 430.766359 540.257875 \nL 431.639891 540.311545 \nL 432.513424 540.208808 \nL 433.386956 540.348608 \nL 434.260488 540.247631 \nL 436.007553 540.354173 \nL 437.754618 540.419012 \nL 438.62815 540.337123 \nL 440.375215 540.427483 \nL 441.248747 540.457854 \nL 442.12228 540.326512 \nL 443.869344 540.483477 \nL 448.237006 540.335218 \nL 449.984071 540.539977 \nL 451.731136 540.363677 \nL 453.4782 540.514988 \nL 460.466459 540.558262 \nL 461.339992 540.624937 \nL 462.213524 540.53804 \nL 465.707654 540.642803 \nL 468.328251 540.500692 \nL 470.948848 540.562928 \nL 471.82238 540.479191 \nL 472.695913 540.649723 \nL 473.569445 540.531918 \nL 474.442977 540.644563 \nL 475.31651 540.550474 \nL 477.937107 540.678835 \nL 482.304769 540.677853 \nL 484.925366 540.717487 \nL 485.798898 540.75398 \nL 486.672431 540.644855 \nL 490.16656 540.740659 \nL 491.040092 540.660487 \nL 495.407754 540.777033 \nL 498.901884 540.764845 \nL 501.522481 540.814203 \nL 505.890143 540.745389 \nL 506.763675 540.818395 \nL 507.637208 540.753132 \nL 510.257805 540.818439 \nL 511.131337 540.815628 \nL 512.004869 540.678974 \nL 513.751934 540.782952 \nL 514.625466 540.754816 \nL 516.372531 540.89097 \nL 517.246064 540.79702 \nL 518.119596 540.84648 \nL 518.993128 540.767757 \nL 520.740193 540.910072 \nL 523.36079 540.887019 \nL 525.981387 540.86165 \nL 527.728452 540.957259 \nL 528.601984 540.858598 \nL 529.475517 540.954207 \nL 531.222582 540.87613 \nL 533.843179 540.910255 \nL 536.463776 540.978506 \nL 537.337308 540.766928 \nL 541.70497 541.01003 \nL 546.072632 540.920157 \nL 546.946164 541.036786 \nL 547.819697 541.008168 \nL 548.693229 540.858231 \nL 551.313826 541.025953 \nL 553.060891 540.93855 \nL 553.934423 540.956562 \nL 554.807956 540.846436 \nL 556.55502 540.897947 \nL 560.922682 541.042465 \nL 561.796215 540.924 \nL 563.543279 541.029074 \nL 564.416812 540.926109 \nL 565.290344 540.948749 \nL 566.163876 541.091994 \nL 569.658006 540.960627 \nL 571.405071 541.045238 \nL 576.646265 541.028207 \nL 577.519797 540.759932 \nL 578.39333 541.038767 \nL 580.140394 541.014632 \nL 593.24338 540.997133 \nL 594.116912 540.916966 \nL 596.737509 541.055969 \nL 598.484574 540.993499 \nL 601.105171 541.111324 \nL 603.725768 541.065681 \nL 604.599301 540.905513 \nL 608.966963 541.125607 \nL 614.208157 541.060927 \nL 615.955222 541.086226 \nL 631.678804 541.103156 \nL 633.425869 541.041274 \nL 634.299401 541.156592 \nL 642.161193 540.945615 \nL 643.908258 541.106195 \nL 645.655322 540.968484 \nL 647.402387 541.134477 \nL 650.022984 541.139017 \nL 652.643581 541.02216 \nL 654.390646 541.10251 \nL 656.137711 540.934795 \nL 657.011243 541.160499 \nL 658.758308 541.127551 \nL 660.505373 541.046732 \nL 662.252437 541.152521 \nL 668.367164 541.035925 \nL 670.114229 541.144423 \nL 676.228955 541.143467 \nL 678.849552 541.002932 \nL 680.596617 541.164975 \nL 682.343682 541.09131 \nL 683.217214 541.161651 \nL 684.090747 541.054969 \nL 685.837811 541.133629 \nL 690.205473 541.114173 \nL 691.079006 541.016874 \nL 692.82607 541.182924 \nL 696.3202 541.14386 \nL 698.067265 541.117978 \nL 698.940797 541.197802 \nL 700.687862 541.105771 \nL 701.561394 541.181373 \nL 702.434926 541.085676 \nL 703.308459 541.167818 \nL 704.181991 541.027409 \nL 706.802588 541.019235 \nL 707.676121 541.145633 \nL 710.296718 541.10265 \nL 712.917315 541.088974 \nL 715.537912 541.159562 \nL 717.284977 540.912965 \nL 718.158509 541.12776 \nL 724.273236 541.12586 \nL 727.767365 541.129273 \nL 730.387962 541.080269 \nL 731.261495 541.163841 \nL 733.882092 541.103644 \nL 734.755624 541.155161 \nL 735.629157 541.086195 \nL 737.376221 541.191357 \nL 738.249754 541.080136 \nL 739.123286 541.120396 \nL 739.996818 541.004889 \nL 740.870351 541.0764 \nL 741.743883 540.988193 \nL 742.617416 541.059958 \nL 743.490948 540.993917 \nL 744.36448 541.104081 \nL 746.111545 541.068018 \nL 748.732142 541.074248 \nL 752.226272 541.121992 \nL 753.099804 541.138947 \nL 753.973336 541.034633 \nL 760.088063 541.085549 \nL 760.961595 541.034589 \nL 761.835128 541.156092 \nL 765.329257 541.01619 \nL 767.949854 541.15758 \nL 774.938113 541.073899 \nL 775.811646 541.146487 \nL 776.685178 541.083365 \nL 779.305775 541.133496 \nL 781.926372 541.109785 \nL 783.673437 541.209034 \nL 786.294034 541.101428 \nL 786.294034 541.101428 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Model MSE -->\n    <g transform=\"translate(398.503125 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"481.787109\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_21\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_22\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p15a2e0b5ee\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "7/7 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}