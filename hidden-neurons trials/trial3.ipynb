{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1)\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df=pd.read_excel('Preprocessed-data.xlsx')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Date  Temp  Dew Point  Humidity  Wind Speed  Pressure  \\\n",
       "0  2020-01-01   5.7        4.8      94.4        2.47    1029.0   \n",
       "1  2020-01-02   9.3        8.2      92.5        7.42    1021.4   \n",
       "2  2020-01-03   8.2        6.8      91.4        6.81    1021.8   \n",
       "3  2020-01-04   5.6        3.6      87.2        3.94    1033.7   \n",
       "4  2020-01-05   7.8        6.1      89.2        3.33    1033.4   \n",
       "\n",
       "   Wind Direction  Pressure Grad  Wind Gradient  \n",
       "0             128             -8              3  \n",
       "1             188              0              8  \n",
       "2             265             11              7  \n",
       "3             248             -1              2  \n",
       "4             224            -11              5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind Direction</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>94.4</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>128</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>92.5</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>91.4</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>265</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>87.2</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>248</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>224</td>\n",
       "      <td>-11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Date              0\n",
       "Temp              0\n",
       "Dew Point         0\n",
       "Humidity          0\n",
       "Wind Speed        0\n",
       "Pressure          0\n",
       "Wind Direction    0\n",
       "Pressure Grad     0\n",
       "Wind Gradient     0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "df['Year']  = df['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "df['Month'] = df['Date'].apply(lambda x: int(str(x)[5:7]))\n",
    "df['Day']=df['Date'].apply(lambda x: int(str(x)[8:10]))\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Date  Temp  Dew Point  Humidity  Wind Speed  Pressure  \\\n",
       "0  2020-01-01   5.7        4.8      94.4        2.47    1029.0   \n",
       "1  2020-01-02   9.3        8.2      92.5        7.42    1021.4   \n",
       "2  2020-01-03   8.2        6.8      91.4        6.81    1021.8   \n",
       "3  2020-01-04   5.6        3.6      87.2        3.94    1033.7   \n",
       "4  2020-01-05   7.8        6.1      89.2        3.33    1033.4   \n",
       "\n",
       "   Wind Direction  Pressure Grad  Wind Gradient  Year  Month  Day  \n",
       "0             128             -8              3  2020      1    1  \n",
       "1             188              0              8  2020      1    2  \n",
       "2             265             11              7  2020      1    3  \n",
       "3             248             -1              2  2020      1    4  \n",
       "4             224            -11              5  2020      1    5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind Direction</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>94.4</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>128</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>92.5</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>91.4</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>265</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>87.2</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>248</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>224</td>\n",
       "      <td>-11</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "del df['Date']\n",
    "#del df['Year']\n",
    "#del df['Month']\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Temp  Dew Point  Humidity  Wind Speed  Pressure  Wind Direction  \\\n",
       "0   5.7        4.8      94.4        2.47    1029.0             128   \n",
       "1   9.3        8.2      92.5        7.42    1021.4             188   \n",
       "2   8.2        6.8      91.4        6.81    1021.8             265   \n",
       "3   5.6        3.6      87.2        3.94    1033.7             248   \n",
       "4   7.8        6.1      89.2        3.33    1033.4             224   \n",
       "\n",
       "   Pressure Grad  Wind Gradient  Year  Month  Day  \n",
       "0             -8              3  2020      1    1  \n",
       "1              0              8  2020      1    2  \n",
       "2             11              7  2020      1    3  \n",
       "3             -1              2  2020      1    4  \n",
       "4            -11              5  2020      1    5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind Direction</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>94.4</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>128</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>92.5</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>91.4</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>265</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>87.2</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>248</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>224</td>\n",
       "      <td>-11</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "X = df.drop(['Wind Speed'], axis=1)\n",
    "#Assign the Target column as the output \n",
    "Y= df['Wind Speed']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "X_norm=(X-X.min())/(X.max()-X.min())\n",
    "X_norm"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Temp  Dew Point  Humidity  Pressure  Wind Direction  Pressure Grad  \\\n",
       "0     0.299639   0.487805  0.923913  0.752055        0.342679       0.449275   \n",
       "1     0.429603   0.606272  0.898098  0.647945        0.529595       0.565217   \n",
       "2     0.389892   0.557491  0.883152  0.653425        0.769470       0.724638   \n",
       "3     0.296029   0.445993  0.826087  0.816438        0.716511       0.550725   \n",
       "4     0.375451   0.533101  0.853261  0.812329        0.641745       0.405797   \n",
       "...        ...        ...       ...       ...             ...            ...   \n",
       "1091  0.198556   0.411150  0.972826  0.706849        0.676012       0.623188   \n",
       "1092  0.259928   0.470383  0.966033  0.767123        0.757009       0.594203   \n",
       "1093  0.415162   0.595819  0.915761  0.795890        0.738318       0.594203   \n",
       "1094  0.346570   0.564460  0.998641  0.831507        0.753894       0.565217   \n",
       "1095  0.397112   0.581882  0.913043  0.836986        0.816199       0.594203   \n",
       "\n",
       "      Wind Gradient  Year  Month       Day  \n",
       "0          0.200000   1.0    0.0  0.000000  \n",
       "1          0.533333   1.0    0.0  0.033333  \n",
       "2          0.466667   1.0    0.0  0.066667  \n",
       "3          0.133333   1.0    0.0  0.100000  \n",
       "4          0.333333   1.0    0.0  0.133333  \n",
       "...             ...   ...    ...       ...  \n",
       "1091       0.066667   0.0    1.0  0.866667  \n",
       "1092       0.200000   0.0    1.0  0.900000  \n",
       "1093       0.266667   0.0    1.0  0.933333  \n",
       "1094       0.200000   0.0    1.0  0.966667  \n",
       "1095       0.200000   0.0    1.0  1.000000  \n",
       "\n",
       "[1096 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind Direction</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299639</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.752055</td>\n",
       "      <td>0.342679</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.429603</td>\n",
       "      <td>0.606272</td>\n",
       "      <td>0.898098</td>\n",
       "      <td>0.647945</td>\n",
       "      <td>0.529595</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.389892</td>\n",
       "      <td>0.557491</td>\n",
       "      <td>0.883152</td>\n",
       "      <td>0.653425</td>\n",
       "      <td>0.769470</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.296029</td>\n",
       "      <td>0.445993</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.716511</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375451</td>\n",
       "      <td>0.533101</td>\n",
       "      <td>0.853261</td>\n",
       "      <td>0.812329</td>\n",
       "      <td>0.641745</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.198556</td>\n",
       "      <td>0.411150</td>\n",
       "      <td>0.972826</td>\n",
       "      <td>0.706849</td>\n",
       "      <td>0.676012</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.259928</td>\n",
       "      <td>0.470383</td>\n",
       "      <td>0.966033</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.415162</td>\n",
       "      <td>0.595819</td>\n",
       "      <td>0.915761</td>\n",
       "      <td>0.795890</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.346570</td>\n",
       "      <td>0.564460</td>\n",
       "      <td>0.998641</td>\n",
       "      <td>0.831507</td>\n",
       "      <td>0.753894</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.397112</td>\n",
       "      <td>0.581882</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.836986</td>\n",
       "      <td>0.816199</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_norm, Y, test_size=0.3, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_train.shape[1], activation=\"sigmoid\", kernel_initializer='normal'))\n",
    "model.add(Dropout(0.2)) #dropping a few neurons for generalizing the model\n",
    "model.add(Dense(1, activation=\"linear\", kernel_initializer='normal'))\n",
    "adam = Adam(learning_rate=1e-3, decay=1e-3)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mse','mae'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "print('Fit model...')\n",
    "filepath=\"/home/m-marouni/Documents/CE-901/Heathrow/best_weights\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_mae', patience=100, verbose=1, mode='min')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "log = model.fit(x_train, y_train,\n",
    "          validation_split=0.40, batch_size=30, epochs=1000, shuffle=True, callbacks=callbacks_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fit model...\n",
      "Epoch 1/1000\n",
      "16/16 [==============================] - 1s 19ms/step - loss: 43.2680 - mse: 43.2680 - mae: 6.0316 - val_loss: 36.3023 - val_mse: 36.3023 - val_mae: 5.5149\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 5.51492, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 2/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 40.8408 - mse: 40.8408 - mae: 5.8643 - val_loss: 33.1300 - val_mse: 33.1300 - val_mae: 5.2193\n",
      "\n",
      "Epoch 00002: val_mae improved from 5.51492 to 5.21931, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 3/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 37.5189 - mse: 37.5189 - mae: 5.5960 - val_loss: 30.0489 - val_mse: 30.0489 - val_mae: 4.9152\n",
      "\n",
      "Epoch 00003: val_mae improved from 5.21931 to 4.91522, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 4/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 31.1529 - mse: 31.1529 - mae: 5.0470 - val_loss: 27.0098 - val_mse: 27.0098 - val_mae: 4.5956\n",
      "\n",
      "Epoch 00004: val_mae improved from 4.91522 to 4.59564, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 5/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 29.8678 - mse: 29.8678 - mae: 4.8791 - val_loss: 23.9602 - val_mse: 23.9602 - val_mae: 4.2527\n",
      "\n",
      "Epoch 00005: val_mae improved from 4.59564 to 4.25268, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 6/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 25.3367 - mse: 25.3367 - mae: 4.3983 - val_loss: 21.0394 - val_mse: 21.0394 - val_mae: 3.8965\n",
      "\n",
      "Epoch 00006: val_mae improved from 4.25268 to 3.89652, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 7/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 23.2135 - mse: 23.2135 - mae: 4.1106 - val_loss: 18.2266 - val_mse: 18.2266 - val_mae: 3.5248\n",
      "\n",
      "Epoch 00007: val_mae improved from 3.89652 to 3.52479, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 8/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 20.7520 - mse: 20.7520 - mae: 3.8118 - val_loss: 15.6390 - val_mse: 15.6390 - val_mae: 3.1575\n",
      "\n",
      "Epoch 00008: val_mae improved from 3.52479 to 3.15754, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 9/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 17.7939 - mse: 17.7939 - mae: 3.3665 - val_loss: 13.3241 - val_mse: 13.3241 - val_mae: 2.8185\n",
      "\n",
      "Epoch 00009: val_mae improved from 3.15754 to 2.81848, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 10/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 13.2642 - mse: 13.2642 - mae: 2.7945 - val_loss: 11.4139 - val_mse: 11.4139 - val_mae: 2.5487\n",
      "\n",
      "Epoch 00010: val_mae improved from 2.81848 to 2.54874, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 11/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 12.4614 - mse: 12.4614 - mae: 2.7038 - val_loss: 9.7942 - val_mse: 9.7942 - val_mae: 2.3377\n",
      "\n",
      "Epoch 00011: val_mae improved from 2.54874 to 2.33774, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 12/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 11.1550 - mse: 11.1550 - mae: 2.4610 - val_loss: 8.5428 - val_mse: 8.5428 - val_mae: 2.1916\n",
      "\n",
      "Epoch 00012: val_mae improved from 2.33774 to 2.19162, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 13/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9.9560 - mse: 9.9560 - mae: 2.2685 - val_loss: 7.6227 - val_mse: 7.6227 - val_mae: 2.0893\n",
      "\n",
      "Epoch 00013: val_mae improved from 2.19162 to 2.08928, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 14/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.4967 - mse: 7.4967 - mae: 2.0212 - val_loss: 6.9725 - val_mse: 6.9725 - val_mae: 2.0167\n",
      "\n",
      "Epoch 00014: val_mae improved from 2.08928 to 2.01673, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 15/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.5831 - mse: 7.5831 - mae: 2.0344 - val_loss: 6.5356 - val_mse: 6.5356 - val_mae: 1.9709\n",
      "\n",
      "Epoch 00015: val_mae improved from 2.01673 to 1.97092, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 16/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.6233 - mse: 7.6233 - mae: 2.0713 - val_loss: 6.2398 - val_mse: 6.2398 - val_mae: 1.9507\n",
      "\n",
      "Epoch 00016: val_mae improved from 1.97092 to 1.95068, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 17/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 8.3927 - mse: 8.3927 - mae: 2.1021 - val_loss: 6.0692 - val_mse: 6.0692 - val_mae: 1.9460\n",
      "\n",
      "Epoch 00017: val_mae improved from 1.95068 to 1.94602, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 18/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1468 - mse: 6.1468 - mae: 1.9555 - val_loss: 5.9797 - val_mse: 5.9797 - val_mae: 1.9465\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 1.94602\n",
      "Epoch 19/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3226 - mse: 7.3226 - mae: 2.0547 - val_loss: 5.9262 - val_mse: 5.9262 - val_mae: 1.9530\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 1.94602\n",
      "Epoch 20/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9448 - mse: 5.9448 - mae: 1.9353 - val_loss: 5.9088 - val_mse: 5.9088 - val_mae: 1.9588\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 1.94602\n",
      "Epoch 21/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.9044 - mse: 7.9044 - mae: 2.1283 - val_loss: 5.9073 - val_mse: 5.9073 - val_mae: 1.9664\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 1.94602\n",
      "Epoch 22/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.9991 - mse: 5.9991 - mae: 1.9601 - val_loss: 5.9098 - val_mse: 5.9098 - val_mae: 1.9708\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 1.94602\n",
      "Epoch 23/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.0958 - mse: 7.0958 - mae: 2.0791 - val_loss: 5.9233 - val_mse: 5.9233 - val_mae: 1.9809\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 1.94602\n",
      "Epoch 24/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.7946 - mse: 6.7946 - mae: 2.1019 - val_loss: 5.9301 - val_mse: 5.9301 - val_mae: 1.9849\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 1.94602\n",
      "Epoch 25/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.0890 - mse: 6.0890 - mae: 1.9927 - val_loss: 5.9312 - val_mse: 5.9312 - val_mae: 1.9859\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 1.94602\n",
      "Epoch 26/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1964 - mse: 6.1964 - mae: 1.9716 - val_loss: 5.9383 - val_mse: 5.9383 - val_mae: 1.9894\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 1.94602\n",
      "Epoch 27/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4600 - mse: 6.4600 - mae: 2.0126 - val_loss: 5.9458 - val_mse: 5.9458 - val_mae: 1.9928\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 1.94602\n",
      "Epoch 28/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.4956 - mse: 7.4956 - mae: 2.1300 - val_loss: 5.9542 - val_mse: 5.9542 - val_mae: 1.9965\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 1.94602\n",
      "Epoch 29/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9545 - mse: 5.9545 - mae: 1.8906 - val_loss: 5.9587 - val_mse: 5.9587 - val_mae: 1.9987\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 1.94602\n",
      "Epoch 30/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.4301 - mse: 6.4301 - mae: 2.0606 - val_loss: 5.9558 - val_mse: 5.9558 - val_mae: 1.9981\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 1.94602\n",
      "Epoch 31/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8735 - mse: 6.8735 - mae: 2.0554 - val_loss: 5.9559 - val_mse: 5.9559 - val_mae: 1.9986\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 1.94602\n",
      "Epoch 32/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 6.8205 - mse: 6.8205 - mae: 2.0311 - val_loss: 5.9472 - val_mse: 5.9472 - val_mae: 1.9957\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 1.94602\n",
      "Epoch 33/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.7235 - mse: 5.7235 - mae: 1.8763 - val_loss: 5.9497 - val_mse: 5.9497 - val_mae: 1.9972\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 1.94602\n",
      "Epoch 34/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.6724 - mse: 6.6724 - mae: 2.0364 - val_loss: 5.9510 - val_mse: 5.9510 - val_mae: 1.9982\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 1.94602\n",
      "Epoch 35/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.1233 - mse: 7.1233 - mae: 2.1463 - val_loss: 5.9450 - val_mse: 5.9450 - val_mae: 1.9965\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 1.94602\n",
      "Epoch 36/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3069 - mse: 5.3069 - mae: 1.8338 - val_loss: 5.9398 - val_mse: 5.9398 - val_mae: 1.9950\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 1.94602\n",
      "Epoch 37/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3434 - mse: 6.3434 - mae: 1.9974 - val_loss: 5.9398 - val_mse: 5.9398 - val_mae: 1.9956\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 1.94602\n",
      "Epoch 38/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.7198 - mse: 6.7198 - mae: 2.0872 - val_loss: 5.9356 - val_mse: 5.9356 - val_mae: 1.9945\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 1.94602\n",
      "Epoch 39/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4908 - mse: 6.4908 - mae: 2.0340 - val_loss: 5.9421 - val_mse: 5.9421 - val_mae: 1.9975\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 1.94602\n",
      "Epoch 40/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9376 - mse: 5.9376 - mae: 1.9411 - val_loss: 5.9433 - val_mse: 5.9433 - val_mae: 1.9985\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 1.94602\n",
      "Epoch 41/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.9083 - mse: 6.9083 - mae: 2.1072 - val_loss: 5.9381 - val_mse: 5.9381 - val_mae: 1.9972\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 1.94602\n",
      "Epoch 42/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0933 - mse: 6.0933 - mae: 1.9866 - val_loss: 5.9158 - val_mse: 5.9158 - val_mae: 1.9893\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 1.94602\n",
      "Epoch 43/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3683 - mse: 6.3683 - mae: 1.9908 - val_loss: 5.9349 - val_mse: 5.9349 - val_mae: 1.9970\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 1.94602\n",
      "Epoch 44/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.2546 - mse: 7.2546 - mae: 2.1394 - val_loss: 5.9391 - val_mse: 5.9391 - val_mae: 1.9990\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 1.94602\n",
      "Epoch 45/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5293 - mse: 6.5293 - mae: 2.0296 - val_loss: 5.9270 - val_mse: 5.9270 - val_mae: 1.9953\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 1.94602\n",
      "Epoch 46/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.0754 - mse: 7.0754 - mae: 2.1134 - val_loss: 5.9269 - val_mse: 5.9269 - val_mae: 1.9959\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 1.94602\n",
      "Epoch 47/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5887 - mse: 6.5887 - mae: 2.0476 - val_loss: 5.9155 - val_mse: 5.9155 - val_mae: 1.9923\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 1.94602\n",
      "Epoch 48/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.8023 - mse: 6.8023 - mae: 2.0542 - val_loss: 5.9028 - val_mse: 5.9028 - val_mae: 1.9881\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 1.94602\n",
      "Epoch 49/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.6061 - mse: 6.6061 - mae: 2.0381 - val_loss: 5.8977 - val_mse: 5.8977 - val_mae: 1.9868\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 1.94602\n",
      "Epoch 50/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.5042 - mse: 6.5042 - mae: 2.0360 - val_loss: 5.8968 - val_mse: 5.8968 - val_mae: 1.9872\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 1.94602\n",
      "Epoch 51/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8877 - mse: 6.8877 - mae: 2.0451 - val_loss: 5.8871 - val_mse: 5.8871 - val_mae: 1.9843\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 1.94602\n",
      "Epoch 52/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.2535 - mse: 7.2535 - mae: 2.1297 - val_loss: 5.8854 - val_mse: 5.8854 - val_mae: 1.9843\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 1.94602\n",
      "Epoch 53/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1058 - mse: 6.1058 - mae: 1.9698 - val_loss: 5.8771 - val_mse: 5.8771 - val_mae: 1.9820\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 1.94602\n",
      "Epoch 54/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.7313 - mse: 6.7313 - mae: 2.0215 - val_loss: 5.8789 - val_mse: 5.8789 - val_mae: 1.9832\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 1.94602\n",
      "Epoch 55/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2935 - mse: 6.2935 - mae: 1.9682 - val_loss: 5.8663 - val_mse: 5.8663 - val_mae: 1.9794\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 1.94602\n",
      "Epoch 56/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3873 - mse: 7.3873 - mae: 2.0649 - val_loss: 5.8757 - val_mse: 5.8757 - val_mae: 1.9835\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 1.94602\n",
      "Epoch 57/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.0169 - mse: 6.0169 - mae: 1.9420 - val_loss: 5.8764 - val_mse: 5.8764 - val_mae: 1.9845\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 1.94602\n",
      "Epoch 58/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.4633 - mse: 6.4633 - mae: 2.0609 - val_loss: 5.8759 - val_mse: 5.8759 - val_mae: 1.9850\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 1.94602\n",
      "Epoch 59/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.5050 - mse: 6.5050 - mae: 2.0286 - val_loss: 5.8749 - val_mse: 5.8749 - val_mae: 1.9854\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 1.94602\n",
      "Epoch 60/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.9355 - mse: 6.9355 - mae: 2.1251 - val_loss: 5.8714 - val_mse: 5.8714 - val_mae: 1.9848\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 1.94602\n",
      "Epoch 61/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.0087 - mse: 7.0087 - mae: 2.0522 - val_loss: 5.8745 - val_mse: 5.8745 - val_mae: 1.9866\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 1.94602\n",
      "Epoch 62/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9727 - mse: 5.9727 - mae: 1.9781 - val_loss: 5.8616 - val_mse: 5.8616 - val_mae: 1.9827\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 1.94602\n",
      "Epoch 63/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5200 - mse: 6.5200 - mae: 2.0298 - val_loss: 5.8581 - val_mse: 5.8581 - val_mae: 1.9821\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 1.94602\n",
      "Epoch 64/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9566 - mse: 5.9566 - mae: 1.9368 - val_loss: 5.8650 - val_mse: 5.8650 - val_mae: 1.9853\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 1.94602\n",
      "Epoch 65/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8404 - mse: 6.8404 - mae: 2.0925 - val_loss: 5.8339 - val_mse: 5.8339 - val_mae: 1.9751\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 1.94602\n",
      "Epoch 66/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.1482 - mse: 7.1482 - mae: 2.1548 - val_loss: 5.8245 - val_mse: 5.8245 - val_mae: 1.9725\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 1.94602\n",
      "Epoch 67/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9657 - mse: 5.9657 - mae: 1.9115 - val_loss: 5.8050 - val_mse: 5.8050 - val_mae: 1.9659\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 1.94602\n",
      "Epoch 68/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6989 - mse: 5.6989 - mae: 1.9126 - val_loss: 5.8024 - val_mse: 5.8024 - val_mae: 1.9659\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 1.94602\n",
      "Epoch 69/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9735 - mse: 5.9735 - mae: 1.9899 - val_loss: 5.8039 - val_mse: 5.8039 - val_mae: 1.9676\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 1.94602\n",
      "Epoch 70/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.7054 - mse: 6.7054 - mae: 2.0728 - val_loss: 5.8200 - val_mse: 5.8200 - val_mae: 1.9743\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 1.94602\n",
      "Epoch 71/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8842 - mse: 6.8842 - mae: 1.9538 - val_loss: 5.8105 - val_mse: 5.8105 - val_mae: 1.9718\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 1.94602\n",
      "Epoch 72/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3543 - mse: 6.3543 - mae: 2.0074 - val_loss: 5.8066 - val_mse: 5.8066 - val_mae: 1.9713\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 1.94602\n",
      "Epoch 73/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9395 - mse: 5.9395 - mae: 1.8925 - val_loss: 5.8249 - val_mse: 5.8249 - val_mae: 1.9785\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 1.94602\n",
      "Epoch 74/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6609 - mse: 5.6609 - mae: 1.8851 - val_loss: 5.8045 - val_mse: 5.8045 - val_mae: 1.9721\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 1.94602\n",
      "Epoch 75/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.9575 - mse: 6.9575 - mae: 2.1006 - val_loss: 5.8147 - val_mse: 5.8147 - val_mae: 1.9765\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 1.94602\n",
      "Epoch 76/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0922 - mse: 6.0922 - mae: 1.9416 - val_loss: 5.8196 - val_mse: 5.8196 - val_mae: 1.9791\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 1.94602\n",
      "Epoch 77/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.0784 - mse: 7.0784 - mae: 2.1593 - val_loss: 5.7829 - val_mse: 5.7829 - val_mae: 1.9672\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 1.94602\n",
      "Epoch 78/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0544 - mse: 6.0544 - mae: 1.9265 - val_loss: 5.7740 - val_mse: 5.7740 - val_mae: 1.9650\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 1.94602\n",
      "Epoch 79/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2725 - mse: 6.2725 - mae: 1.9182 - val_loss: 5.7711 - val_mse: 5.7711 - val_mae: 1.9649\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 1.94602\n",
      "Epoch 80/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2316 - mse: 6.2316 - mae: 1.9435 - val_loss: 5.7726 - val_mse: 5.7726 - val_mae: 1.9665\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 1.94602\n",
      "Epoch 81/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5570 - mse: 6.5570 - mae: 2.0155 - val_loss: 5.7653 - val_mse: 5.7653 - val_mae: 1.9650\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 1.94602\n",
      "Epoch 82/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.1017 - mse: 7.1017 - mae: 2.1153 - val_loss: 5.7616 - val_mse: 5.7616 - val_mae: 1.9647\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 1.94602\n",
      "Epoch 83/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2013 - mse: 6.2013 - mae: 1.9764 - val_loss: 5.7423 - val_mse: 5.7423 - val_mae: 1.9587\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 1.94602\n",
      "Epoch 84/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.6075 - mse: 6.6075 - mae: 2.0773 - val_loss: 5.7374 - val_mse: 5.7374 - val_mae: 1.9582\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 1.94602\n",
      "Epoch 85/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8299 - mse: 6.8299 - mae: 2.0035 - val_loss: 5.7276 - val_mse: 5.7276 - val_mae: 1.9558\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 1.94602\n",
      "Epoch 86/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8680 - mse: 5.8680 - mae: 1.9489 - val_loss: 5.7100 - val_mse: 5.7100 - val_mae: 1.9503\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 1.94602\n",
      "Epoch 87/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.1549 - mse: 7.1549 - mae: 2.0563 - val_loss: 5.7074 - val_mse: 5.7074 - val_mae: 1.9507\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 1.94602\n",
      "Epoch 88/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7190 - mse: 5.7190 - mae: 1.9147 - val_loss: 5.6856 - val_mse: 5.6856 - val_mae: 1.9429\n",
      "\n",
      "Epoch 00088: val_mae improved from 1.94602 to 1.94293, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 89/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.5391 - mse: 6.5391 - mae: 2.0357 - val_loss: 5.7077 - val_mse: 5.7077 - val_mae: 1.9536\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 1.94293\n",
      "Epoch 90/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5126 - mse: 6.5126 - mae: 1.9827 - val_loss: 5.7209 - val_mse: 5.7209 - val_mae: 1.9593\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 1.94293\n",
      "Epoch 91/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2913 - mse: 6.2913 - mae: 1.9844 - val_loss: 5.7005 - val_mse: 5.7005 - val_mae: 1.9536\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 1.94293\n",
      "Epoch 92/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.9629 - mse: 6.9629 - mae: 2.0618 - val_loss: 5.6878 - val_mse: 5.6878 - val_mae: 1.9503\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 1.94293\n",
      "Epoch 93/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.6598 - mse: 6.6598 - mae: 2.0247 - val_loss: 5.6669 - val_mse: 5.6669 - val_mae: 1.9438\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 1.94293\n",
      "Epoch 94/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.9231 - mse: 6.9231 - mae: 2.0588 - val_loss: 5.6615 - val_mse: 5.6615 - val_mae: 1.9433\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 1.94293\n",
      "Epoch 95/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7232 - mse: 5.7232 - mae: 1.8990 - val_loss: 5.6514 - val_mse: 5.6514 - val_mae: 1.9408\n",
      "\n",
      "Epoch 00095: val_mae improved from 1.94293 to 1.94083, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 96/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2992 - mse: 6.2992 - mae: 2.0144 - val_loss: 5.6592 - val_mse: 5.6592 - val_mae: 1.9454\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 1.94083\n",
      "Epoch 97/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4313 - mse: 5.4313 - mae: 1.8796 - val_loss: 5.6455 - val_mse: 5.6455 - val_mae: 1.9417\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 1.94083\n",
      "Epoch 98/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9497 - mse: 5.9497 - mae: 1.9098 - val_loss: 5.6479 - val_mse: 5.6479 - val_mae: 1.9441\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 1.94083\n",
      "Epoch 99/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.4808 - mse: 6.4808 - mae: 1.9910 - val_loss: 5.6340 - val_mse: 5.6340 - val_mae: 1.9404\n",
      "\n",
      "Epoch 00099: val_mae improved from 1.94083 to 1.94039, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 100/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9997 - mse: 5.9997 - mae: 1.9557 - val_loss: 5.6158 - val_mse: 5.6158 - val_mae: 1.9350\n",
      "\n",
      "Epoch 00100: val_mae improved from 1.94039 to 1.93500, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 101/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8611 - mse: 6.8611 - mae: 2.0834 - val_loss: 5.6322 - val_mse: 5.6322 - val_mae: 1.9430\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 1.93500\n",
      "Epoch 102/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.0794 - mse: 6.0794 - mae: 1.9458 - val_loss: 5.6151 - val_mse: 5.6151 - val_mae: 1.9383\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 1.93500\n",
      "Epoch 103/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1925 - mse: 6.1925 - mae: 1.9599 - val_loss: 5.6124 - val_mse: 5.6124 - val_mae: 1.9391\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 1.93500\n",
      "Epoch 104/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2882 - mse: 6.2882 - mae: 1.9988 - val_loss: 5.5863 - val_mse: 5.5863 - val_mae: 1.9309\n",
      "\n",
      "Epoch 00104: val_mae improved from 1.93500 to 1.93094, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 105/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.3875 - mse: 6.3875 - mae: 2.0121 - val_loss: 5.5873 - val_mse: 5.5873 - val_mae: 1.9334\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 1.93094\n",
      "Epoch 106/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.7216 - mse: 6.7216 - mae: 2.0619 - val_loss: 5.5934 - val_mse: 5.5934 - val_mae: 1.9375\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 1.93094\n",
      "Epoch 107/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.0729 - mse: 7.0729 - mae: 2.0811 - val_loss: 5.5850 - val_mse: 5.5850 - val_mae: 1.9362\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 1.93094\n",
      "Epoch 108/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1747 - mse: 6.1747 - mae: 1.9575 - val_loss: 5.5506 - val_mse: 5.5506 - val_mae: 1.9250\n",
      "\n",
      "Epoch 00108: val_mae improved from 1.93094 to 1.92497, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 109/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6707 - mse: 5.6707 - mae: 1.9044 - val_loss: 5.5234 - val_mse: 5.5234 - val_mae: 1.9161\n",
      "\n",
      "Epoch 00109: val_mae improved from 1.92497 to 1.91614, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 110/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.4698 - mse: 6.4698 - mae: 1.9730 - val_loss: 5.5629 - val_mse: 5.5629 - val_mae: 1.9336\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 1.91614\n",
      "Epoch 111/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2653 - mse: 5.2653 - mae: 1.7984 - val_loss: 5.5515 - val_mse: 5.5515 - val_mae: 1.9313\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 1.91614\n",
      "Epoch 112/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.3772 - mse: 6.3772 - mae: 1.9504 - val_loss: 5.5640 - val_mse: 5.5640 - val_mae: 1.9371\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 1.91614\n",
      "Epoch 113/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1643 - mse: 6.1643 - mae: 1.9536 - val_loss: 5.5114 - val_mse: 5.5114 - val_mae: 1.9206\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 1.91614\n",
      "Epoch 114/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8521 - mse: 6.8521 - mae: 1.9991 - val_loss: 5.5000 - val_mse: 5.5000 - val_mae: 1.9181\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 1.91614\n",
      "Epoch 115/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8305 - mse: 5.8305 - mae: 1.9501 - val_loss: 5.4710 - val_mse: 5.4710 - val_mae: 1.9089\n",
      "\n",
      "Epoch 00115: val_mae improved from 1.91614 to 1.90893, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 116/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0071 - mse: 6.0071 - mae: 1.9858 - val_loss: 5.4477 - val_mse: 5.4477 - val_mae: 1.9015\n",
      "\n",
      "Epoch 00116: val_mae improved from 1.90893 to 1.90150, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 117/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1858 - mse: 6.1858 - mae: 1.9784 - val_loss: 5.4640 - val_mse: 5.4640 - val_mae: 1.9110\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 1.90150\n",
      "Epoch 118/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4832 - mse: 5.4832 - mae: 1.8467 - val_loss: 5.4819 - val_mse: 5.4819 - val_mae: 1.9197\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 1.90150\n",
      "Epoch 119/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.6321 - mse: 6.6321 - mae: 2.0206 - val_loss: 5.4672 - val_mse: 5.4672 - val_mae: 1.9165\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 1.90150\n",
      "Epoch 120/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4599 - mse: 6.4599 - mae: 1.9405 - val_loss: 5.4594 - val_mse: 5.4594 - val_mae: 1.9157\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 1.90150\n",
      "Epoch 121/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1140 - mse: 6.1140 - mae: 1.9751 - val_loss: 5.4546 - val_mse: 5.4546 - val_mae: 1.9159\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 1.90150\n",
      "Epoch 122/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.9395 - mse: 6.9395 - mae: 2.0460 - val_loss: 5.4013 - val_mse: 5.4013 - val_mae: 1.8988\n",
      "\n",
      "Epoch 00122: val_mae improved from 1.90150 to 1.89880, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 123/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9413 - mse: 5.9413 - mae: 1.9468 - val_loss: 5.3730 - val_mse: 5.3730 - val_mae: 1.8894\n",
      "\n",
      "Epoch 00123: val_mae improved from 1.89880 to 1.88936, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 124/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.1014 - mse: 6.1014 - mae: 1.9392 - val_loss: 5.3799 - val_mse: 5.3799 - val_mae: 1.8952\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 1.88936\n",
      "Epoch 125/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9886 - mse: 5.9886 - mae: 1.9358 - val_loss: 5.3860 - val_mse: 5.3860 - val_mae: 1.8999\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 1.88936\n",
      "Epoch 126/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0845 - mse: 6.0845 - mae: 1.9687 - val_loss: 5.3967 - val_mse: 5.3967 - val_mae: 1.9058\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 1.88936\n",
      "Epoch 127/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6198 - mse: 5.6198 - mae: 1.8994 - val_loss: 5.3582 - val_mse: 5.3582 - val_mae: 1.8944\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 1.88936\n",
      "Epoch 128/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6171 - mse: 5.6171 - mae: 1.9072 - val_loss: 5.3434 - val_mse: 5.3434 - val_mae: 1.8914\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 1.88936\n",
      "Epoch 129/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7928 - mse: 5.7928 - mae: 1.9291 - val_loss: 5.3362 - val_mse: 5.3362 - val_mae: 1.8909\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 1.88936\n",
      "Epoch 130/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2844 - mse: 5.2844 - mae: 1.8278 - val_loss: 5.3089 - val_mse: 5.3089 - val_mae: 1.8831\n",
      "\n",
      "Epoch 00130: val_mae improved from 1.88936 to 1.88314, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 131/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5778 - mse: 5.5778 - mae: 1.8900 - val_loss: 5.3021 - val_mse: 5.3021 - val_mae: 1.8833\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 1.88314\n",
      "Epoch 132/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.3195 - mse: 6.3195 - mae: 1.9893 - val_loss: 5.3048 - val_mse: 5.3048 - val_mae: 1.8866\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 1.88314\n",
      "Epoch 133/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9000 - mse: 5.9000 - mae: 1.9597 - val_loss: 5.2572 - val_mse: 5.2572 - val_mae: 1.8706\n",
      "\n",
      "Epoch 00133: val_mae improved from 1.88314 to 1.87057, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 134/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.3275 - mse: 6.3275 - mae: 1.9431 - val_loss: 5.2573 - val_mse: 5.2573 - val_mae: 1.8737\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 1.87057\n",
      "Epoch 135/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.7680 - mse: 5.7680 - mae: 1.8880 - val_loss: 5.2386 - val_mse: 5.2386 - val_mae: 1.8689\n",
      "\n",
      "Epoch 00135: val_mae improved from 1.87057 to 1.86885, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 136/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8842 - mse: 5.8842 - mae: 1.8877 - val_loss: 5.2294 - val_mse: 5.2294 - val_mae: 1.8680\n",
      "\n",
      "Epoch 00136: val_mae improved from 1.86885 to 1.86802, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 137/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5922 - mse: 5.5922 - mae: 1.8861 - val_loss: 5.2170 - val_mse: 5.2170 - val_mae: 1.8657\n",
      "\n",
      "Epoch 00137: val_mae improved from 1.86802 to 1.86571, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 138/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.9498 - mse: 6.9498 - mae: 2.0335 - val_loss: 5.2218 - val_mse: 5.2218 - val_mae: 1.8701\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 1.86571\n",
      "Epoch 139/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.1916 - mse: 6.1916 - mae: 1.8961 - val_loss: 5.2024 - val_mse: 5.2024 - val_mae: 1.8654\n",
      "\n",
      "Epoch 00139: val_mae improved from 1.86571 to 1.86543, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 140/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8331 - mse: 5.8331 - mae: 1.9379 - val_loss: 5.1714 - val_mse: 5.1714 - val_mae: 1.8561\n",
      "\n",
      "Epoch 00140: val_mae improved from 1.86543 to 1.85606, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 141/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6171 - mse: 5.6171 - mae: 1.8571 - val_loss: 5.1619 - val_mse: 5.1619 - val_mae: 1.8554\n",
      "\n",
      "Epoch 00141: val_mae improved from 1.85606 to 1.85544, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 142/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9393 - mse: 5.9393 - mae: 1.9791 - val_loss: 5.1391 - val_mse: 5.1391 - val_mae: 1.8488\n",
      "\n",
      "Epoch 00142: val_mae improved from 1.85544 to 1.84884, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 143/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8244 - mse: 5.8244 - mae: 1.9119 - val_loss: 5.1217 - val_mse: 5.1217 - val_mae: 1.8447\n",
      "\n",
      "Epoch 00143: val_mae improved from 1.84884 to 1.84468, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 144/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.2728 - mse: 6.2728 - mae: 1.9285 - val_loss: 5.1186 - val_mse: 5.1186 - val_mae: 1.8469\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 1.84468\n",
      "Epoch 145/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4627 - mse: 5.4627 - mae: 1.8398 - val_loss: 5.1129 - val_mse: 5.1129 - val_mae: 1.8475\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 1.84468\n",
      "Epoch 146/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5102 - mse: 5.5102 - mae: 1.8055 - val_loss: 5.1006 - val_mse: 5.1006 - val_mae: 1.8451\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 1.84468\n",
      "Epoch 147/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7702 - mse: 5.7702 - mae: 1.8745 - val_loss: 5.0896 - val_mse: 5.0896 - val_mae: 1.8436\n",
      "\n",
      "Epoch 00147: val_mae improved from 1.84468 to 1.84361, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 148/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6307 - mse: 5.6307 - mae: 1.8519 - val_loss: 5.0760 - val_mse: 5.0760 - val_mae: 1.8411\n",
      "\n",
      "Epoch 00148: val_mae improved from 1.84361 to 1.84114, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 149/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6923 - mse: 5.6923 - mae: 1.9031 - val_loss: 5.0598 - val_mse: 5.0598 - val_mae: 1.8376\n",
      "\n",
      "Epoch 00149: val_mae improved from 1.84114 to 1.83762, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 150/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3227 - mse: 5.3227 - mae: 1.8100 - val_loss: 5.0455 - val_mse: 5.0455 - val_mae: 1.8346\n",
      "\n",
      "Epoch 00150: val_mae improved from 1.83762 to 1.83458, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 151/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0806 - mse: 6.0806 - mae: 1.9046 - val_loss: 5.0364 - val_mse: 5.0364 - val_mae: 1.8335\n",
      "\n",
      "Epoch 00151: val_mae improved from 1.83458 to 1.83351, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 152/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9469 - mse: 5.9469 - mae: 1.9435 - val_loss: 5.0296 - val_mse: 5.0296 - val_mae: 1.8332\n",
      "\n",
      "Epoch 00152: val_mae improved from 1.83351 to 1.83324, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 153/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6973 - mse: 5.6973 - mae: 1.8925 - val_loss: 5.0024 - val_mse: 5.0024 - val_mae: 1.8254\n",
      "\n",
      "Epoch 00153: val_mae improved from 1.83324 to 1.82543, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 154/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0630 - mse: 6.0630 - mae: 1.9313 - val_loss: 4.9843 - val_mse: 4.9843 - val_mae: 1.8210\n",
      "\n",
      "Epoch 00154: val_mae improved from 1.82543 to 1.82099, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 155/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1502 - mse: 5.1502 - mae: 1.8075 - val_loss: 4.9696 - val_mse: 4.9696 - val_mae: 1.8178\n",
      "\n",
      "Epoch 00155: val_mae improved from 1.82099 to 1.81784, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 156/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0615 - mse: 6.0615 - mae: 1.9368 - val_loss: 4.9624 - val_mse: 4.9624 - val_mae: 1.8176\n",
      "\n",
      "Epoch 00156: val_mae improved from 1.81784 to 1.81763, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 157/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4251 - mse: 5.4251 - mae: 1.8661 - val_loss: 4.9604 - val_mse: 4.9604 - val_mae: 1.8196\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 1.81763\n",
      "Epoch 158/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8475 - mse: 5.8475 - mae: 1.8780 - val_loss: 4.9637 - val_mse: 4.9637 - val_mae: 1.8226\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 1.81763\n",
      "Epoch 159/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7328 - mse: 5.7328 - mae: 1.8794 - val_loss: 4.9385 - val_mse: 4.9385 - val_mae: 1.8165\n",
      "\n",
      "Epoch 00159: val_mae improved from 1.81763 to 1.81652, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 160/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4083 - mse: 5.4083 - mae: 1.8210 - val_loss: 4.9278 - val_mse: 4.9278 - val_mae: 1.8146\n",
      "\n",
      "Epoch 00160: val_mae improved from 1.81652 to 1.81460, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 161/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3555 - mse: 6.3555 - mae: 1.9967 - val_loss: 4.9165 - val_mse: 4.9165 - val_mae: 1.8129\n",
      "\n",
      "Epoch 00161: val_mae improved from 1.81460 to 1.81292, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 162/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9331 - mse: 5.9331 - mae: 1.9256 - val_loss: 4.8823 - val_mse: 4.8823 - val_mae: 1.8032\n",
      "\n",
      "Epoch 00162: val_mae improved from 1.81292 to 1.80322, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 163/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1006 - mse: 6.1006 - mae: 1.9033 - val_loss: 4.8699 - val_mse: 4.8699 - val_mae: 1.8009\n",
      "\n",
      "Epoch 00163: val_mae improved from 1.80322 to 1.80093, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 164/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9477 - mse: 5.9477 - mae: 1.9169 - val_loss: 4.8546 - val_mse: 4.8546 - val_mae: 1.7978\n",
      "\n",
      "Epoch 00164: val_mae improved from 1.80093 to 1.79782, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 165/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5564 - mse: 6.5564 - mae: 1.9502 - val_loss: 4.8509 - val_mse: 4.8509 - val_mae: 1.7989\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 1.79782\n",
      "Epoch 166/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6753 - mse: 5.6753 - mae: 1.9082 - val_loss: 4.8191 - val_mse: 4.8191 - val_mae: 1.7899\n",
      "\n",
      "Epoch 00166: val_mae improved from 1.79782 to 1.78990, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 167/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.2102 - mse: 6.2102 - mae: 1.8932 - val_loss: 4.8118 - val_mse: 4.8118 - val_mae: 1.7894\n",
      "\n",
      "Epoch 00167: val_mae improved from 1.78990 to 1.78941, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 168/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2353 - mse: 5.2353 - mae: 1.8214 - val_loss: 4.7925 - val_mse: 4.7925 - val_mae: 1.7844\n",
      "\n",
      "Epoch 00168: val_mae improved from 1.78941 to 1.78437, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 169/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.5608 - mse: 5.5608 - mae: 1.8523 - val_loss: 4.7909 - val_mse: 4.7909 - val_mae: 1.7862\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 1.78437\n",
      "Epoch 170/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2957 - mse: 5.2957 - mae: 1.7945 - val_loss: 4.7640 - val_mse: 4.7640 - val_mae: 1.7781\n",
      "\n",
      "Epoch 00170: val_mae improved from 1.78437 to 1.77813, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 171/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0971 - mse: 5.0971 - mae: 1.7796 - val_loss: 4.7499 - val_mse: 4.7499 - val_mae: 1.7751\n",
      "\n",
      "Epoch 00171: val_mae improved from 1.77813 to 1.77508, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 172/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6701 - mse: 5.6701 - mae: 1.8469 - val_loss: 4.7600 - val_mse: 4.7600 - val_mae: 1.7812\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 1.77508\n",
      "Epoch 173/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9189 - mse: 5.9189 - mae: 1.8766 - val_loss: 4.7672 - val_mse: 4.7672 - val_mae: 1.7853\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 1.77508\n",
      "Epoch 174/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5098 - mse: 5.5098 - mae: 1.8424 - val_loss: 4.7245 - val_mse: 4.7245 - val_mae: 1.7723\n",
      "\n",
      "Epoch 00174: val_mae improved from 1.77508 to 1.77233, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 175/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6390 - mse: 5.6390 - mae: 1.8928 - val_loss: 4.7138 - val_mse: 4.7138 - val_mae: 1.7705\n",
      "\n",
      "Epoch 00175: val_mae improved from 1.77233 to 1.77045, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 176/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7975 - mse: 5.7975 - mae: 1.8907 - val_loss: 4.6951 - val_mse: 4.6951 - val_mae: 1.7654\n",
      "\n",
      "Epoch 00176: val_mae improved from 1.77045 to 1.76541, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 177/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3641 - mse: 5.3641 - mae: 1.8276 - val_loss: 4.6871 - val_mse: 4.6871 - val_mae: 1.7646\n",
      "\n",
      "Epoch 00177: val_mae improved from 1.76541 to 1.76455, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 178/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9212 - mse: 5.9212 - mae: 1.9000 - val_loss: 4.6854 - val_mse: 4.6854 - val_mae: 1.7662\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 1.76455\n",
      "Epoch 179/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3843 - mse: 5.3843 - mae: 1.8167 - val_loss: 4.6769 - val_mse: 4.6769 - val_mae: 1.7649\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 1.76455\n",
      "Epoch 180/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0371 - mse: 6.0371 - mae: 1.9076 - val_loss: 4.6623 - val_mse: 4.6623 - val_mae: 1.7613\n",
      "\n",
      "Epoch 00180: val_mae improved from 1.76455 to 1.76132, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 181/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4816 - mse: 5.4816 - mae: 1.7928 - val_loss: 4.6369 - val_mse: 4.6369 - val_mae: 1.7533\n",
      "\n",
      "Epoch 00181: val_mae improved from 1.76132 to 1.75334, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 182/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9335 - mse: 4.9335 - mae: 1.7583 - val_loss: 4.6492 - val_mse: 4.6492 - val_mae: 1.7600\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 1.75334\n",
      "Epoch 183/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2101 - mse: 5.2101 - mae: 1.8121 - val_loss: 4.6292 - val_mse: 4.6292 - val_mae: 1.7551\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 1.75334\n",
      "Epoch 184/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5564 - mse: 5.5564 - mae: 1.8244 - val_loss: 4.6160 - val_mse: 4.6160 - val_mae: 1.7520\n",
      "\n",
      "Epoch 00184: val_mae improved from 1.75334 to 1.75205, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 185/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8773 - mse: 4.8773 - mae: 1.7373 - val_loss: 4.5975 - val_mse: 4.5975 - val_mae: 1.7471\n",
      "\n",
      "Epoch 00185: val_mae improved from 1.75205 to 1.74712, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 186/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4962 - mse: 5.4962 - mae: 1.8627 - val_loss: 4.5867 - val_mse: 4.5867 - val_mae: 1.7449\n",
      "\n",
      "Epoch 00186: val_mae improved from 1.74712 to 1.74492, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 187/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4587 - mse: 5.4587 - mae: 1.8495 - val_loss: 4.6040 - val_mse: 4.6040 - val_mae: 1.7525\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 1.74492\n",
      "Epoch 188/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9592 - mse: 5.9592 - mae: 1.8588 - val_loss: 4.5739 - val_mse: 4.5739 - val_mae: 1.7436\n",
      "\n",
      "Epoch 00188: val_mae improved from 1.74492 to 1.74364, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 189/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7443 - mse: 5.7443 - mae: 1.8259 - val_loss: 4.5569 - val_mse: 4.5569 - val_mae: 1.7391\n",
      "\n",
      "Epoch 00189: val_mae improved from 1.74364 to 1.73906, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 190/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8725 - mse: 5.8725 - mae: 1.8707 - val_loss: 4.5400 - val_mse: 4.5400 - val_mae: 1.7345\n",
      "\n",
      "Epoch 00190: val_mae improved from 1.73906 to 1.73454, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 191/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3368 - mse: 5.3368 - mae: 1.8397 - val_loss: 4.5270 - val_mse: 4.5270 - val_mae: 1.7314\n",
      "\n",
      "Epoch 00191: val_mae improved from 1.73454 to 1.73143, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 192/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1533 - mse: 5.1533 - mae: 1.7632 - val_loss: 4.5044 - val_mse: 4.5044 - val_mae: 1.7246\n",
      "\n",
      "Epoch 00192: val_mae improved from 1.73143 to 1.72458, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 193/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2167 - mse: 5.2167 - mae: 1.7812 - val_loss: 4.5050 - val_mse: 4.5050 - val_mae: 1.7270\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 1.72458\n",
      "Epoch 194/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9980 - mse: 4.9980 - mae: 1.7552 - val_loss: 4.5081 - val_mse: 4.5081 - val_mae: 1.7302\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 1.72458\n",
      "Epoch 195/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7214 - mse: 5.7214 - mae: 1.8579 - val_loss: 4.5028 - val_mse: 4.5028 - val_mae: 1.7298\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 1.72458\n",
      "Epoch 196/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8574 - mse: 4.8574 - mae: 1.7746 - val_loss: 4.4954 - val_mse: 4.4954 - val_mae: 1.7286\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 1.72458\n",
      "Epoch 197/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9226 - mse: 4.9226 - mae: 1.7674 - val_loss: 4.4701 - val_mse: 4.4701 - val_mae: 1.7209\n",
      "\n",
      "Epoch 00197: val_mae improved from 1.72458 to 1.72086, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 198/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6814 - mse: 5.6814 - mae: 1.8920 - val_loss: 4.4758 - val_mse: 4.4758 - val_mae: 1.7249\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 1.72086\n",
      "Epoch 199/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2303 - mse: 5.2303 - mae: 1.8076 - val_loss: 4.4426 - val_mse: 4.4426 - val_mae: 1.7146\n",
      "\n",
      "Epoch 00199: val_mae improved from 1.72086 to 1.71456, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 200/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8479 - mse: 5.8479 - mae: 1.8618 - val_loss: 4.4307 - val_mse: 4.4307 - val_mae: 1.7119\n",
      "\n",
      "Epoch 00200: val_mae improved from 1.71456 to 1.71186, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 201/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1118 - mse: 5.1118 - mae: 1.7696 - val_loss: 4.4187 - val_mse: 4.4187 - val_mae: 1.7091\n",
      "\n",
      "Epoch 00201: val_mae improved from 1.71186 to 1.70913, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 202/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9466 - mse: 5.9466 - mae: 1.8905 - val_loss: 4.4119 - val_mse: 4.4119 - val_mae: 1.7085\n",
      "\n",
      "Epoch 00202: val_mae improved from 1.70913 to 1.70849, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 203/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5506 - mse: 5.5506 - mae: 1.8237 - val_loss: 4.4022 - val_mse: 4.4022 - val_mae: 1.7064\n",
      "\n",
      "Epoch 00203: val_mae improved from 1.70849 to 1.70643, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 204/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8990 - mse: 4.8990 - mae: 1.7496 - val_loss: 4.4001 - val_mse: 4.4001 - val_mae: 1.7071\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 1.70643\n",
      "Epoch 205/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7961 - mse: 5.7961 - mae: 1.8956 - val_loss: 4.3841 - val_mse: 4.3841 - val_mae: 1.7027\n",
      "\n",
      "Epoch 00205: val_mae improved from 1.70643 to 1.70273, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 206/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4489 - mse: 5.4489 - mae: 1.8571 - val_loss: 4.3709 - val_mse: 4.3709 - val_mae: 1.6992\n",
      "\n",
      "Epoch 00206: val_mae improved from 1.70273 to 1.69924, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 207/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1266 - mse: 5.1266 - mae: 1.7657 - val_loss: 4.3486 - val_mse: 4.3486 - val_mae: 1.6916\n",
      "\n",
      "Epoch 00207: val_mae improved from 1.69924 to 1.69162, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 208/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6381 - mse: 5.6381 - mae: 1.7780 - val_loss: 4.3304 - val_mse: 4.3304 - val_mae: 1.6852\n",
      "\n",
      "Epoch 00208: val_mae improved from 1.69162 to 1.68516, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 209/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0541 - mse: 5.0541 - mae: 1.7034 - val_loss: 4.3393 - val_mse: 4.3393 - val_mae: 1.6911\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 1.68516\n",
      "Epoch 210/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9791 - mse: 4.9791 - mae: 1.7452 - val_loss: 4.3281 - val_mse: 4.3281 - val_mae: 1.6885\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 1.68516\n",
      "Epoch 211/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2359 - mse: 6.2359 - mae: 1.9239 - val_loss: 4.3413 - val_mse: 4.3413 - val_mae: 1.6949\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 1.68516\n",
      "Epoch 212/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7344 - mse: 5.7344 - mae: 1.8884 - val_loss: 4.3276 - val_mse: 4.3276 - val_mae: 1.6912\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 1.68516\n",
      "Epoch 213/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8152 - mse: 5.8152 - mae: 1.8327 - val_loss: 4.3066 - val_mse: 4.3066 - val_mae: 1.6848\n",
      "\n",
      "Epoch 00213: val_mae improved from 1.68516 to 1.68478, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 214/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7896 - mse: 5.7896 - mae: 1.8570 - val_loss: 4.2915 - val_mse: 4.2915 - val_mae: 1.6804\n",
      "\n",
      "Epoch 00214: val_mae improved from 1.68478 to 1.68035, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 215/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2107 - mse: 5.2107 - mae: 1.7726 - val_loss: 4.2848 - val_mse: 4.2848 - val_mae: 1.6792\n",
      "\n",
      "Epoch 00215: val_mae improved from 1.68035 to 1.67921, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 216/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3585 - mse: 5.3585 - mae: 1.7707 - val_loss: 4.2786 - val_mse: 4.2786 - val_mae: 1.6782\n",
      "\n",
      "Epoch 00216: val_mae improved from 1.67921 to 1.67818, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 217/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3389 - mse: 5.3389 - mae: 1.7743 - val_loss: 4.2911 - val_mse: 4.2911 - val_mae: 1.6837\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 1.67818\n",
      "Epoch 218/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1203 - mse: 5.1203 - mae: 1.7821 - val_loss: 4.2704 - val_mse: 4.2704 - val_mae: 1.6776\n",
      "\n",
      "Epoch 00218: val_mae improved from 1.67818 to 1.67762, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 219/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8722 - mse: 4.8722 - mae: 1.6904 - val_loss: 4.2606 - val_mse: 4.2606 - val_mae: 1.6753\n",
      "\n",
      "Epoch 00219: val_mae improved from 1.67762 to 1.67533, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 220/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8123 - mse: 4.8123 - mae: 1.7182 - val_loss: 4.2510 - val_mse: 4.2510 - val_mae: 1.6730\n",
      "\n",
      "Epoch 00220: val_mae improved from 1.67533 to 1.67299, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 221/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7978 - mse: 5.7978 - mae: 1.8037 - val_loss: 4.2573 - val_mse: 4.2573 - val_mae: 1.6761\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 1.67299\n",
      "Epoch 222/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2045 - mse: 5.2045 - mae: 1.7525 - val_loss: 4.2375 - val_mse: 4.2375 - val_mae: 1.6704\n",
      "\n",
      "Epoch 00222: val_mae improved from 1.67299 to 1.67039, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 223/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1163 - mse: 5.1163 - mae: 1.7784 - val_loss: 4.2312 - val_mse: 4.2312 - val_mae: 1.6692\n",
      "\n",
      "Epoch 00223: val_mae improved from 1.67039 to 1.66922, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 224/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7098 - mse: 4.7098 - mae: 1.6932 - val_loss: 4.2325 - val_mse: 4.2325 - val_mae: 1.6705\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 1.66922\n",
      "Epoch 225/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7176 - mse: 5.7176 - mae: 1.8893 - val_loss: 4.2213 - val_mse: 4.2213 - val_mae: 1.6676\n",
      "\n",
      "Epoch 00225: val_mae improved from 1.66922 to 1.66755, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 226/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0614 - mse: 5.0614 - mae: 1.7693 - val_loss: 4.2159 - val_mse: 4.2159 - val_mae: 1.6666\n",
      "\n",
      "Epoch 00226: val_mae improved from 1.66755 to 1.66661, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 227/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5660 - mse: 5.5660 - mae: 1.7965 - val_loss: 4.2330 - val_mse: 4.2330 - val_mae: 1.6722\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 1.66661\n",
      "Epoch 228/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6874 - mse: 5.6874 - mae: 1.8078 - val_loss: 4.2006 - val_mse: 4.2006 - val_mae: 1.6631\n",
      "\n",
      "Epoch 00228: val_mae improved from 1.66661 to 1.66306, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 229/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7311 - mse: 4.7311 - mae: 1.6977 - val_loss: 4.1907 - val_mse: 4.1907 - val_mae: 1.6605\n",
      "\n",
      "Epoch 00229: val_mae improved from 1.66306 to 1.66049, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 230/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1119 - mse: 5.1119 - mae: 1.7337 - val_loss: 4.1841 - val_mse: 4.1841 - val_mae: 1.6590\n",
      "\n",
      "Epoch 00230: val_mae improved from 1.66049 to 1.65899, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 231/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4178 - mse: 5.4178 - mae: 1.8060 - val_loss: 4.1771 - val_mse: 4.1771 - val_mae: 1.6571\n",
      "\n",
      "Epoch 00231: val_mae improved from 1.65899 to 1.65714, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 232/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1232 - mse: 5.1232 - mae: 1.7576 - val_loss: 4.1594 - val_mse: 4.1594 - val_mae: 1.6514\n",
      "\n",
      "Epoch 00232: val_mae improved from 1.65714 to 1.65137, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 233/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1387 - mse: 5.1387 - mae: 1.7348 - val_loss: 4.1669 - val_mse: 4.1669 - val_mae: 1.6551\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 1.65137\n",
      "Epoch 234/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6663 - mse: 5.6663 - mae: 1.8013 - val_loss: 4.1572 - val_mse: 4.1572 - val_mae: 1.6525\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 1.65137\n",
      "Epoch 235/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1197 - mse: 5.1197 - mae: 1.7525 - val_loss: 4.1635 - val_mse: 4.1635 - val_mae: 1.6553\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 1.65137\n",
      "Epoch 236/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0135 - mse: 6.0135 - mae: 1.8581 - val_loss: 4.1436 - val_mse: 4.1436 - val_mae: 1.6492\n",
      "\n",
      "Epoch 00236: val_mae improved from 1.65137 to 1.64923, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 237/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8281 - mse: 4.8281 - mae: 1.7155 - val_loss: 4.1538 - val_mse: 4.1538 - val_mae: 1.6530\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 1.64923\n",
      "Epoch 238/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5991 - mse: 5.5991 - mae: 1.8493 - val_loss: 4.1315 - val_mse: 4.1315 - val_mae: 1.6465\n",
      "\n",
      "Epoch 00238: val_mae improved from 1.64923 to 1.64649, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 239/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9925 - mse: 4.9925 - mae: 1.7605 - val_loss: 4.1238 - val_mse: 4.1238 - val_mae: 1.6446\n",
      "\n",
      "Epoch 00239: val_mae improved from 1.64649 to 1.64464, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 240/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1465 - mse: 5.1465 - mae: 1.8021 - val_loss: 4.1352 - val_mse: 4.1352 - val_mae: 1.6488\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 1.64464\n",
      "Epoch 241/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0889 - mse: 5.0889 - mae: 1.7791 - val_loss: 4.1151 - val_mse: 4.1151 - val_mae: 1.6431\n",
      "\n",
      "Epoch 00241: val_mae improved from 1.64464 to 1.64310, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 242/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3532 - mse: 5.3532 - mae: 1.7955 - val_loss: 4.0956 - val_mse: 4.0956 - val_mae: 1.6368\n",
      "\n",
      "Epoch 00242: val_mae improved from 1.64310 to 1.63676, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 243/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9563 - mse: 4.9563 - mae: 1.7079 - val_loss: 4.0882 - val_mse: 4.0882 - val_mae: 1.6348\n",
      "\n",
      "Epoch 00243: val_mae improved from 1.63676 to 1.63477, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 244/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0643 - mse: 5.0643 - mae: 1.7804 - val_loss: 4.1045 - val_mse: 4.1045 - val_mae: 1.6412\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 1.63477\n",
      "Epoch 245/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8976 - mse: 4.8976 - mae: 1.7040 - val_loss: 4.1001 - val_mse: 4.1001 - val_mae: 1.6404\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 1.63477\n",
      "Epoch 246/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1700 - mse: 5.1700 - mae: 1.7613 - val_loss: 4.0880 - val_mse: 4.0880 - val_mae: 1.6370\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 1.63477\n",
      "Epoch 247/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9837 - mse: 4.9837 - mae: 1.7333 - val_loss: 4.0727 - val_mse: 4.0727 - val_mae: 1.6319\n",
      "\n",
      "Epoch 00247: val_mae improved from 1.63477 to 1.63190, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 248/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9098 - mse: 5.9098 - mae: 1.8474 - val_loss: 4.0701 - val_mse: 4.0701 - val_mae: 1.6318\n",
      "\n",
      "Epoch 00248: val_mae improved from 1.63190 to 1.63181, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 249/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.9511 - mse: 4.9511 - mae: 1.7255 - val_loss: 4.0554 - val_mse: 4.0554 - val_mae: 1.6266\n",
      "\n",
      "Epoch 00249: val_mae improved from 1.63181 to 1.62655, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 250/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1185 - mse: 5.1185 - mae: 1.7268 - val_loss: 4.0652 - val_mse: 4.0652 - val_mae: 1.6316\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 1.62655\n",
      "Epoch 251/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2743 - mse: 5.2743 - mae: 1.7948 - val_loss: 4.0615 - val_mse: 4.0615 - val_mae: 1.6309\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 1.62655\n",
      "Epoch 252/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7554 - mse: 4.7554 - mae: 1.6829 - val_loss: 4.0505 - val_mse: 4.0505 - val_mae: 1.6278\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 1.62655\n",
      "Epoch 253/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9803 - mse: 4.9803 - mae: 1.7323 - val_loss: 4.0767 - val_mse: 4.0767 - val_mae: 1.6364\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 1.62655\n",
      "Epoch 254/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0254 - mse: 5.0254 - mae: 1.7671 - val_loss: 4.0683 - val_mse: 4.0683 - val_mae: 1.6344\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 1.62655\n",
      "Epoch 255/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0916 - mse: 5.0916 - mae: 1.7637 - val_loss: 4.0633 - val_mse: 4.0633 - val_mae: 1.6331\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 1.62655\n",
      "Epoch 256/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6019 - mse: 5.6019 - mae: 1.8626 - val_loss: 4.0355 - val_mse: 4.0355 - val_mae: 1.6245\n",
      "\n",
      "Epoch 00256: val_mae improved from 1.62655 to 1.62450, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 257/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1467 - mse: 5.1467 - mae: 1.7655 - val_loss: 4.0322 - val_mse: 4.0322 - val_mae: 1.6240\n",
      "\n",
      "Epoch 00257: val_mae improved from 1.62450 to 1.62401, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 258/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1966 - mse: 5.1966 - mae: 1.8079 - val_loss: 4.0218 - val_mse: 4.0218 - val_mae: 1.6207\n",
      "\n",
      "Epoch 00258: val_mae improved from 1.62401 to 1.62068, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 259/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9356 - mse: 4.9356 - mae: 1.6951 - val_loss: 4.0179 - val_mse: 4.0179 - val_mae: 1.6198\n",
      "\n",
      "Epoch 00259: val_mae improved from 1.62068 to 1.61980, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 260/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0948 - mse: 6.0948 - mae: 1.8593 - val_loss: 4.0371 - val_mse: 4.0371 - val_mae: 1.6266\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 1.61980\n",
      "Epoch 261/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6753 - mse: 4.6753 - mae: 1.6532 - val_loss: 4.0310 - val_mse: 4.0310 - val_mae: 1.6252\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 1.61980\n",
      "Epoch 262/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2497 - mse: 5.2497 - mae: 1.7736 - val_loss: 4.0185 - val_mse: 4.0185 - val_mae: 1.6217\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 1.61980\n",
      "Epoch 263/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9853 - mse: 4.9853 - mae: 1.6886 - val_loss: 4.0081 - val_mse: 4.0081 - val_mae: 1.6185\n",
      "\n",
      "Epoch 00263: val_mae improved from 1.61980 to 1.61848, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 264/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7920 - mse: 4.7920 - mae: 1.7086 - val_loss: 3.9917 - val_mse: 3.9917 - val_mae: 1.6129\n",
      "\n",
      "Epoch 00264: val_mae improved from 1.61848 to 1.61288, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 265/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5515 - mse: 5.5515 - mae: 1.7973 - val_loss: 4.0155 - val_mse: 4.0155 - val_mae: 1.6211\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 1.61288\n",
      "Epoch 266/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2855 - mse: 5.2855 - mae: 1.8291 - val_loss: 4.0022 - val_mse: 4.0022 - val_mae: 1.6174\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 1.61288\n",
      "Epoch 267/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1816 - mse: 5.1816 - mae: 1.7632 - val_loss: 3.9997 - val_mse: 3.9997 - val_mae: 1.6168\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 1.61288\n",
      "Epoch 268/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1010 - mse: 5.1010 - mae: 1.7280 - val_loss: 3.9860 - val_mse: 3.9860 - val_mae: 1.6124\n",
      "\n",
      "Epoch 00268: val_mae improved from 1.61288 to 1.61243, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 269/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0063 - mse: 5.0063 - mae: 1.6902 - val_loss: 3.9782 - val_mse: 3.9782 - val_mae: 1.6101\n",
      "\n",
      "Epoch 00269: val_mae improved from 1.61243 to 1.61011, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 270/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9591 - mse: 4.9591 - mae: 1.6798 - val_loss: 3.9787 - val_mse: 3.9787 - val_mae: 1.6105\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 1.61011\n",
      "Epoch 271/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9651 - mse: 4.9651 - mae: 1.7592 - val_loss: 3.9818 - val_mse: 3.9818 - val_mae: 1.6117\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 1.61011\n",
      "Epoch 272/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1640 - mse: 5.1640 - mae: 1.7524 - val_loss: 3.9798 - val_mse: 3.9798 - val_mae: 1.6114\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 1.61011\n",
      "Epoch 273/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0219 - mse: 5.0219 - mae: 1.7503 - val_loss: 3.9680 - val_mse: 3.9680 - val_mae: 1.6079\n",
      "\n",
      "Epoch 00273: val_mae improved from 1.61011 to 1.60788, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 274/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3778 - mse: 5.3778 - mae: 1.7785 - val_loss: 3.9610 - val_mse: 3.9610 - val_mae: 1.6060\n",
      "\n",
      "Epoch 00274: val_mae improved from 1.60788 to 1.60599, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 275/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1608 - mse: 5.1608 - mae: 1.7565 - val_loss: 3.9533 - val_mse: 3.9533 - val_mae: 1.6037\n",
      "\n",
      "Epoch 00275: val_mae improved from 1.60599 to 1.60374, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 276/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2505 - mse: 5.2505 - mae: 1.7763 - val_loss: 3.9510 - val_mse: 3.9510 - val_mae: 1.6031\n",
      "\n",
      "Epoch 00276: val_mae improved from 1.60374 to 1.60310, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 277/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3255 - mse: 5.3255 - mae: 1.7770 - val_loss: 3.9588 - val_mse: 3.9588 - val_mae: 1.6057\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 1.60310\n",
      "Epoch 278/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5450 - mse: 5.5450 - mae: 1.7745 - val_loss: 3.9442 - val_mse: 3.9442 - val_mae: 1.6013\n",
      "\n",
      "Epoch 00278: val_mae improved from 1.60310 to 1.60129, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 279/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4473 - mse: 4.4473 - mae: 1.6578 - val_loss: 3.9400 - val_mse: 3.9400 - val_mae: 1.6003\n",
      "\n",
      "Epoch 00279: val_mae improved from 1.60129 to 1.60028, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 280/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0788 - mse: 5.0788 - mae: 1.7619 - val_loss: 3.9359 - val_mse: 3.9359 - val_mae: 1.5992\n",
      "\n",
      "Epoch 00280: val_mae improved from 1.60028 to 1.59916, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 281/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8692 - mse: 4.8692 - mae: 1.7049 - val_loss: 3.9342 - val_mse: 3.9342 - val_mae: 1.5989\n",
      "\n",
      "Epoch 00281: val_mae improved from 1.59916 to 1.59887, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 282/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8785 - mse: 4.8785 - mae: 1.7116 - val_loss: 3.9295 - val_mse: 3.9295 - val_mae: 1.5975\n",
      "\n",
      "Epoch 00282: val_mae improved from 1.59887 to 1.59751, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 283/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4019 - mse: 5.4019 - mae: 1.7828 - val_loss: 3.9537 - val_mse: 3.9537 - val_mae: 1.6050\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 1.59751\n",
      "Epoch 284/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3250 - mse: 5.3250 - mae: 1.7505 - val_loss: 3.9499 - val_mse: 3.9499 - val_mae: 1.6039\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 1.59751\n",
      "Epoch 285/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0592 - mse: 5.0592 - mae: 1.7266 - val_loss: 3.9325 - val_mse: 3.9325 - val_mae: 1.5988\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 1.59751\n",
      "Epoch 286/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8888 - mse: 4.8888 - mae: 1.7194 - val_loss: 3.9249 - val_mse: 3.9249 - val_mae: 1.5966\n",
      "\n",
      "Epoch 00286: val_mae improved from 1.59751 to 1.59662, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 287/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8589 - mse: 4.8589 - mae: 1.7152 - val_loss: 3.9185 - val_mse: 3.9185 - val_mae: 1.5949\n",
      "\n",
      "Epoch 00287: val_mae improved from 1.59662 to 1.59489, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 288/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9083 - mse: 4.9083 - mae: 1.7257 - val_loss: 3.9174 - val_mse: 3.9174 - val_mae: 1.5945\n",
      "\n",
      "Epoch 00288: val_mae improved from 1.59489 to 1.59449, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 289/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1622 - mse: 6.1622 - mae: 1.9219 - val_loss: 3.9399 - val_mse: 3.9399 - val_mae: 1.6009\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 1.59449\n",
      "Epoch 290/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6744 - mse: 5.6744 - mae: 1.8246 - val_loss: 3.9110 - val_mse: 3.9110 - val_mae: 1.5926\n",
      "\n",
      "Epoch 00290: val_mae improved from 1.59449 to 1.59257, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 291/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.3097 - mse: 5.3097 - mae: 1.7791 - val_loss: 3.8996 - val_mse: 3.8996 - val_mae: 1.5892\n",
      "\n",
      "Epoch 00291: val_mae improved from 1.59257 to 1.58918, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 292/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8658 - mse: 4.8658 - mae: 1.7042 - val_loss: 3.8906 - val_mse: 3.8906 - val_mae: 1.5863\n",
      "\n",
      "Epoch 00292: val_mae improved from 1.58918 to 1.58628, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 293/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3132 - mse: 5.3132 - mae: 1.7271 - val_loss: 3.8866 - val_mse: 3.8866 - val_mae: 1.5849\n",
      "\n",
      "Epoch 00293: val_mae improved from 1.58628 to 1.58486, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 294/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1102 - mse: 5.1102 - mae: 1.6888 - val_loss: 3.8854 - val_mse: 3.8854 - val_mae: 1.5847\n",
      "\n",
      "Epoch 00294: val_mae improved from 1.58486 to 1.58472, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 295/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0133 - mse: 5.0133 - mae: 1.7283 - val_loss: 3.8906 - val_mse: 3.8906 - val_mae: 1.5867\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 1.58472\n",
      "Epoch 296/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0468 - mse: 5.0468 - mae: 1.7206 - val_loss: 3.8940 - val_mse: 3.8940 - val_mae: 1.5877\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 1.58472\n",
      "Epoch 297/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9327 - mse: 4.9327 - mae: 1.7158 - val_loss: 3.8855 - val_mse: 3.8855 - val_mae: 1.5852\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 1.58472\n",
      "Epoch 298/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7501 - mse: 4.7501 - mae: 1.6547 - val_loss: 3.8952 - val_mse: 3.8952 - val_mae: 1.5880\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 1.58472\n",
      "Epoch 299/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1447 - mse: 5.1447 - mae: 1.7718 - val_loss: 3.9021 - val_mse: 3.9021 - val_mae: 1.5899\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 1.58472\n",
      "Epoch 300/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4152 - mse: 4.4152 - mae: 1.6156 - val_loss: 3.9051 - val_mse: 3.9051 - val_mae: 1.5907\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 1.58472\n",
      "Epoch 301/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6778 - mse: 5.6778 - mae: 1.8256 - val_loss: 3.8847 - val_mse: 3.8847 - val_mae: 1.5851\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 1.58472\n",
      "Epoch 302/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7248 - mse: 4.7248 - mae: 1.6819 - val_loss: 3.8702 - val_mse: 3.8702 - val_mae: 1.5809\n",
      "\n",
      "Epoch 00302: val_mae improved from 1.58472 to 1.58088, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 303/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0432 - mse: 5.0432 - mae: 1.6740 - val_loss: 3.8641 - val_mse: 3.8641 - val_mae: 1.5787\n",
      "\n",
      "Epoch 00303: val_mae improved from 1.58088 to 1.57868, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 304/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7666 - mse: 4.7666 - mae: 1.6461 - val_loss: 3.8902 - val_mse: 3.8902 - val_mae: 1.5866\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 1.57868\n",
      "Epoch 305/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0412 - mse: 5.0412 - mae: 1.7083 - val_loss: 3.8748 - val_mse: 3.8748 - val_mae: 1.5822\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 1.57868\n",
      "Epoch 306/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8196 - mse: 4.8196 - mae: 1.7050 - val_loss: 3.8674 - val_mse: 3.8674 - val_mae: 1.5802\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 1.57868\n",
      "Epoch 307/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9859 - mse: 4.9859 - mae: 1.6814 - val_loss: 3.8667 - val_mse: 3.8667 - val_mae: 1.5800\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 1.57868\n",
      "Epoch 308/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7485 - mse: 5.7485 - mae: 1.7838 - val_loss: 3.8599 - val_mse: 3.8599 - val_mae: 1.5780\n",
      "\n",
      "Epoch 00308: val_mae improved from 1.57868 to 1.57803, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 309/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1286 - mse: 5.1286 - mae: 1.7436 - val_loss: 3.8458 - val_mse: 3.8458 - val_mae: 1.5733\n",
      "\n",
      "Epoch 00309: val_mae improved from 1.57803 to 1.57327, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 310/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1855 - mse: 5.1855 - mae: 1.7555 - val_loss: 3.8541 - val_mse: 3.8541 - val_mae: 1.5765\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 1.57327\n",
      "Epoch 311/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1748 - mse: 5.1748 - mae: 1.7074 - val_loss: 3.8524 - val_mse: 3.8524 - val_mae: 1.5759\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 1.57327\n",
      "Epoch 312/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8111 - mse: 4.8111 - mae: 1.6990 - val_loss: 3.8472 - val_mse: 3.8472 - val_mae: 1.5744\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 1.57327\n",
      "Epoch 313/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3361 - mse: 5.3361 - mae: 1.7509 - val_loss: 3.8417 - val_mse: 3.8417 - val_mae: 1.5725\n",
      "\n",
      "Epoch 00313: val_mae improved from 1.57327 to 1.57253, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 314/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2669 - mse: 5.2669 - mae: 1.7434 - val_loss: 3.8322 - val_mse: 3.8322 - val_mae: 1.5688\n",
      "\n",
      "Epoch 00314: val_mae improved from 1.57253 to 1.56884, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 315/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5023 - mse: 4.5023 - mae: 1.6289 - val_loss: 3.8349 - val_mse: 3.8349 - val_mae: 1.5703\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 1.56884\n",
      "Epoch 316/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9274 - mse: 4.9274 - mae: 1.7009 - val_loss: 3.8932 - val_mse: 3.8932 - val_mae: 1.5867\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 1.56884\n",
      "Epoch 317/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1416 - mse: 5.1416 - mae: 1.7268 - val_loss: 3.8436 - val_mse: 3.8436 - val_mae: 1.5736\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 1.56884\n",
      "Epoch 318/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7621 - mse: 5.7621 - mae: 1.7296 - val_loss: 3.8358 - val_mse: 3.8358 - val_mae: 1.5711\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 1.56884\n",
      "Epoch 319/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1880 - mse: 5.1880 - mae: 1.7253 - val_loss: 3.8439 - val_mse: 3.8439 - val_mae: 1.5738\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 1.56884\n",
      "Epoch 320/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6065 - mse: 4.6065 - mae: 1.6473 - val_loss: 3.8411 - val_mse: 3.8411 - val_mae: 1.5728\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 1.56884\n",
      "Epoch 321/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4575 - mse: 4.4575 - mae: 1.6353 - val_loss: 3.8257 - val_mse: 3.8257 - val_mae: 1.5678\n",
      "\n",
      "Epoch 00321: val_mae improved from 1.56884 to 1.56779, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 322/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0789 - mse: 5.0789 - mae: 1.7050 - val_loss: 3.8389 - val_mse: 3.8389 - val_mae: 1.5723\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 1.56779\n",
      "Epoch 323/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4750 - mse: 5.4750 - mae: 1.8227 - val_loss: 3.8338 - val_mse: 3.8338 - val_mae: 1.5709\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 1.56779\n",
      "Epoch 324/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8529 - mse: 4.8529 - mae: 1.7111 - val_loss: 3.8269 - val_mse: 3.8269 - val_mae: 1.5690\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 1.56779\n",
      "Epoch 325/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2886 - mse: 5.2886 - mae: 1.7383 - val_loss: 3.8249 - val_mse: 3.8249 - val_mae: 1.5683\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 1.56779\n",
      "Epoch 326/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3176 - mse: 5.3176 - mae: 1.7382 - val_loss: 3.8359 - val_mse: 3.8359 - val_mae: 1.5713\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 1.56779\n",
      "Epoch 327/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7611 - mse: 4.7611 - mae: 1.6794 - val_loss: 3.8083 - val_mse: 3.8083 - val_mae: 1.5625\n",
      "\n",
      "Epoch 00327: val_mae improved from 1.56779 to 1.56253, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 328/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.3590 - mse: 5.3590 - mae: 1.7639 - val_loss: 3.8131 - val_mse: 3.8131 - val_mae: 1.5645\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 1.56253\n",
      "Epoch 329/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4591 - mse: 4.4591 - mae: 1.6274 - val_loss: 3.8154 - val_mse: 3.8154 - val_mae: 1.5655\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 1.56253\n",
      "Epoch 330/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5983 - mse: 4.5983 - mae: 1.6711 - val_loss: 3.8020 - val_mse: 3.8020 - val_mae: 1.5608\n",
      "\n",
      "Epoch 00330: val_mae improved from 1.56253 to 1.56080, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 331/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5203 - mse: 4.5203 - mae: 1.6120 - val_loss: 3.8049 - val_mse: 3.8049 - val_mae: 1.5622\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 1.56080\n",
      "Epoch 332/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8240 - mse: 4.8240 - mae: 1.7011 - val_loss: 3.8124 - val_mse: 3.8124 - val_mae: 1.5647\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 1.56080\n",
      "Epoch 333/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2787 - mse: 5.2787 - mae: 1.7568 - val_loss: 3.8150 - val_mse: 3.8150 - val_mae: 1.5655\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 1.56080\n",
      "Epoch 334/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8802 - mse: 4.8802 - mae: 1.7061 - val_loss: 3.8115 - val_mse: 3.8115 - val_mae: 1.5645\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 1.56080\n",
      "Epoch 335/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6106 - mse: 4.6106 - mae: 1.6245 - val_loss: 3.8130 - val_mse: 3.8130 - val_mae: 1.5648\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 1.56080\n",
      "Epoch 336/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8588 - mse: 4.8588 - mae: 1.6789 - val_loss: 3.8060 - val_mse: 3.8060 - val_mae: 1.5628\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 1.56080\n",
      "Epoch 337/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8089 - mse: 4.8089 - mae: 1.6529 - val_loss: 3.7939 - val_mse: 3.7939 - val_mae: 1.5590\n",
      "\n",
      "Epoch 00337: val_mae improved from 1.56080 to 1.55899, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 338/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8971 - mse: 4.8971 - mae: 1.7028 - val_loss: 3.7919 - val_mse: 3.7919 - val_mae: 1.5586\n",
      "\n",
      "Epoch 00338: val_mae improved from 1.55899 to 1.55857, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 339/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3733 - mse: 5.3733 - mae: 1.7421 - val_loss: 3.8017 - val_mse: 3.8017 - val_mae: 1.5614\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 1.55857\n",
      "Epoch 340/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1915 - mse: 5.1915 - mae: 1.7511 - val_loss: 3.7935 - val_mse: 3.7935 - val_mae: 1.5590\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 1.55857\n",
      "Epoch 341/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8469 - mse: 4.8469 - mae: 1.6454 - val_loss: 3.7789 - val_mse: 3.7789 - val_mae: 1.5530\n",
      "\n",
      "Epoch 00341: val_mae improved from 1.55857 to 1.55302, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 342/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5252 - mse: 4.5252 - mae: 1.6267 - val_loss: 3.7936 - val_mse: 3.7936 - val_mae: 1.5591\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 1.55302\n",
      "Epoch 343/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4566 - mse: 5.4566 - mae: 1.7484 - val_loss: 3.7932 - val_mse: 3.7932 - val_mae: 1.5589\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 1.55302\n",
      "Epoch 344/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5923 - mse: 4.5923 - mae: 1.6631 - val_loss: 3.7973 - val_mse: 3.7973 - val_mae: 1.5600\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 1.55302\n",
      "Epoch 345/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7519 - mse: 4.7519 - mae: 1.6728 - val_loss: 3.7853 - val_mse: 3.7853 - val_mae: 1.5566\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 1.55302\n",
      "Epoch 346/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3473 - mse: 5.3473 - mae: 1.7221 - val_loss: 3.7923 - val_mse: 3.7923 - val_mae: 1.5586\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 1.55302\n",
      "Epoch 347/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2489 - mse: 5.2489 - mae: 1.7991 - val_loss: 3.7771 - val_mse: 3.7771 - val_mae: 1.5539\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 1.55302\n",
      "Epoch 348/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9691 - mse: 4.9691 - mae: 1.6878 - val_loss: 3.7747 - val_mse: 3.7747 - val_mae: 1.5529\n",
      "\n",
      "Epoch 00348: val_mae improved from 1.55302 to 1.55289, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 349/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8207 - mse: 4.8207 - mae: 1.6733 - val_loss: 3.7779 - val_mse: 3.7779 - val_mae: 1.5543\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 1.55289\n",
      "Epoch 350/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1891 - mse: 5.1891 - mae: 1.7479 - val_loss: 3.7842 - val_mse: 3.7842 - val_mae: 1.5562\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 1.55289\n",
      "Epoch 351/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2998 - mse: 5.2998 - mae: 1.7042 - val_loss: 3.7838 - val_mse: 3.7838 - val_mae: 1.5560\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 1.55289\n",
      "Epoch 352/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6340 - mse: 4.6340 - mae: 1.6671 - val_loss: 3.7669 - val_mse: 3.7669 - val_mae: 1.5504\n",
      "\n",
      "Epoch 00352: val_mae improved from 1.55289 to 1.55039, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 353/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1147 - mse: 5.1147 - mae: 1.6849 - val_loss: 3.7787 - val_mse: 3.7787 - val_mae: 1.5545\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 1.55039\n",
      "Epoch 354/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9888 - mse: 4.9888 - mae: 1.7114 - val_loss: 3.7876 - val_mse: 3.7876 - val_mae: 1.5567\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 1.55039\n",
      "Epoch 355/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8974 - mse: 4.8974 - mae: 1.6708 - val_loss: 3.7605 - val_mse: 3.7605 - val_mae: 1.5480\n",
      "\n",
      "Epoch 00355: val_mae improved from 1.55039 to 1.54803, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 356/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2034 - mse: 5.2034 - mae: 1.7118 - val_loss: 3.7761 - val_mse: 3.7761 - val_mae: 1.5538\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 1.54803\n",
      "Epoch 357/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.7533 - mse: 4.7533 - mae: 1.6741 - val_loss: 3.7738 - val_mse: 3.7738 - val_mae: 1.5530\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 1.54803\n",
      "Epoch 358/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8189 - mse: 4.8189 - mae: 1.6992 - val_loss: 3.7592 - val_mse: 3.7592 - val_mae: 1.5478\n",
      "\n",
      "Epoch 00358: val_mae improved from 1.54803 to 1.54782, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 359/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6264 - mse: 4.6264 - mae: 1.6597 - val_loss: 3.7772 - val_mse: 3.7772 - val_mae: 1.5537\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 1.54782\n",
      "Epoch 360/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8703 - mse: 4.8703 - mae: 1.6792 - val_loss: 3.7595 - val_mse: 3.7595 - val_mae: 1.5482\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 1.54782\n",
      "Epoch 361/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7304 - mse: 4.7304 - mae: 1.6527 - val_loss: 3.7615 - val_mse: 3.7615 - val_mae: 1.5490\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 1.54782\n",
      "Epoch 362/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2472 - mse: 5.2472 - mae: 1.7161 - val_loss: 3.7734 - val_mse: 3.7734 - val_mae: 1.5524\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 1.54782\n",
      "Epoch 363/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6186 - mse: 5.6186 - mae: 1.7409 - val_loss: 3.7623 - val_mse: 3.7623 - val_mae: 1.5492\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 1.54782\n",
      "Epoch 364/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2502 - mse: 5.2502 - mae: 1.7773 - val_loss: 3.7555 - val_mse: 3.7555 - val_mae: 1.5466\n",
      "\n",
      "Epoch 00364: val_mae improved from 1.54782 to 1.54657, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 365/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8102 - mse: 4.8102 - mae: 1.6612 - val_loss: 3.7785 - val_mse: 3.7785 - val_mae: 1.5532\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 1.54657\n",
      "Epoch 366/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3532 - mse: 5.3532 - mae: 1.7584 - val_loss: 3.7545 - val_mse: 3.7545 - val_mae: 1.5463\n",
      "\n",
      "Epoch 00366: val_mae improved from 1.54657 to 1.54629, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 367/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3511 - mse: 5.3511 - mae: 1.7387 - val_loss: 3.7683 - val_mse: 3.7683 - val_mae: 1.5506\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 1.54629\n",
      "Epoch 368/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5529 - mse: 5.5529 - mae: 1.7278 - val_loss: 3.7630 - val_mse: 3.7630 - val_mae: 1.5491\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 1.54629\n",
      "Epoch 369/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7698 - mse: 4.7698 - mae: 1.6777 - val_loss: 3.7494 - val_mse: 3.7494 - val_mae: 1.5444\n",
      "\n",
      "Epoch 00369: val_mae improved from 1.54629 to 1.54444, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 370/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2313 - mse: 5.2313 - mae: 1.7414 - val_loss: 3.7603 - val_mse: 3.7603 - val_mae: 1.5484\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 1.54444\n",
      "Epoch 371/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7651 - mse: 4.7651 - mae: 1.6815 - val_loss: 3.7564 - val_mse: 3.7564 - val_mae: 1.5471\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 1.54444\n",
      "Epoch 372/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4569 - mse: 5.4569 - mae: 1.7261 - val_loss: 3.7520 - val_mse: 3.7520 - val_mae: 1.5457\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 1.54444\n",
      "Epoch 373/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1962 - mse: 5.1962 - mae: 1.7105 - val_loss: 3.7511 - val_mse: 3.7511 - val_mae: 1.5453\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 1.54444\n",
      "Epoch 374/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5905 - mse: 4.5905 - mae: 1.6551 - val_loss: 3.7503 - val_mse: 3.7503 - val_mae: 1.5450\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 1.54444\n",
      "Epoch 375/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4943 - mse: 4.4943 - mae: 1.5698 - val_loss: 3.7549 - val_mse: 3.7549 - val_mae: 1.5465\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 1.54444\n",
      "Epoch 376/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7903 - mse: 4.7903 - mae: 1.6918 - val_loss: 3.7678 - val_mse: 3.7678 - val_mae: 1.5495\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 1.54444\n",
      "Epoch 377/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0442 - mse: 5.0442 - mae: 1.6921 - val_loss: 3.7540 - val_mse: 3.7540 - val_mae: 1.5460\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 1.54444\n",
      "Epoch 378/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9647 - mse: 5.9647 - mae: 1.8185 - val_loss: 3.7666 - val_mse: 3.7666 - val_mae: 1.5492\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 1.54444\n",
      "Epoch 379/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5094 - mse: 5.5094 - mae: 1.7627 - val_loss: 3.7528 - val_mse: 3.7528 - val_mae: 1.5456\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 1.54444\n",
      "Epoch 380/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1396 - mse: 5.1396 - mae: 1.7337 - val_loss: 3.7373 - val_mse: 3.7373 - val_mae: 1.5401\n",
      "\n",
      "Epoch 00380: val_mae improved from 1.54444 to 1.54010, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 381/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8432 - mse: 4.8432 - mae: 1.7016 - val_loss: 3.7521 - val_mse: 3.7521 - val_mae: 1.5453\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 1.54010\n",
      "Epoch 382/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1992 - mse: 5.1992 - mae: 1.7201 - val_loss: 3.7544 - val_mse: 3.7544 - val_mae: 1.5457\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 1.54010\n",
      "Epoch 383/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8278 - mse: 4.8278 - mae: 1.6959 - val_loss: 3.7547 - val_mse: 3.7547 - val_mae: 1.5455\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 1.54010\n",
      "Epoch 384/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8174 - mse: 4.8174 - mae: 1.6755 - val_loss: 3.7391 - val_mse: 3.7391 - val_mae: 1.5410\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 1.54010\n",
      "Epoch 385/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6900 - mse: 4.6900 - mae: 1.6696 - val_loss: 3.7545 - val_mse: 3.7545 - val_mae: 1.5454\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 1.54010\n",
      "Epoch 386/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.5356 - mse: 4.5356 - mae: 1.6279 - val_loss: 3.7499 - val_mse: 3.7499 - val_mae: 1.5442\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 1.54010\n",
      "Epoch 387/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3925 - mse: 4.3925 - mae: 1.6366 - val_loss: 3.7372 - val_mse: 3.7372 - val_mae: 1.5403\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 1.54010\n",
      "Epoch 388/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2285 - mse: 5.2285 - mae: 1.7289 - val_loss: 3.7714 - val_mse: 3.7714 - val_mae: 1.5489\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 1.54010\n",
      "Epoch 389/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2064 - mse: 5.2064 - mae: 1.7342 - val_loss: 3.7398 - val_mse: 3.7398 - val_mae: 1.5410\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 1.54010\n",
      "Epoch 390/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5831 - mse: 5.5831 - mae: 1.7271 - val_loss: 3.7385 - val_mse: 3.7385 - val_mae: 1.5405\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 1.54010\n",
      "Epoch 391/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5900 - mse: 4.5900 - mae: 1.6551 - val_loss: 3.7384 - val_mse: 3.7384 - val_mae: 1.5404\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 1.54010\n",
      "Epoch 392/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6437 - mse: 4.6437 - mae: 1.6529 - val_loss: 3.7499 - val_mse: 3.7499 - val_mae: 1.5435\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 1.54010\n",
      "Epoch 393/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8610 - mse: 4.8610 - mae: 1.6983 - val_loss: 3.7330 - val_mse: 3.7330 - val_mae: 1.5387\n",
      "\n",
      "Epoch 00393: val_mae improved from 1.54010 to 1.53869, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 394/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8059 - mse: 4.8059 - mae: 1.6484 - val_loss: 3.7430 - val_mse: 3.7430 - val_mae: 1.5415\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 1.53869\n",
      "Epoch 395/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0795 - mse: 5.0795 - mae: 1.7556 - val_loss: 3.7317 - val_mse: 3.7317 - val_mae: 1.5383\n",
      "\n",
      "Epoch 00395: val_mae improved from 1.53869 to 1.53833, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 396/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8164 - mse: 4.8164 - mae: 1.6791 - val_loss: 3.7381 - val_mse: 3.7381 - val_mae: 1.5402\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 1.53833\n",
      "Epoch 397/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6467 - mse: 4.6467 - mae: 1.6616 - val_loss: 3.7268 - val_mse: 3.7268 - val_mae: 1.5367\n",
      "\n",
      "Epoch 00397: val_mae improved from 1.53833 to 1.53667, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 398/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8063 - mse: 4.8063 - mae: 1.6331 - val_loss: 3.7210 - val_mse: 3.7210 - val_mae: 1.5343\n",
      "\n",
      "Epoch 00398: val_mae improved from 1.53667 to 1.53434, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 399/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2667 - mse: 5.2667 - mae: 1.7266 - val_loss: 3.7429 - val_mse: 3.7429 - val_mae: 1.5411\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 1.53434\n",
      "Epoch 400/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3197 - mse: 5.3197 - mae: 1.7063 - val_loss: 3.7250 - val_mse: 3.7250 - val_mae: 1.5359\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 1.53434\n",
      "Epoch 401/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6366 - mse: 4.6366 - mae: 1.6303 - val_loss: 3.7279 - val_mse: 3.7279 - val_mae: 1.5369\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 1.53434\n",
      "Epoch 402/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0722 - mse: 5.0722 - mae: 1.6818 - val_loss: 3.7181 - val_mse: 3.7181 - val_mae: 1.5332\n",
      "\n",
      "Epoch 00402: val_mae improved from 1.53434 to 1.53323, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 403/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0604 - mse: 5.0604 - mae: 1.7256 - val_loss: 3.7214 - val_mse: 3.7214 - val_mae: 1.5347\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 1.53323\n",
      "Epoch 404/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7672 - mse: 4.7672 - mae: 1.6571 - val_loss: 3.7173 - val_mse: 3.7173 - val_mae: 1.5329\n",
      "\n",
      "Epoch 00404: val_mae improved from 1.53323 to 1.53286, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 405/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8793 - mse: 4.8793 - mae: 1.6849 - val_loss: 3.7132 - val_mse: 3.7132 - val_mae: 1.5306\n",
      "\n",
      "Epoch 00405: val_mae improved from 1.53286 to 1.53064, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 406/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7809 - mse: 4.7809 - mae: 1.6600 - val_loss: 3.7113 - val_mse: 3.7113 - val_mae: 1.5297\n",
      "\n",
      "Epoch 00406: val_mae improved from 1.53064 to 1.52972, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 407/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8855 - mse: 4.8855 - mae: 1.6771 - val_loss: 3.7267 - val_mse: 3.7267 - val_mae: 1.5361\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 1.52972\n",
      "Epoch 408/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0993 - mse: 5.0993 - mae: 1.7115 - val_loss: 3.7262 - val_mse: 3.7262 - val_mae: 1.5360\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 1.52972\n",
      "Epoch 409/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4271 - mse: 5.4271 - mae: 1.8021 - val_loss: 3.7263 - val_mse: 3.7263 - val_mae: 1.5360\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 1.52972\n",
      "Epoch 410/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2638 - mse: 4.2638 - mae: 1.6145 - val_loss: 3.7304 - val_mse: 3.7304 - val_mae: 1.5371\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 1.52972\n",
      "Epoch 411/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9953 - mse: 4.9953 - mae: 1.7443 - val_loss: 3.7556 - val_mse: 3.7556 - val_mae: 1.5433\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 1.52972\n",
      "Epoch 412/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9350 - mse: 4.9350 - mae: 1.6476 - val_loss: 3.7224 - val_mse: 3.7224 - val_mae: 1.5348\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 1.52972\n",
      "Epoch 413/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1822 - mse: 5.1822 - mae: 1.7193 - val_loss: 3.7327 - val_mse: 3.7327 - val_mae: 1.5377\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 1.52972\n",
      "Epoch 414/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0896 - mse: 5.0896 - mae: 1.6897 - val_loss: 3.7508 - val_mse: 3.7508 - val_mae: 1.5420\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 1.52972\n",
      "Epoch 415/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5793 - mse: 4.5793 - mae: 1.6212 - val_loss: 3.7220 - val_mse: 3.7220 - val_mae: 1.5346\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 1.52972\n",
      "Epoch 416/1000\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 5.2661 - mse: 5.2661 - mae: 1.7599 - val_loss: 3.7193 - val_mse: 3.7193 - val_mae: 1.5338\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 1.52972\n",
      "Epoch 417/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3537 - mse: 5.3537 - mae: 1.7417 - val_loss: 3.7355 - val_mse: 3.7355 - val_mae: 1.5383\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 1.52972\n",
      "Epoch 418/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8475 - mse: 4.8475 - mae: 1.6500 - val_loss: 3.7232 - val_mse: 3.7232 - val_mae: 1.5349\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 1.52972\n",
      "Epoch 419/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2660 - mse: 5.2660 - mae: 1.7362 - val_loss: 3.7369 - val_mse: 3.7369 - val_mae: 1.5384\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 1.52972\n",
      "Epoch 420/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7257 - mse: 4.7257 - mae: 1.6398 - val_loss: 3.7235 - val_mse: 3.7235 - val_mae: 1.5348\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 1.52972\n",
      "Epoch 421/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9748 - mse: 4.9748 - mae: 1.6761 - val_loss: 3.7086 - val_mse: 3.7086 - val_mae: 1.5298\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 1.52972\n",
      "Epoch 422/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6154 - mse: 5.6154 - mae: 1.7922 - val_loss: 3.7140 - val_mse: 3.7140 - val_mae: 1.5320\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 1.52972\n",
      "Epoch 423/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9119 - mse: 4.9119 - mae: 1.7030 - val_loss: 3.7234 - val_mse: 3.7234 - val_mae: 1.5347\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 1.52972\n",
      "Epoch 424/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6967 - mse: 4.6967 - mae: 1.6703 - val_loss: 3.7332 - val_mse: 3.7332 - val_mae: 1.5371\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 1.52972\n",
      "Epoch 425/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7988 - mse: 5.7988 - mae: 1.8371 - val_loss: 3.7245 - val_mse: 3.7245 - val_mae: 1.5348\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 1.52972\n",
      "Epoch 426/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6829 - mse: 4.6829 - mae: 1.6787 - val_loss: 3.7166 - val_mse: 3.7166 - val_mae: 1.5326\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 1.52972\n",
      "Epoch 427/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2677 - mse: 5.2677 - mae: 1.7669 - val_loss: 3.7253 - val_mse: 3.7253 - val_mae: 1.5350\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 1.52972\n",
      "Epoch 428/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0672 - mse: 5.0672 - mae: 1.7030 - val_loss: 3.7274 - val_mse: 3.7274 - val_mae: 1.5355\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 1.52972\n",
      "Epoch 429/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2389 - mse: 5.2389 - mae: 1.7605 - val_loss: 3.7184 - val_mse: 3.7184 - val_mae: 1.5330\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 1.52972\n",
      "Epoch 430/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5519 - mse: 5.5519 - mae: 1.7725 - val_loss: 3.7148 - val_mse: 3.7148 - val_mae: 1.5320\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 1.52972\n",
      "Epoch 431/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4711 - mse: 5.4711 - mae: 1.6960 - val_loss: 3.7149 - val_mse: 3.7149 - val_mae: 1.5320\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 1.52972\n",
      "Epoch 432/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4800 - mse: 5.4800 - mae: 1.7809 - val_loss: 3.7090 - val_mse: 3.7090 - val_mae: 1.5299\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 1.52972\n",
      "Epoch 433/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6241 - mse: 4.6241 - mae: 1.6639 - val_loss: 3.7047 - val_mse: 3.7047 - val_mae: 1.5283\n",
      "\n",
      "Epoch 00433: val_mae improved from 1.52972 to 1.52829, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 434/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.8858 - mse: 4.8858 - mae: 1.6291 - val_loss: 3.7197 - val_mse: 3.7197 - val_mae: 1.5329\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 1.52829\n",
      "Epoch 435/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6059 - mse: 5.6059 - mae: 1.7911 - val_loss: 3.7227 - val_mse: 3.7227 - val_mae: 1.5336\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 1.52829\n",
      "Epoch 436/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5244 - mse: 4.5244 - mae: 1.6220 - val_loss: 3.7111 - val_mse: 3.7111 - val_mae: 1.5303\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 1.52829\n",
      "Epoch 437/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7443 - mse: 4.7443 - mae: 1.6722 - val_loss: 3.7084 - val_mse: 3.7084 - val_mae: 1.5294\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 1.52829\n",
      "Epoch 438/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8481 - mse: 4.8481 - mae: 1.6295 - val_loss: 3.7087 - val_mse: 3.7087 - val_mae: 1.5294\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 1.52829\n",
      "Epoch 439/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5611 - mse: 5.5611 - mae: 1.7893 - val_loss: 3.7253 - val_mse: 3.7253 - val_mae: 1.5338\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 1.52829\n",
      "Epoch 440/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1854 - mse: 5.1854 - mae: 1.7274 - val_loss: 3.7325 - val_mse: 3.7325 - val_mae: 1.5355\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 1.52829\n",
      "Epoch 441/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7735 - mse: 4.7735 - mae: 1.6860 - val_loss: 3.7074 - val_mse: 3.7074 - val_mae: 1.5289\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 1.52829\n",
      "Epoch 442/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3892 - mse: 4.3892 - mae: 1.5934 - val_loss: 3.7113 - val_mse: 3.7113 - val_mae: 1.5300\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 1.52829\n",
      "Epoch 443/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9504 - mse: 4.9504 - mae: 1.6855 - val_loss: 3.7032 - val_mse: 3.7032 - val_mae: 1.5273\n",
      "\n",
      "Epoch 00443: val_mae improved from 1.52829 to 1.52735, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 444/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6338 - mse: 5.6338 - mae: 1.7332 - val_loss: 3.7067 - val_mse: 3.7067 - val_mae: 1.5285\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 1.52735\n",
      "Epoch 445/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4688 - mse: 4.4688 - mae: 1.6271 - val_loss: 3.6903 - val_mse: 3.6903 - val_mae: 1.5221\n",
      "\n",
      "Epoch 00445: val_mae improved from 1.52735 to 1.52214, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 446/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3268 - mse: 5.3268 - mae: 1.7576 - val_loss: 3.6974 - val_mse: 3.6974 - val_mae: 1.5253\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 1.52214\n",
      "Epoch 447/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4005 - mse: 5.4005 - mae: 1.7162 - val_loss: 3.6988 - val_mse: 3.6988 - val_mae: 1.5258\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 1.52214\n",
      "Epoch 448/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8593 - mse: 4.8593 - mae: 1.6983 - val_loss: 3.7028 - val_mse: 3.7028 - val_mae: 1.5270\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 1.52214\n",
      "Epoch 449/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7666 - mse: 5.7666 - mae: 1.7639 - val_loss: 3.6963 - val_mse: 3.6963 - val_mae: 1.5248\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 1.52214\n",
      "Epoch 450/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8583 - mse: 4.8583 - mae: 1.6981 - val_loss: 3.7072 - val_mse: 3.7072 - val_mae: 1.5282\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 1.52214\n",
      "Epoch 451/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0276 - mse: 5.0276 - mae: 1.6870 - val_loss: 3.6989 - val_mse: 3.6989 - val_mae: 1.5255\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 1.52214\n",
      "Epoch 452/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6571 - mse: 5.6571 - mae: 1.7624 - val_loss: 3.7125 - val_mse: 3.7125 - val_mae: 1.5293\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 1.52214\n",
      "Epoch 453/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5215 - mse: 4.5215 - mae: 1.5914 - val_loss: 3.7107 - val_mse: 3.7107 - val_mae: 1.5285\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 1.52214\n",
      "Epoch 454/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7510 - mse: 4.7510 - mae: 1.6723 - val_loss: 3.6952 - val_mse: 3.6952 - val_mae: 1.5240\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 1.52214\n",
      "Epoch 455/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0170 - mse: 5.0170 - mae: 1.7352 - val_loss: 3.6956 - val_mse: 3.6956 - val_mae: 1.5242\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 1.52214\n",
      "Epoch 456/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5880 - mse: 4.5880 - mae: 1.6847 - val_loss: 3.7097 - val_mse: 3.7097 - val_mae: 1.5280\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 1.52214\n",
      "Epoch 457/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6344 - mse: 4.6344 - mae: 1.6319 - val_loss: 3.7042 - val_mse: 3.7042 - val_mae: 1.5264\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 1.52214\n",
      "Epoch 458/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9642 - mse: 4.9642 - mae: 1.7171 - val_loss: 3.6995 - val_mse: 3.6995 - val_mae: 1.5251\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 1.52214\n",
      "Epoch 459/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5057 - mse: 5.5057 - mae: 1.7643 - val_loss: 3.7089 - val_mse: 3.7089 - val_mae: 1.5277\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 1.52214\n",
      "Epoch 460/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4664 - mse: 5.4664 - mae: 1.7402 - val_loss: 3.7061 - val_mse: 3.7061 - val_mae: 1.5268\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 1.52214\n",
      "Epoch 461/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0939 - mse: 5.0939 - mae: 1.7720 - val_loss: 3.6973 - val_mse: 3.6973 - val_mae: 1.5243\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 1.52214\n",
      "Epoch 462/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4453 - mse: 5.4453 - mae: 1.7615 - val_loss: 3.7122 - val_mse: 3.7122 - val_mae: 1.5284\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 1.52214\n",
      "Epoch 463/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1124 - mse: 5.1124 - mae: 1.6922 - val_loss: 3.7039 - val_mse: 3.7039 - val_mae: 1.5258\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 1.52214\n",
      "Epoch 464/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5753 - mse: 4.5753 - mae: 1.6621 - val_loss: 3.6988 - val_mse: 3.6988 - val_mae: 1.5244\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 1.52214\n",
      "Epoch 465/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6040 - mse: 4.6040 - mae: 1.6316 - val_loss: 3.6939 - val_mse: 3.6939 - val_mae: 1.5229\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 1.52214\n",
      "Epoch 466/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3716 - mse: 5.3716 - mae: 1.7450 - val_loss: 3.6823 - val_mse: 3.6823 - val_mae: 1.5182\n",
      "\n",
      "Epoch 00466: val_mae improved from 1.52214 to 1.51824, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 467/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.1348 - mse: 5.1348 - mae: 1.6838 - val_loss: 3.6942 - val_mse: 3.6942 - val_mae: 1.5228\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 1.51824\n",
      "Epoch 468/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5395 - mse: 4.5395 - mae: 1.6403 - val_loss: 3.6957 - val_mse: 3.6957 - val_mae: 1.5234\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 1.51824\n",
      "Epoch 469/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0506 - mse: 5.0506 - mae: 1.7195 - val_loss: 3.6918 - val_mse: 3.6918 - val_mae: 1.5222\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 1.51824\n",
      "Epoch 470/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2806 - mse: 5.2806 - mae: 1.7228 - val_loss: 3.7132 - val_mse: 3.7132 - val_mae: 1.5283\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 1.51824\n",
      "Epoch 471/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6257 - mse: 4.6257 - mae: 1.6011 - val_loss: 3.7039 - val_mse: 3.7039 - val_mae: 1.5257\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 1.51824\n",
      "Epoch 472/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9791 - mse: 4.9791 - mae: 1.6987 - val_loss: 3.6967 - val_mse: 3.6967 - val_mae: 1.5236\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 1.51824\n",
      "Epoch 473/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9176 - mse: 5.9176 - mae: 1.7781 - val_loss: 3.7055 - val_mse: 3.7055 - val_mae: 1.5261\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 1.51824\n",
      "Epoch 474/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9711 - mse: 4.9711 - mae: 1.6810 - val_loss: 3.6947 - val_mse: 3.6947 - val_mae: 1.5229\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 1.51824\n",
      "Epoch 475/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7832 - mse: 4.7832 - mae: 1.6693 - val_loss: 3.6994 - val_mse: 3.6994 - val_mae: 1.5241\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 1.51824\n",
      "Epoch 476/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.7071 - mse: 5.7071 - mae: 1.7538 - val_loss: 3.6832 - val_mse: 3.6832 - val_mae: 1.5189\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 1.51824\n",
      "Epoch 477/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7795 - mse: 4.7795 - mae: 1.6202 - val_loss: 3.6841 - val_mse: 3.6841 - val_mae: 1.5192\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 1.51824\n",
      "Epoch 478/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1049 - mse: 5.1049 - mae: 1.6771 - val_loss: 3.6856 - val_mse: 3.6856 - val_mae: 1.5198\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 1.51824\n",
      "Epoch 479/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4670 - mse: 4.4670 - mae: 1.5795 - val_loss: 3.6929 - val_mse: 3.6929 - val_mae: 1.5222\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 1.51824\n",
      "Epoch 480/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7972 - mse: 4.7972 - mae: 1.6935 - val_loss: 3.6872 - val_mse: 3.6872 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 1.51824\n",
      "Epoch 481/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4671 - mse: 4.4671 - mae: 1.6661 - val_loss: 3.6846 - val_mse: 3.6846 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 1.51824\n",
      "Epoch 482/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8090 - mse: 4.8090 - mae: 1.6517 - val_loss: 3.7027 - val_mse: 3.7027 - val_mae: 1.5248\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 1.51824\n",
      "Epoch 483/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9031 - mse: 4.9031 - mae: 1.7261 - val_loss: 3.6915 - val_mse: 3.6915 - val_mae: 1.5216\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 1.51824\n",
      "Epoch 484/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8447 - mse: 4.8447 - mae: 1.6870 - val_loss: 3.7065 - val_mse: 3.7065 - val_mae: 1.5257\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 1.51824\n",
      "Epoch 485/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7365 - mse: 4.7365 - mae: 1.6449 - val_loss: 3.6957 - val_mse: 3.6957 - val_mae: 1.5225\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 1.51824\n",
      "Epoch 486/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8209 - mse: 4.8209 - mae: 1.6577 - val_loss: 3.6825 - val_mse: 3.6825 - val_mae: 1.5181\n",
      "\n",
      "Epoch 00486: val_mae improved from 1.51824 to 1.51808, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 487/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9120 - mse: 4.9120 - mae: 1.6708 - val_loss: 3.6831 - val_mse: 3.6831 - val_mae: 1.5183\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 1.51808\n",
      "Epoch 488/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5956 - mse: 5.5956 - mae: 1.7747 - val_loss: 3.6886 - val_mse: 3.6886 - val_mae: 1.5203\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 1.51808\n",
      "Epoch 489/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8422 - mse: 4.8422 - mae: 1.6855 - val_loss: 3.6860 - val_mse: 3.6860 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 1.51808\n",
      "Epoch 490/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6250 - mse: 4.6250 - mae: 1.6359 - val_loss: 3.6883 - val_mse: 3.6883 - val_mae: 1.5201\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 1.51808\n",
      "Epoch 491/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5741 - mse: 4.5741 - mae: 1.6478 - val_loss: 3.6810 - val_mse: 3.6810 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00491: val_mae improved from 1.51808 to 1.51720, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 492/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8491 - mse: 4.8491 - mae: 1.6967 - val_loss: 3.6877 - val_mse: 3.6877 - val_mae: 1.5198\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 1.51720\n",
      "Epoch 493/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1854 - mse: 5.1854 - mae: 1.7082 - val_loss: 3.6854 - val_mse: 3.6854 - val_mae: 1.5191\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 1.51720\n",
      "Epoch 494/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9358 - mse: 4.9358 - mae: 1.7204 - val_loss: 3.6880 - val_mse: 3.6880 - val_mae: 1.5200\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 1.51720\n",
      "Epoch 495/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0537 - mse: 5.0537 - mae: 1.6858 - val_loss: 3.6792 - val_mse: 3.6792 - val_mae: 1.5164\n",
      "\n",
      "Epoch 00495: val_mae improved from 1.51720 to 1.51639, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 496/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9407 - mse: 4.9407 - mae: 1.6803 - val_loss: 3.6960 - val_mse: 3.6960 - val_mae: 1.5223\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 1.51639\n",
      "Epoch 497/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1738 - mse: 5.1738 - mae: 1.7003 - val_loss: 3.6865 - val_mse: 3.6865 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 1.51639\n",
      "Epoch 498/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2803 - mse: 5.2803 - mae: 1.6519 - val_loss: 3.6818 - val_mse: 3.6818 - val_mae: 1.5175\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 1.51639\n",
      "Epoch 499/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7637 - mse: 4.7637 - mae: 1.6837 - val_loss: 3.6844 - val_mse: 3.6844 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 1.51639\n",
      "Epoch 500/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2143 - mse: 5.2143 - mae: 1.7099 - val_loss: 3.6847 - val_mse: 3.6847 - val_mae: 1.5184\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 1.51639\n",
      "Epoch 501/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7508 - mse: 4.7508 - mae: 1.6459 - val_loss: 3.6942 - val_mse: 3.6942 - val_mae: 1.5214\n",
      "\n",
      "Epoch 00501: val_mae did not improve from 1.51639\n",
      "Epoch 502/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3077 - mse: 5.3077 - mae: 1.6753 - val_loss: 3.7039 - val_mse: 3.7039 - val_mae: 1.5241\n",
      "\n",
      "Epoch 00502: val_mae did not improve from 1.51639\n",
      "Epoch 503/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6645 - mse: 4.6645 - mae: 1.6135 - val_loss: 3.6885 - val_mse: 3.6885 - val_mae: 1.5196\n",
      "\n",
      "Epoch 00503: val_mae did not improve from 1.51639\n",
      "Epoch 504/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9514 - mse: 4.9514 - mae: 1.7086 - val_loss: 3.6934 - val_mse: 3.6934 - val_mae: 1.5211\n",
      "\n",
      "Epoch 00504: val_mae did not improve from 1.51639\n",
      "Epoch 505/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5562 - mse: 4.5562 - mae: 1.6074 - val_loss: 3.7029 - val_mse: 3.7029 - val_mae: 1.5236\n",
      "\n",
      "Epoch 00505: val_mae did not improve from 1.51639\n",
      "Epoch 506/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.7699 - mse: 4.7699 - mae: 1.6931 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5195\n",
      "\n",
      "Epoch 00506: val_mae did not improve from 1.51639\n",
      "Epoch 507/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.1510 - mse: 5.1510 - mae: 1.6963 - val_loss: 3.6897 - val_mse: 3.6897 - val_mae: 1.5199\n",
      "\n",
      "Epoch 00507: val_mae did not improve from 1.51639\n",
      "Epoch 508/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.0152 - mse: 5.0152 - mae: 1.7267 - val_loss: 3.6837 - val_mse: 3.6837 - val_mae: 1.5180\n",
      "\n",
      "Epoch 00508: val_mae did not improve from 1.51639\n",
      "Epoch 509/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3378 - mse: 4.3378 - mae: 1.5835 - val_loss: 3.6975 - val_mse: 3.6975 - val_mae: 1.5222\n",
      "\n",
      "Epoch 00509: val_mae did not improve from 1.51639\n",
      "Epoch 510/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6926 - mse: 4.6926 - mae: 1.6711 - val_loss: 3.7085 - val_mse: 3.7085 - val_mae: 1.5251\n",
      "\n",
      "Epoch 00510: val_mae did not improve from 1.51639\n",
      "Epoch 511/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3217 - mse: 5.3217 - mae: 1.6999 - val_loss: 3.7033 - val_mse: 3.7033 - val_mae: 1.5237\n",
      "\n",
      "Epoch 00511: val_mae did not improve from 1.51639\n",
      "Epoch 512/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0798 - mse: 5.0798 - mae: 1.7323 - val_loss: 3.6900 - val_mse: 3.6900 - val_mae: 1.5199\n",
      "\n",
      "Epoch 00512: val_mae did not improve from 1.51639\n",
      "Epoch 513/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5386 - mse: 4.5386 - mae: 1.6150 - val_loss: 3.6837 - val_mse: 3.6837 - val_mae: 1.5179\n",
      "\n",
      "Epoch 00513: val_mae did not improve from 1.51639\n",
      "Epoch 514/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8592 - mse: 4.8592 - mae: 1.7178 - val_loss: 3.6866 - val_mse: 3.6866 - val_mae: 1.5187\n",
      "\n",
      "Epoch 00514: val_mae did not improve from 1.51639\n",
      "Epoch 515/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8494 - mse: 4.8494 - mae: 1.6904 - val_loss: 3.7106 - val_mse: 3.7106 - val_mae: 1.5252\n",
      "\n",
      "Epoch 00515: val_mae did not improve from 1.51639\n",
      "Epoch 516/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2250 - mse: 5.2250 - mae: 1.7428 - val_loss: 3.6918 - val_mse: 3.6918 - val_mae: 1.5202\n",
      "\n",
      "Epoch 00516: val_mae did not improve from 1.51639\n",
      "Epoch 517/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3071 - mse: 4.3071 - mae: 1.6287 - val_loss: 3.6838 - val_mse: 3.6838 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00517: val_mae did not improve from 1.51639\n",
      "Epoch 518/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0017 - mse: 5.0017 - mae: 1.6634 - val_loss: 3.7060 - val_mse: 3.7060 - val_mae: 1.5239\n",
      "\n",
      "Epoch 00518: val_mae did not improve from 1.51639\n",
      "Epoch 519/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7908 - mse: 5.7908 - mae: 1.8010 - val_loss: 3.6899 - val_mse: 3.6899 - val_mae: 1.5193\n",
      "\n",
      "Epoch 00519: val_mae did not improve from 1.51639\n",
      "Epoch 520/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9805 - mse: 4.9805 - mae: 1.6561 - val_loss: 3.6835 - val_mse: 3.6835 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00520: val_mae did not improve from 1.51639\n",
      "Epoch 521/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1907 - mse: 5.1907 - mae: 1.7577 - val_loss: 3.6923 - val_mse: 3.6923 - val_mae: 1.5200\n",
      "\n",
      "Epoch 00521: val_mae did not improve from 1.51639\n",
      "Epoch 522/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7178 - mse: 5.7178 - mae: 1.7508 - val_loss: 3.6970 - val_mse: 3.6970 - val_mae: 1.5213\n",
      "\n",
      "Epoch 00522: val_mae did not improve from 1.51639\n",
      "Epoch 523/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4029 - mse: 4.4029 - mae: 1.6052 - val_loss: 3.6960 - val_mse: 3.6960 - val_mae: 1.5210\n",
      "\n",
      "Epoch 00523: val_mae did not improve from 1.51639\n",
      "Epoch 524/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2526 - mse: 5.2526 - mae: 1.7302 - val_loss: 3.6797 - val_mse: 3.6797 - val_mae: 1.5159\n",
      "\n",
      "Epoch 00524: val_mae improved from 1.51639 to 1.51585, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 525/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3707 - mse: 5.3707 - mae: 1.6763 - val_loss: 3.6866 - val_mse: 3.6866 - val_mae: 1.5181\n",
      "\n",
      "Epoch 00525: val_mae did not improve from 1.51585\n",
      "Epoch 526/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7147 - mse: 4.7147 - mae: 1.6200 - val_loss: 3.6819 - val_mse: 3.6819 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00526: val_mae did not improve from 1.51585\n",
      "Epoch 527/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2033 - mse: 5.2033 - mae: 1.7146 - val_loss: 3.6847 - val_mse: 3.6847 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00527: val_mae did not improve from 1.51585\n",
      "Epoch 528/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.0632 - mse: 5.0632 - mae: 1.6642 - val_loss: 3.6766 - val_mse: 3.6766 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00528: val_mae improved from 1.51585 to 1.51506, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 529/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.4375 - mse: 5.4375 - mae: 1.7307 - val_loss: 3.6869 - val_mse: 3.6869 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00529: val_mae did not improve from 1.51506\n",
      "Epoch 530/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5908 - mse: 4.5908 - mae: 1.6211 - val_loss: 3.6803 - val_mse: 3.6803 - val_mae: 1.5164\n",
      "\n",
      "Epoch 00530: val_mae did not improve from 1.51506\n",
      "Epoch 531/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6529 - mse: 4.6529 - mae: 1.6409 - val_loss: 3.6970 - val_mse: 3.6970 - val_mae: 1.5214\n",
      "\n",
      "Epoch 00531: val_mae did not improve from 1.51506\n",
      "Epoch 532/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9451 - mse: 4.9451 - mae: 1.6795 - val_loss: 3.6819 - val_mse: 3.6819 - val_mae: 1.5169\n",
      "\n",
      "Epoch 00532: val_mae did not improve from 1.51506\n",
      "Epoch 533/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6455 - mse: 5.6455 - mae: 1.7259 - val_loss: 3.6848 - val_mse: 3.6848 - val_mae: 1.5178\n",
      "\n",
      "Epoch 00533: val_mae did not improve from 1.51506\n",
      "Epoch 534/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3870 - mse: 4.3870 - mae: 1.5919 - val_loss: 3.6835 - val_mse: 3.6835 - val_mae: 1.5175\n",
      "\n",
      "Epoch 00534: val_mae did not improve from 1.51506\n",
      "Epoch 535/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7678 - mse: 5.7678 - mae: 1.7844 - val_loss: 3.6807 - val_mse: 3.6807 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00535: val_mae did not improve from 1.51506\n",
      "Epoch 536/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6695 - mse: 4.6695 - mae: 1.6698 - val_loss: 3.6765 - val_mse: 3.6765 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00536: val_mae did not improve from 1.51506\n",
      "Epoch 537/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3137 - mse: 4.3137 - mae: 1.6100 - val_loss: 3.6763 - val_mse: 3.6763 - val_mae: 1.5150\n",
      "\n",
      "Epoch 00537: val_mae improved from 1.51506 to 1.51504, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 538/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2202 - mse: 5.2202 - mae: 1.6805 - val_loss: 3.6866 - val_mse: 3.6866 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00538: val_mae did not improve from 1.51504\n",
      "Epoch 539/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2556 - mse: 5.2556 - mae: 1.7038 - val_loss: 3.6816 - val_mse: 3.6816 - val_mae: 1.5170\n",
      "\n",
      "Epoch 00539: val_mae did not improve from 1.51504\n",
      "Epoch 540/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9025 - mse: 4.9025 - mae: 1.6670 - val_loss: 3.6840 - val_mse: 3.6840 - val_mae: 1.5178\n",
      "\n",
      "Epoch 00540: val_mae did not improve from 1.51504\n",
      "Epoch 541/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1502 - mse: 5.1502 - mae: 1.7181 - val_loss: 3.6735 - val_mse: 3.6735 - val_mae: 1.5141\n",
      "\n",
      "Epoch 00541: val_mae improved from 1.51504 to 1.51410, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 542/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5772 - mse: 4.5772 - mae: 1.6564 - val_loss: 3.6796 - val_mse: 3.6796 - val_mae: 1.5163\n",
      "\n",
      "Epoch 00542: val_mae did not improve from 1.51410\n",
      "Epoch 543/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6376 - mse: 5.6376 - mae: 1.8076 - val_loss: 3.6960 - val_mse: 3.6960 - val_mae: 1.5212\n",
      "\n",
      "Epoch 00543: val_mae did not improve from 1.51410\n",
      "Epoch 544/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1690 - mse: 5.1690 - mae: 1.7216 - val_loss: 3.6845 - val_mse: 3.6845 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00544: val_mae did not improve from 1.51410\n",
      "Epoch 545/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2121 - mse: 4.2121 - mae: 1.5494 - val_loss: 3.6767 - val_mse: 3.6767 - val_mae: 1.5150\n",
      "\n",
      "Epoch 00545: val_mae did not improve from 1.51410\n",
      "Epoch 546/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6285 - mse: 4.6285 - mae: 1.6707 - val_loss: 3.6951 - val_mse: 3.6951 - val_mae: 1.5208\n",
      "\n",
      "Epoch 00546: val_mae did not improve from 1.51410\n",
      "Epoch 547/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1297 - mse: 5.1297 - mae: 1.6807 - val_loss: 3.6878 - val_mse: 3.6878 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00547: val_mae did not improve from 1.51410\n",
      "Epoch 548/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1582 - mse: 5.1582 - mae: 1.6774 - val_loss: 3.6853 - val_mse: 3.6853 - val_mae: 1.5176\n",
      "\n",
      "Epoch 00548: val_mae did not improve from 1.51410\n",
      "Epoch 549/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1701 - mse: 5.1701 - mae: 1.7222 - val_loss: 3.6968 - val_mse: 3.6968 - val_mae: 1.5210\n",
      "\n",
      "Epoch 00549: val_mae did not improve from 1.51410\n",
      "Epoch 550/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2342 - mse: 5.2342 - mae: 1.7119 - val_loss: 3.6895 - val_mse: 3.6895 - val_mae: 1.5188\n",
      "\n",
      "Epoch 00550: val_mae did not improve from 1.51410\n",
      "Epoch 551/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.4161 - mse: 5.4161 - mae: 1.7236 - val_loss: 3.6761 - val_mse: 3.6761 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00551: val_mae did not improve from 1.51410\n",
      "Epoch 552/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.5768 - mse: 4.5768 - mae: 1.6272 - val_loss: 3.6756 - val_mse: 3.6756 - val_mae: 1.5143\n",
      "\n",
      "Epoch 00552: val_mae did not improve from 1.51410\n",
      "Epoch 553/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8647 - mse: 5.8647 - mae: 1.7987 - val_loss: 3.6824 - val_mse: 3.6824 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00553: val_mae did not improve from 1.51410\n",
      "Epoch 554/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9846 - mse: 4.9846 - mae: 1.7132 - val_loss: 3.6765 - val_mse: 3.6765 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00554: val_mae did not improve from 1.51410\n",
      "Epoch 555/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4549 - mse: 4.4549 - mae: 1.6279 - val_loss: 3.6796 - val_mse: 3.6796 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00555: val_mae did not improve from 1.51410\n",
      "Epoch 556/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9028 - mse: 4.9028 - mae: 1.6412 - val_loss: 3.6847 - val_mse: 3.6847 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00556: val_mae did not improve from 1.51410\n",
      "Epoch 557/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7702 - mse: 5.7702 - mae: 1.7572 - val_loss: 3.6837 - val_mse: 3.6837 - val_mae: 1.5169\n",
      "\n",
      "Epoch 00557: val_mae did not improve from 1.51410\n",
      "Epoch 558/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8048 - mse: 4.8048 - mae: 1.5812 - val_loss: 3.6852 - val_mse: 3.6852 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00558: val_mae did not improve from 1.51410\n",
      "Epoch 559/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9661 - mse: 4.9661 - mae: 1.6480 - val_loss: 3.6825 - val_mse: 3.6825 - val_mae: 1.5165\n",
      "\n",
      "Epoch 00559: val_mae did not improve from 1.51410\n",
      "Epoch 560/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4450 - mse: 5.4450 - mae: 1.7098 - val_loss: 3.6741 - val_mse: 3.6741 - val_mae: 1.5134\n",
      "\n",
      "Epoch 00560: val_mae improved from 1.51410 to 1.51338, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 561/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8879 - mse: 4.8879 - mae: 1.6466 - val_loss: 3.7020 - val_mse: 3.7020 - val_mae: 1.5219\n",
      "\n",
      "Epoch 00561: val_mae did not improve from 1.51338\n",
      "Epoch 562/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2390 - mse: 5.2390 - mae: 1.6660 - val_loss: 3.6905 - val_mse: 3.6905 - val_mae: 1.5187\n",
      "\n",
      "Epoch 00562: val_mae did not improve from 1.51338\n",
      "Epoch 563/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8269 - mse: 4.8269 - mae: 1.6799 - val_loss: 3.6781 - val_mse: 3.6781 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00563: val_mae did not improve from 1.51338\n",
      "Epoch 564/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5682 - mse: 5.5682 - mae: 1.7556 - val_loss: 3.6942 - val_mse: 3.6942 - val_mae: 1.5197\n",
      "\n",
      "Epoch 00564: val_mae did not improve from 1.51338\n",
      "Epoch 565/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7149 - mse: 4.7149 - mae: 1.6514 - val_loss: 3.6758 - val_mse: 3.6758 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00565: val_mae did not improve from 1.51338\n",
      "Epoch 566/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6311 - mse: 4.6311 - mae: 1.6585 - val_loss: 3.6721 - val_mse: 3.6721 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00566: val_mae improved from 1.51338 to 1.51235, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 567/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8513 - mse: 4.8513 - mae: 1.6411 - val_loss: 3.6887 - val_mse: 3.6887 - val_mae: 1.5178\n",
      "\n",
      "Epoch 00567: val_mae did not improve from 1.51235\n",
      "Epoch 568/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7174 - mse: 4.7174 - mae: 1.6605 - val_loss: 3.6725 - val_mse: 3.6725 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00568: val_mae improved from 1.51235 to 1.51233, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 569/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3891 - mse: 4.3891 - mae: 1.5739 - val_loss: 3.6910 - val_mse: 3.6910 - val_mae: 1.5184\n",
      "\n",
      "Epoch 00569: val_mae did not improve from 1.51233\n",
      "Epoch 570/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2459 - mse: 4.2459 - mae: 1.5692 - val_loss: 3.7019 - val_mse: 3.7019 - val_mae: 1.5214\n",
      "\n",
      "Epoch 00570: val_mae did not improve from 1.51233\n",
      "Epoch 571/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8966 - mse: 4.8966 - mae: 1.6827 - val_loss: 3.7063 - val_mse: 3.7063 - val_mae: 1.5225\n",
      "\n",
      "Epoch 00571: val_mae did not improve from 1.51233\n",
      "Epoch 572/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8733 - mse: 4.8733 - mae: 1.7370 - val_loss: 3.6985 - val_mse: 3.6985 - val_mae: 1.5203\n",
      "\n",
      "Epoch 00572: val_mae did not improve from 1.51233\n",
      "Epoch 573/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0221 - mse: 5.0221 - mae: 1.7150 - val_loss: 3.7144 - val_mse: 3.7144 - val_mae: 1.5243\n",
      "\n",
      "Epoch 00573: val_mae did not improve from 1.51233\n",
      "Epoch 574/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4888 - mse: 4.4888 - mae: 1.5745 - val_loss: 3.6772 - val_mse: 3.6772 - val_mae: 1.5136\n",
      "\n",
      "Epoch 00574: val_mae did not improve from 1.51233\n",
      "Epoch 575/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7679 - mse: 4.7679 - mae: 1.6336 - val_loss: 3.6820 - val_mse: 3.6820 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00575: val_mae did not improve from 1.51233\n",
      "Epoch 576/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7745 - mse: 4.7745 - mae: 1.6661 - val_loss: 3.6925 - val_mse: 3.6925 - val_mae: 1.5187\n",
      "\n",
      "Epoch 00576: val_mae did not improve from 1.51233\n",
      "Epoch 577/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4319 - mse: 4.4319 - mae: 1.6401 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5175\n",
      "\n",
      "Epoch 00577: val_mae did not improve from 1.51233\n",
      "Epoch 578/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5717 - mse: 5.5717 - mae: 1.7271 - val_loss: 3.6768 - val_mse: 3.6768 - val_mae: 1.5135\n",
      "\n",
      "Epoch 00578: val_mae did not improve from 1.51233\n",
      "Epoch 579/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3742 - mse: 5.3742 - mae: 1.7639 - val_loss: 3.6768 - val_mse: 3.6768 - val_mae: 1.5135\n",
      "\n",
      "Epoch 00579: val_mae did not improve from 1.51233\n",
      "Epoch 580/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8551 - mse: 4.8551 - mae: 1.6921 - val_loss: 3.6841 - val_mse: 3.6841 - val_mae: 1.5160\n",
      "\n",
      "Epoch 00580: val_mae did not improve from 1.51233\n",
      "Epoch 581/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6657 - mse: 4.6657 - mae: 1.6504 - val_loss: 3.6808 - val_mse: 3.6808 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00581: val_mae did not improve from 1.51233\n",
      "Epoch 582/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7680 - mse: 5.7680 - mae: 1.8116 - val_loss: 3.6764 - val_mse: 3.6764 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00582: val_mae did not improve from 1.51233\n",
      "Epoch 583/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4002 - mse: 5.4002 - mae: 1.7673 - val_loss: 3.6716 - val_mse: 3.6716 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00583: val_mae improved from 1.51233 to 1.51125, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 584/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9382 - mse: 4.9382 - mae: 1.6845 - val_loss: 3.6725 - val_mse: 3.6725 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00584: val_mae did not improve from 1.51125\n",
      "Epoch 585/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0918 - mse: 5.0918 - mae: 1.7185 - val_loss: 3.6874 - val_mse: 3.6874 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00585: val_mae did not improve from 1.51125\n",
      "Epoch 586/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1797 - mse: 5.1797 - mae: 1.6694 - val_loss: 3.6894 - val_mse: 3.6894 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00586: val_mae did not improve from 1.51125\n",
      "Epoch 587/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7548 - mse: 4.7548 - mae: 1.6632 - val_loss: 3.6723 - val_mse: 3.6723 - val_mae: 1.5114\n",
      "\n",
      "Epoch 00587: val_mae did not improve from 1.51125\n",
      "Epoch 588/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6034 - mse: 4.6034 - mae: 1.6043 - val_loss: 3.6962 - val_mse: 3.6962 - val_mae: 1.5198\n",
      "\n",
      "Epoch 00588: val_mae did not improve from 1.51125\n",
      "Epoch 589/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5533 - mse: 4.5533 - mae: 1.5896 - val_loss: 3.6853 - val_mse: 3.6853 - val_mae: 1.5165\n",
      "\n",
      "Epoch 00589: val_mae did not improve from 1.51125\n",
      "Epoch 590/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4207 - mse: 5.4207 - mae: 1.7906 - val_loss: 3.6940 - val_mse: 3.6940 - val_mae: 1.5193\n",
      "\n",
      "Epoch 00590: val_mae did not improve from 1.51125\n",
      "Epoch 591/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7347 - mse: 4.7347 - mae: 1.6789 - val_loss: 3.6720 - val_mse: 3.6720 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00591: val_mae did not improve from 1.51125\n",
      "Epoch 592/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7844 - mse: 4.7844 - mae: 1.6804 - val_loss: 3.6741 - val_mse: 3.6741 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00592: val_mae did not improve from 1.51125\n",
      "Epoch 593/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3692 - mse: 4.3692 - mae: 1.5703 - val_loss: 3.6914 - val_mse: 3.6914 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00593: val_mae did not improve from 1.51125\n",
      "Epoch 594/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4929 - mse: 4.4929 - mae: 1.6086 - val_loss: 3.6841 - val_mse: 3.6841 - val_mae: 1.5162\n",
      "\n",
      "Epoch 00594: val_mae did not improve from 1.51125\n",
      "Epoch 595/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9705 - mse: 4.9705 - mae: 1.6746 - val_loss: 3.6872 - val_mse: 3.6872 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00595: val_mae did not improve from 1.51125\n",
      "Epoch 596/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9874 - mse: 4.9874 - mae: 1.6186 - val_loss: 3.6693 - val_mse: 3.6693 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00596: val_mae improved from 1.51125 to 1.51043, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 597/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5723 - mse: 4.5723 - mae: 1.6550 - val_loss: 3.6724 - val_mse: 3.6724 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00597: val_mae did not improve from 1.51043\n",
      "Epoch 598/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.1490 - mse: 4.1490 - mae: 1.5659 - val_loss: 3.6942 - val_mse: 3.6942 - val_mae: 1.5193\n",
      "\n",
      "Epoch 00598: val_mae did not improve from 1.51043\n",
      "Epoch 599/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0334 - mse: 5.0334 - mae: 1.6659 - val_loss: 3.6825 - val_mse: 3.6825 - val_mae: 1.5156\n",
      "\n",
      "Epoch 00599: val_mae did not improve from 1.51043\n",
      "Epoch 600/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9590 - mse: 4.9590 - mae: 1.7056 - val_loss: 3.6747 - val_mse: 3.6747 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00600: val_mae did not improve from 1.51043\n",
      "Epoch 601/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1358 - mse: 5.1358 - mae: 1.7362 - val_loss: 3.6816 - val_mse: 3.6816 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00601: val_mae did not improve from 1.51043\n",
      "Epoch 602/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7967 - mse: 4.7967 - mae: 1.6174 - val_loss: 3.6808 - val_mse: 3.6808 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00602: val_mae did not improve from 1.51043\n",
      "Epoch 603/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6493 - mse: 4.6493 - mae: 1.6633 - val_loss: 3.6743 - val_mse: 3.6743 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00603: val_mae did not improve from 1.51043\n",
      "Epoch 604/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7497 - mse: 4.7497 - mae: 1.6483 - val_loss: 3.6748 - val_mse: 3.6748 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00604: val_mae did not improve from 1.51043\n",
      "Epoch 605/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7998 - mse: 4.7998 - mae: 1.6720 - val_loss: 3.6937 - val_mse: 3.6937 - val_mae: 1.5187\n",
      "\n",
      "Epoch 00605: val_mae did not improve from 1.51043\n",
      "Epoch 606/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3708 - mse: 5.3708 - mae: 1.7522 - val_loss: 3.6884 - val_mse: 3.6884 - val_mae: 1.5170\n",
      "\n",
      "Epoch 00606: val_mae did not improve from 1.51043\n",
      "Epoch 607/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2300 - mse: 5.2300 - mae: 1.7101 - val_loss: 3.6902 - val_mse: 3.6902 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00607: val_mae did not improve from 1.51043\n",
      "Epoch 608/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7102 - mse: 4.7102 - mae: 1.6585 - val_loss: 3.6840 - val_mse: 3.6840 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00608: val_mae did not improve from 1.51043\n",
      "Epoch 609/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9173 - mse: 4.9173 - mae: 1.6536 - val_loss: 3.6850 - val_mse: 3.6850 - val_mae: 1.5158\n",
      "\n",
      "Epoch 00609: val_mae did not improve from 1.51043\n",
      "Epoch 610/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1957 - mse: 5.1957 - mae: 1.7443 - val_loss: 3.6931 - val_mse: 3.6931 - val_mae: 1.5183\n",
      "\n",
      "Epoch 00610: val_mae did not improve from 1.51043\n",
      "Epoch 611/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9585 - mse: 4.9585 - mae: 1.7283 - val_loss: 3.7037 - val_mse: 3.7037 - val_mae: 1.5211\n",
      "\n",
      "Epoch 00611: val_mae did not improve from 1.51043\n",
      "Epoch 612/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5393 - mse: 5.5393 - mae: 1.7782 - val_loss: 3.6898 - val_mse: 3.6898 - val_mae: 1.5171\n",
      "\n",
      "Epoch 00612: val_mae did not improve from 1.51043\n",
      "Epoch 613/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8223 - mse: 4.8223 - mae: 1.6851 - val_loss: 3.6824 - val_mse: 3.6824 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00613: val_mae did not improve from 1.51043\n",
      "Epoch 614/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3698 - mse: 4.3698 - mae: 1.5848 - val_loss: 3.6960 - val_mse: 3.6960 - val_mae: 1.5190\n",
      "\n",
      "Epoch 00614: val_mae did not improve from 1.51043\n",
      "Epoch 615/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.4358 - mse: 4.4358 - mae: 1.6564 - val_loss: 3.6837 - val_mse: 3.6837 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00615: val_mae did not improve from 1.51043\n",
      "Epoch 616/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7780 - mse: 4.7780 - mae: 1.6438 - val_loss: 3.6792 - val_mse: 3.6792 - val_mae: 1.5134\n",
      "\n",
      "Epoch 00616: val_mae did not improve from 1.51043\n",
      "Epoch 617/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1233 - mse: 5.1233 - mae: 1.7108 - val_loss: 3.6836 - val_mse: 3.6836 - val_mae: 1.5150\n",
      "\n",
      "Epoch 00617: val_mae did not improve from 1.51043\n",
      "Epoch 618/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8814 - mse: 5.8814 - mae: 1.8059 - val_loss: 3.6877 - val_mse: 3.6877 - val_mae: 1.5161\n",
      "\n",
      "Epoch 00618: val_mae did not improve from 1.51043\n",
      "Epoch 619/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4542 - mse: 5.4542 - mae: 1.7371 - val_loss: 3.6741 - val_mse: 3.6741 - val_mae: 1.5109\n",
      "\n",
      "Epoch 00619: val_mae did not improve from 1.51043\n",
      "Epoch 620/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8690 - mse: 5.8690 - mae: 1.7592 - val_loss: 3.6850 - val_mse: 3.6850 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00620: val_mae did not improve from 1.51043\n",
      "Epoch 621/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3216 - mse: 4.3216 - mae: 1.5816 - val_loss: 3.6948 - val_mse: 3.6948 - val_mae: 1.5181\n",
      "\n",
      "Epoch 00621: val_mae did not improve from 1.51043\n",
      "Epoch 622/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9666 - mse: 4.9666 - mae: 1.6788 - val_loss: 3.6899 - val_mse: 3.6899 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00622: val_mae did not improve from 1.51043\n",
      "Epoch 623/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5244 - mse: 5.5244 - mae: 1.7405 - val_loss: 3.6781 - val_mse: 3.6781 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00623: val_mae did not improve from 1.51043\n",
      "Epoch 624/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3030 - mse: 5.3030 - mae: 1.7341 - val_loss: 3.6978 - val_mse: 3.6978 - val_mae: 1.5190\n",
      "\n",
      "Epoch 00624: val_mae did not improve from 1.51043\n",
      "Epoch 625/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1627 - mse: 5.1627 - mae: 1.7265 - val_loss: 3.6858 - val_mse: 3.6858 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00625: val_mae did not improve from 1.51043\n",
      "Epoch 626/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9507 - mse: 4.9506 - mae: 1.6683 - val_loss: 3.6813 - val_mse: 3.6813 - val_mae: 1.5136\n",
      "\n",
      "Epoch 00626: val_mae did not improve from 1.51043\n",
      "Epoch 627/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4058 - mse: 5.4058 - mae: 1.7257 - val_loss: 3.7045 - val_mse: 3.7045 - val_mae: 1.5207\n",
      "\n",
      "Epoch 00627: val_mae did not improve from 1.51043\n",
      "Epoch 628/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6849 - mse: 4.6849 - mae: 1.6329 - val_loss: 3.6810 - val_mse: 3.6810 - val_mae: 1.5134\n",
      "\n",
      "Epoch 00628: val_mae did not improve from 1.51043\n",
      "Epoch 629/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3803 - mse: 5.3803 - mae: 1.7725 - val_loss: 3.6838 - val_mse: 3.6838 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00629: val_mae did not improve from 1.51043\n",
      "Epoch 630/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.0662 - mse: 5.0662 - mae: 1.6671 - val_loss: 3.6820 - val_mse: 3.6820 - val_mae: 1.5138\n",
      "\n",
      "Epoch 00630: val_mae did not improve from 1.51043\n",
      "Epoch 631/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5630 - mse: 4.5630 - mae: 1.6346 - val_loss: 3.6841 - val_mse: 3.6841 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00631: val_mae did not improve from 1.51043\n",
      "Epoch 632/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7628 - mse: 4.7628 - mae: 1.6501 - val_loss: 3.6857 - val_mse: 3.6857 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00632: val_mae did not improve from 1.51043\n",
      "Epoch 633/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.7398 - mse: 4.7398 - mae: 1.6379 - val_loss: 3.6896 - val_mse: 3.6896 - val_mae: 1.5163\n",
      "\n",
      "Epoch 00633: val_mae did not improve from 1.51043\n",
      "Epoch 634/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.3708 - mse: 4.3708 - mae: 1.5801 - val_loss: 3.6923 - val_mse: 3.6923 - val_mae: 1.5171\n",
      "\n",
      "Epoch 00634: val_mae did not improve from 1.51043\n",
      "Epoch 635/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3044 - mse: 5.3044 - mae: 1.7284 - val_loss: 3.6850 - val_mse: 3.6850 - val_mae: 1.5148\n",
      "\n",
      "Epoch 00635: val_mae did not improve from 1.51043\n",
      "Epoch 636/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7399 - mse: 4.7399 - mae: 1.6413 - val_loss: 3.6826 - val_mse: 3.6826 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00636: val_mae did not improve from 1.51043\n",
      "Epoch 637/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.6399 - mse: 4.6399 - mae: 1.6182 - val_loss: 3.6780 - val_mse: 3.6780 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00637: val_mae did not improve from 1.51043\n",
      "Epoch 638/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4324 - mse: 5.4324 - mae: 1.7280 - val_loss: 3.6930 - val_mse: 3.6930 - val_mae: 1.5173\n",
      "\n",
      "Epoch 00638: val_mae did not improve from 1.51043\n",
      "Epoch 639/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5602 - mse: 4.5602 - mae: 1.6166 - val_loss: 3.7009 - val_mse: 3.7009 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00639: val_mae did not improve from 1.51043\n",
      "Epoch 640/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.0873 - mse: 5.0873 - mae: 1.7299 - val_loss: 3.6896 - val_mse: 3.6896 - val_mae: 1.5161\n",
      "\n",
      "Epoch 00640: val_mae did not improve from 1.51043\n",
      "Epoch 641/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.9614 - mse: 4.9614 - mae: 1.7108 - val_loss: 3.6942 - val_mse: 3.6942 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00641: val_mae did not improve from 1.51043\n",
      "Epoch 642/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9616 - mse: 4.9616 - mae: 1.7263 - val_loss: 3.6811 - val_mse: 3.6811 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00642: val_mae did not improve from 1.51043\n",
      "Epoch 643/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.6166 - mse: 5.6166 - mae: 1.7640 - val_loss: 3.6833 - val_mse: 3.6833 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00643: val_mae did not improve from 1.51043\n",
      "Epoch 644/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.2659 - mse: 5.2659 - mae: 1.7039 - val_loss: 3.6879 - val_mse: 3.6879 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00644: val_mae did not improve from 1.51043\n",
      "Epoch 645/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3633 - mse: 4.3633 - mae: 1.5759 - val_loss: 3.6937 - val_mse: 3.6937 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00645: val_mae did not improve from 1.51043\n",
      "Epoch 646/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5910 - mse: 4.5910 - mae: 1.6081 - val_loss: 3.6844 - val_mse: 3.6844 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00646: val_mae did not improve from 1.51043\n",
      "Epoch 647/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7294 - mse: 5.7294 - mae: 1.7736 - val_loss: 3.7124 - val_mse: 3.7124 - val_mae: 1.5223\n",
      "\n",
      "Epoch 00647: val_mae did not improve from 1.51043\n",
      "Epoch 648/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4874 - mse: 5.4874 - mae: 1.7581 - val_loss: 3.6932 - val_mse: 3.6932 - val_mae: 1.5173\n",
      "\n",
      "Epoch 00648: val_mae did not improve from 1.51043\n",
      "Epoch 649/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4501 - mse: 5.4501 - mae: 1.6907 - val_loss: 3.6858 - val_mse: 3.6858 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00649: val_mae did not improve from 1.51043\n",
      "Epoch 650/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7507 - mse: 4.7507 - mae: 1.6245 - val_loss: 3.6744 - val_mse: 3.6744 - val_mae: 1.5107\n",
      "\n",
      "Epoch 00650: val_mae did not improve from 1.51043\n",
      "Epoch 651/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6657 - mse: 4.6657 - mae: 1.6695 - val_loss: 3.6762 - val_mse: 3.6762 - val_mae: 1.5114\n",
      "\n",
      "Epoch 00651: val_mae did not improve from 1.51043\n",
      "Epoch 652/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7275 - mse: 5.7275 - mae: 1.7428 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5157\n",
      "\n",
      "Epoch 00652: val_mae did not improve from 1.51043\n",
      "Epoch 653/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7060 - mse: 4.7060 - mae: 1.6424 - val_loss: 3.6754 - val_mse: 3.6754 - val_mae: 1.5109\n",
      "\n",
      "Epoch 00653: val_mae did not improve from 1.51043\n",
      "Epoch 654/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5404 - mse: 4.5404 - mae: 1.6025 - val_loss: 3.6945 - val_mse: 3.6945 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00654: val_mae did not improve from 1.51043\n",
      "Epoch 655/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0328 - mse: 5.0328 - mae: 1.6562 - val_loss: 3.6931 - val_mse: 3.6931 - val_mae: 1.5171\n",
      "\n",
      "Epoch 00655: val_mae did not improve from 1.51043\n",
      "Epoch 656/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5312 - mse: 4.5312 - mae: 1.6182 - val_loss: 3.6958 - val_mse: 3.6958 - val_mae: 1.5179\n",
      "\n",
      "Epoch 00656: val_mae did not improve from 1.51043\n",
      "Epoch 657/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3223 - mse: 5.3223 - mae: 1.7482 - val_loss: 3.6810 - val_mse: 3.6810 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00657: val_mae did not improve from 1.51043\n",
      "Epoch 658/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.5999 - mse: 5.5999 - mae: 1.8236 - val_loss: 3.6862 - val_mse: 3.6862 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00658: val_mae did not improve from 1.51043\n",
      "Epoch 659/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8791 - mse: 4.8791 - mae: 1.6991 - val_loss: 3.6850 - val_mse: 3.6850 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00659: val_mae did not improve from 1.51043\n",
      "Epoch 660/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0669 - mse: 5.0669 - mae: 1.7049 - val_loss: 3.6958 - val_mse: 3.6958 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00660: val_mae did not improve from 1.51043\n",
      "Epoch 661/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5321 - mse: 5.5321 - mae: 1.7716 - val_loss: 3.6860 - val_mse: 3.6860 - val_mae: 1.5143\n",
      "\n",
      "Epoch 00661: val_mae did not improve from 1.51043\n",
      "Epoch 662/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0757 - mse: 5.0757 - mae: 1.6694 - val_loss: 3.6802 - val_mse: 3.6802 - val_mae: 1.5122\n",
      "\n",
      "Epoch 00662: val_mae did not improve from 1.51043\n",
      "Epoch 663/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2443 - mse: 4.2443 - mae: 1.5685 - val_loss: 3.6762 - val_mse: 3.6762 - val_mae: 1.5108\n",
      "\n",
      "Epoch 00663: val_mae did not improve from 1.51043\n",
      "Epoch 664/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6656 - mse: 4.6656 - mae: 1.6446 - val_loss: 3.6928 - val_mse: 3.6928 - val_mae: 1.5164\n",
      "\n",
      "Epoch 00664: val_mae did not improve from 1.51043\n",
      "Epoch 665/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4969 - mse: 4.4969 - mae: 1.6222 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00665: val_mae did not improve from 1.51043\n",
      "Epoch 666/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1710 - mse: 5.1710 - mae: 1.7043 - val_loss: 3.6833 - val_mse: 3.6833 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00666: val_mae did not improve from 1.51043\n",
      "Epoch 667/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8157 - mse: 4.8157 - mae: 1.6445 - val_loss: 3.6795 - val_mse: 3.6795 - val_mae: 1.5119\n",
      "\n",
      "Epoch 00667: val_mae did not improve from 1.51043\n",
      "Epoch 668/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.1916 - mse: 4.1916 - mae: 1.5771 - val_loss: 3.6891 - val_mse: 3.6891 - val_mae: 1.5150\n",
      "\n",
      "Epoch 00668: val_mae did not improve from 1.51043\n",
      "Epoch 669/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2421 - mse: 5.2421 - mae: 1.7063 - val_loss: 3.6820 - val_mse: 3.6820 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00669: val_mae did not improve from 1.51043\n",
      "Epoch 670/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6469 - mse: 4.6469 - mae: 1.6234 - val_loss: 3.6831 - val_mse: 3.6831 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00670: val_mae did not improve from 1.51043\n",
      "Epoch 671/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2289 - mse: 5.2289 - mae: 1.6818 - val_loss: 3.6930 - val_mse: 3.6930 - val_mae: 1.5161\n",
      "\n",
      "Epoch 00671: val_mae did not improve from 1.51043\n",
      "Epoch 672/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6979 - mse: 4.6979 - mae: 1.6567 - val_loss: 3.6850 - val_mse: 3.6850 - val_mae: 1.5134\n",
      "\n",
      "Epoch 00672: val_mae did not improve from 1.51043\n",
      "Epoch 673/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1440 - mse: 5.1440 - mae: 1.7400 - val_loss: 3.6913 - val_mse: 3.6913 - val_mae: 1.5154\n",
      "\n",
      "Epoch 00673: val_mae did not improve from 1.51043\n",
      "Epoch 674/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5047 - mse: 4.5047 - mae: 1.6067 - val_loss: 3.6884 - val_mse: 3.6884 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00674: val_mae did not improve from 1.51043\n",
      "Epoch 675/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6808 - mse: 4.6808 - mae: 1.6549 - val_loss: 3.6869 - val_mse: 3.6869 - val_mae: 1.5141\n",
      "\n",
      "Epoch 00675: val_mae did not improve from 1.51043\n",
      "Epoch 676/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9738 - mse: 4.9738 - mae: 1.7211 - val_loss: 3.6890 - val_mse: 3.6890 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00676: val_mae did not improve from 1.51043\n",
      "Epoch 677/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7977 - mse: 4.7977 - mae: 1.6656 - val_loss: 3.6774 - val_mse: 3.6774 - val_mae: 1.5107\n",
      "\n",
      "Epoch 00677: val_mae did not improve from 1.51043\n",
      "Epoch 678/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1146 - mse: 5.1146 - mae: 1.6728 - val_loss: 3.6827 - val_mse: 3.6827 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00678: val_mae did not improve from 1.51043\n",
      "Epoch 679/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2115 - mse: 5.2115 - mae: 1.7045 - val_loss: 3.6726 - val_mse: 3.6726 - val_mae: 1.5086\n",
      "\n",
      "Epoch 00679: val_mae improved from 1.51043 to 1.50858, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 680/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5471 - mse: 5.5471 - mae: 1.7778 - val_loss: 3.6814 - val_mse: 3.6814 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00680: val_mae did not improve from 1.50858\n",
      "Epoch 681/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7186 - mse: 4.7186 - mae: 1.6360 - val_loss: 3.6796 - val_mse: 3.6796 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00681: val_mae did not improve from 1.50858\n",
      "Epoch 682/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3828 - mse: 4.3828 - mae: 1.6134 - val_loss: 3.6847 - val_mse: 3.6847 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00682: val_mae did not improve from 1.50858\n",
      "Epoch 683/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0940 - mse: 5.0940 - mae: 1.7080 - val_loss: 3.6960 - val_mse: 3.6960 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00683: val_mae did not improve from 1.50858\n",
      "Epoch 684/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4956 - mse: 4.4956 - mae: 1.6445 - val_loss: 3.7157 - val_mse: 3.7157 - val_mae: 1.5216\n",
      "\n",
      "Epoch 00684: val_mae did not improve from 1.50858\n",
      "Epoch 685/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7219 - mse: 4.7219 - mae: 1.6222 - val_loss: 3.6920 - val_mse: 3.6920 - val_mae: 1.5156\n",
      "\n",
      "Epoch 00685: val_mae did not improve from 1.50858\n",
      "Epoch 686/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0754 - mse: 5.0754 - mae: 1.6993 - val_loss: 3.6898 - val_mse: 3.6898 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00686: val_mae did not improve from 1.50858\n",
      "Epoch 687/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4825 - mse: 4.4825 - mae: 1.6118 - val_loss: 3.6807 - val_mse: 3.6807 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00687: val_mae did not improve from 1.50858\n",
      "Epoch 688/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6151 - mse: 4.6151 - mae: 1.6542 - val_loss: 3.6899 - val_mse: 3.6899 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00688: val_mae did not improve from 1.50858\n",
      "Epoch 689/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7147 - mse: 4.7147 - mae: 1.6465 - val_loss: 3.6817 - val_mse: 3.6817 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00689: val_mae did not improve from 1.50858\n",
      "Epoch 690/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5661 - mse: 5.5661 - mae: 1.7275 - val_loss: 3.6917 - val_mse: 3.6917 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00690: val_mae did not improve from 1.50858\n",
      "Epoch 691/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3011 - mse: 4.3011 - mae: 1.6030 - val_loss: 3.6875 - val_mse: 3.6875 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00691: val_mae did not improve from 1.50858\n",
      "Epoch 692/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3769 - mse: 5.3769 - mae: 1.7708 - val_loss: 3.6879 - val_mse: 3.6879 - val_mae: 1.5141\n",
      "\n",
      "Epoch 00692: val_mae did not improve from 1.50858\n",
      "Epoch 693/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4027 - mse: 5.4027 - mae: 1.7197 - val_loss: 3.6868 - val_mse: 3.6868 - val_mae: 1.5137\n",
      "\n",
      "Epoch 00693: val_mae did not improve from 1.50858\n",
      "Epoch 694/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7907 - mse: 4.7907 - mae: 1.7094 - val_loss: 3.6896 - val_mse: 3.6896 - val_mae: 1.5148\n",
      "\n",
      "Epoch 00694: val_mae did not improve from 1.50858\n",
      "Epoch 695/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2288 - mse: 4.2288 - mae: 1.5881 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00695: val_mae did not improve from 1.50858\n",
      "Epoch 696/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.0953 - mse: 5.0953 - mae: 1.6950 - val_loss: 3.6974 - val_mse: 3.6974 - val_mae: 1.5168\n",
      "\n",
      "Epoch 00696: val_mae did not improve from 1.50858\n",
      "Epoch 697/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1384 - mse: 5.1384 - mae: 1.7337 - val_loss: 3.6850 - val_mse: 3.6850 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00697: val_mae did not improve from 1.50858\n",
      "Epoch 698/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0115 - mse: 5.0115 - mae: 1.6727 - val_loss: 3.6861 - val_mse: 3.6861 - val_mae: 1.5134\n",
      "\n",
      "Epoch 00698: val_mae did not improve from 1.50858\n",
      "Epoch 699/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5262 - mse: 4.5262 - mae: 1.6149 - val_loss: 3.6854 - val_mse: 3.6854 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00699: val_mae did not improve from 1.50858\n",
      "Epoch 700/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3634 - mse: 4.3634 - mae: 1.6145 - val_loss: 3.7042 - val_mse: 3.7042 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00700: val_mae did not improve from 1.50858\n",
      "Epoch 701/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0767 - mse: 5.0767 - mae: 1.7039 - val_loss: 3.6793 - val_mse: 3.6793 - val_mae: 1.5109\n",
      "\n",
      "Epoch 00701: val_mae did not improve from 1.50858\n",
      "Epoch 702/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2457 - mse: 5.2457 - mae: 1.7176 - val_loss: 3.6823 - val_mse: 3.6823 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00702: val_mae did not improve from 1.50858\n",
      "Epoch 703/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8674 - mse: 4.8674 - mae: 1.6602 - val_loss: 3.6792 - val_mse: 3.6792 - val_mae: 1.5107\n",
      "\n",
      "Epoch 00703: val_mae did not improve from 1.50858\n",
      "Epoch 704/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8683 - mse: 4.8683 - mae: 1.6933 - val_loss: 3.6732 - val_mse: 3.6732 - val_mae: 1.5084\n",
      "\n",
      "Epoch 00704: val_mae improved from 1.50858 to 1.50838, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 705/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8675 - mse: 4.8675 - mae: 1.6106 - val_loss: 3.6835 - val_mse: 3.6835 - val_mae: 1.5122\n",
      "\n",
      "Epoch 00705: val_mae did not improve from 1.50838\n",
      "Epoch 706/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0843 - mse: 5.0843 - mae: 1.7075 - val_loss: 3.6956 - val_mse: 3.6956 - val_mae: 1.5162\n",
      "\n",
      "Epoch 00706: val_mae did not improve from 1.50838\n",
      "Epoch 707/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8109 - mse: 5.8109 - mae: 1.8756 - val_loss: 3.6841 - val_mse: 3.6841 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00707: val_mae did not improve from 1.50838\n",
      "Epoch 708/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7339 - mse: 4.7339 - mae: 1.6161 - val_loss: 3.6785 - val_mse: 3.6785 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00708: val_mae did not improve from 1.50838\n",
      "Epoch 709/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5460 - mse: 4.5460 - mae: 1.6373 - val_loss: 3.6799 - val_mse: 3.6799 - val_mae: 1.5108\n",
      "\n",
      "Epoch 00709: val_mae did not improve from 1.50838\n",
      "Epoch 710/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6564 - mse: 4.6564 - mae: 1.6212 - val_loss: 3.6815 - val_mse: 3.6815 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00710: val_mae did not improve from 1.50838\n",
      "Epoch 711/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2510 - mse: 5.2510 - mae: 1.7193 - val_loss: 3.6776 - val_mse: 3.6776 - val_mae: 1.5096\n",
      "\n",
      "Epoch 00711: val_mae did not improve from 1.50838\n",
      "Epoch 712/1000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.6706 - mse: 4.6706 - mae: 1.6450 - val_loss: 3.6924 - val_mse: 3.6924 - val_mae: 1.5150\n",
      "\n",
      "Epoch 00712: val_mae did not improve from 1.50838\n",
      "Epoch 713/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3366 - mse: 5.3366 - mae: 1.7307 - val_loss: 3.7019 - val_mse: 3.7019 - val_mae: 1.5179\n",
      "\n",
      "Epoch 00713: val_mae did not improve from 1.50838\n",
      "Epoch 714/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5822 - mse: 5.5822 - mae: 1.8094 - val_loss: 3.6923 - val_mse: 3.6923 - val_mae: 1.5150\n",
      "\n",
      "Epoch 00714: val_mae did not improve from 1.50838\n",
      "Epoch 715/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8270 - mse: 4.8270 - mae: 1.6709 - val_loss: 3.6806 - val_mse: 3.6806 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00715: val_mae did not improve from 1.50838\n",
      "Epoch 716/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9855 - mse: 4.9855 - mae: 1.7199 - val_loss: 3.6767 - val_mse: 3.6767 - val_mae: 1.5090\n",
      "\n",
      "Epoch 00716: val_mae did not improve from 1.50838\n",
      "Epoch 717/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0960 - mse: 5.0960 - mae: 1.6609 - val_loss: 3.6871 - val_mse: 3.6871 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00717: val_mae did not improve from 1.50838\n",
      "Epoch 718/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1062 - mse: 5.1062 - mae: 1.7580 - val_loss: 3.6845 - val_mse: 3.6845 - val_mae: 1.5121\n",
      "\n",
      "Epoch 00718: val_mae did not improve from 1.50838\n",
      "Epoch 719/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5799 - mse: 5.5799 - mae: 1.6766 - val_loss: 3.7035 - val_mse: 3.7035 - val_mae: 1.5180\n",
      "\n",
      "Epoch 00719: val_mae did not improve from 1.50838\n",
      "Epoch 720/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7776 - mse: 4.7776 - mae: 1.6314 - val_loss: 3.6798 - val_mse: 3.6798 - val_mae: 1.5102\n",
      "\n",
      "Epoch 00720: val_mae did not improve from 1.50838\n",
      "Epoch 721/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0867 - mse: 5.0867 - mae: 1.6126 - val_loss: 3.6842 - val_mse: 3.6842 - val_mae: 1.5121\n",
      "\n",
      "Epoch 00721: val_mae did not improve from 1.50838\n",
      "Epoch 722/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5981 - mse: 4.5981 - mae: 1.5912 - val_loss: 3.6918 - val_mse: 3.6918 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00722: val_mae did not improve from 1.50838\n",
      "Epoch 723/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.1819 - mse: 4.1819 - mae: 1.5209 - val_loss: 3.6852 - val_mse: 3.6852 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00723: val_mae did not improve from 1.50838\n",
      "Epoch 724/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1127 - mse: 5.1127 - mae: 1.6727 - val_loss: 3.6898 - val_mse: 3.6898 - val_mae: 1.5140\n",
      "\n",
      "Epoch 00724: val_mae did not improve from 1.50838\n",
      "Epoch 725/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2189 - mse: 4.2189 - mae: 1.5730 - val_loss: 3.6966 - val_mse: 3.6966 - val_mae: 1.5160\n",
      "\n",
      "Epoch 00725: val_mae did not improve from 1.50838\n",
      "Epoch 726/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3503 - mse: 4.3503 - mae: 1.6258 - val_loss: 3.7021 - val_mse: 3.7021 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00726: val_mae did not improve from 1.50838\n",
      "Epoch 727/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.0474 - mse: 5.0474 - mae: 1.7219 - val_loss: 3.7180 - val_mse: 3.7180 - val_mae: 1.5215\n",
      "\n",
      "Epoch 00727: val_mae did not improve from 1.50838\n",
      "Epoch 728/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3744 - mse: 4.3744 - mae: 1.5835 - val_loss: 3.7132 - val_mse: 3.7132 - val_mae: 1.5203\n",
      "\n",
      "Epoch 00728: val_mae did not improve from 1.50838\n",
      "Epoch 729/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1678 - mse: 5.1678 - mae: 1.6268 - val_loss: 3.7101 - val_mse: 3.7101 - val_mae: 1.5196\n",
      "\n",
      "Epoch 00729: val_mae did not improve from 1.50838\n",
      "Epoch 730/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6060 - mse: 4.6060 - mae: 1.6631 - val_loss: 3.7109 - val_mse: 3.7109 - val_mae: 1.5197\n",
      "\n",
      "Epoch 00730: val_mae did not improve from 1.50838\n",
      "Epoch 731/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4301 - mse: 4.4301 - mae: 1.6030 - val_loss: 3.6906 - val_mse: 3.6906 - val_mae: 1.5138\n",
      "\n",
      "Epoch 00731: val_mae did not improve from 1.50838\n",
      "Epoch 732/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0973 - mse: 5.0973 - mae: 1.7187 - val_loss: 3.6892 - val_mse: 3.6892 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00732: val_mae did not improve from 1.50838\n",
      "Epoch 733/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4627 - mse: 5.4627 - mae: 1.6992 - val_loss: 3.6987 - val_mse: 3.6987 - val_mae: 1.5161\n",
      "\n",
      "Epoch 00733: val_mae did not improve from 1.50838\n",
      "Epoch 734/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2985 - mse: 4.2985 - mae: 1.5807 - val_loss: 3.6937 - val_mse: 3.6937 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00734: val_mae did not improve from 1.50838\n",
      "Epoch 735/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9026 - mse: 4.9026 - mae: 1.7102 - val_loss: 3.6965 - val_mse: 3.6965 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00735: val_mae did not improve from 1.50838\n",
      "Epoch 736/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9009 - mse: 4.9009 - mae: 1.6777 - val_loss: 3.6960 - val_mse: 3.6960 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00736: val_mae did not improve from 1.50838\n",
      "Epoch 737/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1848 - mse: 5.1848 - mae: 1.7335 - val_loss: 3.6834 - val_mse: 3.6834 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00737: val_mae did not improve from 1.50838\n",
      "Epoch 738/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2022 - mse: 5.2022 - mae: 1.6418 - val_loss: 3.6893 - val_mse: 3.6893 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00738: val_mae did not improve from 1.50838\n",
      "Epoch 739/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4558 - mse: 4.4558 - mae: 1.6238 - val_loss: 3.7113 - val_mse: 3.7113 - val_mae: 1.5198\n",
      "\n",
      "Epoch 00739: val_mae did not improve from 1.50838\n",
      "Epoch 740/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5329 - mse: 4.5329 - mae: 1.6523 - val_loss: 3.6954 - val_mse: 3.6954 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00740: val_mae did not improve from 1.50838\n",
      "Epoch 741/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8829 - mse: 4.8829 - mae: 1.6861 - val_loss: 3.6917 - val_mse: 3.6917 - val_mae: 1.5141\n",
      "\n",
      "Epoch 00741: val_mae did not improve from 1.50838\n",
      "Epoch 742/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7372 - mse: 4.7372 - mae: 1.6561 - val_loss: 3.6831 - val_mse: 3.6831 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00742: val_mae did not improve from 1.50838\n",
      "Epoch 743/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9727 - mse: 4.9727 - mae: 1.6382 - val_loss: 3.6940 - val_mse: 3.6940 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00743: val_mae did not improve from 1.50838\n",
      "Epoch 744/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9731 - mse: 5.9731 - mae: 1.7928 - val_loss: 3.7126 - val_mse: 3.7126 - val_mae: 1.5199\n",
      "\n",
      "Epoch 00744: val_mae did not improve from 1.50838\n",
      "Epoch 745/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0208 - mse: 5.0208 - mae: 1.7240 - val_loss: 3.6955 - val_mse: 3.6955 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00745: val_mae did not improve from 1.50838\n",
      "Epoch 746/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5136 - mse: 5.5136 - mae: 1.6344 - val_loss: 3.6982 - val_mse: 3.6982 - val_mae: 1.5158\n",
      "\n",
      "Epoch 00746: val_mae did not improve from 1.50838\n",
      "Epoch 747/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0421 - mse: 5.0421 - mae: 1.6357 - val_loss: 3.6845 - val_mse: 3.6845 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00747: val_mae did not improve from 1.50838\n",
      "Epoch 748/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7556 - mse: 4.7556 - mae: 1.6451 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00748: val_mae did not improve from 1.50838\n",
      "Epoch 749/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8411 - mse: 4.8411 - mae: 1.6732 - val_loss: 3.7155 - val_mse: 3.7155 - val_mae: 1.5207\n",
      "\n",
      "Epoch 00749: val_mae did not improve from 1.50838\n",
      "Epoch 750/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8934 - mse: 4.8934 - mae: 1.6719 - val_loss: 3.7215 - val_mse: 3.7215 - val_mae: 1.5220\n",
      "\n",
      "Epoch 00750: val_mae did not improve from 1.50838\n",
      "Epoch 751/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9873 - mse: 4.9873 - mae: 1.6281 - val_loss: 3.7067 - val_mse: 3.7067 - val_mae: 1.5181\n",
      "\n",
      "Epoch 00751: val_mae did not improve from 1.50838\n",
      "Epoch 752/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0108 - mse: 5.0108 - mae: 1.6809 - val_loss: 3.6948 - val_mse: 3.6948 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00752: val_mae did not improve from 1.50838\n",
      "Epoch 753/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0876 - mse: 5.0876 - mae: 1.6981 - val_loss: 3.6910 - val_mse: 3.6910 - val_mae: 1.5134\n",
      "\n",
      "Epoch 00753: val_mae did not improve from 1.50838\n",
      "Epoch 754/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2393 - mse: 4.2393 - mae: 1.5776 - val_loss: 3.6894 - val_mse: 3.6894 - val_mae: 1.5129\n",
      "\n",
      "Epoch 00754: val_mae did not improve from 1.50838\n",
      "Epoch 755/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4743 - mse: 4.4743 - mae: 1.6100 - val_loss: 3.7106 - val_mse: 3.7106 - val_mae: 1.5193\n",
      "\n",
      "Epoch 00755: val_mae did not improve from 1.50838\n",
      "Epoch 756/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2033 - mse: 5.2033 - mae: 1.6489 - val_loss: 3.7112 - val_mse: 3.7112 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00756: val_mae did not improve from 1.50838\n",
      "Epoch 757/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5021 - mse: 5.5021 - mae: 1.7398 - val_loss: 3.7035 - val_mse: 3.7035 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00757: val_mae did not improve from 1.50838\n",
      "Epoch 758/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8036 - mse: 4.8036 - mae: 1.6703 - val_loss: 3.6882 - val_mse: 3.6882 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00758: val_mae did not improve from 1.50838\n",
      "Epoch 759/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3699 - mse: 5.3699 - mae: 1.6988 - val_loss: 3.7009 - val_mse: 3.7009 - val_mae: 1.5166\n",
      "\n",
      "Epoch 00759: val_mae did not improve from 1.50838\n",
      "Epoch 760/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1283 - mse: 5.1283 - mae: 1.6911 - val_loss: 3.6971 - val_mse: 3.6971 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00760: val_mae did not improve from 1.50838\n",
      "Epoch 761/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6515 - mse: 4.6515 - mae: 1.6218 - val_loss: 3.6937 - val_mse: 3.6937 - val_mae: 1.5141\n",
      "\n",
      "Epoch 00761: val_mae did not improve from 1.50838\n",
      "Epoch 762/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.3393 - mse: 5.3393 - mae: 1.6984 - val_loss: 3.6932 - val_mse: 3.6932 - val_mae: 1.5140\n",
      "\n",
      "Epoch 00762: val_mae did not improve from 1.50838\n",
      "Epoch 763/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7698 - mse: 4.7698 - mae: 1.6806 - val_loss: 3.6886 - val_mse: 3.6886 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00763: val_mae did not improve from 1.50838\n",
      "Epoch 764/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3650 - mse: 4.3650 - mae: 1.6066 - val_loss: 3.7039 - val_mse: 3.7039 - val_mae: 1.5169\n",
      "\n",
      "Epoch 00764: val_mae did not improve from 1.50838\n",
      "Epoch 765/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7290 - mse: 4.7290 - mae: 1.6608 - val_loss: 3.6986 - val_mse: 3.6986 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00765: val_mae did not improve from 1.50838\n",
      "Epoch 766/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6565 - mse: 4.6565 - mae: 1.6255 - val_loss: 3.6886 - val_mse: 3.6886 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00766: val_mae did not improve from 1.50838\n",
      "Epoch 767/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0251 - mse: 5.0251 - mae: 1.7435 - val_loss: 3.6979 - val_mse: 3.6979 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00767: val_mae did not improve from 1.50838\n",
      "Epoch 768/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5189 - mse: 4.5189 - mae: 1.6323 - val_loss: 3.7095 - val_mse: 3.7095 - val_mae: 1.5182\n",
      "\n",
      "Epoch 00768: val_mae did not improve from 1.50838\n",
      "Epoch 769/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2850 - mse: 4.2850 - mae: 1.5980 - val_loss: 3.6997 - val_mse: 3.6997 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00769: val_mae did not improve from 1.50838\n",
      "Epoch 770/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1712 - mse: 5.1712 - mae: 1.6933 - val_loss: 3.6978 - val_mse: 3.6978 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00770: val_mae did not improve from 1.50838\n",
      "Epoch 771/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.0779 - mse: 4.0779 - mae: 1.5497 - val_loss: 3.6932 - val_mse: 3.6932 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00771: val_mae did not improve from 1.50838\n",
      "Epoch 772/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7933 - mse: 4.7933 - mae: 1.6547 - val_loss: 3.7031 - val_mse: 3.7031 - val_mae: 1.5163\n",
      "\n",
      "Epoch 00772: val_mae did not improve from 1.50838\n",
      "Epoch 773/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9916 - mse: 4.9916 - mae: 1.6981 - val_loss: 3.7138 - val_mse: 3.7138 - val_mae: 1.5191\n",
      "\n",
      "Epoch 00773: val_mae did not improve from 1.50838\n",
      "Epoch 774/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6813 - mse: 4.6813 - mae: 1.6779 - val_loss: 3.7091 - val_mse: 3.7091 - val_mae: 1.5179\n",
      "\n",
      "Epoch 00774: val_mae did not improve from 1.50838\n",
      "Epoch 775/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.1409 - mse: 4.1409 - mae: 1.5623 - val_loss: 3.7112 - val_mse: 3.7112 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00775: val_mae did not improve from 1.50838\n",
      "Epoch 776/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1037 - mse: 5.1037 - mae: 1.7196 - val_loss: 3.7046 - val_mse: 3.7046 - val_mae: 1.5165\n",
      "\n",
      "Epoch 00776: val_mae did not improve from 1.50838\n",
      "Epoch 777/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0398 - mse: 5.0398 - mae: 1.7330 - val_loss: 3.7031 - val_mse: 3.7031 - val_mae: 1.5161\n",
      "\n",
      "Epoch 00777: val_mae did not improve from 1.50838\n",
      "Epoch 778/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2535 - mse: 5.2535 - mae: 1.6884 - val_loss: 3.7130 - val_mse: 3.7130 - val_mae: 1.5188\n",
      "\n",
      "Epoch 00778: val_mae did not improve from 1.50838\n",
      "Epoch 779/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2291 - mse: 4.2291 - mae: 1.5795 - val_loss: 3.6879 - val_mse: 3.6879 - val_mae: 1.5114\n",
      "\n",
      "Epoch 00779: val_mae did not improve from 1.50838\n",
      "Epoch 780/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2292 - mse: 4.2292 - mae: 1.5937 - val_loss: 3.7071 - val_mse: 3.7071 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00780: val_mae did not improve from 1.50838\n",
      "Epoch 781/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1204 - mse: 5.1204 - mae: 1.6415 - val_loss: 3.7060 - val_mse: 3.7060 - val_mae: 1.5168\n",
      "\n",
      "Epoch 00781: val_mae did not improve from 1.50838\n",
      "Epoch 782/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2642 - mse: 5.2642 - mae: 1.7091 - val_loss: 3.7099 - val_mse: 3.7099 - val_mae: 1.5178\n",
      "\n",
      "Epoch 00782: val_mae did not improve from 1.50838\n",
      "Epoch 783/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9551 - mse: 4.9551 - mae: 1.6620 - val_loss: 3.7147 - val_mse: 3.7147 - val_mae: 1.5191\n",
      "\n",
      "Epoch 00783: val_mae did not improve from 1.50838\n",
      "Epoch 784/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7825 - mse: 4.7825 - mae: 1.6270 - val_loss: 3.6967 - val_mse: 3.6967 - val_mae: 1.5138\n",
      "\n",
      "Epoch 00784: val_mae did not improve from 1.50838\n",
      "Epoch 785/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0660 - mse: 5.0660 - mae: 1.6867 - val_loss: 3.6963 - val_mse: 3.6963 - val_mae: 1.5138\n",
      "\n",
      "Epoch 00785: val_mae did not improve from 1.50838\n",
      "Epoch 786/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6302 - mse: 4.6302 - mae: 1.6527 - val_loss: 3.6950 - val_mse: 3.6950 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00786: val_mae did not improve from 1.50838\n",
      "Epoch 787/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9338 - mse: 5.9338 - mae: 1.8223 - val_loss: 3.6992 - val_mse: 3.6992 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00787: val_mae did not improve from 1.50838\n",
      "Epoch 788/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9191 - mse: 5.9191 - mae: 1.8091 - val_loss: 3.6941 - val_mse: 3.6941 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00788: val_mae did not improve from 1.50838\n",
      "Epoch 789/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.7523 - mse: 4.7523 - mae: 1.6280 - val_loss: 3.6985 - val_mse: 3.6985 - val_mae: 1.5142\n",
      "\n",
      "Epoch 00789: val_mae did not improve from 1.50838\n",
      "Epoch 790/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4797 - mse: 4.4797 - mae: 1.6206 - val_loss: 3.7015 - val_mse: 3.7015 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00790: val_mae did not improve from 1.50838\n",
      "Epoch 791/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6023 - mse: 4.6023 - mae: 1.5882 - val_loss: 3.7046 - val_mse: 3.7046 - val_mae: 1.5161\n",
      "\n",
      "Epoch 00791: val_mae did not improve from 1.50838\n",
      "Epoch 792/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7449 - mse: 4.7449 - mae: 1.6054 - val_loss: 3.7042 - val_mse: 3.7042 - val_mae: 1.5160\n",
      "\n",
      "Epoch 00792: val_mae did not improve from 1.50838\n",
      "Epoch 793/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8687 - mse: 4.8687 - mae: 1.6809 - val_loss: 3.6951 - val_mse: 3.6951 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00793: val_mae did not improve from 1.50838\n",
      "Epoch 794/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9315 - mse: 5.9315 - mae: 1.7843 - val_loss: 3.7025 - val_mse: 3.7025 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00794: val_mae did not improve from 1.50838\n",
      "Epoch 795/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4229 - mse: 4.4229 - mae: 1.6169 - val_loss: 3.6866 - val_mse: 3.6866 - val_mae: 1.5100\n",
      "\n",
      "Epoch 00795: val_mae did not improve from 1.50838\n",
      "Epoch 796/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.6967 - mse: 5.6967 - mae: 1.7739 - val_loss: 3.7019 - val_mse: 3.7019 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00796: val_mae did not improve from 1.50838\n",
      "Epoch 797/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.8940 - mse: 4.8940 - mae: 1.6541 - val_loss: 3.6941 - val_mse: 3.6941 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00797: val_mae did not improve from 1.50838\n",
      "Epoch 798/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3330 - mse: 4.3330 - mae: 1.5906 - val_loss: 3.6984 - val_mse: 3.6984 - val_mae: 1.5138\n",
      "\n",
      "Epoch 00798: val_mae did not improve from 1.50838\n",
      "Epoch 799/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.4901 - mse: 4.4901 - mae: 1.6092 - val_loss: 3.6942 - val_mse: 3.6942 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00799: val_mae did not improve from 1.50838\n",
      "Epoch 800/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.0070 - mse: 5.0070 - mae: 1.7187 - val_loss: 3.7076 - val_mse: 3.7076 - val_mae: 1.5164\n",
      "\n",
      "Epoch 00800: val_mae did not improve from 1.50838\n",
      "Epoch 801/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.4633 - mse: 4.4633 - mae: 1.6086 - val_loss: 3.6961 - val_mse: 3.6961 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00801: val_mae did not improve from 1.50838\n",
      "Epoch 802/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2573 - mse: 4.2573 - mae: 1.5874 - val_loss: 3.6907 - val_mse: 3.6907 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00802: val_mae did not improve from 1.50838\n",
      "Epoch 803/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.4038 - mse: 4.4038 - mae: 1.6489 - val_loss: 3.7013 - val_mse: 3.7013 - val_mae: 1.5148\n",
      "\n",
      "Epoch 00803: val_mae did not improve from 1.50838\n",
      "Epoch 804/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.4613 - mse: 4.4613 - mae: 1.6195 - val_loss: 3.7079 - val_mse: 3.7079 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00804: val_mae did not improve from 1.50838\n",
      "Epoch 00804: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def show_info(model, X, y, log, weights = None):\n",
    "    '''\n",
    "    Show metrics about the evaluation model and plots about loss, rmse and rmspe\n",
    "    '''\n",
    "    if (log != None):\n",
    "        # summarize history for loss\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['loss'])\n",
    "        plt.plot(log.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MAE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mae'])\n",
    "        plt.plot(log.history['val_mae'])\n",
    "        plt.title('Model MAE')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MSE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mse'])\n",
    "        plt.plot(log.history['val_mse'])\n",
    "        plt.title('Model MSE')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    if (weights != None):\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    predictions = model.predict(X, verbose=1)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    mae= mean_absolute_error(y, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "show_info(model, x_test, y_test, log, weights='/home/m-marouni/Documents/CE-901/Heathrow/best_weights')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-06T19:53:08.009876</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m4e3f864559\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.553289\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(155.009539 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.994362\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(243.450612 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"341.435436\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(331.891686 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"429.876509\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(420.332759 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"518.317582\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(508.773832 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"606.758655\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(597.214905 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"695.199729\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(685.655979 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"783.640802\" xlink:href=\"#m4e3f864559\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(774.097052 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mceea686c63\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mceea686c63\" y=\"524.263946\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 528.063165)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mceea686c63\" y=\"460.595199\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 464.394418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mceea686c63\" y=\"396.926453\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 400.725671)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mceea686c63\" y=\"333.257706\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 337.056924)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mceea686c63\" y=\"269.588959\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 273.388177)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mceea686c63\" y=\"205.920212\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 209.719431)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mceea686c63\" y=\"142.251465\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 146.050684)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mceea686c63\" y=\"78.582718\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 82.381937)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 303.775937)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p25565917b1)\" d=\"M 76.112216 47.027216 \nL 77.881037 134.743117 \nL 82.303091 334.145251 \nL 83.187502 371.619228 \nL 84.071913 401.920149 \nL 84.956323 426.896101 \nL 85.840734 447.737789 \nL 86.725145 464.778391 \nL 87.609555 478.621093 \nL 88.493966 483.746988 \nL 89.378377 491.153382 \nL 91.147198 499.782419 \nL 92.031609 500.662277 \nL 92.91602 503.257618 \nL 93.800431 500.977653 \nL 94.684841 502.91484 \nL 95.569252 503.097945 \nL 96.453663 499.201451 \nL 97.338073 503.788329 \nL 98.222484 501.87512 \nL 99.106895 503.848204 \nL 99.991306 502.546742 \nL 100.875716 504.048341 \nL 101.760127 502.855481 \nL 102.644538 502.655453 \nL 103.528949 505.12833 \nL 104.413359 506.28256 \nL 105.29777 501.981421 \nL 106.182181 503.330481 \nL 107.066592 503.005342 \nL 107.951002 506.808328 \nL 108.835413 502.106564 \nL 110.604234 504.787088 \nL 111.488645 503.251231 \nL 112.373056 506.240943 \nL 113.257467 504.333157 \nL 114.141877 501.152506 \nL 115.026288 506.090815 \nL 115.910699 507.568315 \nL 116.79511 504.189659 \nL 117.67952 503.466663 \nL 118.563931 504.863242 \nL 119.448342 503.892481 \nL 120.332753 503.523818 \nL 121.217163 504.537677 \nL 122.101574 501.506074 \nL 122.985985 504.364044 \nL 123.870395 503.390393 \nL 124.754806 503.480889 \nL 125.639217 505.766186 \nL 126.523628 505.402787 \nL 127.408038 507.088335 \nL 128.292449 503.256416 \nL 129.17686 503.616924 \nL 130.061271 506.198974 \nL 130.945681 504.720333 \nL 131.830092 506.582604 \nL 132.714503 502.391167 \nL 133.598914 503.418409 \nL 134.483324 504.000549 \nL 135.367735 505.789131 \nL 136.252146 505.755784 \nL 137.136556 504.572858 \nL 138.020967 504.283367 \nL 138.905378 505.571914 \nL 139.789789 507.90051 \nL 140.674199 508.27955 \nL 141.55861 505.071114 \nL 142.443021 506.407933 \nL 143.327432 505.185461 \nL 144.211842 505.48058 \nL 145.096253 507.274075 \nL 145.980664 507.813869 \nL 146.865075 506.85017 \nL 147.749485 503.826965 \nL 148.633896 504.071827 \nL 149.518307 506.425129 \nL 150.402717 504.138284 \nL 151.287128 508.277315 \nL 152.171539 505.711556 \nL 153.05595 507.152175 \nL 153.94036 506.391393 \nL 154.824771 506.253421 \nL 155.709182 507.213119 \nL 156.593593 507.024568 \nL 157.478003 507.835862 \nL 158.362414 506.590577 \nL 159.246825 507.750903 \nL 160.131235 507.012418 \nL 161.015646 508.709175 \nL 161.900057 505.793224 \nL 162.784468 506.601658 \nL 163.668878 506.276221 \nL 164.553289 510.078254 \nL 165.4377 506.55522 \nL 166.322111 505.373289 \nL 167.206521 506.906244 \nL 168.090932 509.463951 \nL 168.975343 510.180712 \nL 170.744164 508.965288 \nL 171.628575 506.15168 \nL 172.512986 509.020203 \nL 173.397396 506.75685 \nL 174.281807 510.365942 \nL 175.166218 504.639012 \nL 176.050629 507.327879 \nL 176.935039 509.177605 \nL 177.81945 508.059448 \nL 178.703861 508.085637 \nL 179.588272 510.095875 \nL 180.472682 509.316598 \nL 182.241504 508.821402 \nL 183.125915 509.033677 \nL 184.010325 509.70613 \nL 184.894736 509.2803 \nL 185.779147 510.445618 \nL 186.663557 512.455826 \nL 188.432379 511.26939 \nL 189.31679 511.574552 \nL 190.2012 511.442488 \nL 191.970022 510.596772 \nL 192.854433 513.060632 \nL 193.738843 509.191577 \nL 194.623254 509.376619 \nL 195.507665 514.797354 \nL 196.392076 511.846702 \nL 197.276486 509.845044 \nL 198.160897 512.456906 \nL 199.045308 513.546507 \nL 199.929718 511.085192 \nL 200.814129 512.748626 \nL 201.69854 510.942702 \nL 202.582951 510.919653 \nL 203.467361 510.496446 \nL 204.351772 511.973211 \nL 205.236183 512.706887 \nL 206.120594 515.550528 \nL 207.005004 510.962782 \nL 207.889415 514.912733 \nL 208.773826 510.628662 \nL 209.658237 513.166046 \nL 210.542647 512.569692 \nL 211.427058 514.253765 \nL 212.311469 513.56159 \nL 213.195879 512.294027 \nL 214.08029 515.282404 \nL 214.964701 512.32109 \nL 215.849112 512.543686 \nL 216.733522 514.612191 \nL 217.617933 514.898446 \nL 218.502344 513.22008 \nL 219.386755 519.072936 \nL 220.271165 516.224323 \nL 221.155576 514.94625 \nL 222.039987 515.661043 \nL 222.924398 515.526252 \nL 223.808808 515.565981 \nL 224.693219 515.105723 \nL 225.57763 515.173067 \nL 226.46204 515.752346 \nL 227.346451 517.815416 \nL 228.230862 516.947902 \nL 229.115273 517.239974 \nL 229.999683 518.652698 \nL 230.884094 515.191871 \nL 231.768505 516.424861 \nL 232.652916 516.126674 \nL 233.537326 515.995551 \nL 234.421737 517.402277 \nL 235.306148 516.598427 \nL 236.190558 517.401232 \nL 237.074969 518.46218 \nL 237.95938 519.077563 \nL 238.843791 519.273541 \nL 239.728201 518.758058 \nL 240.612612 517.330373 \nL 241.497023 517.38686 \nL 242.381434 517.162739 \nL 243.265844 517.223883 \nL 244.150255 520.90803 \nL 245.034666 516.155868 \nL 245.919077 519.764814 \nL 246.803487 519.842711 \nL 247.687898 520.336759 \nL 248.572309 518.852277 \nL 249.456719 519.614115 \nL 250.34113 519.388962 \nL 251.225541 519.892409 \nL 252.109952 519.81549 \nL 252.994362 519.384529 \nL 253.878773 517.493556 \nL 254.763184 519.529484 \nL 255.647595 520.764963 \nL 256.532005 519.839614 \nL 257.416416 519.631978 \nL 258.300827 521.092483 \nL 259.185238 519.324958 \nL 260.069648 522.47761 \nL 260.954059 519.772216 \nL 261.83847 518.753086 \nL 262.72288 519.438952 \nL 263.607291 521.246291 \nL 264.491702 519.979098 \nL 265.376113 519.843816 \nL 266.260523 519.553244 \nL 267.144934 518.940447 \nL 268.029345 520.994282 \nL 268.913756 521.399692 \nL 269.798166 517.98149 \nL 270.682577 520.201313 \nL 271.566988 519.025399 \nL 272.451399 522.343305 \nL 273.335809 523.212939 \nL 274.22022 521.237705 \nL 275.104631 523.22419 \nL 275.989041 520.611082 \nL 276.873452 519.458625 \nL 277.757863 523.272541 \nL 278.642274 520.927041 \nL 279.526684 520.324202 \nL 280.411095 519.203659 \nL 281.295506 523.059319 \nL 282.179917 519.553323 \nL 283.064327 522.087714 \nL 283.948738 519.167828 \nL 284.833149 519.637279 \nL 285.71756 519.00541 \nL 286.60197 522.249907 \nL 287.486381 524.874594 \nL 289.255202 519.299698 \nL 290.139613 520.691341 \nL 291.024024 522.470913 \nL 291.908435 523.991523 \nL 292.792845 522.252221 \nL 293.677256 521.841194 \nL 294.561667 520.243051 \nL 295.446078 522.175399 \nL 296.330488 522.482826 \nL 297.214899 522.14547 \nL 298.09931 521.346599 \nL 298.98372 524.823407 \nL 299.868131 521.805224 \nL 300.752542 521.510577 \nL 301.636953 521.341341 \nL 302.521363 523.201487 \nL 303.405774 522.421384 \nL 304.290185 522.761661 \nL 305.174596 520.475041 \nL 306.059006 523.389875 \nL 306.943417 523.658418 \nL 307.827828 522.441124 \nL 308.712239 522.548221 \nL 309.596649 523.255473 \nL 310.48106 520.283587 \nL 311.365471 523.189629 \nL 312.249881 522.993001 \nL 313.134292 522.119458 \nL 314.018703 524.935532 \nL 314.903114 520.659609 \nL 315.787524 523.8585 \nL 316.671935 524.361109 \nL 317.556346 524.541306 \nL 318.440757 522.515293 \nL 319.325167 526.396746 \nL 320.209578 519.487631 \nL 321.093989 522.20098 \nL 321.9784 525.35209 \nL 322.86281 520.935924 \nL 323.747221 524.257686 \nL 324.631632 522.931317 \nL 325.516042 522.430377 \nL 326.400453 522.996086 \nL 328.169275 524.92462 \nL 329.053685 521.6021 \nL 329.938096 524.044416 \nL 330.822507 520.096863 \nL 332.591328 523.296598 \nL 333.475739 520.851579 \nL 334.36015 523.968444 \nL 335.244561 522.249124 \nL 336.128971 523.16811 \nL 337.013382 522.520891 \nL 337.897793 523.088325 \nL 338.782203 525.026604 \nL 339.666614 526.278162 \nL 340.551025 524.864915 \nL 341.435436 521.395454 \nL 342.319846 523.095593 \nL 344.088668 522.489621 \nL 344.973079 526.338535 \nL 345.857489 525.732126 \nL 346.7419 525.31323 \nL 347.626311 524.033517 \nL 348.510722 523.65274 \nL 349.395132 523.999144 \nL 350.279543 523.138054 \nL 351.163954 524.683037 \nL 352.048364 521.403426 \nL 352.932775 524.066093 \nL 353.817186 522.599541 \nL 354.701597 523.014217 \nL 355.586007 525.174577 \nL 356.470418 524.550426 \nL 357.354829 524.863282 \nL 358.23924 522.925293 \nL 359.12365 522.798129 \nL 360.008061 523.134404 \nL 360.892472 524.946346 \nL 361.776883 522.652524 \nL 362.661293 524.658986 \nL 363.545704 522.471028 \nL 364.430115 523.728737 \nL 365.314525 524.715024 \nL 366.198936 524.213865 \nL 367.083347 524.903514 \nL 368.852168 523.98802 \nL 369.736579 521.699936 \nL 370.62099 524.941907 \nL 371.505401 521.566281 \nL 373.274222 524.452723 \nL 374.158633 523.215659 \nL 375.043043 522.227654 \nL 375.927454 522.962496 \nL 376.811865 524.651657 \nL 377.696276 526.797742 \nL 378.580686 522.296886 \nL 379.465097 524.041763 \nL 380.349508 525.487184 \nL 381.233919 524.97764 \nL 382.118329 525.413495 \nL 383.00274 525.520234 \nL 383.887151 527.244545 \nL 384.771562 523.545626 \nL 385.655972 525.29972 \nL 386.540383 523.602082 \nL 387.424794 523.669887 \nL 388.309204 524.774006 \nL 389.193615 522.227204 \nL 390.962437 525.289045 \nL 391.846847 523.108174 \nL 392.731258 521.700726 \nL 393.615669 523.870619 \nL 394.50008 525.140295 \nL 395.38449 522.52754 \nL 396.268901 523.369424 \nL 397.153312 521.174035 \nL 398.037723 525.329721 \nL 398.922133 523.831795 \nL 399.806544 524.763053 \nL 400.690955 524.33749 \nL 401.575365 524.867702 \nL 402.459776 524.557919 \nL 403.344187 524.03481 \nL 404.228598 524.029552 \nL 405.113008 522.8151 \nL 406.88183 524.930207 \nL 407.766241 524.886112 \nL 408.650651 522.240775 \nL 409.535062 523.733181 \nL 410.419473 524.850063 \nL 411.303884 524.100739 \nL 412.188294 522.140904 \nL 413.072705 525.789943 \nL 413.957116 522.279344 \nL 414.841526 524.538325 \nL 415.725937 524.975782 \nL 416.610348 525.037212 \nL 417.494759 522.404219 \nL 418.379169 524.479585 \nL 419.26358 522.443365 \nL 420.147991 526.855055 \nL 421.032402 525.014394 \nL 421.916812 526.870284 \nL 422.801223 525.328501 \nL 423.685634 524.2896 \nL 424.570045 523.620826 \nL 425.454455 525.492491 \nL 427.223277 524.810329 \nL 428.107687 525.425457 \nL 428.992098 523.669171 \nL 429.876509 526.011799 \nL 430.76092 524.849638 \nL 431.64533 522.85444 \nL 433.414152 524.71772 \nL 434.298563 523.851772 \nL 435.182973 519.202797 \nL 436.067384 523.741645 \nL 436.951795 526.039098 \nL 437.836205 524.410007 \nL 438.720616 525.441833 \nL 439.605027 525.912328 \nL 440.489438 524.299273 \nL 441.373848 523.375521 \nL 442.258259 526.954793 \nL 443.14267 523.153634 \nL 444.027081 523.824066 \nL 444.911491 525.017654 \nL 445.795902 523.283604 \nL 446.680313 523.924331 \nL 447.564724 525.474822 \nL 448.449134 524.052911 \nL 449.333545 523.208209 \nL 450.217956 526.578479 \nL 451.102366 525.053388 \nL 451.986777 525.308348 \nL 452.871188 524.270037 \nL 453.755599 526.265994 \nL 454.640009 524.86432 \nL 455.52442 525.611215 \nL 456.408831 525.410854 \nL 457.293242 524.751953 \nL 458.177652 521.732725 \nL 459.062063 523.880529 \nL 459.946474 523.27775 \nL 460.830885 525.928552 \nL 461.715295 525.826095 \nL 462.599706 525.284947 \nL 463.484117 529.268184 \nL 464.368527 522.880598 \nL 465.252938 522.821433 \nL 466.137349 527.07917 \nL 467.02176 524.321994 \nL 467.90617 525.049963 \nL 468.790581 523.017884 \nL 469.674992 522.058077 \nL 470.559403 525.554358 \nL 471.443813 524.930814 \nL 472.328224 526.855669 \nL 473.212635 528.326641 \nL 474.097046 525.231398 \nL 474.981456 524.931925 \nL 475.865867 524.288331 \nL 477.634688 524.789508 \nL 478.519099 523.572208 \nL 479.40351 525.240937 \nL 480.287921 525.005328 \nL 481.172331 525.335793 \nL 482.056742 525.290363 \nL 482.941153 523.112952 \nL 483.825564 524.860677 \nL 484.709974 525.551826 \nL 485.594385 524.758669 \nL 486.478796 523.389881 \nL 487.363207 524.521706 \nL 488.247617 526.604643 \nL 489.132028 523.227827 \nL 490.016439 523.735567 \nL 490.900849 525.696647 \nL 491.78526 525.88195 \nL 492.669671 525.090372 \nL 493.554082 523.914197 \nL 494.438492 525.406689 \nL 495.322903 523.497038 \nL 496.207314 524.400972 \nL 497.091725 525.675256 \nL 497.976135 526.093023 \nL 498.860546 525.128813 \nL 499.744957 524.676005 \nL 500.629367 525.49506 \nL 501.513778 525.238545 \nL 502.398189 527.40938 \nL 503.2826 525.120847 \nL 504.16701 526.478742 \nL 505.051421 525.996097 \nL 505.935832 524.720707 \nL 506.820243 522.462692 \nL 507.704653 524.807232 \nL 508.589064 523.635041 \nL 509.473475 525.792001 \nL 510.357886 525.805383 \nL 511.242296 525.197912 \nL 512.126707 524.448357 \nL 513.011118 524.550183 \nL 513.895528 525.917696 \nL 514.779939 525.89305 \nL 515.66435 523.208895 \nL 516.548761 526.462615 \nL 517.433171 521.464443 \nL 518.317582 524.780643 \nL 519.201993 526.053233 \nL 520.086404 524.595358 \nL 520.970814 522.798421 \nL 521.855225 527.518941 \nL 522.739636 522.688209 \nL 523.624047 526.689189 \nL 524.508457 525.827771 \nL 526.277279 527.68499 \nL 528.0461 523.477152 \nL 528.930511 526.93365 \nL 529.814922 524.840032 \nL 530.699332 523.811266 \nL 531.583743 525.696587 \nL 532.468154 526.510638 \nL 533.352565 526.368409 \nL 534.236975 522.605995 \nL 535.121386 523.188942 \nL 536.005797 525.785346 \nL 536.890208 522.64525 \nL 537.774618 526.514244 \nL 538.659029 526.270554 \nL 539.54344 525.822385 \nL 540.42785 524.621018 \nL 541.312261 523.960891 \nL 542.196672 520.991665 \nL 543.081083 524.868249 \nL 543.965493 526.240577 \nL 544.849904 524.637309 \nL 545.734315 523.373049 \nL 546.618726 522.884381 \nL 547.503136 526.248166 \nL 548.387547 524.104844 \nL 549.271958 524.236453 \nL 550.156369 525.206771 \nL 551.040779 523.375575 \nL 551.92519 524.807669 \nL 552.809601 524.59953 \nL 553.694011 524.055673 \nL 554.578422 524.704131 \nL 555.462833 522.011608 \nL 556.347244 524.004602 \nL 557.231654 526.671197 \nL 558.116065 526.970489 \nL 559.000476 523.684521 \nL 559.884887 524.398318 \nL 560.769297 526.458498 \nL 561.653708 523.631549 \nL 562.538119 524.896963 \nL 563.42253 522.924267 \nL 564.30694 525.224731 \nL 565.191351 526.715267 \nL 566.075762 525.277927 \nL 566.960172 525.686216 \nL 567.844583 525.347742 \nL 568.728994 526.732269 \nL 569.613405 525.554838 \nL 570.497815 523.894427 \nL 571.382226 526.224619 \nL 572.266637 524.296273 \nL 573.151048 524.581818 \nL 574.035458 525.523895 \nL 574.919869 524.457908 \nL 575.80428 524.670777 \nL 576.68869 523.024582 \nL 577.573101 523.79554 \nL 578.457512 524.719602 \nL 579.341923 524.098116 \nL 580.226333 527.201179 \nL 581.110744 522.887277 \nL 581.995155 524.459171 \nL 582.879566 525.282154 \nL 583.763976 523.536099 \nL 584.648387 524.761941 \nL 585.532798 527.083815 \nL 586.417209 523.69627 \nL 587.301619 525.921861 \nL 588.18603 523.4472 \nL 589.070441 525.589029 \nL 589.954851 522.751855 \nL 590.839262 523.484129 \nL 591.723673 525.2778 \nL 592.608084 523.181474 \nL 593.492494 525.61056 \nL 594.376905 524.564367 \nL 595.261316 524.309638 \nL 596.145727 526.188674 \nL 597.030137 522.563552 \nL 597.914548 525.567273 \nL 598.798959 526.34771 \nL 599.68337 527.790417 \nL 600.56778 525.360093 \nL 601.452191 524.490702 \nL 602.336602 527.176212 \nL 603.221012 525.421887 \nL 604.105423 527.780878 \nL 604.989834 526.676614 \nL 605.874245 525.016033 \nL 606.758655 524.937724 \nL 607.643066 525.836976 \nL 608.527477 525.606874 \nL 609.411888 525.582574 \nL 610.296298 524.736379 \nL 611.180709 524.966377 \nL 612.06512 525.621671 \nL 612.949531 525.230858 \nL 613.833941 525.56295 \nL 614.718352 524.23035 \nL 615.602763 526.042176 \nL 616.487173 523.920955 \nL 617.371584 523.675067 \nL 618.255995 524.269308 \nL 620.024816 526.212269 \nL 620.909227 525.381569 \nL 621.793638 525.186873 \nL 622.678049 526.476902 \nL 623.562459 523.919644 \nL 624.44687 524.810942 \nL 625.331281 524.786769 \nL 626.215692 524.404092 \nL 627.100102 525.95689 \nL 627.984513 523.394854 \nL 628.868924 525.538607 \nL 629.753334 523.330801 \nL 630.637745 523.794392 \nL 631.522156 523.589932 \nL 632.406567 525.803252 \nL 633.290977 524.87035 \nL 634.175388 524.365997 \nL 635.059799 523.607098 \nL 635.94421 524.752688 \nL 636.82862 523.952353 \nL 637.713031 524.733561 \nL 639.481852 524.290056 \nL 640.366263 524.91026 \nL 641.250674 523.397379 \nL 642.135085 524.339566 \nL 643.019495 523.519717 \nL 643.903906 526.607928 \nL 644.788317 523.165468 \nL 645.672728 524.118633 \nL 646.557138 525.917441 \nL 648.32596 525.243809 \nL 649.210371 527.10197 \nL 650.094781 526.679249 \nL 650.979192 524.869342 \nL 651.863603 524.011779 \nL 652.748013 524.814233 \nL 653.632424 524.764832 \nL 654.516835 525.99895 \nL 655.401246 525.956028 \nL 656.285656 525.693284 \nL 657.170067 521.875512 \nL 658.054478 524.371644 \nL 658.938889 525.58663 \nL 659.823299 523.029299 \nL 660.70771 526.075147 \nL 661.592121 526.977787 \nL 662.476532 525.724439 \nL 663.360942 525.18609 \nL 664.245353 526.182547 \nL 665.129764 525.206722 \nL 666.014174 527.490895 \nL 666.898585 525.23438 \nL 667.782996 525.343535 \nL 668.667407 523.75892 \nL 669.551817 524.936667 \nL 670.436228 525.090584 \nL 671.320639 523.601536 \nL 672.20505 525.613335 \nL 673.08946 525.206837 \nL 673.973871 525.734087 \nL 674.858282 524.620957 \nL 675.742693 525.710303 \nL 676.627103 525.729922 \nL 677.511514 524.320488 \nL 678.395925 526.915884 \nL 679.280335 525.094914 \nL 680.164746 525.740596 \nL 681.049157 524.684913 \nL 681.933568 523.827921 \nL 682.817978 525.5883 \nL 683.702389 526.343769 \nL 684.5868 524.494965 \nL 685.471211 523.673798 \nL 686.355621 525.683374 \nL 687.240032 525.223402 \nL 688.124443 525.427145 \nL 689.008854 524.950274 \nL 689.893264 525.894149 \nL 690.777675 526.664907 \nL 691.662086 525.999035 \nL 692.546496 526.877461 \nL 693.430907 528.135485 \nL 694.315318 526.447241 \nL 695.199729 524.243326 \nL 696.084139 523.323126 \nL 696.96855 522.953935 \nL 697.852961 523.146846 \nL 698.737372 524.991181 \nL 699.621782 523.485939 \nL 700.506193 522.318234 \nL 701.390604 526.86525 \nL 702.275015 526.106739 \nL 703.159425 526.244584 \nL 704.043836 526.775993 \nL 704.928247 523.595367 \nL 705.812657 526.81101 \nL 706.697068 523.953507 \nL 707.581479 524.746762 \nL 708.46589 524.914784 \nL 709.3503 527.217252 \nL 710.234711 523.430089 \nL 711.119122 525.848439 \nL 712.003533 527.643118 \nL 712.887943 525.770166 \nL 713.772354 526.639708 \nL 714.656765 526.883435 \nL 715.541175 526.144604 \nL 716.425586 524.578478 \nL 717.309997 528.232866 \nL 718.194408 524.105269 \nL 719.078818 524.461496 \nL 719.963229 526.607703 \nL 720.84764 527.226153 \nL 721.732051 529.105772 \nL 722.616461 523.044746 \nL 723.500872 525.473292 \nL 724.385283 525.508982 \nL 725.269694 523.300775 \nL 726.154104 524.271803 \nL 727.038515 524.548817 \nL 728.807336 526.476319 \nL 729.691747 526.423129 \nL 730.576158 523.458597 \nL 731.460569 524.604521 \nL 732.344979 525.525037 \nL 733.22939 523.101665 \nL 734.113801 524.595419 \nL 734.998212 524.987629 \nL 735.882622 523.633134 \nL 736.767033 526.121943 \nL 737.651444 527.818694 \nL 738.535855 526.54226 \nL 739.420265 524.782993 \nL 740.304676 523.592379 \nL 741.189087 526.962358 \nL 742.073497 524.837142 \nL 742.957908 523.931672 \nL 743.842319 525.243311 \nL 744.72673 526.950846 \nL 746.495551 523.928545 \nL 747.379962 524.605771 \nL 748.264373 527.426946 \nL 749.148783 526.703348 \nL 750.033194 526.880442 \nL 750.917605 526.826311 \nL 751.802016 524.631881 \nL 752.686426 526.775962 \nL 753.570837 524.655221 \nL 754.455248 526.033645 \nL 755.339658 526.399625 \nL 756.224069 526.943402 \nL 757.10848 525.083881 \nL 757.992891 524.740137 \nL 758.877301 524.25162 \nL 759.761712 526.697373 \nL 760.646123 526.442893 \nL 761.530534 525.354282 \nL 762.414944 524.788664 \nL 763.299355 524.566395 \nL 764.183766 525.481216 \nL 765.068177 527.157595 \nL 765.952587 522.914413 \nL 766.836998 526.504383 \nL 767.721409 527.036831 \nL 768.605819 526.439517 \nL 769.49023 525.017138 \nL 770.374641 525.722508 \nL 771.259052 524.273431 \nL 772.143462 523.764622 \nL 773.027873 524.398148 \nL 773.912284 527.349104 \nL 774.796695 526.393468 \nL 775.681105 526.297653 \nL 776.565516 524.405289 \nL 777.449927 525.562355 \nL 778.334337 525.964243 \nL 779.218748 524.900321 \nL 780.103159 524.800176 \nL 780.98757 526.055753 \nL 781.87198 527.898467 \nL 782.756391 524.415623 \nL 783.640802 525.993419 \nL 784.525213 524.115968 \nL 785.409623 526.16792 \nL 786.294034 526.231894 \nL 786.294034 526.231894 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p25565917b1)\" d=\"M 76.112216 125.667727 \nL 79.649859 282.82969 \nL 81.41868 355.839267 \nL 83.187502 418.26752 \nL 84.071913 442.591311 \nL 84.956323 463.215381 \nL 85.840734 479.151403 \nL 86.725145 490.867698 \nL 87.609555 499.146822 \nL 88.493966 504.709604 \nL 89.378377 508.477124 \nL 90.262788 510.648535 \nL 91.147198 511.789189 \nL 92.031609 512.470052 \nL 92.91602 512.691914 \nL 94.684841 512.678495 \nL 96.453663 512.420614 \nL 98.222484 512.315382 \nL 100.875716 512.05673 \nL 108.835413 512.349755 \nL 110.604234 512.252525 \nL 111.488645 512.318278 \nL 112.373056 512.602772 \nL 113.257467 512.3593 \nL 114.141877 512.305679 \nL 115.026288 512.460367 \nL 115.910699 512.461551 \nL 118.563931 512.833032 \nL 119.448342 512.843852 \nL 121.217163 512.989876 \nL 122.101574 513.094938 \nL 122.985985 513.072059 \nL 123.870395 513.232303 \nL 125.639217 513.10373 \nL 129.17686 513.128856 \nL 130.061271 513.292537 \nL 130.945681 513.336686 \nL 131.830092 513.249839 \nL 132.714503 513.64517 \nL 133.598914 513.765048 \nL 134.483324 514.013256 \nL 136.252146 514.026742 \nL 137.136556 513.822136 \nL 138.905378 513.992648 \nL 139.789789 513.760397 \nL 140.674199 514.019832 \nL 142.443021 513.827 \nL 143.327432 514.295261 \nL 145.096253 514.445522 \nL 145.980664 514.426226 \nL 150.402717 514.998493 \nL 151.287128 515.222826 \nL 152.171539 515.25579 \nL 153.05595 515.533885 \nL 154.824771 515.084331 \nL 157.478003 515.772104 \nL 162.784468 516.19083 \nL 163.668878 516.422195 \nL 164.553289 516.213873 \nL 165.4377 516.431564 \nL 166.322111 516.465682 \nL 167.206521 516.798302 \nL 169.859754 516.814757 \nL 171.628575 517.599317 \nL 172.512986 517.095602 \nL 173.397396 517.241887 \nL 174.281807 517.081533 \nL 175.166218 517.75191 \nL 176.050629 517.896616 \nL 177.81945 518.563459 \nL 179.588272 518.127022 \nL 181.357093 518.414545 \nL 182.241504 518.475283 \nL 183.125915 519.153304 \nL 184.010325 519.514189 \nL 186.663557 519.212724 \nL 187.547968 519.702577 \nL 191.085611 520.416562 \nL 191.970022 520.383094 \nL 192.854433 520.989363 \nL 193.738843 520.987323 \nL 194.623254 521.225343 \nL 196.392076 521.500765 \nL 197.276486 521.439317 \nL 198.160897 521.686238 \nL 199.045308 522.081472 \nL 199.929718 522.202777 \nL 201.69854 522.714452 \nL 203.467361 522.826236 \nL 209.658237 523.887451 \nL 211.427058 524.463561 \nL 213.195879 524.742317 \nL 214.964701 524.726682 \nL 215.849112 525.046684 \nL 217.617933 525.327395 \nL 218.502344 525.762431 \nL 221.155576 526.162152 \nL 222.039987 526.567258 \nL 222.924398 526.66014 \nL 223.808808 526.905987 \nL 224.693219 526.925957 \nL 225.57763 527.268754 \nL 226.46204 527.448556 \nL 228.230862 527.227944 \nL 229.115273 527.772426 \nL 234.421737 528.564235 \nL 235.306148 528.887432 \nL 236.190558 528.731365 \nL 238.843791 529.389234 \nL 239.728201 529.526702 \nL 240.612612 529.306146 \nL 241.497023 529.689934 \nL 245.034666 530.574176 \nL 247.687898 530.595768 \nL 248.572309 530.689895 \nL 249.456719 531.011932 \nL 250.34113 530.938419 \nL 251.225541 531.361243 \nL 254.763184 531.875614 \nL 255.647595 531.902313 \nL 259.185238 532.790957 \nL 260.069648 532.677673 \nL 260.954059 532.819726 \nL 261.83847 532.652232 \nL 266.260523 533.450234 \nL 267.144934 533.291447 \nL 268.029345 533.554447 \nL 269.798166 533.801003 \nL 270.682577 533.721 \nL 271.566988 533.97296 \nL 275.104631 534.248723 \nL 275.989041 534.030103 \nL 276.873452 534.443352 \nL 280.411095 534.96773 \nL 281.295506 534.872662 \nL 282.179917 534.996037 \nL 283.064327 534.915347 \nL 283.948738 535.169312 \nL 284.833149 535.039045 \nL 285.71756 535.323508 \nL 286.60197 535.421084 \nL 287.486381 535.276147 \nL 289.255202 535.78019 \nL 290.139613 535.8751 \nL 291.024024 535.667009 \nL 292.792845 535.877754 \nL 293.677256 536.072092 \nL 294.561667 536.105402 \nL 295.446078 536.292369 \nL 296.330488 536.166971 \nL 298.09931 536.354746 \nL 298.98372 536.020668 \nL 300.752542 536.191648 \nL 301.636953 536.546078 \nL 304.290185 536.770071 \nL 305.174596 536.524905 \nL 306.943417 536.761741 \nL 308.712239 537.103107 \nL 309.596649 536.800516 \nL 310.48106 536.970196 \nL 311.365471 537.001038 \nL 313.134292 537.275426 \nL 315.787524 537.254781 \nL 317.556346 537.494303 \nL 319.325167 537.621629 \nL 320.209578 537.522626 \nL 321.093989 537.707646 \nL 324.631632 537.895964 \nL 325.516042 537.586858 \nL 326.400453 537.635973 \nL 327.284864 537.857538 \nL 329.938096 538.049141 \nL 330.822507 537.763065 \nL 331.706918 538.130832 \nL 333.475739 538.390668 \nL 335.244561 538.457584 \nL 337.013382 538.347057 \nL 337.897793 538.455856 \nL 339.666614 538.244827 \nL 340.551025 538.206279 \nL 342.319846 538.650255 \nL 343.204257 538.728525 \nL 344.088668 538.395787 \nL 345.857489 538.686083 \nL 347.626311 538.781767 \nL 348.510722 538.961384 \nL 349.395132 538.855073 \nL 352.048364 539.013815 \nL 352.932775 539.134163 \nL 353.817186 539.100164 \nL 354.701597 538.357649 \nL 355.586007 538.989348 \nL 356.470418 539.08873 \nL 357.354829 538.985987 \nL 358.23924 539.021572 \nL 359.12365 539.216647 \nL 360.008061 539.049086 \nL 362.661293 539.227012 \nL 363.545704 539.087258 \nL 364.430115 539.438956 \nL 366.198936 539.348721 \nL 367.083347 539.519281 \nL 371.505401 539.37854 \nL 374.158633 539.647326 \nL 375.043043 539.522517 \nL 376.811865 539.81332 \nL 377.696276 539.626035 \nL 381.233919 539.642599 \nL 382.118329 539.835795 \nL 383.887151 539.826135 \nL 385.655972 539.750637 \nL 386.540383 539.966266 \nL 388.309204 539.70248 \nL 389.193615 540.046852 \nL 390.078026 539.849126 \nL 390.962437 539.877901 \nL 391.846847 540.063766 \nL 392.731258 539.834174 \nL 393.615669 540.060375 \nL 394.50008 540.034827 \nL 395.38449 539.8837 \nL 397.153312 540.111267 \nL 398.037723 539.818308 \nL 398.922133 540.124112 \nL 399.806544 539.947704 \nL 402.459776 540.050092 \nL 405.997419 540.176691 \nL 407.766241 539.954529 \nL 408.650651 540.130162 \nL 409.535062 539.969187 \nL 411.303884 540.343284 \nL 412.188294 540.154259 \nL 413.957116 540.120669 \nL 414.841526 540.319704 \nL 415.725937 540.123702 \nL 417.494759 540.344489 \nL 418.379169 539.908965 \nL 419.26358 540.311081 \nL 421.032402 540.328614 \nL 421.916812 540.182086 \nL 422.801223 540.397874 \nL 423.685634 540.270752 \nL 424.570045 540.41424 \nL 425.454455 540.333262 \nL 427.223277 540.549899 \nL 428.107687 540.271298 \nL 428.992098 540.499126 \nL 429.876509 540.462491 \nL 430.76092 540.586978 \nL 432.529741 540.596966 \nL 434.298563 540.67336 \nL 435.182973 540.477398 \nL 437.836205 540.431014 \nL 438.720616 540.109378 \nL 439.605027 540.53196 \nL 441.373848 540.17092 \nL 442.258259 540.53764 \nL 443.14267 540.57211 \nL 444.027081 540.365856 \nL 444.911491 540.521883 \nL 445.795902 540.347404 \nL 447.564724 540.708881 \nL 451.102366 540.505413 \nL 451.986777 540.605995 \nL 453.755599 540.46954 \nL 455.52442 540.628795 \nL 457.293242 540.703713 \nL 458.177652 540.757432 \nL 459.062063 540.566983 \nL 459.946474 540.52842 \nL 460.830885 540.676915 \nL 462.599706 540.706667 \nL 463.484117 540.495313 \nL 464.368527 540.404128 \nL 465.252938 540.723192 \nL 466.137349 540.67445 \nL 467.02176 540.777533 \nL 467.90617 540.732048 \nL 468.790581 540.940801 \nL 470.559403 540.833385 \nL 471.443813 540.782402 \nL 472.328224 540.86553 \nL 473.212635 540.726094 \nL 474.097046 540.831333 \nL 474.981456 540.658438 \nL 475.865867 540.681593 \nL 476.750278 540.878913 \nL 477.634688 540.873764 \nL 478.519099 540.694748 \nL 480.287921 540.824608 \nL 481.172331 540.7046 \nL 482.941153 540.851571 \nL 483.825564 540.663083 \nL 487.363207 541.043031 \nL 488.247617 540.891448 \nL 490.016439 540.92199 \nL 490.900849 540.649112 \nL 492.669671 540.859364 \nL 493.554082 540.74778 \nL 494.438492 540.884802 \nL 495.322903 540.82578 \nL 496.207314 541.0317 \nL 500.629367 541.014396 \nL 501.513778 540.782818 \nL 502.398189 540.92604 \nL 503.2826 540.735044 \nL 505.051421 541.041294 \nL 512.126707 540.970386 \nL 513.011118 541.082389 \nL 513.895528 540.868821 \nL 515.66435 541.049913 \nL 518.317582 540.891418 \nL 519.201993 540.767708 \nL 520.086404 540.964806 \nL 521.855225 540.780833 \nL 522.739636 540.959357 \nL 525.392868 540.84943 \nL 526.277279 540.7097 \nL 528.0461 540.944538 \nL 528.930511 541.025331 \nL 529.814922 540.987834 \nL 530.699332 540.682459 \nL 531.583743 540.922002 \nL 532.468154 541.024262 \nL 533.352565 540.741678 \nL 534.236975 540.946123 \nL 535.121386 541.02851 \nL 536.890208 540.856565 \nL 537.774618 540.868836 \nL 538.659029 541.076587 \nL 539.54344 540.98872 \nL 542.196672 541.115341 \nL 543.081083 540.98512 \nL 543.965493 541.06846 \nL 544.849904 540.856553 \nL 545.734315 541.048775 \nL 547.503136 541.028082 \nL 550.156369 541.119756 \nL 551.040779 540.988556 \nL 553.694011 541.155 \nL 554.578422 541.077762 \nL 555.462833 540.868208 \nL 557.231654 541.11416 \nL 558.116065 540.88084 \nL 559.884887 541.005479 \nL 560.769297 540.858563 \nL 563.42253 541.128277 \nL 564.30694 541.04195 \nL 565.191351 541.116896 \nL 568.728994 541.005776 \nL 570.497815 541.148008 \nL 571.382226 540.791738 \nL 573.151048 541.097289 \nL 574.035458 540.892092 \nL 574.919869 541.125451 \nL 575.80428 541.173416 \nL 576.68869 540.961215 \nL 577.573101 541.167684 \nL 579.341923 540.794155 \nL 580.226333 540.737164 \nL 581.110744 540.836509 \nL 581.995155 540.634703 \nL 582.879566 541.108441 \nL 585.532798 540.959262 \nL 586.417209 541.112894 \nL 591.723673 541.168452 \nL 592.608084 540.977639 \nL 593.492494 540.952304 \nL 594.376905 541.17031 \nL 595.261316 540.866289 \nL 596.145727 541.005245 \nL 597.030137 540.894214 \nL 597.914548 541.174373 \nL 598.798959 541.148206 \nL 599.68337 540.927039 \nL 600.56778 541.020804 \nL 601.452191 540.980238 \nL 602.336602 541.209034 \nL 603.221012 541.169822 \nL 604.105423 540.892037 \nL 605.874245 541.140412 \nL 607.643066 541.062136 \nL 609.411888 541.13894 \nL 610.296298 540.898042 \nL 612.949531 541.021424 \nL 614.718352 540.905869 \nL 615.602763 540.770189 \nL 617.371584 541.041601 \nL 618.255995 540.868791 \nL 620.024816 541.082076 \nL 621.793638 540.974542 \nL 622.678049 541.148251 \nL 624.44687 540.884025 \nL 626.215692 541.09681 \nL 627.100102 540.845921 \nL 628.868924 541.0556 \nL 629.753334 540.760723 \nL 630.637745 541.06033 \nL 635.94421 540.915906 \nL 638.597442 541.098048 \nL 640.366263 540.80676 \nL 641.250674 540.949845 \nL 642.135085 540.891964 \nL 643.019495 541.05805 \nL 646.557138 541.016566 \nL 647.441549 540.660172 \nL 648.32596 540.904017 \nL 650.979192 541.12138 \nL 651.863603 540.959712 \nL 652.748013 541.130834 \nL 653.632424 540.888367 \nL 655.401246 540.871818 \nL 656.285656 541.059792 \nL 658.938889 540.870676 \nL 660.70771 541.070476 \nL 661.592121 541.120439 \nL 662.476532 540.909582 \nL 665.129764 541.078576 \nL 666.014174 540.956785 \nL 667.782996 541.0329 \nL 668.667407 540.907241 \nL 669.551817 541.008536 \nL 670.436228 540.928684 \nL 675.742693 541.167056 \nL 676.627103 541.054643 \nL 678.395925 541.012328 \nL 680.164746 540.617629 \nL 681.049157 540.919965 \nL 684.5868 541.050778 \nL 685.471211 540.923377 \nL 688.124443 540.985842 \nL 690.777675 540.850645 \nL 691.662086 541.008551 \nL 693.430907 541.004019 \nL 694.315318 540.763971 \nL 695.199729 541.081123 \nL 696.96855 541.082835 \nL 697.852961 541.159694 \nL 699.621782 540.873976 \nL 701.390604 541.091639 \nL 704.928247 540.914279 \nL 705.812657 540.793502 \nL 708.46589 541.115129 \nL 709.3503 540.981762 \nL 710.234711 541.014611 \nL 711.119122 540.773404 \nL 712.003533 541.075279 \nL 719.078818 540.649367 \nL 720.84764 540.679022 \nL 721.732051 540.937728 \nL 722.616461 540.955191 \nL 723.500872 540.834178 \nL 725.269694 540.862655 \nL 726.154104 540.869383 \nL 727.038515 541.028795 \nL 727.922926 540.953995 \nL 728.807336 540.67417 \nL 729.691747 540.876903 \nL 731.460569 541.032976 \nL 733.22939 540.657057 \nL 734.113801 540.875394 \nL 734.998212 540.840912 \nL 735.882622 541.014693 \nL 736.767033 540.958695 \nL 737.651444 540.620531 \nL 738.535855 540.544079 \nL 740.304676 540.884295 \nL 742.073497 540.952644 \nL 742.957908 540.682762 \nL 744.72673 540.77378 \nL 745.61114 540.968006 \nL 746.495551 540.805977 \nL 750.033194 540.962994 \nL 750.917605 540.768574 \nL 752.686426 540.962866 \nL 754.455248 540.696597 \nL 756.224069 540.845693 \nL 757.10848 540.904321 \nL 758.877301 540.642697 \nL 762.414944 540.778097 \nL 763.299355 540.652916 \nL 764.183766 540.972026 \nL 765.068177 540.726793 \nL 766.836998 540.692374 \nL 767.721409 540.630055 \nL 768.605819 540.859352 \nL 773.912284 540.798493 \nL 775.681105 540.764256 \nL 776.565516 540.880552 \nL 777.449927 540.786601 \nL 778.334337 540.987964 \nL 779.218748 540.793171 \nL 780.103159 540.892781 \nL 782.756391 540.721407 \nL 784.525213 540.936675 \nL 786.294034 540.717445 \nL 786.294034 540.717445 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Model Loss -->\n    <g transform=\"translate(398.119687 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"385.994141\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"447.175781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"499.275391\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_21\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_22\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p25565917b1\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 822.640625 603.474375\" width=\"822.640625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-06T19:53:08.356427</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 822.640625 603.474375 \nL 822.640625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \nL 815.440625 22.318125 \nL 34.240625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me1ba935bde\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.749716\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(66.568466 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.190789\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(148.647039 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.631862\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(237.088112 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"335.072936\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(325.529186 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"423.514009\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(413.970259 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"511.955082\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(502.411332 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"600.396155\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(590.852405 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"688.837229\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(679.293479 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"777.278302\" xlink:href=\"#me1ba935bde\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(767.734552 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(409.6125 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me2e70e7ec5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me2e70e7ec5\" y=\"487.240323\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2 -->\n      <g transform=\"translate(20.878125 491.039542)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me2e70e7ec5\" y=\"377.463954\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 3 -->\n      <g transform=\"translate(20.878125 381.263173)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me2e70e7ec5\" y=\"267.687585\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 4 -->\n      <g transform=\"translate(20.878125 271.486803)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me2e70e7ec5\" y=\"157.911215\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 5 -->\n      <g transform=\"translate(20.878125 161.710434)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#me2e70e7ec5\" y=\"48.134846\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 6 -->\n      <g transform=\"translate(20.878125 51.934064)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- MAE -->\n     <g transform=\"translate(14.798438 305.011875)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"154.6875\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pd009c681cf)\" d=\"M 69.749716 47.027216 \nL 72.402948 146.164686 \nL 74.17177 219.784377 \nL 76.825002 344.790315 \nL 77.709413 380.22023 \nL 78.593823 410.309259 \nL 79.478234 436.428227 \nL 80.362645 454.599898 \nL 81.247055 469.407667 \nL 82.131466 473.826827 \nL 83.015877 479.7968 \nL 83.900288 483.725588 \nL 84.784698 483.396885 \nL 85.669109 486.593177 \nL 86.55352 488.173748 \nL 87.437931 484.841279 \nL 88.322341 483.983442 \nL 89.206752 484.939715 \nL 90.091163 476.28675 \nL 90.975573 483.713208 \nL 91.859984 482.4197 \nL 92.744395 483.46763 \nL 94.513216 486.719224 \nL 95.397627 480.977609 \nL 96.282038 481.58513 \nL 97.166449 484.581331 \nL 98.050859 490.896025 \nL 98.93527 480.587845 \nL 99.819681 482.751256 \nL 100.704092 483.642751 \nL 101.588502 489.057339 \nL 102.472913 478.613924 \nL 103.357324 482.980817 \nL 104.241734 485.042207 \nL 105.126145 481.251768 \nL 106.010556 487.25002 \nL 106.894967 485.016087 \nL 107.779377 478.194663 \nL 108.663788 486.165279 \nL 109.548199 488.606003 \nL 110.43261 482.601312 \nL 111.31702 483.984201 \nL 112.201431 486.94677 \nL 113.085842 484.309292 \nL 113.970253 486.508691 \nL 114.854663 483.06457 \nL 115.739074 480.569314 \nL 116.623485 484.761845 \nL 117.507895 482.31904 \nL 118.392306 486.993096 \nL 119.276717 487.866074 \nL 120.161128 482.876964 \nL 121.045538 487.352094 \nL 121.929949 482.694985 \nL 122.81436 482.166976 \nL 123.698771 486.206422 \nL 124.583181 484.916054 \nL 125.467592 485.953934 \nL 126.352003 483.213205 \nL 127.236414 481.448901 \nL 128.120824 485.271559 \nL 129.005235 487.769052 \nL 129.889646 486.52272 \nL 130.774056 482.741206 \nL 131.658467 487.190124 \nL 132.542878 488.59751 \nL 133.427289 493.033696 \nL 134.311699 489.322652 \nL 135.19611 484.680893 \nL 136.080521 486.668318 \nL 136.964932 482.417161 \nL 137.849342 488.269945 \nL 138.733753 487.373569 \nL 139.618164 488.982288 \nL 140.502575 485.577465 \nL 141.386985 484.757134 \nL 142.271396 483.310672 \nL 143.155807 485.715422 \nL 144.040217 483.904557 \nL 144.924628 489.878246 \nL 145.809039 489.426309 \nL 146.69345 489.811885 \nL 147.57786 486.03329 \nL 148.462271 488.037008 \nL 149.346682 486.51034 \nL 150.231093 489.055009 \nL 151.999914 487.297759 \nL 152.884325 490.192502 \nL 153.768735 487.06829 \nL 154.653146 492.137921 \nL 155.537557 488.053379 \nL 156.421968 488.214525 \nL 158.190789 491.030278 \nL 159.0752 486.801119 \nL 159.959611 486.835117 \nL 160.844021 487.033297 \nL 161.728432 493.394905 \nL 162.612843 493.572854 \nL 163.497254 491.003687 \nL 164.381664 489.672123 \nL 165.266075 488.977617 \nL 166.150486 491.150673 \nL 167.034896 486.600976 \nL 167.919307 492.294172 \nL 168.803718 484.657965 \nL 169.688129 491.453007 \nL 170.572539 491.445443 \nL 172.341361 491.031953 \nL 173.225772 495.678266 \nL 174.110182 491.698677 \nL 174.994593 491.371191 \nL 176.763415 489.871415 \nL 177.647825 492.571904 \nL 178.532236 491.472767 \nL 179.416647 492.961786 \nL 180.301057 495.054112 \nL 181.185468 495.850391 \nL 182.069879 492.555703 \nL 182.95429 494.674084 \nL 183.8387 496.174017 \nL 184.723111 491.971947 \nL 185.607522 493.654578 \nL 186.491933 496.040628 \nL 187.376343 493.105605 \nL 188.260754 492.832545 \nL 189.145165 499.231057 \nL 190.029576 495.964465 \nL 190.913986 494.058685 \nL 191.798397 495.768798 \nL 192.682808 498.835273 \nL 193.567218 495.5373 \nL 194.451629 495.003664 \nL 195.33604 494.102799 \nL 196.220451 495.510866 \nL 197.104861 494.689539 \nL 197.989272 497.921805 \nL 198.873683 496.974143 \nL 199.758094 501.779927 \nL 200.642504 495.876171 \nL 201.526915 501.208511 \nL 202.411326 492.763959 \nL 203.295737 496.200203 \nL 204.180147 498.054946 \nL 205.064558 498.339627 \nL 205.948969 496.654862 \nL 206.833379 496.865422 \nL 207.71779 501.155001 \nL 208.602201 497.203116 \nL 209.486612 497.330223 \nL 210.371022 501.063121 \nL 212.139844 497.855746 \nL 213.024255 507.180224 \nL 213.908665 500.611289 \nL 214.793076 501.43908 \nL 215.677487 502.499847 \nL 216.561898 502.871552 \nL 217.446308 502.309519 \nL 218.330719 499.841418 \nL 219.21513 501.267766 \nL 220.983951 504.860993 \nL 221.868362 500.603358 \nL 222.752773 505.115144 \nL 223.637183 505.440981 \nL 224.521594 501.940248 \nL 225.406005 503.245312 \nL 226.290416 502.688566 \nL 227.174826 501.340801 \nL 228.059237 504.773158 \nL 228.943648 504.682901 \nL 229.828058 505.263491 \nL 230.712469 506.214699 \nL 231.59688 507.409 \nL 232.481291 509.486355 \nL 233.365701 507.34786 \nL 234.250112 503.882421 \nL 235.134523 506.476361 \nL 236.018934 505.333843 \nL 236.903344 506.184784 \nL 237.787755 507.80905 \nL 238.672166 503.72439 \nL 239.556577 508.676139 \nL 240.440987 510.213591 \nL 241.325398 509.548921 \nL 242.209809 505.824398 \nL 243.094219 506.154724 \nL 243.97863 508.207098 \nL 244.863041 509.419706 \nL 245.747452 507.985049 \nL 246.631862 509.058405 \nL 247.516273 507.629505 \nL 248.400684 507.329526 \nL 249.285095 509.65746 \nL 250.169505 508.268813 \nL 251.053916 509.269187 \nL 251.938327 512.006161 \nL 252.822738 510.92736 \nL 253.707148 514.818014 \nL 255.47597 509.012668 \nL 257.244791 511.699901 \nL 259.013613 510.107617 \nL 259.898023 510.131893 \nL 260.782434 508.493519 \nL 262.551256 513.952208 \nL 263.435666 508.069259 \nL 264.320077 509.615073 \nL 265.204488 508.982164 \nL 266.088899 512.270427 \nL 266.973309 514.156433 \nL 267.85772 510.047616 \nL 268.742131 517.109306 \nL 269.626541 510.807555 \nL 270.510952 509.139985 \nL 271.395363 515.508687 \nL 272.279774 513.153848 \nL 273.164184 510.410724 \nL 274.048595 510.73787 \nL 274.933006 517.082099 \nL 275.817417 512.278802 \nL 276.701827 513.682471 \nL 278.470649 507.354692 \nL 279.35506 511.439194 \nL 281.123881 517.08261 \nL 282.008292 512.11746 \nL 282.892702 510.217857 \nL 283.777113 514.755632 \nL 284.661524 511.579859 \nL 285.545935 513.69277 \nL 286.430345 513.333445 \nL 287.314756 516.240437 \nL 288.199167 510.751689 \nL 289.083578 517.198843 \nL 289.967988 516.637817 \nL 290.852399 513.374484 \nL 292.62122 517.508178 \nL 293.505631 512.79362 \nL 294.390042 512.599732 \nL 295.274453 512.80418 \nL 296.158863 516.740388 \nL 297.043274 516.62532 \nL 297.927685 520.100011 \nL 298.812096 514.887333 \nL 299.696506 516.501339 \nL 300.580917 516.773313 \nL 301.465328 514.39098 \nL 302.349739 517.492448 \nL 303.234149 516.905381 \nL 304.11856 509.775839 \nL 305.002971 516.036276 \nL 306.771792 516.769348 \nL 307.656203 521.083988 \nL 308.540614 511.041696 \nL 309.425024 519.019378 \nL 310.309435 518.10612 \nL 311.193846 517.642091 \nL 312.078257 515.559435 \nL 312.962667 521.889676 \nL 313.847078 511.132934 \nL 314.731489 516.940636 \nL 315.6159 519.104139 \nL 316.50031 511.862093 \nL 317.384721 518.041879 \nL 318.269132 517.375796 \nL 319.153542 515.441553 \nL 320.037953 519.065743 \nL 320.922364 516.996213 \nL 321.806775 519.16906 \nL 322.691185 515.107747 \nL 323.575596 518.616789 \nL 324.460007 511.568657 \nL 325.344418 514.11932 \nL 326.228828 515.498649 \nL 327.113239 516.346829 \nL 327.99765 518.867184 \nL 328.882061 516.825502 \nL 329.766471 517.454262 \nL 330.650882 517.633205 \nL 331.535293 515.834825 \nL 332.419703 522.028548 \nL 333.304114 520.624198 \nL 334.188525 518.060436 \nL 335.072936 516.073141 \nL 335.957346 515.410473 \nL 336.841757 518.205328 \nL 337.726168 517.583268 \nL 338.610579 522.357382 \nL 340.3794 522.345984 \nL 341.263811 521.417454 \nL 342.148222 519.565184 \nL 343.032632 520.659832 \nL 343.917043 517.332219 \nL 344.801454 519.275845 \nL 345.685864 514.500579 \nL 346.570275 521.298303 \nL 347.454686 517.468526 \nL 348.339097 517.925123 \nL 349.223507 521.558342 \nL 350.107918 522.587283 \nL 350.992329 520.407867 \nL 351.87674 517.057693 \nL 352.76115 515.975242 \nL 353.645561 516.848364 \nL 354.529972 518.792775 \nL 355.414383 517.491702 \nL 356.298793 521.672206 \nL 357.183204 517.37492 \nL 358.067615 516.595888 \nL 358.952025 519.827252 \nL 360.720847 520.93848 \nL 361.605258 518.664607 \nL 362.489668 520.437796 \nL 363.374079 516.79946 \nL 364.25849 519.072012 \nL 365.142901 517.530438 \nL 366.027311 518.301814 \nL 366.911722 522.338132 \nL 367.796133 517.756727 \nL 368.680543 517.832969 \nL 369.564954 516.045882 \nL 370.449365 521.258442 \nL 371.333776 523.074371 \nL 372.218186 519.186465 \nL 373.102597 519.234282 \nL 373.987008 517.870016 \nL 374.871419 521.38322 \nL 375.755829 518.301172 \nL 376.64024 522.570297 \nL 377.524651 524.742346 \nL 378.409062 518.931464 \nL 379.293472 521.674667 \nL 380.177883 519.174295 \nL 381.062294 519.783962 \nL 381.946704 519.822541 \nL 382.831115 517.858762 \nL 383.715526 521.180198 \nL 384.599937 522.114827 \nL 385.484347 518.075119 \nL 386.368758 516.582645 \nL 388.13758 523.334502 \nL 389.02199 519.58747 \nL 389.906401 518.524975 \nL 390.790812 516.521924 \nL 391.675223 519.496167 \nL 392.559633 520.611583 \nL 393.444044 520.379169 \nL 394.328455 520.548781 \nL 395.212865 520.138877 \nL 396.097276 520.581864 \nL 396.981687 518.455696 \nL 397.866098 520.673246 \nL 398.750508 518.721088 \nL 399.634919 518.909505 \nL 400.51933 522.382246 \nL 401.403741 519.553851 \nL 402.288151 518.401676 \nL 403.172562 519.201854 \nL 404.056973 522.407804 \nL 404.941384 520.723301 \nL 405.825794 516.839125 \nL 406.710205 523.459921 \nL 407.594616 515.311043 \nL 408.479026 521.112921 \nL 409.363437 524.835926 \nL 410.247848 522.37179 \nL 411.132259 516.167022 \nL 412.016669 520.476702 \nL 412.90108 517.98744 \nL 413.785491 526.541773 \nL 414.669902 522.459011 \nL 415.554312 524.838504 \nL 416.438723 522.062246 \nL 417.323134 523.695973 \nL 418.207545 521.091041 \nL 419.091955 520.16437 \nL 419.976366 520.998036 \nL 420.860777 520.460488 \nL 421.745187 525.426082 \nL 422.629598 519.307749 \nL 423.514009 523.711324 \nL 424.39842 522.940615 \nL 425.28283 516.67493 \nL 426.167241 521.101863 \nL 427.051652 521.529565 \nL 427.936063 520.384731 \nL 428.820473 514.551275 \nL 429.704884 521.401646 \nL 430.589295 520.683479 \nL 431.473705 520.69281 \nL 432.358116 520.082082 \nL 433.242527 523.551212 \nL 434.126938 522.148498 \nL 435.011348 521.234311 \nL 435.895759 527.024149 \nL 436.78017 517.675644 \nL 437.664581 519.90471 \nL 438.548991 523.392527 \nL 439.433402 519.843636 \nL 440.317813 518.744002 \nL 441.202224 520.178712 \nL 442.086634 518.52987 \nL 442.971045 517.904656 \nL 443.855456 523.909098 \nL 445.624277 520.31324 \nL 446.508688 520.675915 \nL 447.393099 522.834184 \nL 448.277509 520.718917 \nL 449.16192 521.271005 \nL 450.046331 523.261153 \nL 450.930742 519.434255 \nL 451.815152 517.279245 \nL 452.699563 522.407045 \nL 453.583974 518.636811 \nL 454.468385 524.512379 \nL 455.352795 523.846466 \nL 456.237206 520.954249 \nL 457.121617 529.001774 \nL 458.006027 519.02549 \nL 458.890438 519.277193 \nL 459.774849 523.043422 \nL 460.65926 522.467936 \nL 461.54367 522.907468 \nL 462.428081 520.916443 \nL 463.312492 518.327319 \nL 464.196903 522.354032 \nL 465.081313 520.287682 \nL 465.965724 523.53912 \nL 466.850135 526.35206 \nL 467.734546 520.915789 \nL 468.618956 522.091795 \nL 469.503367 520.218403 \nL 470.387778 523.579465 \nL 471.272188 522.054014 \nL 472.156599 519.756324 \nL 473.04101 523.168213 \nL 473.925421 519.7039 \nL 474.809831 522.807161 \nL 475.694242 521.024667 \nL 476.578653 517.74194 \nL 477.463064 522.547723 \nL 478.347474 524.655112 \nL 479.231885 520.455489 \nL 480.116296 520.715345 \nL 481.000707 520.596481 \nL 481.885117 525.662068 \nL 482.769528 517.834186 \nL 483.653939 520.118777 \nL 484.538349 523.322436 \nL 485.42276 522.94653 \nL 486.307171 523.648614 \nL 487.191582 520.346977 \nL 488.075992 523.320395 \nL 488.960403 520.729125 \nL 489.844814 520.207803 \nL 490.729225 526.308378 \nL 491.613635 525.802263 \nL 492.498046 520.729661 \nL 493.382457 520.319535 \nL 494.266867 522.376423 \nL 495.151278 520.858509 \nL 496.035689 525.076598 \nL 496.9201 521.167701 \nL 497.80451 524.467728 \nL 499.573332 523.071531 \nL 500.457743 517.079312 \nL 501.342153 520.863836 \nL 502.226564 520.05146 \nL 503.110975 521.797639 \nL 503.995386 521.976203 \nL 504.879796 522.622329 \nL 505.764207 520.352146 \nL 506.648618 522.83192 \nL 507.533028 524.214808 \nL 508.417439 523.4957 \nL 509.30185 520.078314 \nL 510.186261 524.734952 \nL 511.070671 516.480925 \nL 511.955082 522.868143 \nL 512.839493 524.40689 \nL 513.723904 522.20066 \nL 514.608314 519.493785 \nL 515.492725 526.225672 \nL 516.377136 515.905151 \nL 517.261547 524.453307 \nL 518.145957 523.230792 \nL 519.030368 526.387354 \nL 519.914779 524.405568 \nL 520.799189 524.025828 \nL 521.6836 519.759831 \nL 522.568011 525.875769 \nL 523.452422 522.075306 \nL 524.336832 520.473443 \nL 526.105654 524.315992 \nL 526.990065 524.114959 \nL 527.874475 519.750697 \nL 528.758886 520.663981 \nL 529.643297 523.136204 \nL 530.527708 519.121007 \nL 531.412118 523.452711 \nL 532.296529 523.928387 \nL 533.18094 525.655512 \nL 534.06535 523.247111 \nL 534.949761 522.205816 \nL 535.834172 519.54388 \nL 536.718583 523.058576 \nL 537.602993 524.468658 \nL 538.487404 521.171117 \nL 539.371815 519.994234 \nL 540.256226 519.113482 \nL 541.140636 524.676665 \nL 542.025047 520.831866 \nL 542.909458 519.944937 \nL 543.793869 523.114808 \nL 544.678279 521.304061 \nL 545.56269 522.229882 \nL 546.447101 519.421588 \nL 547.331511 521.239663 \nL 548.215922 522.342621 \nL 549.100333 518.481097 \nL 549.984744 518.854398 \nL 550.869154 526.184031 \nL 551.753565 522.613338 \nL 552.637976 520.93763 \nL 553.522387 520.737513 \nL 554.406797 526.416209 \nL 555.291208 520.098153 \nL 556.175619 522.962993 \nL 557.06003 521.010468 \nL 557.94444 522.308256 \nL 558.828851 525.490297 \nL 559.713262 524.360433 \nL 560.597672 524.73312 \nL 561.482083 523.334135 \nL 562.366494 528.070169 \nL 563.250905 526.218998 \nL 564.135315 520.999463 \nL 565.019726 525.921034 \nL 565.904137 522.277974 \nL 566.788548 521.146292 \nL 567.672958 523.082877 \nL 568.557369 522.168481 \nL 569.44178 523.993047 \nL 570.32619 521.60947 \nL 571.210601 522.242471 \nL 572.095012 523.705108 \nL 572.979423 522.901369 \nL 573.863833 523.033712 \nL 574.748244 516.626668 \nL 575.632655 521.26505 \nL 576.517066 524.444173 \nL 577.401476 523.298292 \nL 578.285887 523.380173 \nL 579.170298 522.304958 \nL 580.054709 520.910842 \nL 580.939119 521.101576 \nL 581.82353 517.549073 \nL 582.707941 522.609897 \nL 583.592351 517.805553 \nL 584.476762 518.687011 \nL 585.361173 524.009981 \nL 586.245584 521.23999 \nL 587.129994 522.179918 \nL 588.014405 523.444218 \nL 588.898816 522.2141 \nL 589.783227 526.43012 \nL 590.667637 518.184377 \nL 591.552048 524.886675 \nL 592.436459 523.187136 \nL 593.32087 525.621095 \nL 595.089691 519.745671 \nL 595.974102 525.421214 \nL 596.858512 521.285858 \nL 597.742923 527.063081 \nL 598.627334 524.298195 \nL 599.511745 521.99968 \nL 600.396155 520.392151 \nL 601.280566 524.876939 \nL 602.164977 521.826534 \nL 603.049388 523.038527 \nL 603.933798 522.389156 \nL 604.818209 522.92465 \nL 605.70262 521.629846 \nL 606.587031 524.08418 \nL 607.471441 524.119068 \nL 608.355852 520.162891 \nL 609.240263 522.229908 \nL 610.124673 519.256726 \nL 611.009084 521.597496 \nL 611.893495 520.975423 \nL 612.777906 522.902874 \nL 613.662316 523.921464 \nL 615.431138 521.623996 \nL 616.315549 522.670958 \nL 617.199959 522.128868 \nL 618.08437 522.851641 \nL 618.968781 523.141635 \nL 619.853192 522.474859 \nL 620.737602 525.809735 \nL 621.622013 520.40122 \nL 622.506424 523.614105 \nL 623.390834 520.242338 \nL 625.159656 519.125705 \nL 626.044067 525.228714 \nL 626.928477 521.622701 \nL 627.812888 519.9522 \nL 629.58171 522.558389 \nL 630.46612 522.278903 \nL 631.350531 524.07341 \nL 632.234942 522.51311 \nL 633.119352 518.7911 \nL 634.003763 523.473008 \nL 634.888174 521.023031 \nL 635.772585 521.560632 \nL 636.656995 521.001962 \nL 637.541406 524.681246 \nL 638.425817 519.516922 \nL 639.310228 519.563156 \nL 640.194638 525.687312 \nL 641.079049 524.504972 \nL 641.96346 523.076216 \nL 642.847871 526.854236 \nL 643.732281 528.607586 \nL 644.616692 521.05313 \nL 645.501103 521.590626 \nL 646.385513 522.515636 \nL 647.269924 523.20899 \nL 648.154335 523.556433 \nL 649.038746 522.011287 \nL 649.923156 522.542607 \nL 650.807567 518.28259 \nL 651.691978 521.608188 \nL 652.576389 521.751981 \nL 653.460799 520.005292 \nL 654.34521 527.281375 \nL 655.229621 525.596349 \nL 656.114032 524.583726 \nL 656.998442 522.16127 \nL 657.882853 523.968248 \nL 658.767264 523.831325 \nL 659.651674 527.726416 \nL 660.536085 521.63987 \nL 661.420496 524.85163 \nL 662.304907 519.054096 \nL 663.189317 523.223987 \nL 664.073728 520.93657 \nL 664.958139 521.068127 \nL 665.84255 522.219635 \nL 666.72696 522.838241 \nL 667.611371 522.027161 \nL 668.495782 523.343924 \nL 669.380193 525.161764 \nL 671.149014 522.401012 \nL 672.033425 526.739443 \nL 672.917835 523.112047 \nL 673.802246 522.088471 \nL 674.686657 520.770098 \nL 675.571068 521.071215 \nL 676.455478 523.220794 \nL 677.339889 524.377747 \nL 678.2243 521.021487 \nL 679.108711 521.169978 \nL 679.993121 523.540128 \nL 680.877532 520.478953 \nL 681.761943 524.070047 \nL 682.646354 520.304812 \nL 683.530764 523.766509 \nL 684.415175 523.715694 \nL 685.299586 524.012506 \nL 686.183996 525.819314 \nL 687.068407 528.204121 \nL 688.837229 520.995275 \nL 689.721639 522.345474 \nL 690.60605 521.171797 \nL 691.490461 521.975771 \nL 692.374872 526.658883 \nL 693.259282 521.158161 \nL 694.143693 518.42527 \nL 695.028104 527.368007 \nL 695.912515 524.56657 \nL 696.796925 525.27746 \nL 697.681336 526.154953 \nL 698.565747 522.967927 \nL 699.450157 524.039713 \nL 700.334568 521.629506 \nL 701.218979 522.624593 \nL 702.10339 521.57737 \nL 702.9878 527.891199 \nL 703.872211 519.214823 \nL 704.756622 525.164813 \nL 705.641033 528.503851 \nL 706.525443 526.875187 \nL 707.409854 525.495505 \nL 708.294265 528.02116 \nL 709.178675 526.637971 \nL 710.063086 523.054663 \nL 710.947497 527.295665 \nL 711.831908 520.496397 \nL 712.716318 521.997834 \nL 713.600729 525.257988 \nL 714.48514 524.603238 \nL 715.369551 526.292177 \nL 716.253961 521.917851 \nL 717.138372 523.22472 \nL 718.022783 523.338742 \nL 718.907194 518.124768 \nL 719.791604 520.550574 \nL 720.676015 521.24194 \nL 721.560426 526.56419 \nL 722.444836 524.438206 \nL 723.329247 523.197383 \nL 724.213658 519.698495 \nL 725.098069 522.64735 \nL 725.982479 523.24329 \nL 726.86689 521.602862 \nL 727.751301 521.716242 \nL 728.635712 525.388668 \nL 729.520122 521.536553 \nL 730.404533 524.513923 \nL 731.288944 524.145516 \nL 732.173355 525.719413 \nL 733.057765 522.882512 \nL 733.942176 519.432554 \nL 734.826587 523.296276 \nL 735.710997 520.994123 \nL 736.595408 522.136327 \nL 737.479819 522.507391 \nL 738.36423 523.983363 \nL 739.24864 523.864355 \nL 740.133051 523.418411 \nL 741.017462 521.714266 \nL 741.901873 526.836569 \nL 742.786283 525.808871 \nL 743.670694 525.811672 \nL 744.555105 524.302709 \nL 745.439516 520.854387 \nL 746.323926 525.137174 \nL 747.208337 520.382454 \nL 748.092748 523.581297 \nL 748.977158 524.401969 \nL 749.861569 525.766471 \nL 750.74598 524.715427 \nL 751.630391 521.698013 \nL 752.514801 520.169931 \nL 753.399212 526.572199 \nL 754.283623 525.154121 \nL 755.168034 523.514649 \nL 756.052444 520.648081 \nL 757.821266 524.168666 \nL 758.705677 527.286138 \nL 759.590087 520.112639 \nL 760.474498 527.779965 \nL 761.358909 527.538614 \nL 762.243319 523.76944 \nL 763.12773 524.384251 \nL 764.012141 525.521586 \nL 764.896552 521.896467 \nL 765.780962 521.404211 \nL 766.665373 523.993649 \nL 767.549784 525.704887 \nL 768.434195 524.87635 \nL 769.318605 525.311053 \nL 770.203016 522.017987 \nL 771.087427 524.599914 \nL 771.971837 523.588822 \nL 772.856248 523.244611 \nL 773.740659 523.826654 \nL 774.62507 523.789501 \nL 775.50948 529.534114 \nL 776.393891 520.852581 \nL 777.278302 526.510706 \nL 778.162713 521.19814 \nL 779.047123 523.778653 \nL 779.931534 523.376574 \nL 779.931534 523.376574 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pd009c681cf)\" d=\"M 69.749716 101.385246 \nL 72.402948 202.300525 \nL 75.05618 319.854774 \nL 76.825002 397.39069 \nL 77.709413 427.001256 \nL 78.593823 450.164589 \nL 79.478234 466.204545 \nL 80.362645 477.439946 \nL 81.247055 485.404228 \nL 82.131466 490.432768 \nL 83.015877 492.654793 \nL 83.900288 493.16609 \nL 84.784698 493.111481 \nL 87.437931 490.92789 \nL 88.322341 490.442478 \nL 89.206752 489.339481 \nL 90.091163 488.898313 \nL 90.975573 488.783297 \nL 94.513216 487.381617 \nL 95.397627 487.451851 \nL 96.282038 487.391 \nL 97.166449 487.70714 \nL 98.93527 487.434198 \nL 100.704092 487.791377 \nL 101.588502 487.72533 \nL 102.472913 487.840922 \nL 103.357324 487.510727 \nL 104.241734 487.404557 \nL 105.126145 487.552551 \nL 106.010556 488.419823 \nL 106.894967 487.565951 \nL 107.779377 487.345198 \nL 108.663788 487.754343 \nL 109.548199 487.694433 \nL 111.31702 488.550229 \nL 112.201431 488.68956 \nL 113.085842 488.643522 \nL 113.970253 488.966336 \nL 114.854663 488.966428 \nL 115.739074 489.22152 \nL 116.623485 489.079232 \nL 117.507895 489.499579 \nL 118.392306 489.048859 \nL 121.045538 488.846151 \nL 121.929949 488.911557 \nL 122.81436 488.709804 \nL 123.698771 489.143015 \nL 124.583181 489.202309 \nL 125.467592 488.852681 \nL 126.352003 489.975229 \nL 127.236414 490.256429 \nL 128.120824 490.981571 \nL 129.005235 490.979699 \nL 129.889646 490.79369 \nL 130.774056 490.059911 \nL 131.658467 490.334345 \nL 132.542878 490.388771 \nL 133.427289 489.605867 \nL 134.311699 490.304652 \nL 135.19611 489.822224 \nL 136.080521 489.538132 \nL 136.964932 490.843863 \nL 137.849342 491.0835 \nL 138.733753 491.090554 \nL 139.618164 490.918979 \nL 140.502575 491.078344 \nL 141.386985 491.11445 \nL 142.271396 491.770678 \nL 143.155807 491.825785 \nL 144.040217 492.089056 \nL 144.924628 492.697769 \nL 145.809039 492.647766 \nL 146.69345 493.50538 \nL 147.57786 492.3358 \nL 148.462271 491.705639 \nL 149.346682 492.337619 \nL 150.231093 492.69345 \nL 151.115503 493.41074 \nL 151.999914 493.468555 \nL 152.884325 493.735949 \nL 153.768735 493.237843 \nL 154.653146 493.636846 \nL 155.537557 493.37894 \nL 156.421968 493.783976 \nL 157.306378 494.375624 \nL 158.190789 493.502344 \nL 159.0752 494.008852 \nL 159.959611 493.92646 \nL 160.844021 494.82111 \nL 161.728432 494.556346 \nL 162.612843 494.104487 \nL 163.497254 494.243844 \nL 164.381664 495.477116 \nL 165.266075 496.44658 \nL 166.150486 494.528106 \nL 167.034896 494.783656 \nL 167.919307 494.140108 \nL 168.803718 495.961521 \nL 169.688129 496.228561 \nL 170.572539 497.237938 \nL 171.45695 498.053389 \nL 173.225772 496.058752 \nL 174.110182 496.404913 \nL 174.994593 496.496334 \nL 175.879004 496.475645 \nL 176.763415 498.349965 \nL 177.647825 499.386183 \nL 180.301057 497.585722 \nL 181.185468 498.829711 \nL 182.069879 499.163858 \nL 182.95429 499.21746 \nL 183.8387 500.068427 \nL 184.723111 500.053954 \nL 185.607522 499.684041 \nL 186.491933 501.448672 \nL 187.376343 501.103689 \nL 188.260754 501.63743 \nL 189.145165 501.728354 \nL 190.029576 501.982308 \nL 190.913986 501.504852 \nL 191.798397 502.01276 \nL 192.682808 503.041269 \nL 193.567218 503.109999 \nL 194.451629 503.834434 \nL 195.33604 504.290585 \nL 196.220451 504.051654 \nL 197.104861 503.982833 \nL 200.642504 505.065363 \nL 201.526915 505.399393 \nL 203.295737 505.546339 \nL 204.180147 506.403732 \nL 205.064558 506.891212 \nL 205.948969 507.23694 \nL 206.833379 507.260286 \nL 208.602201 506.718681 \nL 209.486612 507.382029 \nL 211.255433 507.77691 \nL 212.139844 508.842415 \nL 213.908665 509.434481 \nL 214.793076 509.313066 \nL 215.677487 510.304305 \nL 216.561898 510.3583 \nL 217.446308 510.911473 \nL 218.330719 510.71527 \nL 219.21513 511.596505 \nL 220.09954 511.930731 \nL 220.983951 511.262868 \nL 221.868362 510.807123 \nL 222.752773 512.233209 \nL 223.637183 512.438992 \nL 224.521594 512.992977 \nL 225.406005 513.086689 \nL 226.290416 512.908465 \nL 227.174826 513.053816 \nL 228.059237 513.441866 \nL 228.943648 514.317775 \nL 229.828058 513.586692 \nL 230.712469 514.127382 \nL 231.59688 514.459396 \nL 232.481291 515.001027 \nL 233.365701 515.242366 \nL 234.250112 514.406985 \nL 235.134523 515.38239 \nL 236.903344 516.381338 \nL 237.787755 516.722421 \nL 238.672166 517.475305 \nL 240.440987 516.853546 \nL 242.209809 517.033745 \nL 243.094219 517.883691 \nL 243.97863 517.434358 \nL 244.863041 518.575057 \nL 246.631862 519.171272 \nL 247.516273 519.241702 \nL 248.400684 519.467874 \nL 249.285095 519.391358 \nL 251.053916 520.257204 \nL 252.822738 521.80201 \nL 253.707148 521.148268 \nL 254.591559 521.432739 \nL 255.47597 520.730996 \nL 256.36038 521.135194 \nL 257.244791 521.843585 \nL 258.129202 522.329849 \nL 259.898023 522.568308 \nL 260.782434 521.958196 \nL 261.666845 522.6295 \nL 263.435666 523.137892 \nL 264.320077 522.795069 \nL 265.204488 523.424117 \nL 266.088899 523.55256 \nL 266.973309 523.415493 \nL 267.85772 523.735402 \nL 268.742131 523.838143 \nL 269.626541 523.22756 \nL 270.510952 524.227921 \nL 271.395363 524.510639 \nL 273.164184 524.878326 \nL 274.048595 525.51134 \nL 274.933006 525.101579 \nL 275.817417 525.38715 \nL 276.701827 525.083651 \nL 277.586238 525.746698 \nL 278.470649 525.330826 \nL 279.35506 526.047161 \nL 280.23947 526.250026 \nL 281.123881 525.794358 \nL 282.892702 527.116029 \nL 283.777113 527.333537 \nL 284.661524 526.632998 \nL 285.545935 526.718635 \nL 286.430345 527.085263 \nL 287.314756 527.648801 \nL 288.199167 527.659296 \nL 289.083578 528.236051 \nL 289.967988 527.682472 \nL 290.852399 527.758006 \nL 291.73681 528.09604 \nL 292.62122 527.159659 \nL 294.390042 527.517662 \nL 295.274453 528.461569 \nL 296.158863 528.514843 \nL 297.043274 528.880895 \nL 297.927685 528.977197 \nL 298.812096 528.225779 \nL 299.696506 528.387722 \nL 301.465328 529.122338 \nL 302.349739 529.737175 \nL 303.234149 528.829753 \nL 304.11856 529.245651 \nL 305.002971 529.304147 \nL 305.887381 529.78655 \nL 306.771792 530.041498 \nL 309.425024 529.896593 \nL 310.309435 530.286174 \nL 312.962667 530.810833 \nL 313.847078 530.525995 \nL 314.731489 531.009732 \nL 318.269132 531.424675 \nL 319.153542 530.606594 \nL 320.037953 530.724515 \nL 320.922364 531.280882 \nL 322.691185 531.711802 \nL 323.575596 531.755943 \nL 324.460007 531.053061 \nL 325.344418 531.966764 \nL 327.113239 532.657515 \nL 327.99765 532.813229 \nL 328.882061 532.827821 \nL 330.650882 532.502768 \nL 331.535293 532.770306 \nL 333.304114 532.25719 \nL 334.188525 532.167483 \nL 335.072936 532.787004 \nL 335.957346 533.250117 \nL 336.841757 533.491522 \nL 337.726168 532.620598 \nL 338.610579 533.099768 \nL 339.494989 533.32772 \nL 340.3794 533.349809 \nL 341.263811 533.562738 \nL 342.148222 534.085276 \nL 343.032632 533.734941 \nL 344.801454 533.962369 \nL 345.685864 534.166189 \nL 346.570275 534.571866 \nL 347.454686 534.411336 \nL 348.339097 532.610299 \nL 349.223507 534.048045 \nL 350.107918 534.326209 \nL 350.992329 534.028494 \nL 351.87674 534.132505 \nL 352.76115 534.686568 \nL 353.645561 534.190883 \nL 356.298793 534.628622 \nL 357.183204 534.300599 \nL 358.067615 535.263703 \nL 358.952025 535.04257 \nL 359.836436 534.938939 \nL 360.720847 535.453678 \nL 361.605258 535.298788 \nL 362.489668 535.023451 \nL 363.374079 534.943283 \nL 364.25849 535.046208 \nL 365.142901 535.011319 \nL 366.027311 535.2335 \nL 366.911722 535.652591 \nL 367.796133 535.699008 \nL 368.680543 535.384922 \nL 369.564954 535.656477 \nL 370.449365 536.30861 \nL 371.333776 535.645223 \nL 372.218186 535.662274 \nL 373.102597 535.539119 \nL 373.987008 535.917943 \nL 374.871419 535.694846 \nL 375.755829 536.215265 \nL 376.64024 536.322913 \nL 378.409062 535.964556 \nL 379.293472 535.982485 \nL 380.177883 536.596876 \nL 381.062294 536.142636 \nL 381.946704 535.908926 \nL 382.831115 536.855698 \nL 383.715526 536.227435 \nL 384.599937 536.312483 \nL 385.484347 536.878586 \nL 386.368758 536.237407 \nL 387.253169 536.84167 \nL 388.13758 536.753258 \nL 389.02199 536.377418 \nL 390.790812 537.016687 \nL 391.675223 536.287868 \nL 392.559633 537.047309 \nL 393.444044 536.578883 \nL 394.328455 536.743378 \nL 395.212865 537.250121 \nL 396.097276 536.818886 \nL 398.750508 537.153884 \nL 399.634919 537.185593 \nL 400.51933 537.028622 \nL 401.403741 536.693035 \nL 402.288151 537.083009 \nL 403.172562 536.731797 \nL 404.056973 537.12427 \nL 404.941384 537.726426 \nL 405.825794 537.160532 \nL 407.594616 537.131088 \nL 408.479026 537.62676 \nL 409.363437 537.14882 \nL 410.247848 537.278466 \nL 411.132259 537.701012 \nL 412.016669 536.762157 \nL 412.90108 537.627087 \nL 414.669902 537.694181 \nL 415.554312 537.352797 \nL 416.438723 537.881172 \nL 417.323134 537.567701 \nL 418.207545 537.921203 \nL 419.091955 537.717265 \nL 419.976366 538.102868 \nL 420.860777 538.358903 \nL 421.745187 537.62129 \nL 422.629598 538.192758 \nL 423.514009 538.080189 \nL 424.39842 538.481025 \nL 425.28283 538.320952 \nL 427.936063 538.865868 \nL 428.820473 538.166638 \nL 430.589295 538.177526 \nL 431.473705 538.052551 \nL 432.358116 537.374887 \nL 433.242527 538.307395 \nL 434.126938 537.988873 \nL 435.011348 537.52309 \nL 435.895759 538.333921 \nL 436.78017 538.414049 \nL 437.664581 537.919881 \nL 438.548991 538.296442 \nL 439.433402 537.912854 \nL 440.317813 538.306021 \nL 441.202224 538.857493 \nL 443.855456 538.056241 \nL 445.624277 538.55457 \nL 446.508688 538.28308 \nL 447.393099 538.234321 \nL 448.277509 538.506857 \nL 451.815152 539.023023 \nL 452.699563 538.521501 \nL 453.583974 538.441805 \nL 454.468385 538.801719 \nL 456.237206 538.900704 \nL 457.121617 538.419872 \nL 458.006027 538.232161 \nL 458.890438 538.960195 \nL 459.774849 538.837 \nL 460.65926 539.12647 \nL 461.54367 539.002595 \nL 462.428081 539.698606 \nL 463.312492 539.347185 \nL 465.081313 539.165049 \nL 465.965724 539.410601 \nL 466.850135 539.030128 \nL 467.734546 539.326391 \nL 468.618956 538.914759 \nL 469.503367 538.995528 \nL 470.387778 539.49332 \nL 471.272188 539.474869 \nL 472.156599 539.060214 \nL 473.925421 539.372978 \nL 474.809831 539.090208 \nL 475.694242 539.189363 \nL 476.578653 539.462777 \nL 477.463064 539.0161 \nL 479.231885 539.453171 \nL 480.116296 539.614107 \nL 481.000707 540.125981 \nL 481.885117 539.624262 \nL 482.769528 539.559223 \nL 483.653939 539.694392 \nL 484.538349 539.017265 \nL 486.307171 539.540196 \nL 487.191582 539.264322 \nL 488.075992 539.610718 \nL 488.960403 539.483937 \nL 489.844814 540.058533 \nL 491.613635 539.951212 \nL 492.498046 539.696604 \nL 494.266867 539.997747 \nL 495.151278 539.410614 \nL 496.035689 539.76218 \nL 496.9201 539.307376 \nL 497.80451 539.654544 \nL 498.688921 540.143765 \nL 499.573332 540.115394 \nL 500.457743 539.89481 \nL 501.342153 539.998153 \nL 502.226564 539.91686 \nL 503.110975 540.23991 \nL 503.995386 539.949799 \nL 504.879796 540.034965 \nL 505.764207 539.935417 \nL 506.648618 540.329513 \nL 507.533028 539.685677 \nL 509.30185 540.207953 \nL 510.186261 540.097007 \nL 511.070671 540.104584 \nL 512.839493 539.479894 \nL 513.723904 539.972334 \nL 514.608314 539.810939 \nL 515.492725 539.535249 \nL 516.377136 539.983012 \nL 517.261547 539.939317 \nL 518.145957 540.151381 \nL 519.030368 539.695531 \nL 519.914779 539.369026 \nL 520.799189 539.529831 \nL 521.6836 539.945415 \nL 522.568011 540.167543 \nL 523.452422 540.072117 \nL 524.336832 539.358282 \nL 525.221243 539.911508 \nL 526.105654 540.186727 \nL 526.990065 539.507885 \nL 527.874475 540.008674 \nL 528.758886 540.221707 \nL 529.643297 539.931988 \nL 530.527708 539.788431 \nL 531.412118 539.822586 \nL 532.296529 540.388231 \nL 533.18094 540.141422 \nL 534.06535 540.297477 \nL 534.949761 540.183534 \nL 535.834172 540.475125 \nL 536.718583 540.092205 \nL 537.602993 540.327798 \nL 538.487404 539.778145 \nL 539.371815 540.272181 \nL 540.256226 540.171351 \nL 542.025047 540.292203 \nL 542.909458 540.467037 \nL 543.793869 540.477075 \nL 544.678279 540.094887 \nL 545.56269 540.262838 \nL 546.447101 540.174845 \nL 547.331511 540.580993 \nL 548.215922 540.33586 \nL 549.100333 539.801085 \nL 550.869154 540.481328 \nL 551.753565 539.849125 \nL 552.637976 540.095764 \nL 553.522387 540.191334 \nL 554.406797 539.81849 \nL 555.291208 540.060012 \nL 556.175619 540.536186 \nL 557.06003 540.563248 \nL 557.94444 540.292086 \nL 558.828851 540.537403 \nL 561.482083 540.276605 \nL 562.366494 540.220111 \nL 563.250905 540.317905 \nL 564.135315 540.660244 \nL 565.019726 539.724818 \nL 566.788548 540.511492 \nL 567.672958 539.969952 \nL 568.557369 540.600047 \nL 569.44178 540.773297 \nL 570.32619 540.169977 \nL 571.210601 540.7753 \nL 572.095012 540.108837 \nL 572.979423 539.780343 \nL 573.863833 539.659373 \nL 574.748244 539.904546 \nL 575.632655 539.460251 \nL 576.517066 540.636938 \nL 577.401476 540.448271 \nL 578.285887 540.077888 \nL 579.170298 540.204525 \nL 580.054709 540.65166 \nL 580.939119 540.648885 \nL 581.82353 540.375655 \nL 583.592351 540.69279 \nL 584.476762 540.893195 \nL 585.361173 540.854839 \nL 586.245584 540.244242 \nL 587.129994 540.184908 \nL 588.014405 540.871641 \nL 588.898816 539.951107 \nL 589.783227 540.319776 \nL 590.667637 540.013307 \nL 591.552048 540.857076 \nL 592.436459 540.755526 \nL 593.32087 540.093539 \nL 594.20528 540.351249 \nL 595.089691 540.237162 \nL 595.974102 540.983294 \nL 596.858512 540.814048 \nL 597.742923 540.009564 \nL 599.511745 540.729694 \nL 600.396155 540.4556 \nL 601.280566 540.491326 \nL 602.164977 540.749546 \nL 603.049388 540.744115 \nL 603.933798 540.071423 \nL 604.818209 540.266737 \nL 605.70262 540.221145 \nL 606.587031 540.429702 \nL 607.471441 540.394814 \nL 609.240263 539.810351 \nL 610.124673 540.255889 \nL 611.009084 540.51496 \nL 611.893495 540.048038 \nL 612.777906 540.470636 \nL 613.662316 540.657352 \nL 615.431138 540.356183 \nL 616.315549 540.931669 \nL 617.199959 540.453493 \nL 618.08437 540.141226 \nL 618.968781 540.300435 \nL 619.853192 540.774214 \nL 620.737602 540.044963 \nL 621.622013 540.452538 \nL 622.506424 540.634621 \nL 623.390834 539.858875 \nL 624.275245 540.658203 \nL 625.159656 540.547662 \nL 626.044067 540.616117 \nL 629.58171 540.251767 \nL 630.46612 540.509228 \nL 632.234942 540.782968 \nL 633.119352 540.229899 \nL 634.003763 539.998271 \nL 634.888174 540.366063 \nL 635.772585 540.217598 \nL 636.656995 540.702854 \nL 638.425817 540.422675 \nL 639.310228 540.216093 \nL 640.194638 540.545097 \nL 641.079049 539.685376 \nL 641.96346 540.23461 \nL 642.847871 540.498131 \nL 643.732281 540.95915 \nL 644.616692 540.879519 \nL 645.501103 540.408803 \nL 646.385513 540.936236 \nL 647.269924 540.21451 \nL 648.154335 540.255653 \nL 649.038746 540.165881 \nL 649.923156 540.70356 \nL 650.807567 540.526646 \nL 651.691978 540.597404 \nL 652.576389 540.221956 \nL 654.34521 540.784539 \nL 655.229621 540.939782 \nL 656.114032 540.331253 \nL 658.767264 540.82652 \nL 659.651674 540.476865 \nL 660.536085 540.772395 \nL 661.420496 540.73307 \nL 662.304907 540.365186 \nL 663.189317 540.653897 \nL 664.073728 540.435578 \nL 665.84255 540.578677 \nL 666.72696 540.529119 \nL 667.611371 540.94961 \nL 668.495782 540.74791 \nL 669.380193 541.186434 \nL 670.264603 540.806628 \nL 671.149014 540.867388 \nL 672.033425 540.67549 \nL 672.917835 540.296051 \nL 673.802246 539.762729 \nL 674.686657 540.420725 \nL 675.571068 540.491718 \nL 676.455478 540.832696 \nL 677.339889 540.492124 \nL 678.2243 540.812334 \nL 679.108711 540.454841 \nL 679.993121 540.600688 \nL 683.530764 540.544888 \nL 684.415175 540.280164 \nL 685.299586 540.687778 \nL 687.068407 540.701663 \nL 687.952818 540.100109 \nL 688.837229 540.93159 \nL 689.721639 540.829687 \nL 690.60605 540.952188 \nL 691.490461 541.209034 \nL 693.259282 540.350215 \nL 694.143693 540.7714 \nL 695.028104 540.981371 \nL 696.796925 540.889465 \nL 697.681336 541.073263 \nL 698.565747 540.485371 \nL 699.450157 540.162701 \nL 700.334568 540.480621 \nL 701.218979 540.918792 \nL 702.10339 541.143432 \nL 702.9878 540.695394 \nL 703.872211 540.802271 \nL 704.756622 540.157375 \nL 705.641033 541.011613 \nL 707.409854 540.518807 \nL 708.294265 540.75516 \nL 710.947497 540.185406 \nL 711.831908 539.764875 \nL 713.600729 539.976521 \nL 714.48514 539.964691 \nL 715.369551 540.616732 \nL 716.253961 540.664851 \nL 717.138372 540.361365 \nL 718.022783 540.535963 \nL 718.907194 540.427137 \nL 719.791604 540.443521 \nL 720.676015 540.907459 \nL 721.560426 540.679049 \nL 722.444836 539.952272 \nL 723.329247 540.468909 \nL 724.213658 540.585914 \nL 725.098069 540.908807 \nL 725.982479 540.525128 \nL 726.86689 539.940756 \nL 727.751301 540.490122 \nL 728.635712 540.391215 \nL 729.520122 540.892815 \nL 730.404533 540.70568 \nL 731.288944 539.855171 \nL 732.173355 539.711797 \nL 733.942176 540.522811 \nL 735.710997 540.717157 \nL 736.595408 540.012613 \nL 737.479819 540.00043 \nL 738.36423 540.236063 \nL 739.24864 540.754558 \nL 740.133051 540.305224 \nL 741.901873 540.576845 \nL 742.786283 540.595048 \nL 743.670694 540.768102 \nL 744.555105 540.274746 \nL 745.439516 540.462248 \nL 746.323926 540.814938 \nL 748.092748 540.131058 \nL 748.977158 540.428066 \nL 749.861569 540.490776 \nL 750.74598 540.666199 \nL 752.514801 540.030306 \nL 753.399212 540.162138 \nL 754.283623 540.102713 \nL 755.168034 540.314463 \nL 756.052444 540.361587 \nL 756.936855 540.063297 \nL 757.821266 540.877177 \nL 758.705677 540.239269 \nL 759.590087 540.279732 \nL 761.358909 540.03202 \nL 762.243319 540.608776 \nL 766.665373 540.574974 \nL 768.434195 540.359795 \nL 769.318605 540.375446 \nL 770.203016 540.669012 \nL 771.087427 540.445824 \nL 771.971837 541.032237 \nL 772.856248 540.472023 \nL 773.740659 540.735151 \nL 774.62507 540.615934 \nL 775.50948 540.76911 \nL 776.393891 540.32615 \nL 777.278302 540.688275 \nL 778.162713 540.893339 \nL 779.047123 540.50224 \nL 779.931534 540.293591 \nL 779.931534 540.293591 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 565.918125 \nL 34.240625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 815.440625 565.918125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 22.318125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Model MAE -->\n    <g transform=\"translate(391.845313 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"486.71875\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 41.240625 59.674375 \nL 96.515625 59.674375 \nQ 98.515625 59.674375 98.515625 57.674375 \nL 98.515625 29.318125 \nQ 98.515625 27.318125 96.515625 27.318125 \nL 41.240625 27.318125 \nQ 39.240625 27.318125 39.240625 29.318125 \nL 39.240625 57.674375 \nQ 39.240625 59.674375 41.240625 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 43.240625 35.416562 \nL 63.240625 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_18\">\n     <!-- train -->\n     <g transform=\"translate(71.240625 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 43.240625 50.094687 \nL 63.240625 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_19\">\n     <!-- test -->\n     <g transform=\"translate(71.240625 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd009c681cf\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"34.240625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-06T19:53:08.709938</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mef27dca615\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.553289\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(155.009539 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.994362\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(243.450612 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"341.435436\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(331.891686 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"429.876509\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(420.332759 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"518.317582\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(508.773832 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"606.758655\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(597.214905 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"695.199729\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(685.655979 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"783.640802\" xlink:href=\"#mef27dca615\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(774.097052 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m87df95cd0a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m87df95cd0a\" y=\"524.263946\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 528.063165)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m87df95cd0a\" y=\"460.595199\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 464.394418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m87df95cd0a\" y=\"396.926453\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 400.725671)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m87df95cd0a\" y=\"333.257706\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 337.056924)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m87df95cd0a\" y=\"269.588959\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 273.388177)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m87df95cd0a\" y=\"205.920212\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 209.719431)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m87df95cd0a\" y=\"142.251465\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 146.050684)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m87df95cd0a\" y=\"78.582718\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 82.381937)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- MSE -->\n     <g transform=\"translate(14.798438 304.765781)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p440740df64)\" d=\"M 76.112216 47.027216 \nL 77.881037 134.743214 \nL 82.303091 334.145251 \nL 83.187502 371.619228 \nL 84.071913 401.920149 \nL 84.956323 426.896101 \nL 85.840734 447.737789 \nL 86.725145 464.778391 \nL 87.609555 478.621093 \nL 88.493966 483.746988 \nL 89.378377 491.153382 \nL 91.147198 499.782419 \nL 92.031609 500.662277 \nL 92.91602 503.257618 \nL 93.800431 500.977653 \nL 94.684841 502.91484 \nL 95.569252 503.097945 \nL 96.453663 499.201451 \nL 97.338073 503.788329 \nL 98.222484 501.87512 \nL 99.106895 503.848204 \nL 99.991306 502.546742 \nL 100.875716 504.048341 \nL 101.760127 502.855481 \nL 102.644538 502.655453 \nL 103.528949 505.12833 \nL 104.413359 506.28256 \nL 105.29777 501.981421 \nL 106.182181 503.330481 \nL 107.066592 503.005342 \nL 107.951002 506.808328 \nL 108.835413 502.106564 \nL 110.604234 504.787088 \nL 111.488645 503.251231 \nL 112.373056 506.240943 \nL 113.257467 504.333157 \nL 114.141877 501.152506 \nL 115.026288 506.090815 \nL 115.910699 507.568315 \nL 116.79511 504.189659 \nL 117.67952 503.466663 \nL 118.563931 504.863242 \nL 119.448342 503.892481 \nL 120.332753 503.523818 \nL 121.217163 504.537677 \nL 122.101574 501.506074 \nL 122.985985 504.364044 \nL 123.870395 503.390393 \nL 124.754806 503.480889 \nL 125.639217 505.766192 \nL 126.523628 505.402787 \nL 127.408038 507.088335 \nL 128.292449 503.256416 \nL 129.17686 503.616924 \nL 130.061271 506.198974 \nL 130.945681 504.720333 \nL 131.830092 506.582604 \nL 132.714503 502.391167 \nL 133.598914 503.418409 \nL 134.483324 504.000543 \nL 135.367735 505.789131 \nL 136.252146 505.755784 \nL 137.136556 504.572858 \nL 138.020967 504.283367 \nL 138.905378 505.571914 \nL 139.789789 507.90051 \nL 140.674199 508.27955 \nL 141.55861 505.071114 \nL 142.443021 506.407933 \nL 143.327432 505.185461 \nL 144.211842 505.48058 \nL 145.096253 507.274075 \nL 145.980664 507.813869 \nL 146.865075 506.85017 \nL 147.749485 503.826965 \nL 148.633896 504.071827 \nL 149.518307 506.425123 \nL 150.402717 504.138284 \nL 151.287128 508.277315 \nL 152.171539 505.711569 \nL 153.05595 507.152175 \nL 153.94036 506.391393 \nL 154.824771 506.253421 \nL 155.709182 507.213119 \nL 156.593593 507.024568 \nL 157.478003 507.835862 \nL 158.362414 506.590577 \nL 159.246825 507.750903 \nL 160.131235 507.012418 \nL 161.015646 508.709175 \nL 161.900057 505.793224 \nL 162.784468 506.601658 \nL 163.668878 506.276221 \nL 164.553289 510.078254 \nL 165.4377 506.55522 \nL 166.322111 505.373289 \nL 167.206521 506.906244 \nL 168.090932 509.463951 \nL 168.975343 510.180712 \nL 170.744164 508.965288 \nL 171.628575 506.15168 \nL 172.512986 509.020203 \nL 173.397396 506.75685 \nL 174.281807 510.365942 \nL 175.166218 504.639012 \nL 176.050629 507.327885 \nL 176.935039 509.177605 \nL 177.81945 508.059448 \nL 178.703861 508.085637 \nL 179.588272 510.095875 \nL 180.472682 509.316598 \nL 182.241504 508.821402 \nL 183.125915 509.033677 \nL 184.010325 509.70613 \nL 184.894736 509.2803 \nL 185.779147 510.445618 \nL 186.663557 512.455826 \nL 188.432379 511.26939 \nL 189.31679 511.574552 \nL 190.2012 511.442488 \nL 191.970022 510.596772 \nL 192.854433 513.060632 \nL 193.738843 509.191577 \nL 194.623254 509.376619 \nL 195.507665 514.797354 \nL 196.392076 511.846702 \nL 197.276486 509.845044 \nL 198.160897 512.456906 \nL 199.045308 513.546507 \nL 199.929718 511.085192 \nL 200.814129 512.748626 \nL 201.69854 510.942702 \nL 202.582951 510.919653 \nL 203.467361 510.496446 \nL 204.351772 511.973211 \nL 205.236183 512.706887 \nL 206.120594 515.550528 \nL 207.005004 510.962782 \nL 207.889415 514.912733 \nL 208.773826 510.628662 \nL 209.658237 513.166046 \nL 210.542647 512.569692 \nL 211.427058 514.253765 \nL 212.311469 513.56159 \nL 213.195879 512.294027 \nL 214.08029 515.282404 \nL 214.964701 512.321096 \nL 215.849112 512.543686 \nL 216.733522 514.612191 \nL 217.617933 514.898446 \nL 218.502344 513.22008 \nL 219.386755 519.072936 \nL 220.271165 516.224311 \nL 221.155576 514.94625 \nL 222.039987 515.661043 \nL 222.924398 515.526252 \nL 223.808808 515.565981 \nL 224.693219 515.105723 \nL 225.57763 515.173067 \nL 226.46204 515.752346 \nL 227.346451 517.815416 \nL 228.230862 516.947902 \nL 229.115273 517.239974 \nL 229.999683 518.652698 \nL 230.884094 515.191871 \nL 231.768505 516.424861 \nL 232.652916 516.126674 \nL 233.537326 515.995551 \nL 234.421737 517.402277 \nL 235.306148 516.598427 \nL 236.190558 517.401232 \nL 237.074969 518.46218 \nL 237.95938 519.077563 \nL 238.843791 519.273541 \nL 239.728201 518.758058 \nL 240.612612 517.330373 \nL 241.497023 517.38686 \nL 242.381434 517.162739 \nL 243.265844 517.223883 \nL 244.150255 520.90803 \nL 245.034666 516.155868 \nL 245.919077 519.764814 \nL 246.803487 519.842711 \nL 247.687898 520.336759 \nL 248.572309 518.852277 \nL 249.456719 519.614115 \nL 250.34113 519.388962 \nL 251.225541 519.892409 \nL 252.109952 519.81549 \nL 252.994362 519.384529 \nL 253.878773 517.493556 \nL 254.763184 519.529484 \nL 255.647595 520.764963 \nL 256.532005 519.839614 \nL 257.416416 519.631978 \nL 258.300827 521.092483 \nL 259.185238 519.324958 \nL 260.069648 522.47761 \nL 260.954059 519.772216 \nL 261.83847 518.753086 \nL 262.72288 519.438952 \nL 263.607291 521.246291 \nL 264.491702 519.979098 \nL 265.376113 519.843816 \nL 266.260523 519.553244 \nL 267.144934 518.940447 \nL 268.029345 520.994282 \nL 268.913756 521.399692 \nL 269.798166 517.98149 \nL 270.682577 520.201313 \nL 271.566988 519.025399 \nL 272.451399 522.343305 \nL 273.335809 523.212939 \nL 274.22022 521.237705 \nL 275.104631 523.22419 \nL 275.989041 520.611082 \nL 276.873452 519.458625 \nL 277.757863 523.272541 \nL 278.642274 520.927041 \nL 279.526684 520.324202 \nL 280.411095 519.203659 \nL 281.295506 523.059319 \nL 282.179917 519.553323 \nL 283.064327 522.087714 \nL 283.948738 519.167828 \nL 284.833149 519.637279 \nL 285.71756 519.00541 \nL 286.60197 522.249907 \nL 287.486381 524.874594 \nL 289.255202 519.299698 \nL 290.139613 520.691341 \nL 291.024024 522.470913 \nL 291.908435 523.991523 \nL 292.792845 522.252221 \nL 293.677256 521.841194 \nL 294.561667 520.243051 \nL 295.446078 522.175399 \nL 296.330488 522.482826 \nL 297.214899 522.14547 \nL 298.09931 521.346599 \nL 298.98372 524.823407 \nL 299.868131 521.805224 \nL 300.752542 521.510577 \nL 301.636953 521.341335 \nL 302.521363 523.201487 \nL 303.405774 522.421384 \nL 304.290185 522.761661 \nL 305.174596 520.475041 \nL 306.059006 523.389875 \nL 306.943417 523.658418 \nL 307.827828 522.441124 \nL 308.712239 522.548221 \nL 309.596649 523.255473 \nL 310.48106 520.283587 \nL 311.365471 523.189629 \nL 312.249881 522.993001 \nL 313.134292 522.119458 \nL 314.018703 524.935532 \nL 314.903114 520.659609 \nL 315.787524 523.8585 \nL 316.671935 524.361109 \nL 317.556346 524.541306 \nL 318.440757 522.515293 \nL 319.325167 526.396746 \nL 320.209578 519.487631 \nL 321.093989 522.20098 \nL 321.9784 525.35209 \nL 322.86281 520.935924 \nL 323.747221 524.257686 \nL 324.631632 522.931317 \nL 325.516042 522.430377 \nL 326.400453 522.996086 \nL 328.169275 524.92462 \nL 329.053685 521.6021 \nL 329.938096 524.044416 \nL 330.822507 520.096863 \nL 332.591328 523.296598 \nL 333.475739 520.851579 \nL 334.36015 523.968444 \nL 335.244561 522.249124 \nL 336.128971 523.16811 \nL 337.013382 522.520891 \nL 337.897793 523.088325 \nL 338.782203 525.026604 \nL 339.666614 526.278162 \nL 340.551025 524.864915 \nL 341.435436 521.395454 \nL 342.319846 523.095593 \nL 344.088668 522.489621 \nL 344.973079 526.338535 \nL 345.857489 525.732126 \nL 346.7419 525.31323 \nL 347.626311 524.033517 \nL 348.510722 523.65274 \nL 349.395132 523.999144 \nL 350.279543 523.138054 \nL 351.163954 524.683037 \nL 352.048364 521.403426 \nL 352.932775 524.066074 \nL 353.817186 522.599541 \nL 354.701597 523.014217 \nL 355.586007 525.174583 \nL 356.470418 524.550426 \nL 357.354829 524.863282 \nL 358.23924 522.925293 \nL 359.12365 522.798129 \nL 360.008061 523.134404 \nL 360.892472 524.946346 \nL 361.776883 522.652524 \nL 362.661293 524.658986 \nL 363.545704 522.471028 \nL 364.430115 523.728737 \nL 365.314525 524.715024 \nL 366.198936 524.213865 \nL 367.083347 524.903514 \nL 368.852168 523.98802 \nL 369.736579 521.699936 \nL 370.62099 524.941907 \nL 371.505401 521.566281 \nL 373.274222 524.452723 \nL 374.158633 523.215659 \nL 375.043043 522.227654 \nL 375.927454 522.962496 \nL 376.811865 524.651657 \nL 377.696276 526.797742 \nL 378.580686 522.296886 \nL 379.465097 524.041763 \nL 380.349508 525.487184 \nL 381.233919 524.97764 \nL 382.118329 525.413495 \nL 383.00274 525.520234 \nL 383.887151 527.244545 \nL 384.771562 523.545626 \nL 385.655972 525.29972 \nL 386.540383 523.602082 \nL 387.424794 523.669887 \nL 388.309204 524.774006 \nL 389.193615 522.227204 \nL 390.962437 525.289045 \nL 391.846847 523.108174 \nL 392.731258 521.700726 \nL 393.615669 523.870619 \nL 394.50008 525.140295 \nL 395.38449 522.52754 \nL 396.268901 523.369424 \nL 397.153312 521.174035 \nL 398.037723 525.329721 \nL 398.922133 523.831795 \nL 399.806544 524.763053 \nL 400.690955 524.33749 \nL 401.575365 524.867702 \nL 402.459776 524.557919 \nL 403.344187 524.03481 \nL 404.228598 524.029552 \nL 405.113008 522.8151 \nL 406.88183 524.930207 \nL 407.766241 524.886112 \nL 408.650651 522.240775 \nL 409.535062 523.733181 \nL 410.419473 524.850063 \nL 411.303884 524.100739 \nL 412.188294 522.140904 \nL 413.072705 525.789943 \nL 413.957116 522.279344 \nL 414.841526 524.538325 \nL 415.725937 524.975782 \nL 416.610348 525.037212 \nL 417.494759 522.404219 \nL 418.379169 524.479585 \nL 419.26358 522.443365 \nL 420.147991 526.855055 \nL 421.032402 525.014394 \nL 421.916812 526.870284 \nL 422.801223 525.328501 \nL 423.685634 524.2896 \nL 424.570045 523.620826 \nL 425.454455 525.492491 \nL 427.223277 524.810329 \nL 428.107687 525.425457 \nL 428.992098 523.669171 \nL 429.876509 526.011799 \nL 430.76092 524.849638 \nL 431.64533 522.85444 \nL 433.414152 524.71772 \nL 434.298563 523.851772 \nL 435.182973 519.202797 \nL 436.067384 523.741645 \nL 436.951795 526.039098 \nL 437.836205 524.410007 \nL 438.720616 525.441833 \nL 439.605027 525.912328 \nL 440.489438 524.299273 \nL 441.373848 523.375521 \nL 442.258259 526.954793 \nL 443.14267 523.153634 \nL 444.027081 523.824066 \nL 444.911491 525.017654 \nL 445.795902 523.283598 \nL 446.680313 523.924331 \nL 447.564724 525.474822 \nL 448.449134 524.052911 \nL 449.333545 523.208209 \nL 450.217956 526.578479 \nL 451.102366 525.053388 \nL 451.986777 525.308348 \nL 452.871188 524.270037 \nL 453.755599 526.265994 \nL 454.640009 524.86432 \nL 455.52442 525.611215 \nL 456.408831 525.410854 \nL 457.293242 524.751953 \nL 458.177652 521.732725 \nL 459.062063 523.880529 \nL 459.946474 523.27775 \nL 460.830885 525.928552 \nL 461.715295 525.826095 \nL 462.599706 525.284947 \nL 463.484117 529.268184 \nL 464.368527 522.880598 \nL 465.252938 522.821433 \nL 466.137349 527.07917 \nL 467.02176 524.321994 \nL 467.90617 525.049963 \nL 468.790581 523.017884 \nL 469.674992 522.058077 \nL 470.559403 525.554358 \nL 471.443813 524.930814 \nL 472.328224 526.855669 \nL 473.212635 528.326641 \nL 474.097046 525.231398 \nL 474.981456 524.931925 \nL 475.865867 524.288331 \nL 477.634688 524.789508 \nL 478.519099 523.572208 \nL 479.40351 525.240937 \nL 480.287921 525.005328 \nL 481.172331 525.335793 \nL 482.056742 525.290363 \nL 482.941153 523.112952 \nL 483.825564 524.860677 \nL 484.709974 525.551826 \nL 485.594385 524.758669 \nL 486.478796 523.389881 \nL 487.363207 524.521706 \nL 488.247617 526.604643 \nL 489.132028 523.227827 \nL 490.016439 523.735567 \nL 490.900849 525.696647 \nL 491.78526 525.88195 \nL 492.669671 525.090372 \nL 493.554082 523.914197 \nL 494.438492 525.406689 \nL 495.322903 523.497038 \nL 496.207314 524.400972 \nL 497.091725 525.675256 \nL 497.976135 526.093023 \nL 498.860546 525.128813 \nL 499.744957 524.676005 \nL 500.629367 525.49506 \nL 501.513778 525.238545 \nL 502.398189 527.40938 \nL 503.2826 525.120847 \nL 504.16701 526.478742 \nL 505.051421 525.996097 \nL 505.935832 524.720707 \nL 506.820243 522.462692 \nL 507.704653 524.807232 \nL 508.589064 523.635041 \nL 509.473475 525.792001 \nL 510.357886 525.805383 \nL 511.242296 525.197912 \nL 512.126707 524.448357 \nL 513.011118 524.550183 \nL 513.895528 525.917696 \nL 514.779939 525.89305 \nL 515.66435 523.208895 \nL 516.548761 526.462615 \nL 517.433171 521.464443 \nL 518.317582 524.780643 \nL 519.201993 526.053233 \nL 520.086404 524.595358 \nL 520.970814 522.798421 \nL 521.855225 527.518941 \nL 522.739636 522.688209 \nL 523.624047 526.689189 \nL 524.508457 525.827771 \nL 526.277279 527.68499 \nL 528.0461 523.477152 \nL 528.930511 526.93365 \nL 529.814922 524.840032 \nL 530.699332 523.811266 \nL 531.583743 525.696587 \nL 532.468154 526.510638 \nL 533.352565 526.368421 \nL 534.236975 522.605995 \nL 535.121386 523.188942 \nL 536.005797 525.785346 \nL 536.890208 522.64525 \nL 537.774618 526.514244 \nL 538.659029 526.270554 \nL 539.54344 525.822385 \nL 540.42785 524.621018 \nL 541.312261 523.960891 \nL 542.196672 520.991665 \nL 543.081083 524.868249 \nL 543.965493 526.240577 \nL 544.849904 524.637309 \nL 545.734315 523.373049 \nL 546.618726 522.884381 \nL 547.503136 526.248166 \nL 548.387547 524.104844 \nL 549.271958 524.236453 \nL 550.156369 525.206771 \nL 551.040779 523.375575 \nL 551.92519 524.807669 \nL 552.809601 524.59953 \nL 553.694011 524.055673 \nL 554.578422 524.704131 \nL 555.462833 522.011608 \nL 556.347244 524.004602 \nL 557.231654 526.671197 \nL 558.116065 526.970489 \nL 559.000476 523.684521 \nL 559.884887 524.398318 \nL 560.769297 526.458498 \nL 561.653708 523.631549 \nL 562.538119 524.896963 \nL 563.42253 522.924267 \nL 564.30694 525.224731 \nL 565.191351 526.715267 \nL 566.075762 525.277927 \nL 566.960172 525.686216 \nL 567.844583 525.347742 \nL 568.728994 526.732269 \nL 569.613405 525.554838 \nL 570.497815 523.894427 \nL 571.382226 526.224619 \nL 572.266637 524.296273 \nL 573.151048 524.581818 \nL 574.035458 525.523895 \nL 574.919869 524.457908 \nL 575.80428 524.670777 \nL 576.68869 523.024582 \nL 577.573101 523.79554 \nL 578.457512 524.719602 \nL 579.341923 524.098116 \nL 580.226333 527.201179 \nL 581.110744 522.887277 \nL 581.995155 524.459171 \nL 582.879566 525.282154 \nL 583.763976 523.536099 \nL 584.648387 524.761941 \nL 585.532798 527.083815 \nL 586.417209 523.69627 \nL 587.301619 525.921861 \nL 588.18603 523.4472 \nL 589.070441 525.589029 \nL 589.954851 522.751855 \nL 590.839262 523.484129 \nL 591.723673 525.2778 \nL 592.608084 523.181474 \nL 593.492494 525.61056 \nL 594.376905 524.564367 \nL 595.261316 524.309638 \nL 596.145727 526.188674 \nL 597.030137 522.563552 \nL 597.914548 525.567273 \nL 598.798959 526.34771 \nL 599.68337 527.790417 \nL 600.56778 525.360093 \nL 601.452191 524.490702 \nL 602.336602 527.176218 \nL 603.221012 525.421887 \nL 604.105423 527.780878 \nL 604.989834 526.676614 \nL 605.874245 525.016033 \nL 606.758655 524.937724 \nL 607.643066 525.836976 \nL 608.527477 525.606862 \nL 609.411888 525.582574 \nL 610.296298 524.736379 \nL 611.180709 524.966377 \nL 612.06512 525.621671 \nL 612.949531 525.230858 \nL 613.833941 525.56295 \nL 614.718352 524.23035 \nL 615.602763 526.042176 \nL 616.487173 523.920955 \nL 617.371584 523.675067 \nL 618.255995 524.269308 \nL 620.024816 526.212269 \nL 620.909227 525.381569 \nL 621.793638 525.186873 \nL 622.678049 526.476902 \nL 623.562459 523.919644 \nL 624.44687 524.810942 \nL 625.331281 524.786769 \nL 626.215692 524.404092 \nL 627.100102 525.95689 \nL 627.984513 523.394854 \nL 628.868924 525.538607 \nL 629.753334 523.330801 \nL 630.637745 523.794392 \nL 631.522156 523.589932 \nL 632.406567 525.803252 \nL 633.290977 524.87035 \nL 634.175388 524.365997 \nL 635.059799 523.607098 \nL 635.94421 524.752688 \nL 636.82862 523.952353 \nL 637.713031 524.733561 \nL 639.481852 524.290056 \nL 640.366263 524.91026 \nL 641.250674 523.397379 \nL 642.135085 524.339566 \nL 643.019495 523.519717 \nL 643.903906 526.607928 \nL 644.788317 523.165468 \nL 645.672728 524.118633 \nL 646.557138 525.917441 \nL 648.32596 525.243809 \nL 649.210371 527.10197 \nL 650.094781 526.679249 \nL 650.979192 524.869342 \nL 651.863603 524.011779 \nL 652.748013 524.814233 \nL 653.632424 524.764832 \nL 654.516835 525.99895 \nL 655.401246 525.956028 \nL 656.285656 525.693284 \nL 657.170067 521.875512 \nL 658.054478 524.371656 \nL 658.938889 525.58663 \nL 659.823299 523.029299 \nL 660.70771 526.075147 \nL 661.592121 526.977787 \nL 662.476532 525.724439 \nL 663.360942 525.18609 \nL 664.245353 526.182547 \nL 665.129764 525.206722 \nL 666.014174 527.490895 \nL 666.898585 525.23438 \nL 667.782996 525.343535 \nL 668.667407 523.75892 \nL 669.551817 524.936667 \nL 670.436228 525.090584 \nL 671.320639 523.601536 \nL 672.20505 525.613335 \nL 673.08946 525.206837 \nL 673.973871 525.734087 \nL 674.858282 524.620957 \nL 675.742693 525.710303 \nL 676.627103 525.729922 \nL 677.511514 524.320488 \nL 678.395925 526.915884 \nL 679.280335 525.094914 \nL 680.164746 525.740596 \nL 681.049157 524.684913 \nL 681.933568 523.827921 \nL 682.817978 525.5883 \nL 683.702389 526.343769 \nL 684.5868 524.494965 \nL 685.471211 523.673798 \nL 686.355621 525.683374 \nL 687.240032 525.223402 \nL 688.124443 525.427145 \nL 689.008854 524.950274 \nL 689.893264 525.894149 \nL 690.777675 526.664907 \nL 691.662086 525.999035 \nL 692.546496 526.877461 \nL 693.430907 528.135485 \nL 694.315318 526.447241 \nL 695.199729 524.243326 \nL 696.084139 523.323126 \nL 696.96855 522.953935 \nL 697.852961 523.146846 \nL 698.737372 524.991181 \nL 699.621782 523.485939 \nL 700.506193 522.318234 \nL 701.390604 526.86525 \nL 702.275015 526.106739 \nL 703.159425 526.244584 \nL 704.043836 526.775993 \nL 704.928247 523.595367 \nL 705.812657 526.81101 \nL 706.697068 523.953507 \nL 707.581479 524.746762 \nL 708.46589 524.914784 \nL 709.3503 527.217252 \nL 710.234711 523.430089 \nL 711.119122 525.848439 \nL 712.003533 527.643118 \nL 712.887943 525.770166 \nL 713.772354 526.639708 \nL 714.656765 526.883435 \nL 715.541175 526.144604 \nL 716.425586 524.578478 \nL 717.309997 528.232866 \nL 718.194408 524.105269 \nL 719.078818 524.461496 \nL 719.963229 526.607703 \nL 720.84764 527.226153 \nL 721.732051 529.105772 \nL 722.616461 523.044746 \nL 723.500872 525.473292 \nL 724.385283 525.508982 \nL 725.269694 523.300775 \nL 726.154104 524.271803 \nL 727.038515 524.548817 \nL 728.807336 526.476319 \nL 729.691747 526.423129 \nL 730.576158 523.458597 \nL 731.460569 524.604521 \nL 732.344979 525.525037 \nL 733.22939 523.101665 \nL 734.113801 524.595419 \nL 734.998212 524.987635 \nL 735.882622 523.633134 \nL 736.767033 526.121943 \nL 737.651444 527.818694 \nL 738.535855 526.54226 \nL 739.420265 524.782993 \nL 740.304676 523.592379 \nL 741.189087 526.962358 \nL 742.073497 524.837142 \nL 742.957908 523.931672 \nL 743.842319 525.243311 \nL 744.72673 526.950846 \nL 746.495551 523.928545 \nL 747.379962 524.605771 \nL 748.264373 527.426946 \nL 749.148783 526.703348 \nL 750.033194 526.880442 \nL 750.917605 526.826311 \nL 751.802016 524.631881 \nL 752.686426 526.775962 \nL 753.570837 524.655221 \nL 754.455248 526.033645 \nL 755.339658 526.399625 \nL 756.224069 526.943402 \nL 757.10848 525.083881 \nL 757.992891 524.740137 \nL 758.877301 524.25162 \nL 759.761712 526.697373 \nL 760.646123 526.442893 \nL 761.530534 525.354282 \nL 762.414944 524.788664 \nL 763.299355 524.566395 \nL 764.183766 525.481216 \nL 765.068177 527.157595 \nL 765.952587 522.914413 \nL 766.836998 526.504383 \nL 767.721409 527.036831 \nL 768.605819 526.439517 \nL 769.49023 525.017138 \nL 770.374641 525.722508 \nL 771.259052 524.273431 \nL 772.143462 523.764622 \nL 773.027873 524.398148 \nL 773.912284 527.349104 \nL 774.796695 526.393468 \nL 775.681105 526.297653 \nL 776.565516 524.405289 \nL 777.449927 525.562355 \nL 778.334337 525.964243 \nL 779.218748 524.900321 \nL 780.103159 524.800176 \nL 780.98757 526.055753 \nL 781.87198 527.898467 \nL 782.756391 524.415623 \nL 783.640802 525.993419 \nL 784.525213 524.115968 \nL 785.409623 526.16792 \nL 786.294034 526.231894 \nL 786.294034 526.231894 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p440740df64)\" d=\"M 76.112216 125.667727 \nL 79.649859 282.82969 \nL 81.41868 355.839267 \nL 83.187502 418.26752 \nL 84.071913 442.591311 \nL 84.956323 463.215381 \nL 85.840734 479.151403 \nL 86.725145 490.867698 \nL 87.609555 499.146822 \nL 88.493966 504.709604 \nL 89.378377 508.477124 \nL 90.262788 510.648535 \nL 91.147198 511.789189 \nL 92.031609 512.470052 \nL 92.91602 512.691914 \nL 94.684841 512.678495 \nL 96.453663 512.420614 \nL 98.222484 512.315382 \nL 100.875716 512.05673 \nL 108.835413 512.349755 \nL 110.604234 512.252525 \nL 111.488645 512.318278 \nL 112.373056 512.602772 \nL 113.257467 512.3593 \nL 114.141877 512.305679 \nL 115.026288 512.460367 \nL 115.910699 512.461551 \nL 118.563931 512.833032 \nL 119.448342 512.843852 \nL 121.217163 512.989876 \nL 122.101574 513.094938 \nL 122.985985 513.072059 \nL 123.870395 513.232303 \nL 125.639217 513.10373 \nL 129.17686 513.128856 \nL 130.061271 513.292537 \nL 130.945681 513.336686 \nL 131.830092 513.249839 \nL 132.714503 513.64517 \nL 133.598914 513.765048 \nL 134.483324 514.013256 \nL 136.252146 514.026742 \nL 137.136556 513.822136 \nL 138.905378 513.992648 \nL 139.789789 513.760397 \nL 140.674199 514.019832 \nL 142.443021 513.827 \nL 143.327432 514.295261 \nL 145.096253 514.445522 \nL 145.980664 514.426226 \nL 150.402717 514.998493 \nL 151.287128 515.222826 \nL 152.171539 515.25579 \nL 153.05595 515.533885 \nL 154.824771 515.084331 \nL 157.478003 515.772104 \nL 162.784468 516.19083 \nL 163.668878 516.422195 \nL 164.553289 516.213873 \nL 165.4377 516.431564 \nL 166.322111 516.465682 \nL 167.206521 516.798302 \nL 169.859754 516.814757 \nL 171.628575 517.599317 \nL 172.512986 517.095602 \nL 173.397396 517.241887 \nL 174.281807 517.081533 \nL 175.166218 517.75191 \nL 176.050629 517.896616 \nL 177.81945 518.563459 \nL 179.588272 518.127022 \nL 181.357093 518.414545 \nL 182.241504 518.475283 \nL 183.125915 519.153304 \nL 184.010325 519.514189 \nL 186.663557 519.212724 \nL 187.547968 519.702577 \nL 191.085611 520.416562 \nL 191.970022 520.383094 \nL 192.854433 520.989363 \nL 193.738843 520.987323 \nL 194.623254 521.225343 \nL 196.392076 521.500765 \nL 197.276486 521.439317 \nL 198.160897 521.686238 \nL 199.045308 522.081472 \nL 199.929718 522.202777 \nL 201.69854 522.714452 \nL 203.467361 522.826236 \nL 209.658237 523.887451 \nL 211.427058 524.463561 \nL 213.195879 524.742317 \nL 214.964701 524.726682 \nL 215.849112 525.046684 \nL 217.617933 525.327395 \nL 218.502344 525.762431 \nL 221.155576 526.162152 \nL 222.039987 526.567258 \nL 222.924398 526.66014 \nL 223.808808 526.905987 \nL 224.693219 526.925957 \nL 225.57763 527.268754 \nL 226.46204 527.448556 \nL 228.230862 527.227944 \nL 229.115273 527.772426 \nL 234.421737 528.564235 \nL 235.306148 528.887432 \nL 236.190558 528.731365 \nL 238.843791 529.389234 \nL 239.728201 529.526702 \nL 240.612612 529.306146 \nL 241.497023 529.689934 \nL 245.034666 530.574176 \nL 247.687898 530.595768 \nL 248.572309 530.689895 \nL 249.456719 531.011932 \nL 250.34113 530.938419 \nL 251.225541 531.361243 \nL 254.763184 531.875614 \nL 255.647595 531.902313 \nL 259.185238 532.790957 \nL 260.069648 532.677673 \nL 260.954059 532.819726 \nL 261.83847 532.652232 \nL 266.260523 533.450234 \nL 267.144934 533.291447 \nL 268.029345 533.554447 \nL 269.798166 533.801003 \nL 270.682577 533.721 \nL 271.566988 533.97296 \nL 275.104631 534.248723 \nL 275.989041 534.030103 \nL 276.873452 534.443352 \nL 280.411095 534.96773 \nL 281.295506 534.872662 \nL 282.179917 534.996037 \nL 283.064327 534.915347 \nL 283.948738 535.169312 \nL 284.833149 535.039045 \nL 285.71756 535.323508 \nL 286.60197 535.421084 \nL 287.486381 535.276147 \nL 289.255202 535.78019 \nL 290.139613 535.8751 \nL 291.024024 535.667009 \nL 292.792845 535.877754 \nL 293.677256 536.072092 \nL 294.561667 536.105402 \nL 295.446078 536.292369 \nL 296.330488 536.166971 \nL 298.09931 536.354746 \nL 298.98372 536.020668 \nL 300.752542 536.191648 \nL 301.636953 536.546078 \nL 304.290185 536.770071 \nL 305.174596 536.524905 \nL 306.943417 536.761741 \nL 308.712239 537.103107 \nL 309.596649 536.800516 \nL 310.48106 536.970196 \nL 311.365471 537.001038 \nL 313.134292 537.275426 \nL 315.787524 537.254781 \nL 317.556346 537.494303 \nL 319.325167 537.621629 \nL 320.209578 537.522626 \nL 321.093989 537.707646 \nL 324.631632 537.895964 \nL 325.516042 537.586858 \nL 326.400453 537.635973 \nL 327.284864 537.857538 \nL 329.938096 538.049141 \nL 330.822507 537.763065 \nL 331.706918 538.130832 \nL 333.475739 538.390668 \nL 335.244561 538.457584 \nL 337.013382 538.347057 \nL 337.897793 538.455856 \nL 339.666614 538.244827 \nL 340.551025 538.206279 \nL 342.319846 538.650255 \nL 343.204257 538.728525 \nL 344.088668 538.395787 \nL 345.857489 538.686083 \nL 347.626311 538.781767 \nL 348.510722 538.961384 \nL 349.395132 538.855073 \nL 352.048364 539.013815 \nL 352.932775 539.134163 \nL 353.817186 539.100164 \nL 354.701597 538.357649 \nL 355.586007 538.989348 \nL 356.470418 539.08873 \nL 357.354829 538.985987 \nL 358.23924 539.021572 \nL 359.12365 539.216647 \nL 360.008061 539.049086 \nL 362.661293 539.227012 \nL 363.545704 539.087258 \nL 364.430115 539.438956 \nL 366.198936 539.348721 \nL 367.083347 539.519281 \nL 371.505401 539.37854 \nL 374.158633 539.647326 \nL 375.043043 539.522517 \nL 376.811865 539.81332 \nL 377.696276 539.626035 \nL 381.233919 539.642599 \nL 382.118329 539.835795 \nL 383.887151 539.826135 \nL 385.655972 539.750637 \nL 386.540383 539.966266 \nL 388.309204 539.70248 \nL 389.193615 540.046852 \nL 390.078026 539.849126 \nL 390.962437 539.877901 \nL 391.846847 540.063766 \nL 392.731258 539.834174 \nL 393.615669 540.060375 \nL 394.50008 540.034827 \nL 395.38449 539.8837 \nL 397.153312 540.111267 \nL 398.037723 539.818308 \nL 398.922133 540.124112 \nL 399.806544 539.947704 \nL 402.459776 540.050092 \nL 405.997419 540.176691 \nL 407.766241 539.954529 \nL 408.650651 540.130162 \nL 409.535062 539.969187 \nL 411.303884 540.343284 \nL 412.188294 540.154259 \nL 413.957116 540.120669 \nL 414.841526 540.319704 \nL 415.725937 540.123702 \nL 417.494759 540.344489 \nL 418.379169 539.908965 \nL 419.26358 540.311081 \nL 421.032402 540.328614 \nL 421.916812 540.182086 \nL 422.801223 540.397874 \nL 423.685634 540.270752 \nL 424.570045 540.41424 \nL 425.454455 540.333262 \nL 427.223277 540.549899 \nL 428.107687 540.271298 \nL 428.992098 540.499126 \nL 429.876509 540.462491 \nL 430.76092 540.586978 \nL 432.529741 540.596966 \nL 434.298563 540.67336 \nL 435.182973 540.477398 \nL 437.836205 540.431014 \nL 438.720616 540.109378 \nL 439.605027 540.53196 \nL 441.373848 540.170914 \nL 442.258259 540.53764 \nL 443.14267 540.57211 \nL 444.027081 540.365856 \nL 444.911491 540.521883 \nL 445.795902 540.347404 \nL 447.564724 540.708881 \nL 451.102366 540.505413 \nL 451.986777 540.605995 \nL 453.755599 540.46954 \nL 455.52442 540.628795 \nL 457.293242 540.703713 \nL 458.177652 540.757432 \nL 459.062063 540.566983 \nL 459.946474 540.52842 \nL 460.830885 540.676915 \nL 462.599706 540.706667 \nL 463.484117 540.495316 \nL 464.368527 540.404128 \nL 465.252938 540.723192 \nL 466.137349 540.67445 \nL 467.02176 540.777533 \nL 467.90617 540.732048 \nL 468.790581 540.940801 \nL 470.559403 540.833385 \nL 471.443813 540.782402 \nL 472.328224 540.86553 \nL 473.212635 540.726094 \nL 474.097046 540.831333 \nL 474.981456 540.658438 \nL 475.865867 540.681593 \nL 476.750278 540.878913 \nL 477.634688 540.873764 \nL 478.519099 540.694748 \nL 480.287921 540.824608 \nL 481.172331 540.7046 \nL 482.941153 540.851571 \nL 483.825564 540.663083 \nL 487.363207 541.043031 \nL 488.247617 540.891448 \nL 490.016439 540.92199 \nL 490.900849 540.649112 \nL 492.669671 540.859364 \nL 493.554082 540.74778 \nL 494.438492 540.884802 \nL 495.322903 540.82578 \nL 496.207314 541.0317 \nL 500.629367 541.014396 \nL 501.513778 540.782818 \nL 502.398189 540.92604 \nL 503.2826 540.735044 \nL 505.051421 541.041294 \nL 512.126707 540.970386 \nL 513.011118 541.082389 \nL 513.895528 540.868821 \nL 515.66435 541.049913 \nL 518.317582 540.891418 \nL 519.201993 540.767708 \nL 520.086404 540.964806 \nL 521.855225 540.780833 \nL 522.739636 540.959357 \nL 525.392868 540.84943 \nL 526.277279 540.7097 \nL 528.0461 540.944538 \nL 528.930511 541.025331 \nL 529.814922 540.987834 \nL 530.699332 540.682459 \nL 531.583743 540.922002 \nL 532.468154 541.024262 \nL 533.352565 540.741678 \nL 534.236975 540.946123 \nL 535.121386 541.02851 \nL 536.890208 540.856565 \nL 537.774618 540.868836 \nL 538.659029 541.076587 \nL 539.54344 540.98872 \nL 542.196672 541.115341 \nL 543.081083 540.98512 \nL 543.965493 541.06846 \nL 544.849904 540.856553 \nL 545.734315 541.048775 \nL 547.503136 541.028082 \nL 550.156369 541.119756 \nL 551.040779 540.988556 \nL 553.694011 541.155 \nL 554.578422 541.077762 \nL 555.462833 540.868208 \nL 557.231654 541.11416 \nL 558.116065 540.88084 \nL 559.884887 541.005479 \nL 560.769297 540.858563 \nL 563.42253 541.128277 \nL 564.30694 541.04195 \nL 565.191351 541.116896 \nL 568.728994 541.005776 \nL 570.497815 541.148008 \nL 571.382226 540.791738 \nL 573.151048 541.097283 \nL 574.035458 540.892092 \nL 574.919869 541.125451 \nL 575.80428 541.173416 \nL 576.68869 540.961215 \nL 577.573101 541.167684 \nL 579.341923 540.794155 \nL 580.226333 540.737164 \nL 581.110744 540.836509 \nL 581.995155 540.634703 \nL 582.879566 541.108441 \nL 585.532798 540.959262 \nL 586.417209 541.112894 \nL 591.723673 541.168452 \nL 592.608084 540.977639 \nL 593.492494 540.952304 \nL 594.376905 541.17031 \nL 595.261316 540.866289 \nL 596.145727 541.005245 \nL 597.030137 540.894214 \nL 597.914548 541.174373 \nL 598.798959 541.148206 \nL 599.68337 540.927039 \nL 600.56778 541.020804 \nL 601.452191 540.980238 \nL 602.336602 541.209034 \nL 603.221012 541.169822 \nL 604.105423 540.892037 \nL 605.874245 541.140412 \nL 607.643066 541.062136 \nL 609.411888 541.13894 \nL 610.296298 540.898042 \nL 612.949531 541.021424 \nL 614.718352 540.905869 \nL 615.602763 540.770189 \nL 617.371584 541.041601 \nL 618.255995 540.868791 \nL 620.024816 541.082076 \nL 621.793638 540.974542 \nL 622.678049 541.148251 \nL 624.44687 540.884025 \nL 626.215692 541.09681 \nL 627.100102 540.845921 \nL 628.868924 541.0556 \nL 629.753334 540.760723 \nL 630.637745 541.06033 \nL 635.94421 540.915906 \nL 638.597442 541.098048 \nL 640.366263 540.80676 \nL 641.250674 540.949845 \nL 642.135085 540.891964 \nL 643.019495 541.05805 \nL 646.557138 541.016566 \nL 647.441549 540.660172 \nL 648.32596 540.904017 \nL 650.979192 541.12138 \nL 651.863603 540.959712 \nL 652.748013 541.130834 \nL 653.632424 540.888367 \nL 655.401246 540.871818 \nL 656.285656 541.059792 \nL 658.938889 540.870676 \nL 660.70771 541.070476 \nL 661.592121 541.120439 \nL 662.476532 540.909582 \nL 665.129764 541.078576 \nL 666.014174 540.956785 \nL 667.782996 541.0329 \nL 668.667407 540.907241 \nL 669.551817 541.008536 \nL 670.436228 540.928684 \nL 675.742693 541.167056 \nL 676.627103 541.054643 \nL 678.395925 541.012328 \nL 680.164746 540.617629 \nL 681.049157 540.919971 \nL 684.5868 541.050778 \nL 685.471211 540.923377 \nL 688.124443 540.985842 \nL 690.777675 540.850645 \nL 691.662086 541.008551 \nL 693.430907 541.004019 \nL 694.315318 540.763971 \nL 695.199729 541.081123 \nL 696.96855 541.082835 \nL 697.852961 541.159694 \nL 699.621782 540.873976 \nL 701.390604 541.091639 \nL 704.928247 540.914279 \nL 705.812657 540.793502 \nL 708.46589 541.115129 \nL 709.3503 540.981762 \nL 710.234711 541.014611 \nL 711.119122 540.773404 \nL 712.003533 541.075279 \nL 719.078818 540.649367 \nL 720.84764 540.679022 \nL 721.732051 540.937728 \nL 722.616461 540.955191 \nL 723.500872 540.834178 \nL 725.269694 540.862655 \nL 726.154104 540.869383 \nL 727.038515 541.028795 \nL 727.922926 540.953995 \nL 728.807336 540.67417 \nL 729.691747 540.876903 \nL 731.460569 541.032976 \nL 733.22939 540.657057 \nL 734.113801 540.875394 \nL 734.998212 540.840912 \nL 735.882622 541.014693 \nL 736.767033 540.958695 \nL 737.651444 540.620531 \nL 738.535855 540.544079 \nL 740.304676 540.884295 \nL 742.073497 540.952647 \nL 742.957908 540.682762 \nL 744.72673 540.77378 \nL 745.61114 540.968006 \nL 746.495551 540.805977 \nL 750.033194 540.962994 \nL 750.917605 540.768574 \nL 752.686426 540.962866 \nL 754.455248 540.696597 \nL 756.224069 540.845693 \nL 757.10848 540.904321 \nL 758.877301 540.642697 \nL 762.414944 540.778097 \nL 763.299355 540.652916 \nL 764.183766 540.972032 \nL 765.068177 540.726793 \nL 766.836998 540.692368 \nL 767.721409 540.630055 \nL 768.605819 540.859352 \nL 773.912284 540.798493 \nL 775.681105 540.764256 \nL 776.565516 540.880549 \nL 777.449927 540.786601 \nL 778.334337 540.987964 \nL 779.218748 540.793171 \nL 780.103159 540.892781 \nL 782.756391 540.721407 \nL 784.525213 540.936675 \nL 786.294034 540.717445 \nL 786.294034 540.717445 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Model MSE -->\n    <g transform=\"translate(398.503125 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"481.787109\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_21\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_22\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p440740df64\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "11/11 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}