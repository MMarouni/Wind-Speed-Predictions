{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1)\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1)\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df=pd.read_excel('Pre-Processed-Data.xlsx')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Wind Speed  Pressure  Pressure Grad  Wind Gradient\n",
       "0        2.47    1029.0             -8              3\n",
       "1        7.42    1021.4              0              8\n",
       "2        6.81    1021.8             11              7\n",
       "3        3.94    1033.7             -1              2\n",
       "4        3.33    1033.4            -11              5"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>-11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Wind Speed       0\n",
       "Pressure         0\n",
       "Pressure Grad    0\n",
       "Wind Gradient    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "X = df.drop(['Wind Speed'], axis=1)\n",
    "#Assign the Target column as the output \n",
    "Y= df['Wind Speed']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "X_norm=(X-X.min())/(X.max()-X.min())\n",
    "X_norm"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Pressure  Pressure Grad  Wind Gradient\n",
       "0     0.752055       0.449275       0.200000\n",
       "1     0.647945       0.565217       0.533333\n",
       "2     0.653425       0.724638       0.466667\n",
       "3     0.816438       0.550725       0.133333\n",
       "4     0.812329       0.405797       0.333333\n",
       "...        ...            ...            ...\n",
       "1091  0.706849       0.623188       0.066667\n",
       "1092  0.767123       0.594203       0.200000\n",
       "1093  0.795890       0.594203       0.266667\n",
       "1094  0.831507       0.565217       0.200000\n",
       "1095  0.836986       0.594203       0.200000\n",
       "\n",
       "[1096 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.752055</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.647945</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.653425</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.812329</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.706849</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.795890</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.831507</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.836986</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows Ã— 3 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_norm, Y, test_size=0.3, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=x_train.shape[1], activation=\"sigmoid\", kernel_initializer='normal'))\n",
    "model.add(Dropout(0.2)) #dropping a few neurons for generalizing the model\n",
    "model.add(Dense(1, activation=\"linear\", kernel_initializer='normal'))\n",
    "adam = Adam(learning_rate=1e-3, decay=1e-3)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mse','mae'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "print('Fit model...')\n",
    "filepath=\"/home/m-marouni/Documents/CE-901/Heathrow/best_weights\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_mae', patience=100, verbose=1, mode='min')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "log = model.fit(x_train, y_train,\n",
    "          validation_split=0.40, batch_size=30, epochs=1000, shuffle=True, callbacks=callbacks_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fit model...\n",
      "Epoch 1/1000\n",
      "16/16 [==============================] - 1s 17ms/step - loss: 43.3544 - mse: 43.3544 - mae: 6.0389 - val_loss: 36.5055 - val_mse: 36.5055 - val_mae: 5.5334\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 5.53338, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 2/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 41.1034 - mse: 41.1034 - mae: 5.8869 - val_loss: 33.4959 - val_mse: 33.4959 - val_mae: 5.2544\n",
      "\n",
      "Epoch 00002: val_mae improved from 5.53338 to 5.25443, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 3/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 37.9761 - mse: 37.9761 - mae: 5.6370 - val_loss: 30.6430 - val_mse: 30.6430 - val_mae: 4.9756\n",
      "\n",
      "Epoch 00003: val_mae improved from 5.25443 to 4.97560, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 4/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 31.8544 - mse: 31.8544 - mae: 5.1166 - val_loss: 27.9334 - val_mse: 27.9334 - val_mae: 4.6955\n",
      "\n",
      "Epoch 00004: val_mae improved from 4.97560 to 4.69550, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 5/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 30.9908 - mse: 30.9908 - mae: 4.9938 - val_loss: 25.3090 - val_mse: 25.3090 - val_mae: 4.4082\n",
      "\n",
      "Epoch 00005: val_mae improved from 4.69550 to 4.40818, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 6/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 26.8888 - mse: 26.8888 - mae: 4.5722 - val_loss: 22.8497 - val_mse: 22.8497 - val_mae: 4.1218\n",
      "\n",
      "Epoch 00006: val_mae improved from 4.40818 to 4.12179, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 7/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 25.3131 - mse: 25.3131 - mae: 4.3606 - val_loss: 20.4904 - val_mse: 20.4904 - val_mae: 3.8275\n",
      "\n",
      "Epoch 00007: val_mae improved from 4.12179 to 3.82750, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 8/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 23.3674 - mse: 23.3674 - mae: 4.1382 - val_loss: 18.2822 - val_mse: 18.2822 - val_mae: 3.5342\n",
      "\n",
      "Epoch 00008: val_mae improved from 3.82750 to 3.53423, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 9/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.8219 - mse: 20.8219 - mae: 3.7709 - val_loss: 16.2216 - val_mse: 16.2216 - val_mae: 3.2423\n",
      "\n",
      "Epoch 00009: val_mae improved from 3.53423 to 3.24227, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 10/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 16.3663 - mse: 16.3663 - mae: 3.2565 - val_loss: 14.4035 - val_mse: 14.4035 - val_mae: 2.9757\n",
      "\n",
      "Epoch 00010: val_mae improved from 3.24227 to 2.97567, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 11/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 15.7765 - mse: 15.7765 - mae: 3.1580 - val_loss: 12.7280 - val_mse: 12.7280 - val_mae: 2.7328\n",
      "\n",
      "Epoch 00011: val_mae improved from 2.97567 to 2.73282, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 12/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 14.2888 - mse: 14.2888 - mae: 2.8921 - val_loss: 11.2795 - val_mse: 11.2795 - val_mae: 2.5334\n",
      "\n",
      "Epoch 00012: val_mae improved from 2.73282 to 2.53337, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 13/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.0367 - mse: 13.0367 - mae: 2.6939 - val_loss: 10.0539 - val_mse: 10.0539 - val_mae: 2.3688\n",
      "\n",
      "Epoch 00013: val_mae improved from 2.53337 to 2.36879, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 14/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 10.1733 - mse: 10.1733 - mae: 2.4095 - val_loss: 9.0366 - val_mse: 9.0366 - val_mae: 2.2439\n",
      "\n",
      "Epoch 00014: val_mae improved from 2.36879 to 2.24394, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 15/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.7432 - mse: 9.7432 - mae: 2.2753 - val_loss: 8.2137 - val_mse: 8.2137 - val_mae: 2.1478\n",
      "\n",
      "Epoch 00015: val_mae improved from 2.24394 to 2.14777, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 16/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.7800 - mse: 9.7800 - mae: 2.3307 - val_loss: 7.5369 - val_mse: 7.5369 - val_mae: 2.0740\n",
      "\n",
      "Epoch 00016: val_mae improved from 2.14777 to 2.07397, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 17/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 10.1273 - mse: 10.1273 - mae: 2.2713 - val_loss: 7.0232 - val_mse: 7.0232 - val_mae: 2.0209\n",
      "\n",
      "Epoch 00017: val_mae improved from 2.07397 to 2.02091, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 18/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.2929 - mse: 7.2929 - mae: 2.0353 - val_loss: 6.6493 - val_mse: 6.6493 - val_mae: 1.9838\n",
      "\n",
      "Epoch 00018: val_mae improved from 2.02091 to 1.98385, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 19/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.3169 - mse: 8.3169 - mae: 2.1020 - val_loss: 6.3507 - val_mse: 6.3507 - val_mae: 1.9559\n",
      "\n",
      "Epoch 00019: val_mae improved from 1.98385 to 1.95591, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 20/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6226 - mse: 6.6226 - mae: 1.9667 - val_loss: 6.1525 - val_mse: 6.1525 - val_mae: 1.9480\n",
      "\n",
      "Epoch 00020: val_mae improved from 1.95591 to 1.94803, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 21/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.3564 - mse: 8.3564 - mae: 2.1289 - val_loss: 6.0115 - val_mse: 6.0115 - val_mae: 1.9465\n",
      "\n",
      "Epoch 00021: val_mae improved from 1.94803 to 1.94653, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 22/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1955 - mse: 6.1955 - mae: 1.9235 - val_loss: 5.9322 - val_mse: 5.9322 - val_mae: 1.9462\n",
      "\n",
      "Epoch 00022: val_mae improved from 1.94653 to 1.94625, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 23/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.4621 - mse: 7.4621 - mae: 2.0780 - val_loss: 5.8701 - val_mse: 5.8701 - val_mae: 1.9498\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 1.94625\n",
      "Epoch 24/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9024 - mse: 6.9024 - mae: 2.0646 - val_loss: 5.8418 - val_mse: 5.8418 - val_mae: 1.9543\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 1.94625\n",
      "Epoch 25/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0080 - mse: 6.0080 - mae: 1.9322 - val_loss: 5.8289 - val_mse: 5.8289 - val_mae: 1.9585\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 1.94625\n",
      "Epoch 26/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1187 - mse: 6.1187 - mae: 1.9157 - val_loss: 5.8236 - val_mse: 5.8236 - val_mae: 1.9634\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 1.94625\n",
      "Epoch 27/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4017 - mse: 6.4017 - mae: 1.9905 - val_loss: 5.8251 - val_mse: 5.8251 - val_mae: 1.9688\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 1.94625\n",
      "Epoch 28/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.5346 - mse: 7.5346 - mae: 2.1182 - val_loss: 5.8311 - val_mse: 5.8311 - val_mae: 1.9741\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 1.94625\n",
      "Epoch 29/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7999 - mse: 5.7999 - mae: 1.8576 - val_loss: 5.8389 - val_mse: 5.8389 - val_mae: 1.9787\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 1.94625\n",
      "Epoch 30/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2865 - mse: 6.2865 - mae: 2.0309 - val_loss: 5.8436 - val_mse: 5.8436 - val_mae: 1.9815\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 1.94625\n",
      "Epoch 31/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8319 - mse: 6.8319 - mae: 2.0456 - val_loss: 5.8484 - val_mse: 5.8484 - val_mae: 1.9842\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 1.94625\n",
      "Epoch 32/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7267 - mse: 6.7267 - mae: 2.0037 - val_loss: 5.8468 - val_mse: 5.8468 - val_mae: 1.9845\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 1.94625\n",
      "Epoch 33/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6281 - mse: 5.6281 - mae: 1.8585 - val_loss: 5.8508 - val_mse: 5.8508 - val_mae: 1.9866\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 1.94625\n",
      "Epoch 34/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5478 - mse: 6.5478 - mae: 2.0112 - val_loss: 5.8531 - val_mse: 5.8531 - val_mae: 1.9881\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 1.94625\n",
      "Epoch 35/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9906 - mse: 6.9906 - mae: 2.1176 - val_loss: 5.8503 - val_mse: 5.8503 - val_mae: 1.9880\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 1.94625\n",
      "Epoch 36/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2283 - mse: 5.2283 - mae: 1.8184 - val_loss: 5.8459 - val_mse: 5.8459 - val_mae: 1.9872\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 1.94625\n",
      "Epoch 37/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1888 - mse: 6.1888 - mae: 1.9699 - val_loss: 5.8450 - val_mse: 5.8450 - val_mae: 1.9878\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 1.94625\n",
      "Epoch 38/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5892 - mse: 6.5892 - mae: 2.0723 - val_loss: 5.8408 - val_mse: 5.8408 - val_mae: 1.9872\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 1.94625\n",
      "Epoch 39/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3586 - mse: 6.3586 - mae: 2.0139 - val_loss: 5.8437 - val_mse: 5.8437 - val_mae: 1.9890\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 1.94625\n",
      "Epoch 40/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7799 - mse: 5.7799 - mae: 1.9063 - val_loss: 5.8429 - val_mse: 5.8429 - val_mae: 1.9896\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 1.94625\n",
      "Epoch 41/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7730 - mse: 6.7730 - mae: 2.0847 - val_loss: 5.8376 - val_mse: 5.8376 - val_mae: 1.9886\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 1.94625\n",
      "Epoch 42/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9467 - mse: 5.9467 - mae: 1.9615 - val_loss: 5.8181 - val_mse: 5.8181 - val_mae: 1.9831\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 1.94625\n",
      "Epoch 43/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2308 - mse: 6.2308 - mae: 1.9718 - val_loss: 5.8295 - val_mse: 5.8295 - val_mae: 1.9876\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 1.94625\n",
      "Epoch 44/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.1333 - mse: 7.1333 - mae: 2.1138 - val_loss: 5.8307 - val_mse: 5.8307 - val_mae: 1.9889\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 1.94625\n",
      "Epoch 45/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4014 - mse: 6.4014 - mae: 2.0063 - val_loss: 5.8196 - val_mse: 5.8196 - val_mae: 1.9861\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 1.94625\n",
      "Epoch 46/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8784 - mse: 6.8784 - mae: 2.0800 - val_loss: 5.8169 - val_mse: 5.8169 - val_mae: 1.9860\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 1.94625\n",
      "Epoch 47/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4506 - mse: 6.4506 - mae: 2.0273 - val_loss: 5.8052 - val_mse: 5.8052 - val_mae: 1.9831\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 1.94625\n",
      "Epoch 48/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6710 - mse: 6.6710 - mae: 2.0296 - val_loss: 5.7929 - val_mse: 5.7929 - val_mae: 1.9800\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 1.94625\n",
      "Epoch 49/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4003 - mse: 6.4003 - mae: 2.0058 - val_loss: 5.7865 - val_mse: 5.7865 - val_mae: 1.9787\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 1.94625\n",
      "Epoch 50/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3943 - mse: 6.3943 - mae: 2.0159 - val_loss: 5.7815 - val_mse: 5.7815 - val_mae: 1.9781\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 1.94625\n",
      "Epoch 51/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7648 - mse: 6.7648 - mae: 2.0264 - val_loss: 5.7699 - val_mse: 5.7699 - val_mae: 1.9751\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 1.94625\n",
      "Epoch 52/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.1211 - mse: 7.1211 - mae: 2.1074 - val_loss: 5.7647 - val_mse: 5.7647 - val_mae: 1.9744\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 1.94625\n",
      "Epoch 53/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0159 - mse: 6.0159 - mae: 1.9555 - val_loss: 5.7551 - val_mse: 5.7551 - val_mae: 1.9721\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 1.94625\n",
      "Epoch 54/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6665 - mse: 6.6665 - mae: 2.0079 - val_loss: 5.7537 - val_mse: 5.7537 - val_mae: 1.9725\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 1.94625\n",
      "Epoch 55/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1554 - mse: 6.1554 - mae: 1.9390 - val_loss: 5.7410 - val_mse: 5.7410 - val_mae: 1.9693\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 1.94625\n",
      "Epoch 56/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.2526 - mse: 7.2526 - mae: 2.0461 - val_loss: 5.7443 - val_mse: 5.7443 - val_mae: 1.9714\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 1.94625\n",
      "Epoch 57/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9182 - mse: 5.9182 - mae: 1.9352 - val_loss: 5.7426 - val_mse: 5.7426 - val_mae: 1.9718\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 1.94625\n",
      "Epoch 58/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2299 - mse: 6.2299 - mae: 2.0265 - val_loss: 5.7403 - val_mse: 5.7403 - val_mae: 1.9720\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 1.94625\n",
      "Epoch 59/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3893 - mse: 6.3893 - mae: 2.0182 - val_loss: 5.7368 - val_mse: 5.7368 - val_mae: 1.9718\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 1.94625\n",
      "Epoch 60/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7462 - mse: 6.7462 - mae: 2.0989 - val_loss: 5.7318 - val_mse: 5.7318 - val_mae: 1.9712\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 1.94625\n",
      "Epoch 61/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8830 - mse: 6.8830 - mae: 2.0253 - val_loss: 5.7313 - val_mse: 5.7313 - val_mae: 1.9719\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 1.94625\n",
      "Epoch 62/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8161 - mse: 5.8161 - mae: 1.9528 - val_loss: 5.7190 - val_mse: 5.7190 - val_mae: 1.9689\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 1.94625\n",
      "Epoch 63/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.3469 - mse: 6.3469 - mae: 2.0005 - val_loss: 5.7146 - val_mse: 5.7146 - val_mae: 1.9684\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 1.94625\n",
      "Epoch 64/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8305 - mse: 5.8305 - mae: 1.9179 - val_loss: 5.7169 - val_mse: 5.7169 - val_mae: 1.9701\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 1.94625\n",
      "Epoch 65/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7217 - mse: 6.7217 - mae: 2.0791 - val_loss: 5.6902 - val_mse: 5.6902 - val_mae: 1.9625\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 1.94625\n",
      "Epoch 66/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9859 - mse: 6.9859 - mae: 2.1266 - val_loss: 5.6792 - val_mse: 5.6792 - val_mae: 1.9599\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 1.94625\n",
      "Epoch 67/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8003 - mse: 5.8003 - mae: 1.8863 - val_loss: 5.6595 - val_mse: 5.6595 - val_mae: 1.9541\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 1.94625\n",
      "Epoch 68/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6033 - mse: 5.6033 - mae: 1.8986 - val_loss: 5.6518 - val_mse: 5.6518 - val_mae: 1.9526\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 1.94625\n",
      "Epoch 69/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8195 - mse: 5.8195 - mae: 1.9625 - val_loss: 5.6492 - val_mse: 5.6492 - val_mae: 1.9529\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 1.94625\n",
      "Epoch 70/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4986 - mse: 6.4986 - mae: 2.0379 - val_loss: 5.6582 - val_mse: 5.6582 - val_mae: 1.9573\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 1.94625\n",
      "Epoch 71/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7379 - mse: 6.7379 - mae: 1.9317 - val_loss: 5.6484 - val_mse: 5.6484 - val_mae: 1.9550\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 1.94625\n",
      "Epoch 72/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1286 - mse: 6.1286 - mae: 1.9778 - val_loss: 5.6438 - val_mse: 5.6438 - val_mae: 1.9546\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 1.94625\n",
      "Epoch 73/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8270 - mse: 5.8270 - mae: 1.8720 - val_loss: 5.6554 - val_mse: 5.6554 - val_mae: 1.9594\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 1.94625\n",
      "Epoch 74/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5592 - mse: 5.5592 - mae: 1.8744 - val_loss: 5.6386 - val_mse: 5.6386 - val_mae: 1.9549\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 1.94625\n",
      "Epoch 75/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8020 - mse: 6.8020 - mae: 2.0802 - val_loss: 5.6432 - val_mse: 5.6432 - val_mae: 1.9574\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 1.94625\n",
      "Epoch 76/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9325 - mse: 5.9325 - mae: 1.9156 - val_loss: 5.6463 - val_mse: 5.6463 - val_mae: 1.9592\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 1.94625\n",
      "Epoch 77/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9009 - mse: 6.9009 - mae: 2.1250 - val_loss: 5.6176 - val_mse: 5.6176 - val_mae: 1.9512\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 1.94625\n",
      "Epoch 78/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8093 - mse: 5.8093 - mae: 1.8908 - val_loss: 5.6056 - val_mse: 5.6056 - val_mae: 1.9483\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 1.94625\n",
      "Epoch 79/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1593 - mse: 6.1593 - mae: 1.9093 - val_loss: 5.5994 - val_mse: 5.5994 - val_mae: 1.9473\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 1.94625\n",
      "Epoch 80/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0507 - mse: 6.0507 - mae: 1.9167 - val_loss: 5.5986 - val_mse: 5.5986 - val_mae: 1.9482\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 1.94625\n",
      "Epoch 81/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4142 - mse: 6.4142 - mae: 1.9944 - val_loss: 5.5912 - val_mse: 5.5912 - val_mae: 1.9470\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 1.94625\n",
      "Epoch 82/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9127 - mse: 6.9127 - mae: 2.0963 - val_loss: 5.5872 - val_mse: 5.5872 - val_mae: 1.9467\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 1.94625\n",
      "Epoch 83/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0531 - mse: 6.0531 - mae: 1.9516 - val_loss: 5.5697 - val_mse: 5.5697 - val_mae: 1.9420\n",
      "\n",
      "Epoch 00083: val_mae improved from 1.94625 to 1.94196, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 84/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4427 - mse: 6.4427 - mae: 2.0567 - val_loss: 5.5605 - val_mse: 5.5605 - val_mae: 1.9401\n",
      "\n",
      "Epoch 00084: val_mae improved from 1.94196 to 1.94011, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 85/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6923 - mse: 6.6923 - mae: 1.9864 - val_loss: 5.5498 - val_mse: 5.5498 - val_mae: 1.9377\n",
      "\n",
      "Epoch 00085: val_mae improved from 1.94011 to 1.93770, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 86/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6865 - mse: 5.6865 - mae: 1.9250 - val_loss: 5.5342 - val_mse: 5.5342 - val_mae: 1.9334\n",
      "\n",
      "Epoch 00086: val_mae improved from 1.93770 to 1.93344, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 87/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9979 - mse: 6.9979 - mae: 2.0312 - val_loss: 5.5281 - val_mse: 5.5281 - val_mae: 1.9326\n",
      "\n",
      "Epoch 00087: val_mae improved from 1.93344 to 1.93264, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 88/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5913 - mse: 5.5913 - mae: 1.8956 - val_loss: 5.5077 - val_mse: 5.5077 - val_mae: 1.9263\n",
      "\n",
      "Epoch 00088: val_mae improved from 1.93264 to 1.92630, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 89/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3490 - mse: 6.3490 - mae: 2.0076 - val_loss: 5.5202 - val_mse: 5.5202 - val_mae: 1.9325\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 1.92630\n",
      "Epoch 90/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3136 - mse: 6.3136 - mae: 1.9529 - val_loss: 5.5293 - val_mse: 5.5293 - val_mae: 1.9369\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 1.92630\n",
      "Epoch 91/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1304 - mse: 6.1304 - mae: 1.9524 - val_loss: 5.5148 - val_mse: 5.5148 - val_mae: 1.9333\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 1.92630\n",
      "Epoch 92/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.8541 - mse: 6.8541 - mae: 2.0328 - val_loss: 5.5037 - val_mse: 5.5037 - val_mae: 1.9308\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 1.92630\n",
      "Epoch 93/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4814 - mse: 6.4814 - mae: 1.9946 - val_loss: 5.4863 - val_mse: 5.4863 - val_mae: 1.9260\n",
      "\n",
      "Epoch 00093: val_mae improved from 1.92630 to 1.92597, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 94/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8055 - mse: 6.8055 - mae: 2.0421 - val_loss: 5.4793 - val_mse: 5.4793 - val_mae: 1.9248\n",
      "\n",
      "Epoch 00094: val_mae improved from 1.92597 to 1.92483, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 95/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6087 - mse: 5.6087 - mae: 1.8830 - val_loss: 5.4678 - val_mse: 5.4678 - val_mae: 1.9220\n",
      "\n",
      "Epoch 00095: val_mae improved from 1.92483 to 1.92202, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 96/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1425 - mse: 6.1425 - mae: 1.9928 - val_loss: 5.4707 - val_mse: 5.4707 - val_mae: 1.9245\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 1.92202\n",
      "Epoch 97/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2721 - mse: 5.2721 - mae: 1.8586 - val_loss: 5.4587 - val_mse: 5.4587 - val_mae: 1.9216\n",
      "\n",
      "Epoch 00097: val_mae improved from 1.92202 to 1.92157, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 98/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7543 - mse: 5.7543 - mae: 1.8777 - val_loss: 5.4585 - val_mse: 5.4585 - val_mae: 1.9227\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 1.92157\n",
      "Epoch 99/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3292 - mse: 6.3292 - mae: 1.9659 - val_loss: 5.4483 - val_mse: 5.4483 - val_mae: 1.9205\n",
      "\n",
      "Epoch 00099: val_mae improved from 1.92157 to 1.92050, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 100/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8918 - mse: 5.8918 - mae: 1.9423 - val_loss: 5.4325 - val_mse: 5.4325 - val_mae: 1.9163\n",
      "\n",
      "Epoch 00100: val_mae improved from 1.92050 to 1.91632, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 101/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.7315 - mse: 6.7315 - mae: 2.0651 - val_loss: 5.4420 - val_mse: 5.4420 - val_mae: 1.9209\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 1.91632\n",
      "Epoch 102/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9737 - mse: 5.9737 - mae: 1.9321 - val_loss: 5.4293 - val_mse: 5.4293 - val_mae: 1.9179\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 1.91632\n",
      "Epoch 103/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0084 - mse: 6.0084 - mae: 1.9307 - val_loss: 5.4274 - val_mse: 5.4274 - val_mae: 1.9186\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 1.91632\n",
      "Epoch 104/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1363 - mse: 6.1363 - mae: 1.9776 - val_loss: 5.4067 - val_mse: 5.4067 - val_mae: 1.9128\n",
      "\n",
      "Epoch 00104: val_mae improved from 1.91632 to 1.91283, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 105/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.2779 - mse: 6.2779 - mae: 2.0012 - val_loss: 5.4055 - val_mse: 5.4055 - val_mae: 1.9139\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 1.91283\n",
      "Epoch 106/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.5235 - mse: 6.5235 - mae: 2.0329 - val_loss: 5.4076 - val_mse: 5.4076 - val_mae: 1.9157\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 1.91283\n",
      "Epoch 107/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9402 - mse: 6.9402 - mae: 2.0591 - val_loss: 5.4018 - val_mse: 5.4018 - val_mae: 1.9150\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 1.91283\n",
      "Epoch 108/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0380 - mse: 6.0380 - mae: 1.9369 - val_loss: 5.3766 - val_mse: 5.3766 - val_mae: 1.9079\n",
      "\n",
      "Epoch 00108: val_mae improved from 1.91283 to 1.90791, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 109/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5724 - mse: 5.5724 - mae: 1.8870 - val_loss: 5.3526 - val_mse: 5.3526 - val_mae: 1.9010\n",
      "\n",
      "Epoch 00109: val_mae improved from 1.90791 to 1.90097, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 110/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3910 - mse: 6.3910 - mae: 1.9631 - val_loss: 5.3789 - val_mse: 5.3789 - val_mae: 1.9111\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 1.90097\n",
      "Epoch 111/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1520 - mse: 5.1520 - mae: 1.7843 - val_loss: 5.3708 - val_mse: 5.3708 - val_mae: 1.9096\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 1.90097\n",
      "Epoch 112/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2338 - mse: 6.2338 - mae: 1.9304 - val_loss: 5.3825 - val_mse: 5.3825 - val_mae: 1.9139\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 1.90097\n",
      "Epoch 113/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0191 - mse: 6.0191 - mae: 1.9359 - val_loss: 5.3473 - val_mse: 5.3473 - val_mae: 1.9044\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 1.90097\n",
      "Epoch 114/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7601 - mse: 6.7601 - mae: 1.9876 - val_loss: 5.3351 - val_mse: 5.3351 - val_mae: 1.9015\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 1.90097\n",
      "Epoch 115/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7062 - mse: 5.7062 - mae: 1.9268 - val_loss: 5.3113 - val_mse: 5.3113 - val_mae: 1.8949\n",
      "\n",
      "Epoch 00115: val_mae improved from 1.90097 to 1.89485, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 116/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7585 - mse: 5.7585 - mae: 1.9468 - val_loss: 5.2912 - val_mse: 5.2912 - val_mae: 1.8891\n",
      "\n",
      "Epoch 00116: val_mae improved from 1.89485 to 1.88909, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 117/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0467 - mse: 6.0467 - mae: 1.9569 - val_loss: 5.2981 - val_mse: 5.2981 - val_mae: 1.8930\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 1.88909\n",
      "Epoch 118/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4237 - mse: 5.4237 - mae: 1.8373 - val_loss: 5.3082 - val_mse: 5.3082 - val_mae: 1.8975\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 1.88909\n",
      "Epoch 119/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5723 - mse: 6.5723 - mae: 2.0094 - val_loss: 5.3030 - val_mse: 5.3030 - val_mae: 1.8969\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 1.88909\n",
      "Epoch 120/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3490 - mse: 6.3490 - mae: 1.9258 - val_loss: 5.3015 - val_mse: 5.3015 - val_mae: 1.8974\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 1.88909\n",
      "Epoch 121/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0093 - mse: 6.0093 - mae: 1.9637 - val_loss: 5.3009 - val_mse: 5.3009 - val_mae: 1.8981\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 1.88909\n",
      "Epoch 122/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6880 - mse: 6.6880 - mae: 2.0155 - val_loss: 5.2630 - val_mse: 5.2630 - val_mae: 1.8875\n",
      "\n",
      "Epoch 00122: val_mae improved from 1.88909 to 1.88751, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 123/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8057 - mse: 5.8057 - mae: 1.9294 - val_loss: 5.2368 - val_mse: 5.2368 - val_mae: 1.8799\n",
      "\n",
      "Epoch 00123: val_mae improved from 1.88751 to 1.87995, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 124/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0402 - mse: 6.0402 - mae: 1.9365 - val_loss: 5.2364 - val_mse: 5.2364 - val_mae: 1.8813\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 1.87995\n",
      "Epoch 125/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8816 - mse: 5.8816 - mae: 1.9172 - val_loss: 5.2380 - val_mse: 5.2380 - val_mae: 1.8831\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 1.87995\n",
      "Epoch 126/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9957 - mse: 5.9957 - mae: 1.9692 - val_loss: 5.2488 - val_mse: 5.2488 - val_mae: 1.8876\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 1.87995\n",
      "Epoch 127/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4948 - mse: 5.4948 - mae: 1.8865 - val_loss: 5.2258 - val_mse: 5.2258 - val_mae: 1.8816\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 1.87995\n",
      "Epoch 128/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5615 - mse: 5.5615 - mae: 1.9049 - val_loss: 5.2150 - val_mse: 5.2150 - val_mae: 1.8794\n",
      "\n",
      "Epoch 00128: val_mae improved from 1.87995 to 1.87938, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 129/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7169 - mse: 5.7169 - mae: 1.9231 - val_loss: 5.2094 - val_mse: 5.2094 - val_mae: 1.8786\n",
      "\n",
      "Epoch 00129: val_mae improved from 1.87938 to 1.87865, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 130/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2253 - mse: 5.2253 - mae: 1.8260 - val_loss: 5.1888 - val_mse: 5.1888 - val_mae: 1.8733\n",
      "\n",
      "Epoch 00130: val_mae improved from 1.87865 to 1.87328, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 131/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5225 - mse: 5.5225 - mae: 1.8881 - val_loss: 5.1836 - val_mse: 5.1836 - val_mae: 1.8729\n",
      "\n",
      "Epoch 00131: val_mae improved from 1.87328 to 1.87288, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 132/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2483 - mse: 6.2483 - mae: 1.9801 - val_loss: 5.1850 - val_mse: 5.1850 - val_mae: 1.8744\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 1.87288\n",
      "Epoch 133/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8307 - mse: 5.8307 - mae: 1.9474 - val_loss: 5.1499 - val_mse: 5.1499 - val_mae: 1.8643\n",
      "\n",
      "Epoch 00133: val_mae improved from 1.87288 to 1.86432, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 134/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2207 - mse: 6.2207 - mae: 1.9331 - val_loss: 5.1482 - val_mse: 5.1482 - val_mae: 1.8650\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 1.86432\n",
      "Epoch 135/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6734 - mse: 5.6734 - mae: 1.8795 - val_loss: 5.1331 - val_mse: 5.1331 - val_mae: 1.8613\n",
      "\n",
      "Epoch 00135: val_mae improved from 1.86432 to 1.86128, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 136/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8188 - mse: 5.8188 - mae: 1.8805 - val_loss: 5.1266 - val_mse: 5.1266 - val_mae: 1.8604\n",
      "\n",
      "Epoch 00136: val_mae improved from 1.86128 to 1.86037, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 137/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4447 - mse: 5.4447 - mae: 1.8704 - val_loss: 5.1174 - val_mse: 5.1174 - val_mae: 1.8585\n",
      "\n",
      "Epoch 00137: val_mae improved from 1.86037 to 1.85850, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 138/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9007 - mse: 6.9007 - mae: 2.0332 - val_loss: 5.1205 - val_mse: 5.1205 - val_mae: 1.8608\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 1.85850\n",
      "Epoch 139/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1686 - mse: 6.1686 - mae: 1.8886 - val_loss: 5.1117 - val_mse: 5.1117 - val_mae: 1.8592\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 1.85850\n",
      "Epoch 140/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8149 - mse: 5.8149 - mae: 1.9464 - val_loss: 5.0911 - val_mse: 5.0911 - val_mae: 1.8537\n",
      "\n",
      "Epoch 00140: val_mae improved from 1.85850 to 1.85368, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 141/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5118 - mse: 5.5118 - mae: 1.8464 - val_loss: 5.0833 - val_mse: 5.0833 - val_mae: 1.8524\n",
      "\n",
      "Epoch 00141: val_mae improved from 1.85368 to 1.85242, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 142/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8847 - mse: 5.8847 - mae: 1.9761 - val_loss: 5.0639 - val_mse: 5.0639 - val_mae: 1.8469\n",
      "\n",
      "Epoch 00142: val_mae improved from 1.85242 to 1.84693, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 143/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.8025 - mse: 5.8025 - mae: 1.9175 - val_loss: 5.0486 - val_mse: 5.0486 - val_mae: 1.8429\n",
      "\n",
      "Epoch 00143: val_mae improved from 1.84693 to 1.84287, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 144/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2487 - mse: 6.2487 - mae: 1.9317 - val_loss: 5.0462 - val_mse: 5.0462 - val_mae: 1.8437\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 1.84287\n",
      "Epoch 145/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4067 - mse: 5.4067 - mae: 1.8373 - val_loss: 5.0420 - val_mse: 5.0420 - val_mae: 1.8438\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 1.84287\n",
      "Epoch 146/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5017 - mse: 5.5017 - mae: 1.8118 - val_loss: 5.0334 - val_mse: 5.0334 - val_mae: 1.8421\n",
      "\n",
      "Epoch 00146: val_mae improved from 1.84287 to 1.84211, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 147/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7702 - mse: 5.7702 - mae: 1.8802 - val_loss: 5.0270 - val_mse: 5.0270 - val_mae: 1.8414\n",
      "\n",
      "Epoch 00147: val_mae improved from 1.84211 to 1.84139, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 148/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5837 - mse: 5.5837 - mae: 1.8449 - val_loss: 5.0227 - val_mse: 5.0227 - val_mae: 1.8413\n",
      "\n",
      "Epoch 00148: val_mae improved from 1.84139 to 1.84128, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 149/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6560 - mse: 5.6560 - mae: 1.9030 - val_loss: 5.0115 - val_mse: 5.0115 - val_mae: 1.8389\n",
      "\n",
      "Epoch 00149: val_mae improved from 1.84128 to 1.83888, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 150/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2900 - mse: 5.2900 - mae: 1.8068 - val_loss: 5.0020 - val_mse: 5.0020 - val_mae: 1.8370\n",
      "\n",
      "Epoch 00150: val_mae improved from 1.83888 to 1.83695, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 151/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0574 - mse: 6.0574 - mae: 1.9037 - val_loss: 4.9944 - val_mse: 4.9944 - val_mae: 1.8357\n",
      "\n",
      "Epoch 00151: val_mae improved from 1.83695 to 1.83567, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 152/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9309 - mse: 5.9309 - mae: 1.9422 - val_loss: 4.9906 - val_mse: 4.9906 - val_mae: 1.8358\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 1.83567\n",
      "Epoch 153/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6835 - mse: 5.6835 - mae: 1.8939 - val_loss: 4.9710 - val_mse: 4.9710 - val_mae: 1.8304\n",
      "\n",
      "Epoch 00153: val_mae improved from 1.83567 to 1.83041, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 154/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0405 - mse: 6.0405 - mae: 1.9284 - val_loss: 4.9570 - val_mse: 4.9570 - val_mae: 1.8268\n",
      "\n",
      "Epoch 00154: val_mae improved from 1.83041 to 1.82683, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 155/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1604 - mse: 5.1604 - mae: 1.8143 - val_loss: 4.9454 - val_mse: 4.9454 - val_mae: 1.8241\n",
      "\n",
      "Epoch 00155: val_mae improved from 1.82683 to 1.82415, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 156/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0548 - mse: 6.0548 - mae: 1.9421 - val_loss: 4.9395 - val_mse: 4.9395 - val_mae: 1.8235\n",
      "\n",
      "Epoch 00156: val_mae improved from 1.82415 to 1.82351, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 157/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4726 - mse: 5.4726 - mae: 1.8722 - val_loss: 4.9412 - val_mse: 4.9412 - val_mae: 1.8260\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 1.82351\n",
      "Epoch 158/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8793 - mse: 5.8793 - mae: 1.8797 - val_loss: 4.9448 - val_mse: 4.9448 - val_mae: 1.8286\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 1.82351\n",
      "Epoch 159/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7340 - mse: 5.7340 - mae: 1.8807 - val_loss: 4.9337 - val_mse: 4.9337 - val_mae: 1.8264\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 1.82351\n",
      "Epoch 160/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4446 - mse: 5.4446 - mae: 1.8344 - val_loss: 4.9285 - val_mse: 4.9285 - val_mae: 1.8260\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 1.82351\n",
      "Epoch 161/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4042 - mse: 6.4042 - mae: 2.0139 - val_loss: 4.9234 - val_mse: 4.9234 - val_mae: 1.8257\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 1.82351\n",
      "Epoch 162/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9781 - mse: 5.9781 - mae: 1.9443 - val_loss: 4.8955 - val_mse: 4.8955 - val_mae: 1.8172\n",
      "\n",
      "Epoch 00162: val_mae improved from 1.82351 to 1.81724, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 163/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1686 - mse: 6.1686 - mae: 1.9303 - val_loss: 4.8847 - val_mse: 4.8847 - val_mae: 1.8148\n",
      "\n",
      "Epoch 00163: val_mae improved from 1.81724 to 1.81482, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 164/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9850 - mse: 5.9850 - mae: 1.9201 - val_loss: 4.8687 - val_mse: 4.8687 - val_mae: 1.8107\n",
      "\n",
      "Epoch 00164: val_mae improved from 1.81482 to 1.81067, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 165/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.6021 - mse: 6.6021 - mae: 1.9648 - val_loss: 4.8654 - val_mse: 4.8654 - val_mae: 1.8111\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 1.81067\n",
      "Epoch 166/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6837 - mse: 5.6837 - mae: 1.9201 - val_loss: 4.8435 - val_mse: 4.8435 - val_mae: 1.8047\n",
      "\n",
      "Epoch 00166: val_mae improved from 1.81067 to 1.80473, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 167/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.3175 - mse: 6.3175 - mae: 1.9168 - val_loss: 4.8352 - val_mse: 4.8352 - val_mae: 1.8032\n",
      "\n",
      "Epoch 00167: val_mae improved from 1.80473 to 1.80319, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 168/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3112 - mse: 5.3112 - mae: 1.8441 - val_loss: 4.8176 - val_mse: 4.8176 - val_mae: 1.7983\n",
      "\n",
      "Epoch 00168: val_mae improved from 1.80319 to 1.79826, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 169/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6141 - mse: 5.6141 - mae: 1.8723 - val_loss: 4.8127 - val_mse: 4.8127 - val_mae: 1.7982\n",
      "\n",
      "Epoch 00169: val_mae improved from 1.79826 to 1.79819, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 170/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.3524 - mse: 5.3524 - mae: 1.8273 - val_loss: 4.7949 - val_mse: 4.7949 - val_mae: 1.7930\n",
      "\n",
      "Epoch 00170: val_mae improved from 1.79819 to 1.79297, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 171/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1228 - mse: 5.1228 - mae: 1.7925 - val_loss: 4.7835 - val_mse: 4.7835 - val_mae: 1.7903\n",
      "\n",
      "Epoch 00171: val_mae improved from 1.79297 to 1.79028, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 172/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7643 - mse: 5.7643 - mae: 1.8716 - val_loss: 4.7903 - val_mse: 4.7903 - val_mae: 1.7947\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 1.79028\n",
      "Epoch 173/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0113 - mse: 6.0113 - mae: 1.8876 - val_loss: 4.7984 - val_mse: 4.7984 - val_mae: 1.7993\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 1.79028\n",
      "Epoch 174/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5976 - mse: 5.5976 - mae: 1.8584 - val_loss: 4.7702 - val_mse: 4.7702 - val_mae: 1.7903\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 1.79028\n",
      "Epoch 175/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7019 - mse: 5.7019 - mae: 1.9076 - val_loss: 4.7641 - val_mse: 4.7641 - val_mae: 1.7897\n",
      "\n",
      "Epoch 00175: val_mae improved from 1.79028 to 1.78968, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 176/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8637 - mse: 5.8637 - mae: 1.9243 - val_loss: 4.7436 - val_mse: 4.7436 - val_mae: 1.7835\n",
      "\n",
      "Epoch 00176: val_mae improved from 1.78968 to 1.78350, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 177/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4541 - mse: 5.4541 - mae: 1.8452 - val_loss: 4.7360 - val_mse: 4.7360 - val_mae: 1.7823\n",
      "\n",
      "Epoch 00177: val_mae improved from 1.78350 to 1.78231, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 178/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9848 - mse: 5.9848 - mae: 1.9135 - val_loss: 4.7341 - val_mse: 4.7341 - val_mae: 1.7833\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 1.78231\n",
      "Epoch 179/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5306 - mse: 5.5306 - mae: 1.8520 - val_loss: 4.7295 - val_mse: 4.7295 - val_mae: 1.7832\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 1.78231\n",
      "Epoch 180/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1784 - mse: 6.1784 - mae: 1.9504 - val_loss: 4.7172 - val_mse: 4.7172 - val_mae: 1.7800\n",
      "\n",
      "Epoch 00180: val_mae improved from 1.78231 to 1.78003, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 181/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6491 - mse: 5.6491 - mae: 1.8186 - val_loss: 4.6962 - val_mse: 4.6962 - val_mae: 1.7733\n",
      "\n",
      "Epoch 00181: val_mae improved from 1.78003 to 1.77334, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 182/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9841 - mse: 4.9841 - mae: 1.7806 - val_loss: 4.7036 - val_mse: 4.7036 - val_mae: 1.7780\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 1.77334\n",
      "Epoch 183/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2680 - mse: 5.2680 - mae: 1.8248 - val_loss: 4.6902 - val_mse: 4.6902 - val_mae: 1.7747\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 1.77334\n",
      "Epoch 184/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6736 - mse: 5.6736 - mae: 1.8472 - val_loss: 4.6820 - val_mse: 4.6820 - val_mae: 1.7731\n",
      "\n",
      "Epoch 00184: val_mae improved from 1.77334 to 1.77310, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 185/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9152 - mse: 4.9152 - mae: 1.7665 - val_loss: 4.6684 - val_mse: 4.6684 - val_mae: 1.7694\n",
      "\n",
      "Epoch 00185: val_mae improved from 1.77310 to 1.76944, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 186/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5780 - mse: 5.5780 - mae: 1.8849 - val_loss: 4.6569 - val_mse: 4.6569 - val_mae: 1.7668\n",
      "\n",
      "Epoch 00186: val_mae improved from 1.76944 to 1.76681, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 187/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5129 - mse: 5.5129 - mae: 1.8701 - val_loss: 4.6647 - val_mse: 4.6647 - val_mae: 1.7710\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 1.76681\n",
      "Epoch 188/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1166 - mse: 6.1166 - mae: 1.8934 - val_loss: 4.6453 - val_mse: 4.6453 - val_mae: 1.7655\n",
      "\n",
      "Epoch 00188: val_mae improved from 1.76681 to 1.76548, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 189/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.8387 - mse: 5.8387 - mae: 1.8508 - val_loss: 4.6325 - val_mse: 4.6325 - val_mae: 1.7623\n",
      "\n",
      "Epoch 00189: val_mae improved from 1.76548 to 1.76233, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 190/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0288 - mse: 6.0288 - mae: 1.8902 - val_loss: 4.6173 - val_mse: 4.6173 - val_mae: 1.7583\n",
      "\n",
      "Epoch 00190: val_mae improved from 1.76233 to 1.75829, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 191/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4820 - mse: 5.4820 - mae: 1.8698 - val_loss: 4.6031 - val_mse: 4.6031 - val_mae: 1.7547\n",
      "\n",
      "Epoch 00191: val_mae improved from 1.75829 to 1.75473, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 192/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2499 - mse: 5.2499 - mae: 1.7956 - val_loss: 4.5835 - val_mse: 4.5835 - val_mae: 1.7488\n",
      "\n",
      "Epoch 00192: val_mae improved from 1.75473 to 1.74883, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 193/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2598 - mse: 5.2598 - mae: 1.7925 - val_loss: 4.5798 - val_mse: 4.5798 - val_mae: 1.7492\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 1.74883\n",
      "Epoch 194/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0727 - mse: 5.0727 - mae: 1.7720 - val_loss: 4.5837 - val_mse: 4.5837 - val_mae: 1.7521\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 1.74883\n",
      "Epoch 195/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8045 - mse: 5.8045 - mae: 1.8775 - val_loss: 4.5817 - val_mse: 4.5817 - val_mae: 1.7525\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 1.74883\n",
      "Epoch 196/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.9233 - mse: 4.9233 - mae: 1.8067 - val_loss: 4.5769 - val_mse: 4.5769 - val_mae: 1.7520\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 1.74883\n",
      "Epoch 197/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0476 - mse: 5.0476 - mae: 1.7979 - val_loss: 4.5575 - val_mse: 4.5575 - val_mae: 1.7466\n",
      "\n",
      "Epoch 00197: val_mae improved from 1.74883 to 1.74662, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 198/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.8346 - mse: 5.8346 - mae: 1.9226 - val_loss: 4.5593 - val_mse: 4.5593 - val_mae: 1.7485\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 1.74662\n",
      "Epoch 199/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3068 - mse: 5.3068 - mae: 1.8416 - val_loss: 4.5356 - val_mse: 4.5356 - val_mae: 1.7417\n",
      "\n",
      "Epoch 00199: val_mae improved from 1.74662 to 1.74174, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 200/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0036 - mse: 6.0036 - mae: 1.9015 - val_loss: 4.5205 - val_mse: 4.5205 - val_mae: 1.7379\n",
      "\n",
      "Epoch 00200: val_mae improved from 1.74174 to 1.73785, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 201/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2433 - mse: 5.2433 - mae: 1.8077 - val_loss: 4.5068 - val_mse: 4.5068 - val_mae: 1.7345\n",
      "\n",
      "Epoch 00201: val_mae improved from 1.73785 to 1.73447, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 202/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0965 - mse: 6.0965 - mae: 1.9304 - val_loss: 4.5011 - val_mse: 4.5011 - val_mae: 1.7337\n",
      "\n",
      "Epoch 00202: val_mae improved from 1.73447 to 1.73375, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 203/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7506 - mse: 5.7506 - mae: 1.8587 - val_loss: 4.4946 - val_mse: 4.4946 - val_mae: 1.7326\n",
      "\n",
      "Epoch 00203: val_mae improved from 1.73375 to 1.73263, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 204/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.0483 - mse: 5.0483 - mae: 1.7791 - val_loss: 4.4896 - val_mse: 4.4896 - val_mae: 1.7321\n",
      "\n",
      "Epoch 00204: val_mae improved from 1.73263 to 1.73206, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 205/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8604 - mse: 5.8604 - mae: 1.9198 - val_loss: 4.4764 - val_mse: 4.4764 - val_mae: 1.7288\n",
      "\n",
      "Epoch 00205: val_mae improved from 1.73206 to 1.72878, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 206/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5390 - mse: 5.5390 - mae: 1.8829 - val_loss: 4.4644 - val_mse: 4.4644 - val_mae: 1.7259\n",
      "\n",
      "Epoch 00206: val_mae improved from 1.72878 to 1.72587, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 207/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2938 - mse: 5.2938 - mae: 1.8064 - val_loss: 4.4429 - val_mse: 4.4429 - val_mae: 1.7189\n",
      "\n",
      "Epoch 00207: val_mae improved from 1.72587 to 1.71893, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 208/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7461 - mse: 5.7461 - mae: 1.8060 - val_loss: 4.4202 - val_mse: 4.4202 - val_mae: 1.7103\n",
      "\n",
      "Epoch 00208: val_mae improved from 1.71893 to 1.71035, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 209/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.0763 - mse: 5.0763 - mae: 1.7340 - val_loss: 4.4221 - val_mse: 4.4221 - val_mae: 1.7136\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 1.71035\n",
      "Epoch 210/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0819 - mse: 5.0819 - mae: 1.7726 - val_loss: 4.4159 - val_mse: 4.4159 - val_mae: 1.7128\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 1.71035\n",
      "Epoch 211/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3485 - mse: 6.3485 - mae: 1.9501 - val_loss: 4.4252 - val_mse: 4.4252 - val_mae: 1.7182\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 1.71035\n",
      "Epoch 212/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8689 - mse: 5.8689 - mae: 1.9045 - val_loss: 4.4178 - val_mse: 4.4178 - val_mae: 1.7168\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 1.71035\n",
      "Epoch 213/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0124 - mse: 6.0124 - mae: 1.8716 - val_loss: 4.4004 - val_mse: 4.4004 - val_mae: 1.7117\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 1.71035\n",
      "Epoch 214/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0146 - mse: 6.0146 - mae: 1.9106 - val_loss: 4.3841 - val_mse: 4.3841 - val_mae: 1.7066\n",
      "\n",
      "Epoch 00214: val_mae improved from 1.71035 to 1.70655, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 215/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3585 - mse: 5.3585 - mae: 1.8211 - val_loss: 4.3749 - val_mse: 4.3749 - val_mae: 1.7042\n",
      "\n",
      "Epoch 00215: val_mae improved from 1.70655 to 1.70418, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 216/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4728 - mse: 5.4728 - mae: 1.8136 - val_loss: 4.3688 - val_mse: 4.3688 - val_mae: 1.7032\n",
      "\n",
      "Epoch 00216: val_mae improved from 1.70418 to 1.70323, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 217/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4871 - mse: 5.4871 - mae: 1.8252 - val_loss: 4.3784 - val_mse: 4.3784 - val_mae: 1.7084\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 1.70323\n",
      "Epoch 218/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2807 - mse: 5.2807 - mae: 1.8193 - val_loss: 4.3621 - val_mse: 4.3621 - val_mae: 1.7036\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 1.70323\n",
      "Epoch 219/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9125 - mse: 4.9125 - mae: 1.7014 - val_loss: 4.3508 - val_mse: 4.3508 - val_mae: 1.7006\n",
      "\n",
      "Epoch 00219: val_mae improved from 1.70323 to 1.70056, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 220/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9972 - mse: 4.9972 - mae: 1.7616 - val_loss: 4.3415 - val_mse: 4.3415 - val_mae: 1.6981\n",
      "\n",
      "Epoch 00220: val_mae improved from 1.70056 to 1.69814, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 221/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9345 - mse: 5.9345 - mae: 1.8299 - val_loss: 4.3438 - val_mse: 4.3438 - val_mae: 1.7006\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 1.69814\n",
      "Epoch 222/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4348 - mse: 5.4348 - mae: 1.8065 - val_loss: 4.3267 - val_mse: 4.3267 - val_mae: 1.6955\n",
      "\n",
      "Epoch 00222: val_mae improved from 1.69814 to 1.69551, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 223/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3786 - mse: 5.3786 - mae: 1.8312 - val_loss: 4.3186 - val_mse: 4.3186 - val_mae: 1.6938\n",
      "\n",
      "Epoch 00223: val_mae improved from 1.69551 to 1.69375, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 224/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8859 - mse: 4.8859 - mae: 1.7302 - val_loss: 4.3168 - val_mse: 4.3168 - val_mae: 1.6943\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 1.69375\n",
      "Epoch 225/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8431 - mse: 5.8431 - mae: 1.9117 - val_loss: 4.3072 - val_mse: 4.3072 - val_mae: 1.6919\n",
      "\n",
      "Epoch 00225: val_mae improved from 1.69375 to 1.69189, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 226/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.1800 - mse: 5.1800 - mae: 1.8042 - val_loss: 4.3025 - val_mse: 4.3025 - val_mae: 1.6914\n",
      "\n",
      "Epoch 00226: val_mae improved from 1.69189 to 1.69140, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 227/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7407 - mse: 5.7407 - mae: 1.8260 - val_loss: 4.3134 - val_mse: 4.3134 - val_mae: 1.6965\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 1.69140\n",
      "Epoch 228/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8561 - mse: 5.8561 - mae: 1.8523 - val_loss: 4.2925 - val_mse: 4.2925 - val_mae: 1.6899\n",
      "\n",
      "Epoch 00228: val_mae improved from 1.69140 to 1.68993, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 229/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8967 - mse: 4.8967 - mae: 1.7435 - val_loss: 4.2785 - val_mse: 4.2785 - val_mae: 1.6858\n",
      "\n",
      "Epoch 00229: val_mae improved from 1.68993 to 1.68578, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 230/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2677 - mse: 5.2677 - mae: 1.7755 - val_loss: 4.2654 - val_mse: 4.2654 - val_mae: 1.6819\n",
      "\n",
      "Epoch 00230: val_mae improved from 1.68578 to 1.68195, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 231/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6459 - mse: 5.6459 - mae: 1.8528 - val_loss: 4.2567 - val_mse: 4.2567 - val_mae: 1.6798\n",
      "\n",
      "Epoch 00231: val_mae improved from 1.68195 to 1.67981, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 232/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3438 - mse: 5.3438 - mae: 1.8179 - val_loss: 4.2379 - val_mse: 4.2379 - val_mae: 1.6731\n",
      "\n",
      "Epoch 00232: val_mae improved from 1.67981 to 1.67307, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 233/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3330 - mse: 5.3330 - mae: 1.7769 - val_loss: 4.2393 - val_mse: 4.2393 - val_mae: 1.6755\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 1.67307\n",
      "Epoch 234/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9291 - mse: 5.9291 - mae: 1.8662 - val_loss: 4.2292 - val_mse: 4.2292 - val_mae: 1.6725\n",
      "\n",
      "Epoch 00234: val_mae improved from 1.67307 to 1.67255, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 235/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3509 - mse: 5.3509 - mae: 1.8114 - val_loss: 4.2329 - val_mse: 4.2329 - val_mae: 1.6756\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 1.67255\n",
      "Epoch 236/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1218 - mse: 6.1218 - mae: 1.8955 - val_loss: 4.2181 - val_mse: 4.2181 - val_mae: 1.6706\n",
      "\n",
      "Epoch 00236: val_mae improved from 1.67255 to 1.67060, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 237/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9418 - mse: 4.9418 - mae: 1.7526 - val_loss: 4.2240 - val_mse: 4.2240 - val_mae: 1.6745\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 1.67060\n",
      "Epoch 238/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6799 - mse: 5.6799 - mae: 1.8552 - val_loss: 4.2069 - val_mse: 4.2069 - val_mae: 1.6686\n",
      "\n",
      "Epoch 00238: val_mae improved from 1.67060 to 1.66860, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 239/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1813 - mse: 5.1813 - mae: 1.8186 - val_loss: 4.1971 - val_mse: 4.1971 - val_mae: 1.6659\n",
      "\n",
      "Epoch 00239: val_mae improved from 1.66860 to 1.66594, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 240/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1779 - mse: 5.1779 - mae: 1.8179 - val_loss: 4.2004 - val_mse: 4.2004 - val_mae: 1.6687\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 1.66594\n",
      "Epoch 241/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2051 - mse: 5.2051 - mae: 1.7929 - val_loss: 4.1855 - val_mse: 4.1855 - val_mae: 1.6638\n",
      "\n",
      "Epoch 00241: val_mae improved from 1.66594 to 1.66384, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 242/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.4994 - mse: 5.4994 - mae: 1.8338 - val_loss: 4.1690 - val_mse: 4.1690 - val_mae: 1.6578\n",
      "\n",
      "Epoch 00242: val_mae improved from 1.66384 to 1.65783, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 243/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0981 - mse: 5.0981 - mae: 1.7483 - val_loss: 4.1577 - val_mse: 4.1577 - val_mae: 1.6538\n",
      "\n",
      "Epoch 00243: val_mae improved from 1.65783 to 1.65377, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 244/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1456 - mse: 5.1456 - mae: 1.8105 - val_loss: 4.1662 - val_mse: 4.1662 - val_mae: 1.6595\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 1.65377\n",
      "Epoch 245/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.9944 - mse: 4.9944 - mae: 1.7331 - val_loss: 4.1657 - val_mse: 4.1657 - val_mae: 1.6605\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 1.65377\n",
      "Epoch 246/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.2163 - mse: 5.2163 - mae: 1.7806 - val_loss: 4.1534 - val_mse: 4.1534 - val_mae: 1.6566\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 1.65377\n",
      "Epoch 247/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.0366 - mse: 5.0366 - mae: 1.7515 - val_loss: 4.1388 - val_mse: 4.1388 - val_mae: 1.6510\n",
      "\n",
      "Epoch 00247: val_mae improved from 1.65377 to 1.65095, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 248/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0917 - mse: 6.0917 - mae: 1.8990 - val_loss: 4.1336 - val_mse: 4.1336 - val_mae: 1.6498\n",
      "\n",
      "Epoch 00248: val_mae improved from 1.65095 to 1.64983, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 249/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0808 - mse: 5.0808 - mae: 1.7595 - val_loss: 4.1200 - val_mse: 4.1200 - val_mae: 1.6442\n",
      "\n",
      "Epoch 00249: val_mae improved from 1.64983 to 1.64423, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 250/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3065 - mse: 5.3065 - mae: 1.7901 - val_loss: 4.1242 - val_mse: 4.1242 - val_mae: 1.6486\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 1.64423\n",
      "Epoch 251/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3693 - mse: 5.3693 - mae: 1.8337 - val_loss: 4.1208 - val_mse: 4.1208 - val_mae: 1.6486\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 1.64423\n",
      "Epoch 252/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8850 - mse: 4.8850 - mae: 1.7336 - val_loss: 4.1131 - val_mse: 4.1131 - val_mae: 1.6464\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 1.64423\n",
      "Epoch 253/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1552 - mse: 5.1552 - mae: 1.7677 - val_loss: 4.1262 - val_mse: 4.1262 - val_mae: 1.6535\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 1.64423\n",
      "Epoch 254/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1130 - mse: 5.1130 - mae: 1.8011 - val_loss: 4.1273 - val_mse: 4.1273 - val_mae: 1.6551\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 1.64423\n",
      "Epoch 255/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1777 - mse: 5.1777 - mae: 1.7816 - val_loss: 4.1235 - val_mse: 4.1235 - val_mae: 1.6543\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 1.64423\n",
      "Epoch 256/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7823 - mse: 5.7823 - mae: 1.9043 - val_loss: 4.0971 - val_mse: 4.0971 - val_mae: 1.6444\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 1.64423\n",
      "Epoch 257/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2311 - mse: 5.2311 - mae: 1.8105 - val_loss: 4.0899 - val_mse: 4.0899 - val_mae: 1.6425\n",
      "\n",
      "Epoch 00257: val_mae improved from 1.64423 to 1.64248, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 258/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3451 - mse: 5.3451 - mae: 1.8477 - val_loss: 4.0779 - val_mse: 4.0779 - val_mae: 1.6382\n",
      "\n",
      "Epoch 00258: val_mae improved from 1.64248 to 1.63824, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 259/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0727 - mse: 5.0727 - mae: 1.7489 - val_loss: 4.0685 - val_mse: 4.0685 - val_mae: 1.6350\n",
      "\n",
      "Epoch 00259: val_mae improved from 1.63824 to 1.63500, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 260/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2087 - mse: 6.2087 - mae: 1.8965 - val_loss: 4.0798 - val_mse: 4.0798 - val_mae: 1.6416\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 1.63500\n",
      "Epoch 261/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8306 - mse: 4.8306 - mae: 1.6940 - val_loss: 4.0759 - val_mse: 4.0759 - val_mae: 1.6412\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 1.63500\n",
      "Epoch 262/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4305 - mse: 5.4305 - mae: 1.8168 - val_loss: 4.0666 - val_mse: 4.0666 - val_mae: 1.6383\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 1.63500\n",
      "Epoch 263/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1639 - mse: 5.1639 - mae: 1.7282 - val_loss: 4.0569 - val_mse: 4.0569 - val_mae: 1.6350\n",
      "\n",
      "Epoch 00263: val_mae improved from 1.63500 to 1.63499, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 264/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9112 - mse: 4.9112 - mae: 1.7472 - val_loss: 4.0410 - val_mse: 4.0410 - val_mae: 1.6285\n",
      "\n",
      "Epoch 00264: val_mae improved from 1.63499 to 1.62852, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 265/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7496 - mse: 5.7496 - mae: 1.8468 - val_loss: 4.0547 - val_mse: 4.0547 - val_mae: 1.6360\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 1.62852\n",
      "Epoch 266/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2781 - mse: 5.2781 - mae: 1.8478 - val_loss: 4.0467 - val_mse: 4.0467 - val_mae: 1.6333\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 1.62852\n",
      "Epoch 267/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2333 - mse: 5.2333 - mae: 1.7911 - val_loss: 4.0412 - val_mse: 4.0412 - val_mae: 1.6318\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 1.62852\n",
      "Epoch 268/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2782 - mse: 5.2782 - mae: 1.7577 - val_loss: 4.0300 - val_mse: 4.0300 - val_mae: 1.6281\n",
      "\n",
      "Epoch 00268: val_mae improved from 1.62852 to 1.62805, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 269/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1356 - mse: 5.1356 - mae: 1.7251 - val_loss: 4.0214 - val_mse: 4.0214 - val_mae: 1.6251\n",
      "\n",
      "Epoch 00269: val_mae improved from 1.62805 to 1.62509, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 270/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1186 - mse: 5.1186 - mae: 1.7237 - val_loss: 4.0165 - val_mse: 4.0165 - val_mae: 1.6239\n",
      "\n",
      "Epoch 00270: val_mae improved from 1.62509 to 1.62391, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 271/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1265 - mse: 5.1265 - mae: 1.8051 - val_loss: 4.0166 - val_mse: 4.0166 - val_mae: 1.6250\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 1.62391\n",
      "Epoch 272/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3068 - mse: 5.3068 - mae: 1.7933 - val_loss: 4.0159 - val_mse: 4.0159 - val_mae: 1.6257\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 1.62391\n",
      "Epoch 273/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.0910 - mse: 5.0910 - mae: 1.7674 - val_loss: 4.0065 - val_mse: 4.0065 - val_mae: 1.6225\n",
      "\n",
      "Epoch 00273: val_mae improved from 1.62391 to 1.62248, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 274/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4893 - mse: 5.4893 - mae: 1.8089 - val_loss: 3.9992 - val_mse: 3.9992 - val_mae: 1.6204\n",
      "\n",
      "Epoch 00274: val_mae improved from 1.62248 to 1.62039, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 275/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2225 - mse: 5.2225 - mae: 1.7734 - val_loss: 3.9887 - val_mse: 3.9887 - val_mae: 1.6164\n",
      "\n",
      "Epoch 00275: val_mae improved from 1.62039 to 1.61642, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 276/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3464 - mse: 5.3464 - mae: 1.8185 - val_loss: 3.9823 - val_mse: 3.9823 - val_mae: 1.6142\n",
      "\n",
      "Epoch 00276: val_mae improved from 1.61642 to 1.61419, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 277/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.4361 - mse: 5.4361 - mae: 1.8007 - val_loss: 3.9837 - val_mse: 3.9837 - val_mae: 1.6161\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 1.61419\n",
      "Epoch 278/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6117 - mse: 5.6117 - mae: 1.8173 - val_loss: 3.9742 - val_mse: 3.9742 - val_mae: 1.6125\n",
      "\n",
      "Epoch 00278: val_mae improved from 1.61419 to 1.61249, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 279/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4752 - mse: 4.4752 - mae: 1.6800 - val_loss: 3.9706 - val_mse: 3.9706 - val_mae: 1.6119\n",
      "\n",
      "Epoch 00279: val_mae improved from 1.61249 to 1.61194, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 280/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1444 - mse: 5.1444 - mae: 1.7767 - val_loss: 3.9657 - val_mse: 3.9657 - val_mae: 1.6108\n",
      "\n",
      "Epoch 00280: val_mae improved from 1.61194 to 1.61080, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 281/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8916 - mse: 4.8916 - mae: 1.7281 - val_loss: 3.9609 - val_mse: 3.9609 - val_mae: 1.6095\n",
      "\n",
      "Epoch 00281: val_mae improved from 1.61080 to 1.60948, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 282/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0149 - mse: 5.0149 - mae: 1.7518 - val_loss: 3.9558 - val_mse: 3.9558 - val_mae: 1.6082\n",
      "\n",
      "Epoch 00282: val_mae improved from 1.60948 to 1.60820, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 283/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4946 - mse: 5.4946 - mae: 1.8139 - val_loss: 3.9700 - val_mse: 3.9700 - val_mae: 1.6156\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 1.60820\n",
      "Epoch 284/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4229 - mse: 5.4229 - mae: 1.7860 - val_loss: 3.9698 - val_mse: 3.9698 - val_mae: 1.6163\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 1.60820\n",
      "Epoch 285/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1278 - mse: 5.1278 - mae: 1.7683 - val_loss: 3.9579 - val_mse: 3.9579 - val_mae: 1.6124\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 1.60820\n",
      "Epoch 286/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0102 - mse: 5.0102 - mae: 1.7530 - val_loss: 3.9496 - val_mse: 3.9496 - val_mae: 1.6096\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 1.60820\n",
      "Epoch 287/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9738 - mse: 4.9738 - mae: 1.7415 - val_loss: 3.9421 - val_mse: 3.9421 - val_mae: 1.6071\n",
      "\n",
      "Epoch 00287: val_mae improved from 1.60820 to 1.60713, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 288/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9808 - mse: 4.9808 - mae: 1.7591 - val_loss: 3.9362 - val_mse: 3.9362 - val_mae: 1.6053\n",
      "\n",
      "Epoch 00288: val_mae improved from 1.60713 to 1.60532, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 289/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2334 - mse: 6.2334 - mae: 1.9384 - val_loss: 3.9469 - val_mse: 3.9469 - val_mae: 1.6105\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 1.60532\n",
      "Epoch 290/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7373 - mse: 5.7373 - mae: 1.8474 - val_loss: 3.9298 - val_mse: 3.9298 - val_mae: 1.6043\n",
      "\n",
      "Epoch 00290: val_mae improved from 1.60532 to 1.60427, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 291/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3914 - mse: 5.3914 - mae: 1.8094 - val_loss: 3.9189 - val_mse: 3.9189 - val_mae: 1.6002\n",
      "\n",
      "Epoch 00291: val_mae improved from 1.60427 to 1.60016, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 292/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9810 - mse: 4.9810 - mae: 1.7428 - val_loss: 3.9078 - val_mse: 3.9078 - val_mae: 1.5958\n",
      "\n",
      "Epoch 00292: val_mae improved from 1.60016 to 1.59582, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 293/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4265 - mse: 5.4265 - mae: 1.7606 - val_loss: 3.8998 - val_mse: 3.8998 - val_mae: 1.5923\n",
      "\n",
      "Epoch 00293: val_mae improved from 1.59582 to 1.59226, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 294/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1631 - mse: 5.1631 - mae: 1.7254 - val_loss: 3.8968 - val_mse: 3.8968 - val_mae: 1.5916\n",
      "\n",
      "Epoch 00294: val_mae improved from 1.59226 to 1.59155, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 295/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1432 - mse: 5.1432 - mae: 1.7546 - val_loss: 3.8966 - val_mse: 3.8966 - val_mae: 1.5926\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 1.59155\n",
      "Epoch 296/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1206 - mse: 5.1206 - mae: 1.7526 - val_loss: 3.8996 - val_mse: 3.8996 - val_mae: 1.5950\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 1.59155\n",
      "Epoch 297/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9590 - mse: 4.9590 - mae: 1.7199 - val_loss: 3.8928 - val_mse: 3.8928 - val_mae: 1.5925\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 1.59155\n",
      "Epoch 298/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8200 - mse: 4.8200 - mae: 1.6881 - val_loss: 3.8977 - val_mse: 3.8977 - val_mae: 1.5956\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 1.59155\n",
      "Epoch 299/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2593 - mse: 5.2593 - mae: 1.7954 - val_loss: 3.9080 - val_mse: 3.9080 - val_mae: 1.6003\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 1.59155\n",
      "Epoch 300/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5305 - mse: 4.5305 - mae: 1.6296 - val_loss: 3.9105 - val_mse: 3.9105 - val_mae: 1.6016\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 1.59155\n",
      "Epoch 301/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7596 - mse: 5.7596 - mae: 1.8512 - val_loss: 3.8927 - val_mse: 3.8927 - val_mae: 1.5954\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 1.59155\n",
      "Epoch 302/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8042 - mse: 4.8042 - mae: 1.7117 - val_loss: 3.8790 - val_mse: 3.8790 - val_mae: 1.5899\n",
      "\n",
      "Epoch 00302: val_mae improved from 1.59155 to 1.58994, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 303/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2436 - mse: 5.2436 - mae: 1.7295 - val_loss: 3.8668 - val_mse: 3.8668 - val_mae: 1.5846\n",
      "\n",
      "Epoch 00303: val_mae improved from 1.58994 to 1.58464, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 304/1000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.8214 - mse: 4.8214 - mae: 1.6551 - val_loss: 3.8809 - val_mse: 3.8809 - val_mae: 1.5922\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 1.58464\n",
      "Epoch 305/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1474 - mse: 5.1474 - mae: 1.7270 - val_loss: 3.8732 - val_mse: 3.8732 - val_mae: 1.5895\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 1.58464\n",
      "Epoch 306/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9007 - mse: 4.9007 - mae: 1.7329 - val_loss: 3.8683 - val_mse: 3.8683 - val_mae: 1.5879\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 1.58464\n",
      "Epoch 307/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0458 - mse: 5.0458 - mae: 1.7097 - val_loss: 3.8664 - val_mse: 3.8664 - val_mae: 1.5876\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 1.58464\n",
      "Epoch 308/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8193 - mse: 5.8193 - mae: 1.7917 - val_loss: 3.8598 - val_mse: 3.8598 - val_mae: 1.5853\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 1.58464\n",
      "Epoch 309/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2607 - mse: 5.2607 - mae: 1.7795 - val_loss: 3.8469 - val_mse: 3.8469 - val_mae: 1.5794\n",
      "\n",
      "Epoch 00309: val_mae improved from 1.58464 to 1.57942, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 310/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2773 - mse: 5.2773 - mae: 1.7723 - val_loss: 3.8490 - val_mse: 3.8490 - val_mae: 1.5814\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 1.57942\n",
      "Epoch 311/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2763 - mse: 5.2763 - mae: 1.7231 - val_loss: 3.8447 - val_mse: 3.8447 - val_mae: 1.5800\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 1.57942\n",
      "Epoch 312/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8410 - mse: 4.8410 - mae: 1.7042 - val_loss: 3.8425 - val_mse: 3.8425 - val_mae: 1.5798\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 1.57942\n",
      "Epoch 313/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4164 - mse: 5.4164 - mae: 1.7767 - val_loss: 3.8363 - val_mse: 3.8363 - val_mae: 1.5774\n",
      "\n",
      "Epoch 00313: val_mae improved from 1.57942 to 1.57736, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 314/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3751 - mse: 5.3751 - mae: 1.7655 - val_loss: 3.8253 - val_mse: 3.8253 - val_mae: 1.5713\n",
      "\n",
      "Epoch 00314: val_mae improved from 1.57736 to 1.57129, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 315/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5789 - mse: 4.5789 - mae: 1.6640 - val_loss: 3.8247 - val_mse: 3.8247 - val_mae: 1.5722\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 1.57129\n",
      "Epoch 316/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0067 - mse: 5.0067 - mae: 1.7183 - val_loss: 3.8572 - val_mse: 3.8572 - val_mae: 1.5875\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 1.57129\n",
      "Epoch 317/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2042 - mse: 5.2042 - mae: 1.7377 - val_loss: 3.8349 - val_mse: 3.8349 - val_mae: 1.5793\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 1.57129\n",
      "Epoch 318/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8625 - mse: 5.8625 - mae: 1.7603 - val_loss: 3.8269 - val_mse: 3.8269 - val_mae: 1.5762\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 1.57129\n",
      "Epoch 319/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3179 - mse: 5.3179 - mae: 1.7469 - val_loss: 3.8300 - val_mse: 3.8300 - val_mae: 1.5781\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 1.57129\n",
      "Epoch 320/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7275 - mse: 4.7275 - mae: 1.6745 - val_loss: 3.8252 - val_mse: 3.8252 - val_mae: 1.5764\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 1.57129\n",
      "Epoch 321/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4926 - mse: 4.4926 - mae: 1.6630 - val_loss: 3.8144 - val_mse: 3.8144 - val_mae: 1.5718\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 1.57129\n",
      "Epoch 322/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2444 - mse: 5.2444 - mae: 1.7298 - val_loss: 3.8208 - val_mse: 3.8208 - val_mae: 1.5755\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 1.57129\n",
      "Epoch 323/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6283 - mse: 5.6283 - mae: 1.8461 - val_loss: 3.8173 - val_mse: 3.8173 - val_mae: 1.5743\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 1.57129\n",
      "Epoch 324/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9245 - mse: 4.9245 - mae: 1.7292 - val_loss: 3.8112 - val_mse: 3.8112 - val_mae: 1.5723\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 1.57129\n",
      "Epoch 325/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3371 - mse: 5.3371 - mae: 1.7523 - val_loss: 3.8066 - val_mse: 3.8066 - val_mae: 1.5706\n",
      "\n",
      "Epoch 00325: val_mae improved from 1.57129 to 1.57059, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 326/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.4075 - mse: 5.4075 - mae: 1.7518 - val_loss: 3.8112 - val_mse: 3.8112 - val_mae: 1.5731\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 1.57059\n",
      "Epoch 327/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8162 - mse: 4.8162 - mae: 1.6993 - val_loss: 3.7942 - val_mse: 3.7942 - val_mae: 1.5652\n",
      "\n",
      "Epoch 00327: val_mae improved from 1.57059 to 1.56523, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 328/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3934 - mse: 5.3934 - mae: 1.7709 - val_loss: 3.7937 - val_mse: 3.7937 - val_mae: 1.5656\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 1.56523\n",
      "Epoch 329/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4552 - mse: 4.4552 - mae: 1.6413 - val_loss: 3.7929 - val_mse: 3.7929 - val_mae: 1.5657\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 1.56523\n",
      "Epoch 330/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.8000 - mse: 4.8000 - mae: 1.7122 - val_loss: 3.7847 - val_mse: 3.7847 - val_mae: 1.5617\n",
      "\n",
      "Epoch 00330: val_mae improved from 1.56523 to 1.56168, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 331/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6484 - mse: 4.6484 - mae: 1.6417 - val_loss: 3.7835 - val_mse: 3.7835 - val_mae: 1.5617\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 1.56168\n",
      "Epoch 332/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9509 - mse: 4.9509 - mae: 1.7275 - val_loss: 3.7872 - val_mse: 3.7872 - val_mae: 1.5645\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 1.56168\n",
      "Epoch 333/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2981 - mse: 5.2981 - mae: 1.7678 - val_loss: 3.7893 - val_mse: 3.7893 - val_mae: 1.5659\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 1.56168\n",
      "Epoch 334/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9889 - mse: 4.9889 - mae: 1.7265 - val_loss: 3.7900 - val_mse: 3.7900 - val_mae: 1.5666\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 1.56168\n",
      "Epoch 335/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6610 - mse: 4.6610 - mae: 1.6479 - val_loss: 3.7914 - val_mse: 3.7914 - val_mae: 1.5676\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 1.56168\n",
      "Epoch 336/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9659 - mse: 4.9659 - mae: 1.7169 - val_loss: 3.7841 - val_mse: 3.7841 - val_mae: 1.5649\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 1.56168\n",
      "Epoch 337/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8545 - mse: 4.8545 - mae: 1.6595 - val_loss: 3.7724 - val_mse: 3.7724 - val_mae: 1.5597\n",
      "\n",
      "Epoch 00337: val_mae improved from 1.56168 to 1.55970, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 338/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9608 - mse: 4.9608 - mae: 1.7061 - val_loss: 3.7681 - val_mse: 3.7681 - val_mae: 1.5580\n",
      "\n",
      "Epoch 00338: val_mae improved from 1.55970 to 1.55797, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 339/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4256 - mse: 5.4256 - mae: 1.7657 - val_loss: 3.7715 - val_mse: 3.7715 - val_mae: 1.5604\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 1.55797\n",
      "Epoch 340/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2268 - mse: 5.2268 - mae: 1.7521 - val_loss: 3.7645 - val_mse: 3.7645 - val_mae: 1.5573\n",
      "\n",
      "Epoch 00340: val_mae improved from 1.55797 to 1.55730, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 341/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8172 - mse: 4.8172 - mae: 1.6639 - val_loss: 3.7532 - val_mse: 3.7532 - val_mae: 1.5502\n",
      "\n",
      "Epoch 00341: val_mae improved from 1.55730 to 1.55019, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 342/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5972 - mse: 4.5972 - mae: 1.6423 - val_loss: 3.7609 - val_mse: 3.7609 - val_mae: 1.5562\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 1.55019\n",
      "Epoch 343/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5203 - mse: 5.5203 - mae: 1.7728 - val_loss: 3.7605 - val_mse: 3.7605 - val_mae: 1.5564\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 1.55019\n",
      "Epoch 344/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7308 - mse: 4.7308 - mae: 1.6923 - val_loss: 3.7646 - val_mse: 3.7646 - val_mae: 1.5588\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 1.55019\n",
      "Epoch 345/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9723 - mse: 4.9723 - mae: 1.7179 - val_loss: 3.7562 - val_mse: 3.7562 - val_mae: 1.5553\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 1.55019\n",
      "Epoch 346/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4802 - mse: 5.4802 - mae: 1.7606 - val_loss: 3.7597 - val_mse: 3.7597 - val_mae: 1.5574\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 1.55019\n",
      "Epoch 347/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2854 - mse: 5.2854 - mae: 1.8149 - val_loss: 3.7477 - val_mse: 3.7477 - val_mae: 1.5514\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 1.55019\n",
      "Epoch 348/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1961 - mse: 5.1961 - mae: 1.7318 - val_loss: 3.7434 - val_mse: 3.7434 - val_mae: 1.5492\n",
      "\n",
      "Epoch 00348: val_mae improved from 1.55019 to 1.54920, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 349/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9330 - mse: 4.9330 - mae: 1.7004 - val_loss: 3.7435 - val_mse: 3.7435 - val_mae: 1.5501\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 1.54920\n",
      "Epoch 350/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3196 - mse: 5.3196 - mae: 1.7782 - val_loss: 3.7473 - val_mse: 3.7473 - val_mae: 1.5526\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 1.54920\n",
      "Epoch 351/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4892 - mse: 5.4892 - mae: 1.7441 - val_loss: 3.7484 - val_mse: 3.7484 - val_mae: 1.5536\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 1.54920\n",
      "Epoch 352/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7535 - mse: 4.7535 - mae: 1.6960 - val_loss: 3.7364 - val_mse: 3.7364 - val_mae: 1.5474\n",
      "\n",
      "Epoch 00352: val_mae improved from 1.54920 to 1.54738, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 353/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1873 - mse: 5.1873 - mae: 1.7141 - val_loss: 3.7421 - val_mse: 3.7421 - val_mae: 1.5510\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 1.54738\n",
      "Epoch 354/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1452 - mse: 5.1452 - mae: 1.7377 - val_loss: 3.7451 - val_mse: 3.7451 - val_mae: 1.5528\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 1.54738\n",
      "Epoch 355/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8595 - mse: 4.8595 - mae: 1.6802 - val_loss: 3.7295 - val_mse: 3.7295 - val_mae: 1.5443\n",
      "\n",
      "Epoch 00355: val_mae improved from 1.54738 to 1.54432, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 356/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1904 - mse: 5.1904 - mae: 1.7326 - val_loss: 3.7372 - val_mse: 3.7372 - val_mae: 1.5497\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 1.54432\n",
      "Epoch 357/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8753 - mse: 4.8753 - mae: 1.6996 - val_loss: 3.7363 - val_mse: 3.7363 - val_mae: 1.5494\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 1.54432\n",
      "Epoch 358/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9716 - mse: 4.9716 - mae: 1.7317 - val_loss: 3.7250 - val_mse: 3.7250 - val_mae: 1.5432\n",
      "\n",
      "Epoch 00358: val_mae improved from 1.54432 to 1.54320, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 359/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.7592 - mse: 4.7592 - mae: 1.6893 - val_loss: 3.7332 - val_mse: 3.7332 - val_mae: 1.5486\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 1.54320\n",
      "Epoch 360/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9046 - mse: 4.9046 - mae: 1.6963 - val_loss: 3.7232 - val_mse: 3.7232 - val_mae: 1.5432\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 1.54320\n",
      "Epoch 361/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8613 - mse: 4.8613 - mae: 1.6816 - val_loss: 3.7240 - val_mse: 3.7240 - val_mae: 1.5440\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 1.54320\n",
      "Epoch 362/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3540 - mse: 5.3540 - mae: 1.7342 - val_loss: 3.7294 - val_mse: 3.7294 - val_mae: 1.5474\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 1.54320\n",
      "Epoch 363/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7414 - mse: 5.7414 - mae: 1.7563 - val_loss: 3.7242 - val_mse: 3.7242 - val_mae: 1.5451\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 1.54320\n",
      "Epoch 364/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4015 - mse: 5.4015 - mae: 1.8167 - val_loss: 3.7177 - val_mse: 3.7177 - val_mae: 1.5414\n",
      "\n",
      "Epoch 00364: val_mae improved from 1.54320 to 1.54139, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 365/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8642 - mse: 4.8642 - mae: 1.6750 - val_loss: 3.7298 - val_mse: 3.7298 - val_mae: 1.5482\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 1.54139\n",
      "Epoch 366/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4993 - mse: 5.4993 - mae: 1.7827 - val_loss: 3.7166 - val_mse: 3.7166 - val_mae: 1.5417\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 1.54139\n",
      "Epoch 367/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5252 - mse: 5.5252 - mae: 1.7741 - val_loss: 3.7240 - val_mse: 3.7240 - val_mae: 1.5459\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 1.54139\n",
      "Epoch 368/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6429 - mse: 5.6429 - mae: 1.7494 - val_loss: 3.7210 - val_mse: 3.7210 - val_mae: 1.5448\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 1.54139\n",
      "Epoch 369/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8162 - mse: 4.8162 - mae: 1.6987 - val_loss: 3.7105 - val_mse: 3.7105 - val_mae: 1.5392\n",
      "\n",
      "Epoch 00369: val_mae improved from 1.54139 to 1.53921, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 370/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3658 - mse: 5.3658 - mae: 1.7669 - val_loss: 3.7155 - val_mse: 3.7155 - val_mae: 1.5427\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 1.53921\n",
      "Epoch 371/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8939 - mse: 4.8939 - mae: 1.7081 - val_loss: 3.7131 - val_mse: 3.7131 - val_mae: 1.5417\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 1.53921\n",
      "Epoch 372/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5603 - mse: 5.5603 - mae: 1.7426 - val_loss: 3.7092 - val_mse: 3.7092 - val_mae: 1.5399\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 1.53921\n",
      "Epoch 373/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2849 - mse: 5.2849 - mae: 1.7460 - val_loss: 3.7079 - val_mse: 3.7079 - val_mae: 1.5396\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 1.53921\n",
      "Epoch 374/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.7313 - mse: 4.7313 - mae: 1.6926 - val_loss: 3.7052 - val_mse: 3.7052 - val_mae: 1.5384\n",
      "\n",
      "Epoch 00374: val_mae improved from 1.53921 to 1.53840, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 375/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6452 - mse: 4.6452 - mae: 1.6167 - val_loss: 3.7075 - val_mse: 3.7075 - val_mae: 1.5403\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 1.53840\n",
      "Epoch 376/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9202 - mse: 4.9202 - mae: 1.7049 - val_loss: 3.7125 - val_mse: 3.7125 - val_mae: 1.5429\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 1.53840\n",
      "Epoch 377/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1854 - mse: 5.1854 - mae: 1.7374 - val_loss: 3.7072 - val_mse: 3.7072 - val_mae: 1.5405\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 1.53840\n",
      "Epoch 378/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0355 - mse: 6.0355 - mae: 1.8347 - val_loss: 3.7146 - val_mse: 3.7146 - val_mae: 1.5442\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 1.53840\n",
      "Epoch 379/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6734 - mse: 5.6734 - mae: 1.7924 - val_loss: 3.7070 - val_mse: 3.7070 - val_mae: 1.5409\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 1.53840\n",
      "Epoch 380/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3412 - mse: 5.3412 - mae: 1.7837 - val_loss: 3.6933 - val_mse: 3.6933 - val_mae: 1.5332\n",
      "\n",
      "Epoch 00380: val_mae improved from 1.53840 to 1.53324, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 381/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8525 - mse: 4.8525 - mae: 1.7018 - val_loss: 3.6993 - val_mse: 3.6993 - val_mae: 1.5374\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 1.53324\n",
      "Epoch 382/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3023 - mse: 5.3023 - mae: 1.7355 - val_loss: 3.7003 - val_mse: 3.7003 - val_mae: 1.5383\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 1.53324\n",
      "Epoch 383/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0024 - mse: 5.0024 - mae: 1.7268 - val_loss: 3.7017 - val_mse: 3.7017 - val_mae: 1.5394\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 1.53324\n",
      "Epoch 384/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9340 - mse: 4.9340 - mae: 1.6859 - val_loss: 3.6919 - val_mse: 3.6919 - val_mae: 1.5344\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 1.53324\n",
      "Epoch 385/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7564 - mse: 4.7564 - mae: 1.6886 - val_loss: 3.7008 - val_mse: 3.7008 - val_mae: 1.5393\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 1.53324\n",
      "Epoch 386/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6958 - mse: 4.6958 - mae: 1.6628 - val_loss: 3.6962 - val_mse: 3.6962 - val_mae: 1.5372\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 1.53324\n",
      "Epoch 387/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.4461 - mse: 4.4461 - mae: 1.6464 - val_loss: 3.6873 - val_mse: 3.6873 - val_mae: 1.5325\n",
      "\n",
      "Epoch 00387: val_mae improved from 1.53324 to 1.53245, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 388/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3204 - mse: 5.3204 - mae: 1.7644 - val_loss: 3.7070 - val_mse: 3.7070 - val_mae: 1.5422\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 1.53245\n",
      "Epoch 389/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3876 - mse: 5.3876 - mae: 1.7605 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5342\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 1.53245\n",
      "Epoch 390/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7867 - mse: 5.7867 - mae: 1.7712 - val_loss: 3.6862 - val_mse: 3.6862 - val_mae: 1.5331\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 1.53245\n",
      "Epoch 391/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6666 - mse: 4.6666 - mae: 1.6695 - val_loss: 3.6836 - val_mse: 3.6836 - val_mae: 1.5320\n",
      "\n",
      "Epoch 00391: val_mae improved from 1.53245 to 1.53198, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 392/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6777 - mse: 4.6777 - mae: 1.6778 - val_loss: 3.6927 - val_mse: 3.6927 - val_mae: 1.5367\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 1.53198\n",
      "Epoch 393/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9150 - mse: 4.9150 - mae: 1.7222 - val_loss: 3.6817 - val_mse: 3.6817 - val_mae: 1.5312\n",
      "\n",
      "Epoch 00393: val_mae improved from 1.53198 to 1.53123, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 394/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9701 - mse: 4.9701 - mae: 1.6930 - val_loss: 3.6849 - val_mse: 3.6849 - val_mae: 1.5333\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 1.53123\n",
      "Epoch 395/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1681 - mse: 5.1681 - mae: 1.7645 - val_loss: 3.6793 - val_mse: 3.6793 - val_mae: 1.5303\n",
      "\n",
      "Epoch 00395: val_mae improved from 1.53123 to 1.53028, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 396/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8255 - mse: 4.8255 - mae: 1.6790 - val_loss: 3.6827 - val_mse: 3.6827 - val_mae: 1.5326\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 1.53028\n",
      "Epoch 397/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7667 - mse: 4.7667 - mae: 1.6997 - val_loss: 3.6762 - val_mse: 3.6762 - val_mae: 1.5291\n",
      "\n",
      "Epoch 00397: val_mae improved from 1.53028 to 1.52913, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 398/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8397 - mse: 4.8397 - mae: 1.6571 - val_loss: 3.6691 - val_mse: 3.6691 - val_mae: 1.5251\n",
      "\n",
      "Epoch 00398: val_mae improved from 1.52913 to 1.52510, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 399/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3544 - mse: 5.3544 - mae: 1.7573 - val_loss: 3.6806 - val_mse: 3.6806 - val_mae: 1.5321\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 1.52510\n",
      "Epoch 400/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3952 - mse: 5.3952 - mae: 1.7149 - val_loss: 3.6731 - val_mse: 3.6731 - val_mae: 1.5282\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 1.52510\n",
      "Epoch 401/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8089 - mse: 4.8089 - mae: 1.6580 - val_loss: 3.6741 - val_mse: 3.6741 - val_mae: 1.5289\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 1.52510\n",
      "Epoch 402/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2178 - mse: 5.2178 - mae: 1.7180 - val_loss: 3.6669 - val_mse: 3.6669 - val_mae: 1.5248\n",
      "\n",
      "Epoch 00402: val_mae improved from 1.52510 to 1.52481, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 403/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1079 - mse: 5.1079 - mae: 1.7444 - val_loss: 3.6680 - val_mse: 3.6680 - val_mae: 1.5256\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 1.52481\n",
      "Epoch 404/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8543 - mse: 4.8543 - mae: 1.6704 - val_loss: 3.6638 - val_mse: 3.6638 - val_mae: 1.5226\n",
      "\n",
      "Epoch 00404: val_mae improved from 1.52481 to 1.52262, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 405/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9504 - mse: 4.9504 - mae: 1.7086 - val_loss: 3.6604 - val_mse: 3.6604 - val_mae: 1.5202\n",
      "\n",
      "Epoch 00405: val_mae improved from 1.52262 to 1.52019, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 406/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8805 - mse: 4.8805 - mae: 1.6920 - val_loss: 3.6587 - val_mse: 3.6587 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00406: val_mae improved from 1.52019 to 1.51848, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 407/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9710 - mse: 4.9710 - mae: 1.6961 - val_loss: 3.6644 - val_mse: 3.6644 - val_mae: 1.5240\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 1.51848\n",
      "Epoch 408/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1266 - mse: 5.1266 - mae: 1.7206 - val_loss: 3.6682 - val_mse: 3.6682 - val_mae: 1.5265\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 1.51848\n",
      "Epoch 409/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4834 - mse: 5.4834 - mae: 1.8237 - val_loss: 3.6706 - val_mse: 3.6706 - val_mae: 1.5279\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 1.51848\n",
      "Epoch 410/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4136 - mse: 4.4136 - mae: 1.6429 - val_loss: 3.6729 - val_mse: 3.6729 - val_mae: 1.5293\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 1.51848\n",
      "Epoch 411/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0201 - mse: 5.0201 - mae: 1.7591 - val_loss: 3.6874 - val_mse: 3.6874 - val_mae: 1.5360\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 1.51848\n",
      "Epoch 412/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0343 - mse: 5.0343 - mae: 1.6757 - val_loss: 3.6708 - val_mse: 3.6708 - val_mae: 1.5283\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 1.51848\n",
      "Epoch 413/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2638 - mse: 5.2638 - mae: 1.7415 - val_loss: 3.6767 - val_mse: 3.6767 - val_mae: 1.5314\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 1.51848\n",
      "Epoch 414/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1730 - mse: 5.1730 - mae: 1.7088 - val_loss: 3.6855 - val_mse: 3.6855 - val_mae: 1.5354\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 1.51848\n",
      "Epoch 415/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6967 - mse: 4.6967 - mae: 1.6489 - val_loss: 3.6695 - val_mse: 3.6695 - val_mae: 1.5281\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 1.51848\n",
      "Epoch 416/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3574 - mse: 5.3574 - mae: 1.7851 - val_loss: 3.6658 - val_mse: 3.6658 - val_mae: 1.5263\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 1.51848\n",
      "Epoch 417/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4008 - mse: 5.4008 - mae: 1.7513 - val_loss: 3.6750 - val_mse: 3.6750 - val_mae: 1.5310\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 1.51848\n",
      "Epoch 418/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8957 - mse: 4.8957 - mae: 1.6575 - val_loss: 3.6677 - val_mse: 3.6677 - val_mae: 1.5275\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 1.51848\n",
      "Epoch 419/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3226 - mse: 5.3226 - mae: 1.7430 - val_loss: 3.6755 - val_mse: 3.6755 - val_mae: 1.5315\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 1.51848\n",
      "Epoch 420/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8454 - mse: 4.8454 - mae: 1.6717 - val_loss: 3.6692 - val_mse: 3.6692 - val_mae: 1.5288\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 1.51848\n",
      "Epoch 421/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0772 - mse: 5.0772 - mae: 1.7095 - val_loss: 3.6576 - val_mse: 3.6576 - val_mae: 1.5229\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 1.51848\n",
      "Epoch 422/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6679 - mse: 5.6679 - mae: 1.8022 - val_loss: 3.6589 - val_mse: 3.6589 - val_mae: 1.5237\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 1.51848\n",
      "Epoch 423/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9337 - mse: 4.9337 - mae: 1.7208 - val_loss: 3.6627 - val_mse: 3.6627 - val_mae: 1.5259\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 1.51848\n",
      "Epoch 424/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8545 - mse: 4.8545 - mae: 1.7061 - val_loss: 3.6719 - val_mse: 3.6719 - val_mae: 1.5305\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 1.51848\n",
      "Epoch 425/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9527 - mse: 5.9527 - mae: 1.8756 - val_loss: 3.6662 - val_mse: 3.6662 - val_mae: 1.5280\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 1.51848\n",
      "Epoch 426/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8132 - mse: 4.8132 - mae: 1.7156 - val_loss: 3.6598 - val_mse: 3.6598 - val_mae: 1.5249\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 1.51848\n",
      "Epoch 427/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3736 - mse: 5.3736 - mae: 1.7971 - val_loss: 3.6646 - val_mse: 3.6646 - val_mae: 1.5274\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 1.51848\n",
      "Epoch 428/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1203 - mse: 5.1203 - mae: 1.7115 - val_loss: 3.6676 - val_mse: 3.6676 - val_mae: 1.5290\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 1.51848\n",
      "Epoch 429/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3455 - mse: 5.3455 - mae: 1.7741 - val_loss: 3.6627 - val_mse: 3.6627 - val_mae: 1.5268\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 1.51848\n",
      "Epoch 430/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6217 - mse: 5.6217 - mae: 1.7964 - val_loss: 3.6597 - val_mse: 3.6597 - val_mae: 1.5255\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 1.51848\n",
      "Epoch 431/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5350 - mse: 5.5350 - mae: 1.7229 - val_loss: 3.6568 - val_mse: 3.6568 - val_mae: 1.5241\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 1.51848\n",
      "Epoch 432/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6636 - mse: 5.6636 - mae: 1.8177 - val_loss: 3.6511 - val_mse: 3.6511 - val_mae: 1.5213\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 1.51848\n",
      "Epoch 433/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7994 - mse: 4.7994 - mae: 1.7038 - val_loss: 3.6468 - val_mse: 3.6468 - val_mae: 1.5188\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 1.51848\n",
      "Epoch 434/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9830 - mse: 4.9830 - mae: 1.6532 - val_loss: 3.6529 - val_mse: 3.6529 - val_mae: 1.5223\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 1.51848\n",
      "Epoch 435/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6836 - mse: 5.6836 - mae: 1.8055 - val_loss: 3.6578 - val_mse: 3.6578 - val_mae: 1.5249\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 1.51848\n",
      "Epoch 436/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6108 - mse: 4.6108 - mae: 1.6448 - val_loss: 3.6544 - val_mse: 3.6544 - val_mae: 1.5234\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 1.51848\n",
      "Epoch 437/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7558 - mse: 4.7558 - mae: 1.6883 - val_loss: 3.6517 - val_mse: 3.6517 - val_mae: 1.5223\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 1.51848\n",
      "Epoch 438/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9552 - mse: 4.9552 - mae: 1.6474 - val_loss: 3.6489 - val_mse: 3.6489 - val_mae: 1.5212\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 1.51848\n",
      "Epoch 439/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6932 - mse: 5.6932 - mae: 1.8073 - val_loss: 3.6591 - val_mse: 3.6591 - val_mae: 1.5262\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 1.51848\n",
      "Epoch 440/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1852 - mse: 5.1852 - mae: 1.7343 - val_loss: 3.6633 - val_mse: 3.6633 - val_mae: 1.5283\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 1.51848\n",
      "Epoch 441/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8280 - mse: 4.8280 - mae: 1.6881 - val_loss: 3.6510 - val_mse: 3.6510 - val_mae: 1.5226\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 1.51848\n",
      "Epoch 442/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5020 - mse: 4.5020 - mae: 1.6124 - val_loss: 3.6507 - val_mse: 3.6507 - val_mae: 1.5225\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 1.51848\n",
      "Epoch 443/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0808 - mse: 5.0808 - mae: 1.7093 - val_loss: 3.6438 - val_mse: 3.6438 - val_mae: 1.5192\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 1.51848\n",
      "Epoch 444/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7178 - mse: 5.7178 - mae: 1.7540 - val_loss: 3.6442 - val_mse: 3.6442 - val_mae: 1.5196\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 1.51848\n",
      "Epoch 445/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.5198 - mse: 4.5198 - mae: 1.6340 - val_loss: 3.6333 - val_mse: 3.6333 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00445: val_mae improved from 1.51848 to 1.51283, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 446/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4412 - mse: 5.4412 - mae: 1.7663 - val_loss: 3.6363 - val_mse: 3.6363 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 1.51283\n",
      "Epoch 447/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6186 - mse: 5.6186 - mae: 1.7469 - val_loss: 3.6358 - val_mse: 3.6358 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 1.51283\n",
      "Epoch 448/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9723 - mse: 4.9723 - mae: 1.7247 - val_loss: 3.6380 - val_mse: 3.6380 - val_mae: 1.5165\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 1.51283\n",
      "Epoch 449/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.8679 - mse: 5.8679 - mae: 1.7812 - val_loss: 3.6364 - val_mse: 3.6364 - val_mae: 1.5156\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 1.51283\n",
      "Epoch 450/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9117 - mse: 4.9117 - mae: 1.7230 - val_loss: 3.6426 - val_mse: 3.6426 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 1.51283\n",
      "Epoch 451/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1092 - mse: 5.1092 - mae: 1.6955 - val_loss: 3.6379 - val_mse: 3.6379 - val_mae: 1.5173\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 1.51283\n",
      "Epoch 452/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.7565 - mse: 5.7565 - mae: 1.7893 - val_loss: 3.6445 - val_mse: 3.6445 - val_mae: 1.5207\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 1.51283\n",
      "Epoch 453/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6145 - mse: 4.6145 - mae: 1.6122 - val_loss: 3.6461 - val_mse: 3.6461 - val_mae: 1.5216\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 1.51283\n",
      "Epoch 454/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9249 - mse: 4.9249 - mae: 1.6999 - val_loss: 3.6366 - val_mse: 3.6366 - val_mae: 1.5170\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 1.51283\n",
      "Epoch 455/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0685 - mse: 5.0685 - mae: 1.7541 - val_loss: 3.6340 - val_mse: 3.6340 - val_mae: 1.5157\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 1.51283\n",
      "Epoch 456/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6217 - mse: 4.6217 - mae: 1.6937 - val_loss: 3.6394 - val_mse: 3.6394 - val_mae: 1.5188\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 1.51283\n",
      "Epoch 457/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6904 - mse: 4.6904 - mae: 1.6537 - val_loss: 3.6378 - val_mse: 3.6378 - val_mae: 1.5181\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 1.51283\n",
      "Epoch 458/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1625 - mse: 5.1625 - mae: 1.7528 - val_loss: 3.6370 - val_mse: 3.6370 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 1.51283\n",
      "Epoch 459/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6067 - mse: 5.6067 - mae: 1.7767 - val_loss: 3.6402 - val_mse: 3.6402 - val_mae: 1.5193\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 1.51283\n",
      "Epoch 460/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6038 - mse: 5.6038 - mae: 1.7628 - val_loss: 3.6392 - val_mse: 3.6392 - val_mae: 1.5189\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 1.51283\n",
      "Epoch 461/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1017 - mse: 5.1017 - mae: 1.7756 - val_loss: 3.6352 - val_mse: 3.6352 - val_mae: 1.5170\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 1.51283\n",
      "Epoch 462/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4407 - mse: 5.4407 - mae: 1.7537 - val_loss: 3.6423 - val_mse: 3.6423 - val_mae: 1.5204\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 1.51283\n",
      "Epoch 463/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1807 - mse: 5.1807 - mae: 1.7178 - val_loss: 3.6378 - val_mse: 3.6378 - val_mae: 1.5184\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 1.51283\n",
      "Epoch 464/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7634 - mse: 4.7634 - mae: 1.6953 - val_loss: 3.6345 - val_mse: 3.6345 - val_mae: 1.5169\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 1.51283\n",
      "Epoch 465/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6959 - mse: 4.6959 - mae: 1.6594 - val_loss: 3.6310 - val_mse: 3.6310 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 1.51283\n",
      "Epoch 466/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5034 - mse: 5.5034 - mae: 1.7692 - val_loss: 3.6215 - val_mse: 3.6215 - val_mae: 1.5092\n",
      "\n",
      "Epoch 00466: val_mae improved from 1.51283 to 1.50918, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 467/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2671 - mse: 5.2671 - mae: 1.7115 - val_loss: 3.6265 - val_mse: 3.6265 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 1.50918\n",
      "Epoch 468/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6629 - mse: 4.6629 - mae: 1.6771 - val_loss: 3.6271 - val_mse: 3.6271 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 1.50918\n",
      "Epoch 469/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0803 - mse: 5.0803 - mae: 1.7251 - val_loss: 3.6254 - val_mse: 3.6254 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 1.50918\n",
      "Epoch 470/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3714 - mse: 5.3714 - mae: 1.7400 - val_loss: 3.6388 - val_mse: 3.6388 - val_mae: 1.5195\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 1.50918\n",
      "Epoch 471/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7304 - mse: 4.7304 - mae: 1.6307 - val_loss: 3.6359 - val_mse: 3.6359 - val_mae: 1.5182\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 1.50918\n",
      "Epoch 472/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0933 - mse: 5.0933 - mae: 1.7237 - val_loss: 3.6317 - val_mse: 3.6317 - val_mae: 1.5163\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 1.50918\n",
      "Epoch 473/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0241 - mse: 6.0241 - mae: 1.8157 - val_loss: 3.6362 - val_mse: 3.6362 - val_mae: 1.5186\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 1.50918\n",
      "Epoch 474/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0286 - mse: 5.0286 - mae: 1.7121 - val_loss: 3.6283 - val_mse: 3.6283 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 1.50918\n",
      "Epoch 475/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9016 - mse: 4.9016 - mae: 1.6974 - val_loss: 3.6289 - val_mse: 3.6289 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 1.50918\n",
      "Epoch 476/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7974 - mse: 5.7974 - mae: 1.7728 - val_loss: 3.6186 - val_mse: 3.6186 - val_mae: 1.5094\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 1.50918\n",
      "Epoch 477/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8814 - mse: 4.8814 - mae: 1.6634 - val_loss: 3.6169 - val_mse: 3.6169 - val_mae: 1.5085\n",
      "\n",
      "Epoch 00477: val_mae improved from 1.50918 to 1.50846, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 478/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2186 - mse: 5.2186 - mae: 1.7046 - val_loss: 3.6169 - val_mse: 3.6169 - val_mae: 1.5085\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 1.50846\n",
      "Epoch 479/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5925 - mse: 4.5925 - mae: 1.6009 - val_loss: 3.6207 - val_mse: 3.6207 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 1.50846\n",
      "Epoch 480/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9415 - mse: 4.9415 - mae: 1.7239 - val_loss: 3.6200 - val_mse: 3.6200 - val_mae: 1.5105\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 1.50846\n",
      "Epoch 481/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5043 - mse: 4.5043 - mae: 1.6807 - val_loss: 3.6185 - val_mse: 3.6185 - val_mae: 1.5097\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 1.50846\n",
      "Epoch 482/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8431 - mse: 4.8431 - mae: 1.6538 - val_loss: 3.6264 - val_mse: 3.6264 - val_mae: 1.5143\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 1.50846\n",
      "Epoch 483/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0131 - mse: 5.0131 - mae: 1.7429 - val_loss: 3.6235 - val_mse: 3.6235 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 1.50846\n",
      "Epoch 484/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8611 - mse: 4.8611 - mae: 1.7079 - val_loss: 3.6311 - val_mse: 3.6311 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 1.50846\n",
      "Epoch 485/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7961 - mse: 4.7961 - mae: 1.6608 - val_loss: 3.6262 - val_mse: 3.6262 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 1.50846\n",
      "Epoch 486/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8696 - mse: 4.8696 - mae: 1.6718 - val_loss: 3.6157 - val_mse: 3.6157 - val_mae: 1.5086\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 1.50846\n",
      "Epoch 487/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0771 - mse: 5.0771 - mae: 1.6881 - val_loss: 3.6143 - val_mse: 3.6143 - val_mae: 1.5078\n",
      "\n",
      "Epoch 00487: val_mae improved from 1.50846 to 1.50783, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 488/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7786 - mse: 5.7786 - mae: 1.8205 - val_loss: 3.6166 - val_mse: 3.6166 - val_mae: 1.5093\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 1.50783\n",
      "Epoch 489/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9129 - mse: 4.9129 - mae: 1.6984 - val_loss: 3.6151 - val_mse: 3.6151 - val_mae: 1.5084\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 1.50783\n",
      "Epoch 490/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6984 - mse: 4.6984 - mae: 1.6575 - val_loss: 3.6166 - val_mse: 3.6166 - val_mae: 1.5094\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 1.50783\n",
      "Epoch 491/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6024 - mse: 4.6024 - mae: 1.6599 - val_loss: 3.6125 - val_mse: 3.6125 - val_mae: 1.5068\n",
      "\n",
      "Epoch 00491: val_mae improved from 1.50783 to 1.50677, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 492/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9771 - mse: 4.9771 - mae: 1.7148 - val_loss: 3.6165 - val_mse: 3.6165 - val_mae: 1.5094\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 1.50677\n",
      "Epoch 493/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1872 - mse: 5.1872 - mae: 1.7064 - val_loss: 3.6141 - val_mse: 3.6141 - val_mae: 1.5080\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 1.50677\n",
      "Epoch 494/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9734 - mse: 4.9734 - mae: 1.7192 - val_loss: 3.6164 - val_mse: 3.6164 - val_mae: 1.5094\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 1.50677\n",
      "Epoch 495/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1001 - mse: 5.1001 - mae: 1.7035 - val_loss: 3.6110 - val_mse: 3.6110 - val_mae: 1.5061\n",
      "\n",
      "Epoch 00495: val_mae improved from 1.50677 to 1.50610, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 496/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9271 - mse: 4.9271 - mae: 1.6850 - val_loss: 3.6208 - val_mse: 3.6208 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 1.50610\n",
      "Epoch 497/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1346 - mse: 5.1346 - mae: 1.6975 - val_loss: 3.6169 - val_mse: 3.6169 - val_mae: 1.5097\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 1.50610\n",
      "Epoch 498/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4106 - mse: 5.4106 - mae: 1.6806 - val_loss: 3.6129 - val_mse: 3.6129 - val_mae: 1.5074\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 1.50610\n",
      "Epoch 499/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8170 - mse: 4.8170 - mae: 1.7100 - val_loss: 3.6142 - val_mse: 3.6142 - val_mae: 1.5083\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 1.50610\n",
      "Epoch 500/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3083 - mse: 5.3083 - mae: 1.7385 - val_loss: 3.6137 - val_mse: 3.6137 - val_mae: 1.5079\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 1.50610\n",
      "Epoch 501/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9259 - mse: 4.9259 - mae: 1.6750 - val_loss: 3.6193 - val_mse: 3.6193 - val_mae: 1.5112\n",
      "\n",
      "Epoch 00501: val_mae did not improve from 1.50610\n",
      "Epoch 502/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2427 - mse: 5.2427 - mae: 1.6714 - val_loss: 3.6262 - val_mse: 3.6262 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00502: val_mae did not improve from 1.50610\n",
      "Epoch 503/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7455 - mse: 4.7455 - mae: 1.6461 - val_loss: 3.6193 - val_mse: 3.6193 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00503: val_mae did not improve from 1.50610\n",
      "Epoch 504/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0630 - mse: 5.0630 - mae: 1.7338 - val_loss: 3.6213 - val_mse: 3.6213 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00504: val_mae did not improve from 1.50610\n",
      "Epoch 505/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5970 - mse: 4.5970 - mae: 1.6227 - val_loss: 3.6278 - val_mse: 3.6278 - val_mae: 1.5158\n",
      "\n",
      "Epoch 00505: val_mae did not improve from 1.50610\n",
      "Epoch 506/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8753 - mse: 4.8753 - mae: 1.7196 - val_loss: 3.6195 - val_mse: 3.6195 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00506: val_mae did not improve from 1.50610\n",
      "Epoch 507/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2678 - mse: 5.2678 - mae: 1.7237 - val_loss: 3.6193 - val_mse: 3.6193 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00507: val_mae did not improve from 1.50610\n",
      "Epoch 508/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0617 - mse: 5.0617 - mae: 1.7403 - val_loss: 3.6147 - val_mse: 3.6147 - val_mae: 1.5092\n",
      "\n",
      "Epoch 00508: val_mae did not improve from 1.50610\n",
      "Epoch 509/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3549 - mse: 4.3549 - mae: 1.6043 - val_loss: 3.6206 - val_mse: 3.6206 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00509: val_mae did not improve from 1.50610\n",
      "Epoch 510/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8203 - mse: 4.8203 - mae: 1.7043 - val_loss: 3.6296 - val_mse: 3.6296 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00510: val_mae did not improve from 1.50610\n",
      "Epoch 511/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4240 - mse: 5.4240 - mae: 1.7248 - val_loss: 3.6294 - val_mse: 3.6294 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00511: val_mae did not improve from 1.50610\n",
      "Epoch 512/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1952 - mse: 5.1952 - mae: 1.7514 - val_loss: 3.6207 - val_mse: 3.6207 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00512: val_mae did not improve from 1.50610\n",
      "Epoch 513/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5935 - mse: 4.5935 - mae: 1.6360 - val_loss: 3.6148 - val_mse: 3.6148 - val_mae: 1.5094\n",
      "\n",
      "Epoch 00513: val_mae did not improve from 1.50610\n",
      "Epoch 514/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8554 - mse: 4.8554 - mae: 1.7190 - val_loss: 3.6148 - val_mse: 3.6148 - val_mae: 1.5094\n",
      "\n",
      "Epoch 00514: val_mae did not improve from 1.50610\n",
      "Epoch 515/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9488 - mse: 4.9488 - mae: 1.7032 - val_loss: 3.6270 - val_mse: 3.6270 - val_mae: 1.5158\n",
      "\n",
      "Epoch 00515: val_mae did not improve from 1.50610\n",
      "Epoch 516/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3120 - mse: 5.3120 - mae: 1.7550 - val_loss: 3.6206 - val_mse: 3.6206 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00516: val_mae did not improve from 1.50610\n",
      "Epoch 517/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3558 - mse: 4.3558 - mae: 1.6349 - val_loss: 3.6140 - val_mse: 3.6140 - val_mae: 1.5092\n",
      "\n",
      "Epoch 00517: val_mae did not improve from 1.50610\n",
      "Epoch 518/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2288 - mse: 5.2288 - mae: 1.7140 - val_loss: 3.6246 - val_mse: 3.6246 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00518: val_mae did not improve from 1.50610\n",
      "Epoch 519/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8826 - mse: 5.8826 - mae: 1.8277 - val_loss: 3.6155 - val_mse: 3.6155 - val_mae: 1.5101\n",
      "\n",
      "Epoch 00519: val_mae did not improve from 1.50610\n",
      "Epoch 520/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1136 - mse: 5.1136 - mae: 1.6935 - val_loss: 3.6125 - val_mse: 3.6125 - val_mae: 1.5084\n",
      "\n",
      "Epoch 00520: val_mae did not improve from 1.50610\n",
      "Epoch 521/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2665 - mse: 5.2665 - mae: 1.7742 - val_loss: 3.6159 - val_mse: 3.6159 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00521: val_mae did not improve from 1.50610\n",
      "Epoch 522/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7851 - mse: 5.7851 - mae: 1.7563 - val_loss: 3.6186 - val_mse: 3.6186 - val_mae: 1.5119\n",
      "\n",
      "Epoch 00522: val_mae did not improve from 1.50610\n",
      "Epoch 523/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5624 - mse: 4.5624 - mae: 1.6332 - val_loss: 3.6205 - val_mse: 3.6205 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00523: val_mae did not improve from 1.50610\n",
      "Epoch 524/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4098 - mse: 5.4098 - mae: 1.7595 - val_loss: 3.6104 - val_mse: 3.6104 - val_mae: 1.5076\n",
      "\n",
      "Epoch 00524: val_mae did not improve from 1.50610\n",
      "Epoch 525/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4838 - mse: 5.4838 - mae: 1.7042 - val_loss: 3.6132 - val_mse: 3.6132 - val_mae: 1.5092\n",
      "\n",
      "Epoch 00525: val_mae did not improve from 1.50610\n",
      "Epoch 526/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8358 - mse: 4.8358 - mae: 1.6554 - val_loss: 3.6104 - val_mse: 3.6104 - val_mae: 1.5077\n",
      "\n",
      "Epoch 00526: val_mae did not improve from 1.50610\n",
      "Epoch 527/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3215 - mse: 5.3215 - mae: 1.7491 - val_loss: 3.6105 - val_mse: 3.6105 - val_mae: 1.5076\n",
      "\n",
      "Epoch 00527: val_mae did not improve from 1.50610\n",
      "Epoch 528/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0734 - mse: 5.0734 - mae: 1.6724 - val_loss: 3.6067 - val_mse: 3.6067 - val_mae: 1.5054\n",
      "\n",
      "Epoch 00528: val_mae improved from 1.50610 to 1.50537, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 529/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4956 - mse: 5.4956 - mae: 1.7397 - val_loss: 3.6124 - val_mse: 3.6124 - val_mae: 1.5087\n",
      "\n",
      "Epoch 00529: val_mae did not improve from 1.50537\n",
      "Epoch 530/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6164 - mse: 4.6164 - mae: 1.6364 - val_loss: 3.6079 - val_mse: 3.6079 - val_mae: 1.5064\n",
      "\n",
      "Epoch 00530: val_mae did not improve from 1.50537\n",
      "Epoch 531/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7213 - mse: 4.7213 - mae: 1.6594 - val_loss: 3.6155 - val_mse: 3.6155 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00531: val_mae did not improve from 1.50537\n",
      "Epoch 532/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9877 - mse: 4.9877 - mae: 1.6886 - val_loss: 3.6100 - val_mse: 3.6100 - val_mae: 1.5074\n",
      "\n",
      "Epoch 00532: val_mae did not improve from 1.50537\n",
      "Epoch 533/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.8285 - mse: 5.8285 - mae: 1.7561 - val_loss: 3.6112 - val_mse: 3.6112 - val_mae: 1.5081\n",
      "\n",
      "Epoch 00533: val_mae did not improve from 1.50537\n",
      "Epoch 534/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.3925 - mse: 4.3925 - mae: 1.6038 - val_loss: 3.6091 - val_mse: 3.6091 - val_mae: 1.5072\n",
      "\n",
      "Epoch 00534: val_mae did not improve from 1.50537\n",
      "Epoch 535/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8700 - mse: 5.8700 - mae: 1.8019 - val_loss: 3.6083 - val_mse: 3.6083 - val_mae: 1.5068\n",
      "\n",
      "Epoch 00535: val_mae did not improve from 1.50537\n",
      "Epoch 536/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8060 - mse: 4.8060 - mae: 1.7095 - val_loss: 3.6057 - val_mse: 3.6057 - val_mae: 1.5053\n",
      "\n",
      "Epoch 00536: val_mae improved from 1.50537 to 1.50531, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 537/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3550 - mse: 4.3550 - mae: 1.6174 - val_loss: 3.6049 - val_mse: 3.6049 - val_mae: 1.5048\n",
      "\n",
      "Epoch 00537: val_mae improved from 1.50531 to 1.50481, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 538/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3848 - mse: 5.3848 - mae: 1.7081 - val_loss: 3.6107 - val_mse: 3.6107 - val_mae: 1.5081\n",
      "\n",
      "Epoch 00538: val_mae did not improve from 1.50481\n",
      "Epoch 539/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3063 - mse: 5.3063 - mae: 1.7278 - val_loss: 3.6094 - val_mse: 3.6094 - val_mae: 1.5071\n",
      "\n",
      "Epoch 00539: val_mae did not improve from 1.50481\n",
      "Epoch 540/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9394 - mse: 4.9394 - mae: 1.6858 - val_loss: 3.6098 - val_mse: 3.6098 - val_mae: 1.5074\n",
      "\n",
      "Epoch 00540: val_mae did not improve from 1.50481\n",
      "Epoch 541/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2776 - mse: 5.2776 - mae: 1.7429 - val_loss: 3.6049 - val_mse: 3.6049 - val_mae: 1.5046\n",
      "\n",
      "Epoch 00541: val_mae improved from 1.50481 to 1.50461, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 542/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6454 - mse: 4.6454 - mae: 1.6792 - val_loss: 3.6062 - val_mse: 3.6062 - val_mae: 1.5053\n",
      "\n",
      "Epoch 00542: val_mae did not improve from 1.50461\n",
      "Epoch 543/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7252 - mse: 5.7252 - mae: 1.8229 - val_loss: 3.6151 - val_mse: 3.6151 - val_mae: 1.5103\n",
      "\n",
      "Epoch 00543: val_mae did not improve from 1.50461\n",
      "Epoch 544/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2611 - mse: 5.2611 - mae: 1.7417 - val_loss: 3.6124 - val_mse: 3.6124 - val_mae: 1.5089\n",
      "\n",
      "Epoch 00544: val_mae did not improve from 1.50461\n",
      "Epoch 545/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.2651 - mse: 4.2651 - mae: 1.5789 - val_loss: 3.6071 - val_mse: 3.6071 - val_mae: 1.5061\n",
      "\n",
      "Epoch 00545: val_mae did not improve from 1.50461\n",
      "Epoch 546/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7351 - mse: 4.7351 - mae: 1.7125 - val_loss: 3.6168 - val_mse: 3.6168 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00546: val_mae did not improve from 1.50461\n",
      "Epoch 547/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2393 - mse: 5.2393 - mae: 1.7139 - val_loss: 3.6124 - val_mse: 3.6124 - val_mae: 1.5093\n",
      "\n",
      "Epoch 00547: val_mae did not improve from 1.50461\n",
      "Epoch 548/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3062 - mse: 5.3062 - mae: 1.7043 - val_loss: 3.6101 - val_mse: 3.6101 - val_mae: 1.5081\n",
      "\n",
      "Epoch 00548: val_mae did not improve from 1.50461\n",
      "Epoch 549/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1530 - mse: 5.1530 - mae: 1.7237 - val_loss: 3.6176 - val_mse: 3.6176 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00549: val_mae did not improve from 1.50461\n",
      "Epoch 550/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3349 - mse: 5.3349 - mae: 1.7103 - val_loss: 3.6144 - val_mse: 3.6144 - val_mae: 1.5105\n",
      "\n",
      "Epoch 00550: val_mae did not improve from 1.50461\n",
      "Epoch 551/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5404 - mse: 5.5404 - mae: 1.7270 - val_loss: 3.6063 - val_mse: 3.6063 - val_mae: 1.5063\n",
      "\n",
      "Epoch 00551: val_mae did not improve from 1.50461\n",
      "Epoch 552/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6028 - mse: 4.6028 - mae: 1.6328 - val_loss: 3.6038 - val_mse: 3.6038 - val_mae: 1.5050\n",
      "\n",
      "Epoch 00552: val_mae did not improve from 1.50461\n",
      "Epoch 553/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9117 - mse: 5.9117 - mae: 1.8006 - val_loss: 3.6061 - val_mse: 3.6061 - val_mae: 1.5064\n",
      "\n",
      "Epoch 00553: val_mae did not improve from 1.50461\n",
      "Epoch 554/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1363 - mse: 5.1363 - mae: 1.7460 - val_loss: 3.6022 - val_mse: 3.6022 - val_mae: 1.5042\n",
      "\n",
      "Epoch 00554: val_mae improved from 1.50461 to 1.50417, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 555/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5950 - mse: 4.5950 - mae: 1.6631 - val_loss: 3.6030 - val_mse: 3.6030 - val_mae: 1.5047\n",
      "\n",
      "Epoch 00555: val_mae did not improve from 1.50417\n",
      "Epoch 556/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0872 - mse: 5.0872 - mae: 1.6680 - val_loss: 3.6074 - val_mse: 3.6074 - val_mae: 1.5070\n",
      "\n",
      "Epoch 00556: val_mae did not improve from 1.50417\n",
      "Epoch 557/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8601 - mse: 5.8601 - mae: 1.7662 - val_loss: 3.6075 - val_mse: 3.6075 - val_mae: 1.5071\n",
      "\n",
      "Epoch 00557: val_mae did not improve from 1.50417\n",
      "Epoch 558/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8496 - mse: 4.8496 - mae: 1.5840 - val_loss: 3.6099 - val_mse: 3.6099 - val_mae: 1.5084\n",
      "\n",
      "Epoch 00558: val_mae did not improve from 1.50417\n",
      "Epoch 559/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0298 - mse: 5.0298 - mae: 1.6706 - val_loss: 3.6081 - val_mse: 3.6081 - val_mae: 1.5074\n",
      "\n",
      "Epoch 00559: val_mae did not improve from 1.50417\n",
      "Epoch 560/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5184 - mse: 5.5184 - mae: 1.7256 - val_loss: 3.6020 - val_mse: 3.6020 - val_mae: 1.5040\n",
      "\n",
      "Epoch 00560: val_mae improved from 1.50417 to 1.50403, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 561/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9743 - mse: 4.9743 - mae: 1.6630 - val_loss: 3.6179 - val_mse: 3.6179 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00561: val_mae did not improve from 1.50403\n",
      "Epoch 562/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2553 - mse: 5.2553 - mae: 1.6687 - val_loss: 3.6128 - val_mse: 3.6128 - val_mae: 1.5100\n",
      "\n",
      "Epoch 00562: val_mae did not improve from 1.50403\n",
      "Epoch 563/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9557 - mse: 4.9557 - mae: 1.6973 - val_loss: 3.6076 - val_mse: 3.6076 - val_mae: 1.5073\n",
      "\n",
      "Epoch 00563: val_mae did not improve from 1.50403\n",
      "Epoch 564/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6164 - mse: 5.6164 - mae: 1.7598 - val_loss: 3.6168 - val_mse: 3.6168 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00564: val_mae did not improve from 1.50403\n",
      "Epoch 565/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8196 - mse: 4.8196 - mae: 1.6726 - val_loss: 3.6058 - val_mse: 3.6058 - val_mae: 1.5063\n",
      "\n",
      "Epoch 00565: val_mae did not improve from 1.50403\n",
      "Epoch 566/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7148 - mse: 4.7148 - mae: 1.6829 - val_loss: 3.6009 - val_mse: 3.6009 - val_mae: 1.5038\n",
      "\n",
      "Epoch 00566: val_mae improved from 1.50403 to 1.50375, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 567/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9107 - mse: 4.9107 - mae: 1.6528 - val_loss: 3.6097 - val_mse: 3.6097 - val_mae: 1.5087\n",
      "\n",
      "Epoch 00567: val_mae did not improve from 1.50375\n",
      "Epoch 568/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8271 - mse: 4.8271 - mae: 1.6929 - val_loss: 3.6014 - val_mse: 3.6014 - val_mae: 1.5040\n",
      "\n",
      "Epoch 00568: val_mae did not improve from 1.50375\n",
      "Epoch 569/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4653 - mse: 4.4653 - mae: 1.5944 - val_loss: 3.6096 - val_mse: 3.6096 - val_mae: 1.5085\n",
      "\n",
      "Epoch 00569: val_mae did not improve from 1.50375\n",
      "Epoch 570/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3603 - mse: 4.3603 - mae: 1.5877 - val_loss: 3.6170 - val_mse: 3.6170 - val_mae: 1.5122\n",
      "\n",
      "Epoch 00570: val_mae did not improve from 1.50375\n",
      "Epoch 571/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0000 - mse: 5.0000 - mae: 1.7069 - val_loss: 3.6218 - val_mse: 3.6218 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00571: val_mae did not improve from 1.50375\n",
      "Epoch 572/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9835 - mse: 4.9835 - mae: 1.7579 - val_loss: 3.6202 - val_mse: 3.6202 - val_mae: 1.5137\n",
      "\n",
      "Epoch 00572: val_mae did not improve from 1.50375\n",
      "Epoch 573/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0299 - mse: 5.0299 - mae: 1.7210 - val_loss: 3.6318 - val_mse: 3.6318 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00573: val_mae did not improve from 1.50375\n",
      "Epoch 574/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6155 - mse: 4.6155 - mae: 1.5901 - val_loss: 3.6066 - val_mse: 3.6066 - val_mae: 1.5067\n",
      "\n",
      "Epoch 00574: val_mae did not improve from 1.50375\n",
      "Epoch 575/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.8611 - mse: 4.8611 - mae: 1.6608 - val_loss: 3.6067 - val_mse: 3.6067 - val_mae: 1.5066\n",
      "\n",
      "Epoch 00575: val_mae did not improve from 1.50375\n",
      "Epoch 576/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7823 - mse: 4.7823 - mae: 1.6713 - val_loss: 3.6118 - val_mse: 3.6118 - val_mae: 1.5093\n",
      "\n",
      "Epoch 00576: val_mae did not improve from 1.50375\n",
      "Epoch 577/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.4896 - mse: 4.4896 - mae: 1.6470 - val_loss: 3.6109 - val_mse: 3.6109 - val_mae: 1.5089\n",
      "\n",
      "Epoch 00577: val_mae did not improve from 1.50375\n",
      "Epoch 578/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.7273 - mse: 5.7273 - mae: 1.7657 - val_loss: 3.6040 - val_mse: 3.6040 - val_mae: 1.5051\n",
      "\n",
      "Epoch 00578: val_mae did not improve from 1.50375\n",
      "Epoch 579/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4161 - mse: 5.4161 - mae: 1.7663 - val_loss: 3.6012 - val_mse: 3.6012 - val_mae: 1.5036\n",
      "\n",
      "Epoch 00579: val_mae improved from 1.50375 to 1.50362, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 580/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9111 - mse: 4.9111 - mae: 1.7029 - val_loss: 3.6051 - val_mse: 3.6051 - val_mae: 1.5058\n",
      "\n",
      "Epoch 00580: val_mae did not improve from 1.50362\n",
      "Epoch 581/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6765 - mse: 4.6765 - mae: 1.6537 - val_loss: 3.6045 - val_mse: 3.6045 - val_mae: 1.5053\n",
      "\n",
      "Epoch 00581: val_mae did not improve from 1.50362\n",
      "Epoch 582/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8766 - mse: 5.8766 - mae: 1.8373 - val_loss: 3.6020 - val_mse: 3.6020 - val_mae: 1.5040\n",
      "\n",
      "Epoch 00582: val_mae did not improve from 1.50362\n",
      "Epoch 583/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4804 - mse: 5.4804 - mae: 1.7786 - val_loss: 3.5987 - val_mse: 3.5987 - val_mae: 1.5018\n",
      "\n",
      "Epoch 00583: val_mae improved from 1.50362 to 1.50176, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 584/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1212 - mse: 5.1212 - mae: 1.7316 - val_loss: 3.5979 - val_mse: 3.5979 - val_mae: 1.5007\n",
      "\n",
      "Epoch 00584: val_mae improved from 1.50176 to 1.50072, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 585/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2072 - mse: 5.2072 - mae: 1.7443 - val_loss: 3.6045 - val_mse: 3.6045 - val_mae: 1.5053\n",
      "\n",
      "Epoch 00585: val_mae did not improve from 1.50072\n",
      "Epoch 586/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2970 - mse: 5.2970 - mae: 1.6867 - val_loss: 3.6088 - val_mse: 3.6088 - val_mae: 1.5075\n",
      "\n",
      "Epoch 00586: val_mae did not improve from 1.50072\n",
      "Epoch 587/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9333 - mse: 4.9333 - mae: 1.7027 - val_loss: 3.6004 - val_mse: 3.6004 - val_mae: 1.5027\n",
      "\n",
      "Epoch 00587: val_mae did not improve from 1.50072\n",
      "Epoch 588/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6361 - mse: 4.6361 - mae: 1.6233 - val_loss: 3.6119 - val_mse: 3.6119 - val_mae: 1.5091\n",
      "\n",
      "Epoch 00588: val_mae did not improve from 1.50072\n",
      "Epoch 589/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6914 - mse: 4.6914 - mae: 1.6115 - val_loss: 3.6093 - val_mse: 3.6093 - val_mae: 1.5077\n",
      "\n",
      "Epoch 00589: val_mae did not improve from 1.50072\n",
      "Epoch 590/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4383 - mse: 5.4383 - mae: 1.7864 - val_loss: 3.6157 - val_mse: 3.6157 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00590: val_mae did not improve from 1.50072\n",
      "Epoch 591/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8342 - mse: 4.8342 - mae: 1.6947 - val_loss: 3.6018 - val_mse: 3.6018 - val_mae: 1.5036\n",
      "\n",
      "Epoch 00591: val_mae did not improve from 1.50072\n",
      "Epoch 592/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8855 - mse: 4.8855 - mae: 1.7112 - val_loss: 3.6014 - val_mse: 3.6014 - val_mae: 1.5032\n",
      "\n",
      "Epoch 00592: val_mae did not improve from 1.50072\n",
      "Epoch 593/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3603 - mse: 4.3603 - mae: 1.5739 - val_loss: 3.6080 - val_mse: 3.6080 - val_mae: 1.5070\n",
      "\n",
      "Epoch 00593: val_mae did not improve from 1.50072\n",
      "Epoch 594/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5970 - mse: 4.5970 - mae: 1.6306 - val_loss: 3.6059 - val_mse: 3.6059 - val_mae: 1.5058\n",
      "\n",
      "Epoch 00594: val_mae did not improve from 1.50072\n",
      "Epoch 595/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0734 - mse: 5.0734 - mae: 1.6882 - val_loss: 3.6103 - val_mse: 3.6103 - val_mae: 1.5083\n",
      "\n",
      "Epoch 00595: val_mae did not improve from 1.50072\n",
      "Epoch 596/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0265 - mse: 5.0265 - mae: 1.6213 - val_loss: 3.5984 - val_mse: 3.5984 - val_mae: 1.5014\n",
      "\n",
      "Epoch 00596: val_mae did not improve from 1.50072\n",
      "Epoch 597/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6979 - mse: 4.6979 - mae: 1.6791 - val_loss: 3.5998 - val_mse: 3.5998 - val_mae: 1.5023\n",
      "\n",
      "Epoch 00597: val_mae did not improve from 1.50072\n",
      "Epoch 598/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2901 - mse: 4.2901 - mae: 1.6002 - val_loss: 3.6094 - val_mse: 3.6094 - val_mae: 1.5079\n",
      "\n",
      "Epoch 00598: val_mae did not improve from 1.50072\n",
      "Epoch 599/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1045 - mse: 5.1045 - mae: 1.6920 - val_loss: 3.6052 - val_mse: 3.6052 - val_mae: 1.5057\n",
      "\n",
      "Epoch 00599: val_mae did not improve from 1.50072\n",
      "Epoch 600/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0127 - mse: 5.0127 - mae: 1.7122 - val_loss: 3.6023 - val_mse: 3.6023 - val_mae: 1.5042\n",
      "\n",
      "Epoch 00600: val_mae did not improve from 1.50072\n",
      "Epoch 601/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2177 - mse: 5.2177 - mae: 1.7621 - val_loss: 3.6050 - val_mse: 3.6050 - val_mae: 1.5056\n",
      "\n",
      "Epoch 00601: val_mae did not improve from 1.50072\n",
      "Epoch 602/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9282 - mse: 4.9282 - mae: 1.6534 - val_loss: 3.6046 - val_mse: 3.6046 - val_mae: 1.5054\n",
      "\n",
      "Epoch 00602: val_mae did not improve from 1.50072\n",
      "Epoch 603/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.7250 - mse: 4.7250 - mae: 1.6794 - val_loss: 3.6008 - val_mse: 3.6008 - val_mae: 1.5034\n",
      "\n",
      "Epoch 00603: val_mae did not improve from 1.50072\n",
      "Epoch 604/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8973 - mse: 4.8973 - mae: 1.6882 - val_loss: 3.6008 - val_mse: 3.6008 - val_mae: 1.5032\n",
      "\n",
      "Epoch 00604: val_mae did not improve from 1.50072\n",
      "Epoch 605/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8929 - mse: 4.8929 - mae: 1.6917 - val_loss: 3.6104 - val_mse: 3.6104 - val_mae: 1.5086\n",
      "\n",
      "Epoch 00605: val_mae did not improve from 1.50072\n",
      "Epoch 606/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5111 - mse: 5.5111 - mae: 1.7791 - val_loss: 3.6084 - val_mse: 3.6084 - val_mae: 1.5072\n",
      "\n",
      "Epoch 00606: val_mae did not improve from 1.50072\n",
      "Epoch 607/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3254 - mse: 5.3254 - mae: 1.7194 - val_loss: 3.6108 - val_mse: 3.6108 - val_mae: 1.5088\n",
      "\n",
      "Epoch 00607: val_mae did not improve from 1.50072\n",
      "Epoch 608/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7964 - mse: 4.7964 - mae: 1.6768 - val_loss: 3.6065 - val_mse: 3.6065 - val_mae: 1.5066\n",
      "\n",
      "Epoch 00608: val_mae did not improve from 1.50072\n",
      "Epoch 609/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9114 - mse: 4.9114 - mae: 1.6587 - val_loss: 3.6064 - val_mse: 3.6064 - val_mae: 1.5065\n",
      "\n",
      "Epoch 00609: val_mae did not improve from 1.50072\n",
      "Epoch 610/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2488 - mse: 5.2488 - mae: 1.7567 - val_loss: 3.6090 - val_mse: 3.6090 - val_mae: 1.5080\n",
      "\n",
      "Epoch 00610: val_mae did not improve from 1.50072\n",
      "Epoch 611/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1407 - mse: 5.1407 - mae: 1.7650 - val_loss: 3.6170 - val_mse: 3.6170 - val_mae: 1.5121\n",
      "\n",
      "Epoch 00611: val_mae did not improve from 1.50072\n",
      "Epoch 612/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6817 - mse: 5.6817 - mae: 1.8136 - val_loss: 3.6110 - val_mse: 3.6110 - val_mae: 1.5091\n",
      "\n",
      "Epoch 00612: val_mae did not improve from 1.50072\n",
      "Epoch 613/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0025 - mse: 5.0025 - mae: 1.7116 - val_loss: 3.6061 - val_mse: 3.6061 - val_mae: 1.5065\n",
      "\n",
      "Epoch 00613: val_mae did not improve from 1.50072\n",
      "Epoch 614/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4394 - mse: 4.4393 - mae: 1.5974 - val_loss: 3.6124 - val_mse: 3.6124 - val_mae: 1.5100\n",
      "\n",
      "Epoch 00614: val_mae did not improve from 1.50072\n",
      "Epoch 615/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5568 - mse: 4.5568 - mae: 1.6868 - val_loss: 3.6053 - val_mse: 3.6053 - val_mae: 1.5061\n",
      "\n",
      "Epoch 00615: val_mae did not improve from 1.50072\n",
      "Epoch 616/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8873 - mse: 4.8873 - mae: 1.6753 - val_loss: 3.6012 - val_mse: 3.6012 - val_mae: 1.5039\n",
      "\n",
      "Epoch 00616: val_mae did not improve from 1.50072\n",
      "Epoch 617/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2034 - mse: 5.2034 - mae: 1.7237 - val_loss: 3.6033 - val_mse: 3.6033 - val_mae: 1.5050\n",
      "\n",
      "Epoch 00617: val_mae did not improve from 1.50072\n",
      "Epoch 618/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9941 - mse: 5.9941 - mae: 1.8108 - val_loss: 3.6051 - val_mse: 3.6051 - val_mae: 1.5058\n",
      "\n",
      "Epoch 00618: val_mae did not improve from 1.50072\n",
      "Epoch 619/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5568 - mse: 5.5568 - mae: 1.7585 - val_loss: 3.5967 - val_mse: 3.5967 - val_mae: 1.5011\n",
      "\n",
      "Epoch 00619: val_mae did not improve from 1.50072\n",
      "Epoch 620/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9752 - mse: 5.9752 - mae: 1.7769 - val_loss: 3.6027 - val_mse: 3.6027 - val_mae: 1.5047\n",
      "\n",
      "Epoch 00620: val_mae did not improve from 1.50072\n",
      "Epoch 621/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4761 - mse: 4.4761 - mae: 1.6100 - val_loss: 3.6059 - val_mse: 3.6059 - val_mae: 1.5064\n",
      "\n",
      "Epoch 00621: val_mae did not improve from 1.50072\n",
      "Epoch 622/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0228 - mse: 5.0228 - mae: 1.7065 - val_loss: 3.6050 - val_mse: 3.6050 - val_mae: 1.5058\n",
      "\n",
      "Epoch 00622: val_mae did not improve from 1.50072\n",
      "Epoch 623/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6805 - mse: 5.6805 - mae: 1.7599 - val_loss: 3.5993 - val_mse: 3.5993 - val_mae: 1.5027\n",
      "\n",
      "Epoch 00623: val_mae did not improve from 1.50072\n",
      "Epoch 624/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3837 - mse: 5.3837 - mae: 1.7578 - val_loss: 3.6099 - val_mse: 3.6099 - val_mae: 1.5085\n",
      "\n",
      "Epoch 00624: val_mae did not improve from 1.50072\n",
      "Epoch 625/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2105 - mse: 5.2105 - mae: 1.7416 - val_loss: 3.6035 - val_mse: 3.6035 - val_mae: 1.5051\n",
      "\n",
      "Epoch 00625: val_mae did not improve from 1.50072\n",
      "Epoch 626/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0990 - mse: 5.0990 - mae: 1.7146 - val_loss: 3.6000 - val_mse: 3.6000 - val_mae: 1.5032\n",
      "\n",
      "Epoch 00626: val_mae did not improve from 1.50072\n",
      "Epoch 627/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4869 - mse: 5.4869 - mae: 1.7386 - val_loss: 3.6115 - val_mse: 3.6115 - val_mae: 1.5094\n",
      "\n",
      "Epoch 00627: val_mae did not improve from 1.50072\n",
      "Epoch 628/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8115 - mse: 4.8115 - mae: 1.6564 - val_loss: 3.6004 - val_mse: 3.6004 - val_mae: 1.5035\n",
      "\n",
      "Epoch 00628: val_mae did not improve from 1.50072\n",
      "Epoch 629/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5299 - mse: 5.5299 - mae: 1.7971 - val_loss: 3.6024 - val_mse: 3.6024 - val_mae: 1.5046\n",
      "\n",
      "Epoch 00629: val_mae did not improve from 1.50072\n",
      "Epoch 630/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0278 - mse: 5.0278 - mae: 1.6651 - val_loss: 3.5995 - val_mse: 3.5995 - val_mae: 1.5030\n",
      "\n",
      "Epoch 00630: val_mae did not improve from 1.50072\n",
      "Epoch 631/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7149 - mse: 4.7149 - mae: 1.6589 - val_loss: 3.6001 - val_mse: 3.6001 - val_mae: 1.5035\n",
      "\n",
      "Epoch 00631: val_mae did not improve from 1.50072\n",
      "Epoch 632/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8061 - mse: 4.8061 - mae: 1.6698 - val_loss: 3.6009 - val_mse: 3.6009 - val_mae: 1.5039\n",
      "\n",
      "Epoch 00632: val_mae did not improve from 1.50072\n",
      "Epoch 633/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8668 - mse: 4.8668 - mae: 1.6617 - val_loss: 3.6035 - val_mse: 3.6035 - val_mae: 1.5052\n",
      "\n",
      "Epoch 00633: val_mae did not improve from 1.50072\n",
      "Epoch 634/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4952 - mse: 4.4952 - mae: 1.5992 - val_loss: 3.6070 - val_mse: 3.6070 - val_mae: 1.5072\n",
      "\n",
      "Epoch 00634: val_mae did not improve from 1.50072\n",
      "Epoch 635/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4872 - mse: 5.4872 - mae: 1.7597 - val_loss: 3.6019 - val_mse: 3.6019 - val_mae: 1.5045\n",
      "\n",
      "Epoch 00635: val_mae did not improve from 1.50072\n",
      "Epoch 636/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8920 - mse: 4.8920 - mae: 1.6748 - val_loss: 3.6013 - val_mse: 3.6013 - val_mae: 1.5042\n",
      "\n",
      "Epoch 00636: val_mae did not improve from 1.50072\n",
      "Epoch 637/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6928 - mse: 4.6928 - mae: 1.6394 - val_loss: 3.5974 - val_mse: 3.5974 - val_mae: 1.5021\n",
      "\n",
      "Epoch 00637: val_mae did not improve from 1.50072\n",
      "Epoch 638/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5479 - mse: 5.5479 - mae: 1.7551 - val_loss: 3.6049 - val_mse: 3.6049 - val_mae: 1.5061\n",
      "\n",
      "Epoch 00638: val_mae did not improve from 1.50072\n",
      "Epoch 639/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.6621 - mse: 4.6621 - mae: 1.6288 - val_loss: 3.6126 - val_mse: 3.6126 - val_mae: 1.5102\n",
      "\n",
      "Epoch 00639: val_mae did not improve from 1.50072\n",
      "Epoch 640/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2445 - mse: 5.2445 - mae: 1.7548 - val_loss: 3.6066 - val_mse: 3.6066 - val_mae: 1.5070\n",
      "\n",
      "Epoch 00640: val_mae did not improve from 1.50072\n",
      "Epoch 641/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.0497 - mse: 5.0497 - mae: 1.7192 - val_loss: 3.6082 - val_mse: 3.6082 - val_mae: 1.5079\n",
      "\n",
      "Epoch 00641: val_mae did not improve from 1.50072\n",
      "Epoch 642/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1700 - mse: 5.1700 - mae: 1.7587 - val_loss: 3.6005 - val_mse: 3.6005 - val_mae: 1.5038\n",
      "\n",
      "Epoch 00642: val_mae did not improve from 1.50072\n",
      "Epoch 643/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6257 - mse: 5.6257 - mae: 1.7841 - val_loss: 3.5999 - val_mse: 3.5999 - val_mae: 1.5034\n",
      "\n",
      "Epoch 00643: val_mae did not improve from 1.50072\n",
      "Epoch 644/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3438 - mse: 5.3438 - mae: 1.7126 - val_loss: 3.6026 - val_mse: 3.6026 - val_mae: 1.5048\n",
      "\n",
      "Epoch 00644: val_mae did not improve from 1.50072\n",
      "Epoch 645/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4276 - mse: 4.4276 - mae: 1.5945 - val_loss: 3.6063 - val_mse: 3.6063 - val_mae: 1.5068\n",
      "\n",
      "Epoch 00645: val_mae did not improve from 1.50072\n",
      "Epoch 646/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7513 - mse: 4.7513 - mae: 1.6274 - val_loss: 3.6032 - val_mse: 3.6032 - val_mae: 1.5050\n",
      "\n",
      "Epoch 00646: val_mae did not improve from 1.50072\n",
      "Epoch 647/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7557 - mse: 5.7557 - mae: 1.7781 - val_loss: 3.6187 - val_mse: 3.6187 - val_mae: 1.5129\n",
      "\n",
      "Epoch 00647: val_mae did not improve from 1.50072\n",
      "Epoch 648/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6238 - mse: 5.6238 - mae: 1.7915 - val_loss: 3.6098 - val_mse: 3.6098 - val_mae: 1.5086\n",
      "\n",
      "Epoch 00648: val_mae did not improve from 1.50072\n",
      "Epoch 649/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5772 - mse: 5.5772 - mae: 1.7146 - val_loss: 3.6049 - val_mse: 3.6049 - val_mae: 1.5061\n",
      "\n",
      "Epoch 00649: val_mae did not improve from 1.50072\n",
      "Epoch 650/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8672 - mse: 4.8672 - mae: 1.6582 - val_loss: 3.5963 - val_mse: 3.5963 - val_mae: 1.5012\n",
      "\n",
      "Epoch 00650: val_mae did not improve from 1.50072\n",
      "Epoch 651/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6807 - mse: 4.6807 - mae: 1.6693 - val_loss: 3.5955 - val_mse: 3.5955 - val_mae: 1.5004\n",
      "\n",
      "Epoch 00651: val_mae improved from 1.50072 to 1.50043, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 652/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.8424 - mse: 5.8424 - mae: 1.7729 - val_loss: 3.6002 - val_mse: 3.6002 - val_mae: 1.5034\n",
      "\n",
      "Epoch 00652: val_mae did not improve from 1.50043\n",
      "Epoch 653/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7712 - mse: 4.7712 - mae: 1.6581 - val_loss: 3.5955 - val_mse: 3.5955 - val_mae: 1.5005\n",
      "\n",
      "Epoch 00653: val_mae did not improve from 1.50043\n",
      "Epoch 654/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6521 - mse: 4.6521 - mae: 1.6283 - val_loss: 3.6073 - val_mse: 3.6073 - val_mae: 1.5072\n",
      "\n",
      "Epoch 00654: val_mae did not improve from 1.50043\n",
      "Epoch 655/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1176 - mse: 5.1176 - mae: 1.6694 - val_loss: 3.6091 - val_mse: 3.6091 - val_mae: 1.5082\n",
      "\n",
      "Epoch 00655: val_mae did not improve from 1.50043\n",
      "Epoch 656/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5615 - mse: 4.5615 - mae: 1.6240 - val_loss: 3.6095 - val_mse: 3.6095 - val_mae: 1.5086\n",
      "\n",
      "Epoch 00656: val_mae did not improve from 1.50043\n",
      "Epoch 657/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4171 - mse: 5.4171 - mae: 1.7611 - val_loss: 3.6002 - val_mse: 3.6002 - val_mae: 1.5037\n",
      "\n",
      "Epoch 00657: val_mae did not improve from 1.50043\n",
      "Epoch 658/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7049 - mse: 5.7049 - mae: 1.8458 - val_loss: 3.6022 - val_mse: 3.6022 - val_mae: 1.5047\n",
      "\n",
      "Epoch 00658: val_mae did not improve from 1.50043\n",
      "Epoch 659/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.0405 - mse: 5.0405 - mae: 1.7370 - val_loss: 3.6007 - val_mse: 3.6007 - val_mae: 1.5039\n",
      "\n",
      "Epoch 00659: val_mae did not improve from 1.50043\n",
      "Epoch 660/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1811 - mse: 5.1811 - mae: 1.7363 - val_loss: 3.6062 - val_mse: 3.6062 - val_mae: 1.5067\n",
      "\n",
      "Epoch 00660: val_mae did not improve from 1.50043\n",
      "Epoch 661/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5996 - mse: 5.5996 - mae: 1.7767 - val_loss: 3.6040 - val_mse: 3.6040 - val_mae: 1.5057\n",
      "\n",
      "Epoch 00661: val_mae did not improve from 1.50043\n",
      "Epoch 662/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2150 - mse: 5.2150 - mae: 1.7013 - val_loss: 3.5985 - val_mse: 3.5985 - val_mae: 1.5029\n",
      "\n",
      "Epoch 00662: val_mae did not improve from 1.50043\n",
      "Epoch 663/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2891 - mse: 4.2891 - mae: 1.5958 - val_loss: 3.5960 - val_mse: 3.5960 - val_mae: 1.5014\n",
      "\n",
      "Epoch 00663: val_mae did not improve from 1.50043\n",
      "Epoch 664/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7011 - mse: 4.7011 - mae: 1.6441 - val_loss: 3.6041 - val_mse: 3.6041 - val_mae: 1.5058\n",
      "\n",
      "Epoch 00664: val_mae did not improve from 1.50043\n",
      "Epoch 665/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5036 - mse: 4.5036 - mae: 1.6231 - val_loss: 3.6018 - val_mse: 3.6018 - val_mae: 1.5046\n",
      "\n",
      "Epoch 00665: val_mae did not improve from 1.50043\n",
      "Epoch 666/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2579 - mse: 5.2579 - mae: 1.7032 - val_loss: 3.5990 - val_mse: 3.5990 - val_mae: 1.5033\n",
      "\n",
      "Epoch 00666: val_mae did not improve from 1.50043\n",
      "Epoch 667/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8871 - mse: 4.8871 - mae: 1.6500 - val_loss: 3.5964 - val_mse: 3.5964 - val_mae: 1.5018\n",
      "\n",
      "Epoch 00667: val_mae did not improve from 1.50043\n",
      "Epoch 668/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2542 - mse: 4.2542 - mae: 1.5870 - val_loss: 3.6005 - val_mse: 3.6005 - val_mae: 1.5040\n",
      "\n",
      "Epoch 00668: val_mae did not improve from 1.50043\n",
      "Epoch 669/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3863 - mse: 5.3863 - mae: 1.7391 - val_loss: 3.5975 - val_mse: 3.5975 - val_mae: 1.5024\n",
      "\n",
      "Epoch 00669: val_mae did not improve from 1.50043\n",
      "Epoch 670/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7625 - mse: 4.7625 - mae: 1.6433 - val_loss: 3.5979 - val_mse: 3.5979 - val_mae: 1.5026\n",
      "\n",
      "Epoch 00670: val_mae did not improve from 1.50043\n",
      "Epoch 671/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3536 - mse: 5.3536 - mae: 1.7047 - val_loss: 3.6030 - val_mse: 3.6030 - val_mae: 1.5053\n",
      "\n",
      "Epoch 00671: val_mae did not improve from 1.50043\n",
      "Epoch 672/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8037 - mse: 4.8037 - mae: 1.6833 - val_loss: 3.6009 - val_mse: 3.6009 - val_mae: 1.5043\n",
      "\n",
      "Epoch 00672: val_mae did not improve from 1.50043\n",
      "Epoch 673/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2284 - mse: 5.2284 - mae: 1.7552 - val_loss: 3.6028 - val_mse: 3.6028 - val_mae: 1.5053\n",
      "\n",
      "Epoch 00673: val_mae did not improve from 1.50043\n",
      "Epoch 674/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6565 - mse: 4.6565 - mae: 1.6394 - val_loss: 3.6016 - val_mse: 3.6016 - val_mae: 1.5046\n",
      "\n",
      "Epoch 00674: val_mae did not improve from 1.50043\n",
      "Epoch 675/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7936 - mse: 4.7936 - mae: 1.6684 - val_loss: 3.6025 - val_mse: 3.6025 - val_mae: 1.5051\n",
      "\n",
      "Epoch 00675: val_mae did not improve from 1.50043\n",
      "Epoch 676/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0346 - mse: 5.0346 - mae: 1.7166 - val_loss: 3.6022 - val_mse: 3.6022 - val_mae: 1.5051\n",
      "\n",
      "Epoch 00676: val_mae did not improve from 1.50043\n",
      "Epoch 677/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9060 - mse: 4.9060 - mae: 1.6817 - val_loss: 3.5955 - val_mse: 3.5955 - val_mae: 1.5015\n",
      "\n",
      "Epoch 00677: val_mae did not improve from 1.50043\n",
      "Epoch 678/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1176 - mse: 5.1176 - mae: 1.6948 - val_loss: 3.5986 - val_mse: 3.5986 - val_mae: 1.5032\n",
      "\n",
      "Epoch 00678: val_mae did not improve from 1.50043\n",
      "Epoch 679/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2434 - mse: 5.2434 - mae: 1.7096 - val_loss: 3.5926 - val_mse: 3.5926 - val_mae: 1.4990\n",
      "\n",
      "Epoch 00679: val_mae improved from 1.50043 to 1.49905, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 680/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6592 - mse: 5.6592 - mae: 1.8115 - val_loss: 3.5959 - val_mse: 3.5959 - val_mae: 1.5017\n",
      "\n",
      "Epoch 00680: val_mae did not improve from 1.49905\n",
      "Epoch 681/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8249 - mse: 4.8249 - mae: 1.6578 - val_loss: 3.5957 - val_mse: 3.5957 - val_mae: 1.5016\n",
      "\n",
      "Epoch 00681: val_mae did not improve from 1.49905\n",
      "Epoch 682/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3652 - mse: 4.3652 - mae: 1.6124 - val_loss: 3.5980 - val_mse: 3.5980 - val_mae: 1.5029\n",
      "\n",
      "Epoch 00682: val_mae did not improve from 1.49905\n",
      "Epoch 683/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2578 - mse: 5.2578 - mae: 1.7226 - val_loss: 3.6047 - val_mse: 3.6047 - val_mae: 1.5064\n",
      "\n",
      "Epoch 00683: val_mae did not improve from 1.49905\n",
      "Epoch 684/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4281 - mse: 4.4281 - mae: 1.6291 - val_loss: 3.6168 - val_mse: 3.6168 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00684: val_mae did not improve from 1.49905\n",
      "Epoch 685/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7821 - mse: 4.7821 - mae: 1.6411 - val_loss: 3.6076 - val_mse: 3.6076 - val_mae: 1.5080\n",
      "\n",
      "Epoch 00685: val_mae did not improve from 1.49905\n",
      "Epoch 686/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1844 - mse: 5.1844 - mae: 1.7112 - val_loss: 3.6065 - val_mse: 3.6065 - val_mae: 1.5075\n",
      "\n",
      "Epoch 00686: val_mae did not improve from 1.49905\n",
      "Epoch 687/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5456 - mse: 4.5456 - mae: 1.6172 - val_loss: 3.5991 - val_mse: 3.5991 - val_mae: 1.5035\n",
      "\n",
      "Epoch 00687: val_mae did not improve from 1.49905\n",
      "Epoch 688/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6411 - mse: 4.6411 - mae: 1.6587 - val_loss: 3.5999 - val_mse: 3.5999 - val_mae: 1.5040\n",
      "\n",
      "Epoch 00688: val_mae did not improve from 1.49905\n",
      "Epoch 689/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8609 - mse: 4.8609 - mae: 1.6703 - val_loss: 3.5967 - val_mse: 3.5967 - val_mae: 1.5024\n",
      "\n",
      "Epoch 00689: val_mae did not improve from 1.49905\n",
      "Epoch 690/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6676 - mse: 5.6676 - mae: 1.7446 - val_loss: 3.6008 - val_mse: 3.6008 - val_mae: 1.5045\n",
      "\n",
      "Epoch 00690: val_mae did not improve from 1.49905\n",
      "Epoch 691/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3496 - mse: 4.3496 - mae: 1.6306 - val_loss: 3.6012 - val_mse: 3.6012 - val_mae: 1.5047\n",
      "\n",
      "Epoch 00691: val_mae did not improve from 1.49905\n",
      "Epoch 692/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5316 - mse: 5.5316 - mae: 1.7959 - val_loss: 3.6018 - val_mse: 3.6018 - val_mae: 1.5050\n",
      "\n",
      "Epoch 00692: val_mae did not improve from 1.49905\n",
      "Epoch 693/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4914 - mse: 5.4914 - mae: 1.7381 - val_loss: 3.6006 - val_mse: 3.6006 - val_mae: 1.5043\n",
      "\n",
      "Epoch 00693: val_mae did not improve from 1.49905\n",
      "Epoch 694/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8308 - mse: 4.8308 - mae: 1.7146 - val_loss: 3.6029 - val_mse: 3.6029 - val_mae: 1.5056\n",
      "\n",
      "Epoch 00694: val_mae did not improve from 1.49905\n",
      "Epoch 695/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3651 - mse: 4.3651 - mae: 1.6124 - val_loss: 3.6012 - val_mse: 3.6012 - val_mae: 1.5047\n",
      "\n",
      "Epoch 00695: val_mae did not improve from 1.49905\n",
      "Epoch 696/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3510 - mse: 5.3510 - mae: 1.7485 - val_loss: 3.6057 - val_mse: 3.6057 - val_mae: 1.5072\n",
      "\n",
      "Epoch 00696: val_mae did not improve from 1.49905\n",
      "Epoch 697/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2044 - mse: 5.2044 - mae: 1.7442 - val_loss: 3.5995 - val_mse: 3.5995 - val_mae: 1.5039\n",
      "\n",
      "Epoch 00697: val_mae did not improve from 1.49905\n",
      "Epoch 698/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0701 - mse: 5.0701 - mae: 1.6659 - val_loss: 3.5992 - val_mse: 3.5992 - val_mae: 1.5038\n",
      "\n",
      "Epoch 00698: val_mae did not improve from 1.49905\n",
      "Epoch 699/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5985 - mse: 4.5985 - mae: 1.6290 - val_loss: 3.5973 - val_mse: 3.5973 - val_mae: 1.5028\n",
      "\n",
      "Epoch 00699: val_mae did not improve from 1.49905\n",
      "Epoch 700/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4130 - mse: 4.4130 - mae: 1.6245 - val_loss: 3.6093 - val_mse: 3.6093 - val_mae: 1.5090\n",
      "\n",
      "Epoch 00700: val_mae did not improve from 1.49905\n",
      "Epoch 701/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1496 - mse: 5.1496 - mae: 1.7174 - val_loss: 3.5973 - val_mse: 3.5973 - val_mae: 1.5027\n",
      "\n",
      "Epoch 00701: val_mae did not improve from 1.49905\n",
      "Epoch 702/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2613 - mse: 5.2613 - mae: 1.7217 - val_loss: 3.5971 - val_mse: 3.5971 - val_mae: 1.5027\n",
      "\n",
      "Epoch 00702: val_mae did not improve from 1.49905\n",
      "Epoch 703/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9757 - mse: 4.9757 - mae: 1.6787 - val_loss: 3.5945 - val_mse: 3.5945 - val_mae: 1.5014\n",
      "\n",
      "Epoch 00703: val_mae did not improve from 1.49905\n",
      "Epoch 704/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9191 - mse: 4.9191 - mae: 1.7004 - val_loss: 3.5906 - val_mse: 3.5906 - val_mae: 1.4986\n",
      "\n",
      "Epoch 00704: val_mae improved from 1.49905 to 1.49858, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 705/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9696 - mse: 4.9696 - mae: 1.6277 - val_loss: 3.5944 - val_mse: 3.5944 - val_mae: 1.5010\n",
      "\n",
      "Epoch 00705: val_mae did not improve from 1.49858\n",
      "Epoch 706/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1111 - mse: 5.1111 - mae: 1.7272 - val_loss: 3.6012 - val_mse: 3.6012 - val_mae: 1.5045\n",
      "\n",
      "Epoch 00706: val_mae did not improve from 1.49858\n",
      "Epoch 707/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9284 - mse: 5.9284 - mae: 1.9000 - val_loss: 3.5982 - val_mse: 3.5982 - val_mae: 1.5029\n",
      "\n",
      "Epoch 00707: val_mae did not improve from 1.49858\n",
      "Epoch 708/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6881 - mse: 4.6881 - mae: 1.6113 - val_loss: 3.5957 - val_mse: 3.5957 - val_mae: 1.5015\n",
      "\n",
      "Epoch 00708: val_mae did not improve from 1.49858\n",
      "Epoch 709/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6239 - mse: 4.6239 - mae: 1.6553 - val_loss: 3.5951 - val_mse: 3.5951 - val_mae: 1.5012\n",
      "\n",
      "Epoch 00709: val_mae did not improve from 1.49858\n",
      "Epoch 710/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8352 - mse: 4.8352 - mae: 1.6536 - val_loss: 3.5955 - val_mse: 3.5955 - val_mae: 1.5013\n",
      "\n",
      "Epoch 00710: val_mae did not improve from 1.49858\n",
      "Epoch 711/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3666 - mse: 5.3666 - mae: 1.7503 - val_loss: 3.5933 - val_mse: 3.5933 - val_mae: 1.4989\n",
      "\n",
      "Epoch 00711: val_mae did not improve from 1.49858\n",
      "Epoch 712/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7649 - mse: 4.7649 - mae: 1.6681 - val_loss: 3.6007 - val_mse: 3.6007 - val_mae: 1.5039\n",
      "\n",
      "Epoch 00712: val_mae did not improve from 1.49858\n",
      "Epoch 713/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3582 - mse: 5.3582 - mae: 1.7288 - val_loss: 3.6092 - val_mse: 3.6092 - val_mae: 1.5082\n",
      "\n",
      "Epoch 00713: val_mae did not improve from 1.49858\n",
      "Epoch 714/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7207 - mse: 5.7207 - mae: 1.8307 - val_loss: 3.6058 - val_mse: 3.6058 - val_mae: 1.5064\n",
      "\n",
      "Epoch 00714: val_mae did not improve from 1.49858\n",
      "Epoch 715/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9638 - mse: 4.9638 - mae: 1.6842 - val_loss: 3.5975 - val_mse: 3.5975 - val_mae: 1.5021\n",
      "\n",
      "Epoch 00715: val_mae did not improve from 1.49858\n",
      "Epoch 716/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1291 - mse: 5.1291 - mae: 1.7517 - val_loss: 3.5946 - val_mse: 3.5946 - val_mae: 1.4999\n",
      "\n",
      "Epoch 00716: val_mae did not improve from 1.49858\n",
      "Epoch 717/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1907 - mse: 5.1907 - mae: 1.6727 - val_loss: 3.5980 - val_mse: 3.5980 - val_mae: 1.5024\n",
      "\n",
      "Epoch 00717: val_mae did not improve from 1.49858\n",
      "Epoch 718/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1790 - mse: 5.1790 - mae: 1.7696 - val_loss: 3.5956 - val_mse: 3.5956 - val_mae: 1.5009\n",
      "\n",
      "Epoch 00718: val_mae did not improve from 1.49858\n",
      "Epoch 719/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6963 - mse: 5.6963 - mae: 1.7131 - val_loss: 3.6068 - val_mse: 3.6068 - val_mae: 1.5069\n",
      "\n",
      "Epoch 00719: val_mae did not improve from 1.49858\n",
      "Epoch 720/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8647 - mse: 4.8647 - mae: 1.6660 - val_loss: 3.5965 - val_mse: 3.5965 - val_mae: 1.5014\n",
      "\n",
      "Epoch 00720: val_mae did not improve from 1.49858\n",
      "Epoch 721/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1265 - mse: 5.1265 - mae: 1.6228 - val_loss: 3.5985 - val_mse: 3.5985 - val_mae: 1.5026\n",
      "\n",
      "Epoch 00721: val_mae did not improve from 1.49858\n",
      "Epoch 722/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6736 - mse: 4.6736 - mae: 1.6085 - val_loss: 3.6017 - val_mse: 3.6017 - val_mae: 1.5044\n",
      "\n",
      "Epoch 00722: val_mae did not improve from 1.49858\n",
      "Epoch 723/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3429 - mse: 4.3429 - mae: 1.5541 - val_loss: 3.5990 - val_mse: 3.5990 - val_mae: 1.5030\n",
      "\n",
      "Epoch 00723: val_mae did not improve from 1.49858\n",
      "Epoch 724/1000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.2226 - mse: 5.2226 - mae: 1.6906 - val_loss: 3.6004 - val_mse: 3.6004 - val_mae: 1.5037\n",
      "\n",
      "Epoch 00724: val_mae did not improve from 1.49858\n",
      "Epoch 725/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3603 - mse: 4.3603 - mae: 1.6028 - val_loss: 3.6038 - val_mse: 3.6038 - val_mae: 1.5054\n",
      "\n",
      "Epoch 00725: val_mae did not improve from 1.49858\n",
      "Epoch 726/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3577 - mse: 4.3577 - mae: 1.6350 - val_loss: 3.6078 - val_mse: 3.6078 - val_mae: 1.5075\n",
      "\n",
      "Epoch 00726: val_mae did not improve from 1.49858\n",
      "Epoch 727/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0469 - mse: 5.0469 - mae: 1.7218 - val_loss: 3.6182 - val_mse: 3.6182 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00727: val_mae did not improve from 1.49858\n",
      "Epoch 728/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4220 - mse: 4.4220 - mae: 1.6021 - val_loss: 3.6182 - val_mse: 3.6182 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00728: val_mae did not improve from 1.49858\n",
      "Epoch 729/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2628 - mse: 5.2628 - mae: 1.6427 - val_loss: 3.6179 - val_mse: 3.6179 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00729: val_mae did not improve from 1.49858\n",
      "Epoch 730/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7959 - mse: 4.7959 - mae: 1.7027 - val_loss: 3.6167 - val_mse: 3.6167 - val_mae: 1.5119\n",
      "\n",
      "Epoch 00730: val_mae did not improve from 1.49858\n",
      "Epoch 731/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5751 - mse: 4.5751 - mae: 1.6285 - val_loss: 3.6023 - val_mse: 3.6023 - val_mae: 1.5047\n",
      "\n",
      "Epoch 00731: val_mae did not improve from 1.49858\n",
      "Epoch 732/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1076 - mse: 5.1076 - mae: 1.7236 - val_loss: 3.5998 - val_mse: 3.5998 - val_mae: 1.5035\n",
      "\n",
      "Epoch 00732: val_mae did not improve from 1.49858\n",
      "Epoch 733/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6355 - mse: 5.6355 - mae: 1.7347 - val_loss: 3.6040 - val_mse: 3.6040 - val_mae: 1.5057\n",
      "\n",
      "Epoch 00733: val_mae did not improve from 1.49858\n",
      "Epoch 734/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4583 - mse: 4.4583 - mae: 1.6198 - val_loss: 3.6019 - val_mse: 3.6019 - val_mae: 1.5046\n",
      "\n",
      "Epoch 00734: val_mae did not improve from 1.49858\n",
      "Epoch 735/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9672 - mse: 4.9672 - mae: 1.7048 - val_loss: 3.6041 - val_mse: 3.6041 - val_mae: 1.5057\n",
      "\n",
      "Epoch 00735: val_mae did not improve from 1.49858\n",
      "Epoch 736/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9840 - mse: 4.9840 - mae: 1.6866 - val_loss: 3.6054 - val_mse: 3.6054 - val_mae: 1.5064\n",
      "\n",
      "Epoch 00736: val_mae did not improve from 1.49858\n",
      "Epoch 737/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1957 - mse: 5.1957 - mae: 1.7458 - val_loss: 3.5966 - val_mse: 3.5966 - val_mae: 1.5020\n",
      "\n",
      "Epoch 00737: val_mae did not improve from 1.49858\n",
      "Epoch 738/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2765 - mse: 5.2765 - mae: 1.6512 - val_loss: 3.5981 - val_mse: 3.5981 - val_mae: 1.5026\n",
      "\n",
      "Epoch 00738: val_mae did not improve from 1.49858\n",
      "Epoch 739/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4713 - mse: 4.4713 - mae: 1.6246 - val_loss: 3.6098 - val_mse: 3.6098 - val_mae: 1.5088\n",
      "\n",
      "Epoch 00739: val_mae did not improve from 1.49858\n",
      "Epoch 740/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6801 - mse: 4.6801 - mae: 1.6741 - val_loss: 3.6044 - val_mse: 3.6044 - val_mae: 1.5059\n",
      "\n",
      "Epoch 00740: val_mae did not improve from 1.49858\n",
      "Epoch 741/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9753 - mse: 4.9753 - mae: 1.6855 - val_loss: 3.6022 - val_mse: 3.6022 - val_mae: 1.5047\n",
      "\n",
      "Epoch 00741: val_mae did not improve from 1.49858\n",
      "Epoch 742/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7835 - mse: 4.7835 - mae: 1.6636 - val_loss: 3.5957 - val_mse: 3.5957 - val_mae: 1.5013\n",
      "\n",
      "Epoch 00742: val_mae did not improve from 1.49858\n",
      "Epoch 743/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0852 - mse: 5.0852 - mae: 1.6550 - val_loss: 3.5995 - val_mse: 3.5995 - val_mae: 1.5034\n",
      "\n",
      "Epoch 00743: val_mae did not improve from 1.49858\n",
      "Epoch 744/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1744 - mse: 6.1744 - mae: 1.8215 - val_loss: 3.6091 - val_mse: 3.6091 - val_mae: 1.5084\n",
      "\n",
      "Epoch 00744: val_mae did not improve from 1.49858\n",
      "Epoch 745/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1131 - mse: 5.1131 - mae: 1.7321 - val_loss: 3.6019 - val_mse: 3.6019 - val_mae: 1.5046\n",
      "\n",
      "Epoch 00745: val_mae did not improve from 1.49858\n",
      "Epoch 746/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6292 - mse: 5.6292 - mae: 1.6583 - val_loss: 3.6037 - val_mse: 3.6037 - val_mae: 1.5055\n",
      "\n",
      "Epoch 00746: val_mae did not improve from 1.49858\n",
      "Epoch 747/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1244 - mse: 5.1244 - mae: 1.6545 - val_loss: 3.5959 - val_mse: 3.5959 - val_mae: 1.5014\n",
      "\n",
      "Epoch 00747: val_mae did not improve from 1.49858\n",
      "Epoch 748/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8609 - mse: 4.8609 - mae: 1.6542 - val_loss: 3.5961 - val_mse: 3.5961 - val_mae: 1.5016\n",
      "\n",
      "Epoch 00748: val_mae did not improve from 1.49858\n",
      "Epoch 749/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9565 - mse: 4.9565 - mae: 1.6930 - val_loss: 3.6085 - val_mse: 3.6085 - val_mae: 1.5080\n",
      "\n",
      "Epoch 00749: val_mae did not improve from 1.49858\n",
      "Epoch 750/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0312 - mse: 5.0312 - mae: 1.7033 - val_loss: 3.6158 - val_mse: 3.6158 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00750: val_mae did not improve from 1.49858\n",
      "Epoch 751/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1656 - mse: 5.1656 - mae: 1.6502 - val_loss: 3.6097 - val_mse: 3.6097 - val_mae: 1.5088\n",
      "\n",
      "Epoch 00751: val_mae did not improve from 1.49858\n",
      "Epoch 752/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9946 - mse: 4.9946 - mae: 1.6794 - val_loss: 3.6034 - val_mse: 3.6034 - val_mae: 1.5055\n",
      "\n",
      "Epoch 00752: val_mae did not improve from 1.49858\n",
      "Epoch 753/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2222 - mse: 5.2222 - mae: 1.7170 - val_loss: 3.5980 - val_mse: 3.5980 - val_mae: 1.5028\n",
      "\n",
      "Epoch 00753: val_mae did not improve from 1.49858\n",
      "Epoch 754/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3248 - mse: 4.3248 - mae: 1.5995 - val_loss: 3.5948 - val_mse: 3.5948 - val_mae: 1.5012\n",
      "\n",
      "Epoch 00754: val_mae did not improve from 1.49858\n",
      "Epoch 755/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5525 - mse: 4.5525 - mae: 1.6261 - val_loss: 3.6047 - val_mse: 3.6047 - val_mae: 1.5064\n",
      "\n",
      "Epoch 00755: val_mae did not improve from 1.49858\n",
      "Epoch 756/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4025 - mse: 5.4025 - mae: 1.6924 - val_loss: 3.6075 - val_mse: 3.6075 - val_mae: 1.5079\n",
      "\n",
      "Epoch 00756: val_mae did not improve from 1.49858\n",
      "Epoch 757/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5449 - mse: 5.5449 - mae: 1.7365 - val_loss: 3.6072 - val_mse: 3.6072 - val_mae: 1.5079\n",
      "\n",
      "Epoch 00757: val_mae did not improve from 1.49858\n",
      "Epoch 758/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9092 - mse: 4.9092 - mae: 1.7092 - val_loss: 3.5970 - val_mse: 3.5970 - val_mae: 1.5026\n",
      "\n",
      "Epoch 00758: val_mae did not improve from 1.49858\n",
      "Epoch 759/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4251 - mse: 5.4251 - mae: 1.7142 - val_loss: 3.6017 - val_mse: 3.6017 - val_mae: 1.5048\n",
      "\n",
      "Epoch 00759: val_mae did not improve from 1.49858\n",
      "Epoch 760/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2759 - mse: 5.2759 - mae: 1.7197 - val_loss: 3.6005 - val_mse: 3.6005 - val_mae: 1.5042\n",
      "\n",
      "Epoch 00760: val_mae did not improve from 1.49858\n",
      "Epoch 761/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8322 - mse: 4.8322 - mae: 1.6523 - val_loss: 3.5991 - val_mse: 3.5991 - val_mae: 1.5036\n",
      "\n",
      "Epoch 00761: val_mae did not improve from 1.49858\n",
      "Epoch 762/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4601 - mse: 5.4601 - mae: 1.7311 - val_loss: 3.5984 - val_mse: 3.5984 - val_mae: 1.5032\n",
      "\n",
      "Epoch 00762: val_mae did not improve from 1.49858\n",
      "Epoch 763/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8853 - mse: 4.8853 - mae: 1.7026 - val_loss: 3.5945 - val_mse: 3.5945 - val_mae: 1.5011\n",
      "\n",
      "Epoch 00763: val_mae did not improve from 1.49858\n",
      "Epoch 764/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4710 - mse: 4.4710 - mae: 1.6189 - val_loss: 3.6007 - val_mse: 3.6007 - val_mae: 1.5043\n",
      "\n",
      "Epoch 00764: val_mae did not improve from 1.49858\n",
      "Epoch 765/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9124 - mse: 4.9124 - mae: 1.6922 - val_loss: 3.5995 - val_mse: 3.5995 - val_mae: 1.5037\n",
      "\n",
      "Epoch 00765: val_mae did not improve from 1.49858\n",
      "Epoch 766/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7510 - mse: 4.7510 - mae: 1.6356 - val_loss: 3.5932 - val_mse: 3.5932 - val_mae: 1.5005\n",
      "\n",
      "Epoch 00766: val_mae did not improve from 1.49858\n",
      "Epoch 767/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0402 - mse: 5.0402 - mae: 1.7351 - val_loss: 3.5982 - val_mse: 3.5982 - val_mae: 1.5032\n",
      "\n",
      "Epoch 00767: val_mae did not improve from 1.49858\n",
      "Epoch 768/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6095 - mse: 4.6095 - mae: 1.6330 - val_loss: 3.6051 - val_mse: 3.6051 - val_mae: 1.5068\n",
      "\n",
      "Epoch 00768: val_mae did not improve from 1.49858\n",
      "Epoch 769/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2860 - mse: 4.2860 - mae: 1.5937 - val_loss: 3.6031 - val_mse: 3.6031 - val_mae: 1.5057\n",
      "\n",
      "Epoch 00769: val_mae did not improve from 1.49858\n",
      "Epoch 770/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1658 - mse: 5.1658 - mae: 1.7034 - val_loss: 3.6021 - val_mse: 3.6021 - val_mae: 1.5051\n",
      "\n",
      "Epoch 00770: val_mae did not improve from 1.49858\n",
      "Epoch 771/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.1220 - mse: 4.1220 - mae: 1.5599 - val_loss: 3.5984 - val_mse: 3.5984 - val_mae: 1.5033\n",
      "\n",
      "Epoch 00771: val_mae did not improve from 1.49858\n",
      "Epoch 772/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8611 - mse: 4.8611 - mae: 1.6681 - val_loss: 3.6016 - val_mse: 3.6016 - val_mae: 1.5050\n",
      "\n",
      "Epoch 00772: val_mae did not improve from 1.49858\n",
      "Epoch 773/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0977 - mse: 5.0977 - mae: 1.7077 - val_loss: 3.6094 - val_mse: 3.6094 - val_mae: 1.5090\n",
      "\n",
      "Epoch 00773: val_mae did not improve from 1.49858\n",
      "Epoch 774/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6967 - mse: 4.6967 - mae: 1.6920 - val_loss: 3.6086 - val_mse: 3.6086 - val_mae: 1.5086\n",
      "\n",
      "Epoch 00774: val_mae did not improve from 1.49858\n",
      "Epoch 775/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.1597 - mse: 4.1597 - mae: 1.5709 - val_loss: 3.6121 - val_mse: 3.6121 - val_mae: 1.5102\n",
      "\n",
      "Epoch 00775: val_mae did not improve from 1.49858\n",
      "Epoch 776/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1794 - mse: 5.1794 - mae: 1.7488 - val_loss: 3.6074 - val_mse: 3.6074 - val_mae: 1.5080\n",
      "\n",
      "Epoch 00776: val_mae did not improve from 1.49858\n",
      "Epoch 777/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1424 - mse: 5.1424 - mae: 1.7608 - val_loss: 3.6066 - val_mse: 3.6066 - val_mae: 1.5076\n",
      "\n",
      "Epoch 00777: val_mae did not improve from 1.49858\n",
      "Epoch 778/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5118 - mse: 5.5118 - mae: 1.7316 - val_loss: 3.6116 - val_mse: 3.6116 - val_mae: 1.5100\n",
      "\n",
      "Epoch 00778: val_mae did not improve from 1.49858\n",
      "Epoch 779/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3450 - mse: 4.3450 - mae: 1.6020 - val_loss: 3.5947 - val_mse: 3.5947 - val_mae: 1.5014\n",
      "\n",
      "Epoch 00779: val_mae did not improve from 1.49858\n",
      "Epoch 780/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3070 - mse: 4.3070 - mae: 1.6182 - val_loss: 3.6038 - val_mse: 3.6038 - val_mae: 1.5061\n",
      "\n",
      "Epoch 00780: val_mae did not improve from 1.49858\n",
      "Epoch 781/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2798 - mse: 5.2798 - mae: 1.6600 - val_loss: 3.6039 - val_mse: 3.6039 - val_mae: 1.5060\n",
      "\n",
      "Epoch 00781: val_mae did not improve from 1.49858\n",
      "Epoch 782/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3514 - mse: 5.3514 - mae: 1.7415 - val_loss: 3.6068 - val_mse: 3.6068 - val_mae: 1.5077\n",
      "\n",
      "Epoch 00782: val_mae did not improve from 1.49858\n",
      "Epoch 783/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0177 - mse: 5.0177 - mae: 1.6690 - val_loss: 3.6108 - val_mse: 3.6108 - val_mae: 1.5095\n",
      "\n",
      "Epoch 00783: val_mae did not improve from 1.49858\n",
      "Epoch 784/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8123 - mse: 4.8123 - mae: 1.6316 - val_loss: 3.5993 - val_mse: 3.5993 - val_mae: 1.5036\n",
      "\n",
      "Epoch 00784: val_mae did not improve from 1.49858\n",
      "Epoch 785/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0164 - mse: 5.0164 - mae: 1.6671 - val_loss: 3.5984 - val_mse: 3.5984 - val_mae: 1.5031\n",
      "\n",
      "Epoch 00785: val_mae did not improve from 1.49858\n",
      "Epoch 786/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7609 - mse: 4.7609 - mae: 1.6830 - val_loss: 3.5962 - val_mse: 3.5962 - val_mae: 1.5022\n",
      "\n",
      "Epoch 00786: val_mae did not improve from 1.49858\n",
      "Epoch 787/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0870 - mse: 6.0870 - mae: 1.8328 - val_loss: 3.5982 - val_mse: 3.5982 - val_mae: 1.5031\n",
      "\n",
      "Epoch 00787: val_mae did not improve from 1.49858\n",
      "Epoch 788/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0433 - mse: 6.0433 - mae: 1.8260 - val_loss: 3.5955 - val_mse: 3.5955 - val_mae: 1.5018\n",
      "\n",
      "Epoch 00788: val_mae did not improve from 1.49858\n",
      "Epoch 789/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8005 - mse: 4.8005 - mae: 1.6323 - val_loss: 3.5964 - val_mse: 3.5964 - val_mae: 1.5023\n",
      "\n",
      "Epoch 00789: val_mae did not improve from 1.49858\n",
      "Epoch 790/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5566 - mse: 4.5566 - mae: 1.6462 - val_loss: 3.5988 - val_mse: 3.5988 - val_mae: 1.5035\n",
      "\n",
      "Epoch 00790: val_mae did not improve from 1.49858\n",
      "Epoch 791/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6384 - mse: 4.6384 - mae: 1.6139 - val_loss: 3.6044 - val_mse: 3.6044 - val_mae: 1.5065\n",
      "\n",
      "Epoch 00791: val_mae did not improve from 1.49858\n",
      "Epoch 792/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7836 - mse: 4.7836 - mae: 1.6210 - val_loss: 3.6025 - val_mse: 3.6025 - val_mae: 1.5055\n",
      "\n",
      "Epoch 00792: val_mae did not improve from 1.49858\n",
      "Epoch 793/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9147 - mse: 4.9147 - mae: 1.6850 - val_loss: 3.5965 - val_mse: 3.5965 - val_mae: 1.5024\n",
      "\n",
      "Epoch 00793: val_mae did not improve from 1.49858\n",
      "Epoch 794/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0737 - mse: 6.0737 - mae: 1.8014 - val_loss: 3.6007 - val_mse: 3.6007 - val_mae: 1.5045\n",
      "\n",
      "Epoch 00794: val_mae did not improve from 1.49858\n",
      "Epoch 795/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5007 - mse: 4.5007 - mae: 1.6256 - val_loss: 3.5919 - val_mse: 3.5919 - val_mae: 1.4997\n",
      "\n",
      "Epoch 00795: val_mae did not improve from 1.49858\n",
      "Epoch 796/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8217 - mse: 5.8217 - mae: 1.7949 - val_loss: 3.5986 - val_mse: 3.5986 - val_mae: 1.5034\n",
      "\n",
      "Epoch 00796: val_mae did not improve from 1.49858\n",
      "Epoch 797/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9494 - mse: 4.9494 - mae: 1.6763 - val_loss: 3.5942 - val_mse: 3.5942 - val_mae: 1.5012\n",
      "\n",
      "Epoch 00797: val_mae did not improve from 1.49858\n",
      "Epoch 798/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5601 - mse: 4.5601 - mae: 1.6263 - val_loss: 3.5957 - val_mse: 3.5957 - val_mae: 1.5019\n",
      "\n",
      "Epoch 00798: val_mae did not improve from 1.49858\n",
      "Epoch 799/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5423 - mse: 4.5423 - mae: 1.6262 - val_loss: 3.5948 - val_mse: 3.5948 - val_mae: 1.5015\n",
      "\n",
      "Epoch 00799: val_mae did not improve from 1.49858\n",
      "Epoch 800/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1158 - mse: 5.1158 - mae: 1.7386 - val_loss: 3.6019 - val_mse: 3.6019 - val_mae: 1.5051\n",
      "\n",
      "Epoch 00800: val_mae did not improve from 1.49858\n",
      "Epoch 801/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4757 - mse: 4.4757 - mae: 1.6042 - val_loss: 3.5971 - val_mse: 3.5971 - val_mae: 1.5026\n",
      "\n",
      "Epoch 00801: val_mae did not improve from 1.49858\n",
      "Epoch 802/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3027 - mse: 4.3027 - mae: 1.6035 - val_loss: 3.5923 - val_mse: 3.5923 - val_mae: 1.4999\n",
      "\n",
      "Epoch 00802: val_mae did not improve from 1.49858\n",
      "Epoch 803/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4230 - mse: 4.4230 - mae: 1.6627 - val_loss: 3.5975 - val_mse: 3.5975 - val_mae: 1.5027\n",
      "\n",
      "Epoch 00803: val_mae did not improve from 1.49858\n",
      "Epoch 804/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5525 - mse: 4.5525 - mae: 1.6264 - val_loss: 3.6015 - val_mse: 3.6015 - val_mae: 1.5048\n",
      "\n",
      "Epoch 00804: val_mae did not improve from 1.49858\n",
      "Epoch 00804: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def show_info(model, X, y, log, weights = None):\n",
    "    '''\n",
    "    Show metrics about the evaluation model and plots about loss, rmse and rmspe\n",
    "    '''\n",
    "    if (log != None):\n",
    "        # summarize history for loss\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['loss'])\n",
    "        plt.plot(log.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MAE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mae'])\n",
    "        plt.plot(log.history['val_mae'])\n",
    "        plt.title('Model MAE')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MSE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mse'])\n",
    "        plt.plot(log.history['val_mse'])\n",
    "        plt.title('Model MSE')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    if (weights != None):\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    predictions = model.predict(X, verbose=1)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    mae= mean_absolute_error(y, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "show_info(model, x_test, y_test, log, weights='/home/m-marouni/Documents/CE-901/Heathrow/best_weights')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-13T20:17:01.172135</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md2613ee01f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.553289\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(155.009539 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.994362\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(243.450612 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"341.435436\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(331.891686 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"429.876509\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(420.332759 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"518.317582\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(508.773832 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"606.758655\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(597.214905 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"695.199729\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(685.655979 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"783.640802\" xlink:href=\"#md2613ee01f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(774.097052 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5e7cc466c2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5e7cc466c2\" y=\"523.355526\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 527.154745)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5e7cc466c2\" y=\"460.019815\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 463.819033)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5e7cc466c2\" y=\"396.684103\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 400.483322)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5e7cc466c2\" y=\"333.348392\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 337.14761)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5e7cc466c2\" y=\"270.01268\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 273.811899)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5e7cc466c2\" y=\"206.676969\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 210.476187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5e7cc466c2\" y=\"143.341257\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 147.140476)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5e7cc466c2\" y=\"80.005546\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 83.804764)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 303.775937)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p0e4b88aa24)\" d=\"M 76.112216 47.027216 \nL 77.881037 129.551756 \nL 80.53427 235.385313 \nL 81.41868 270.590216 \nL 84.071913 359.697617 \nL 85.840734 405.606187 \nL 86.725145 425.638149 \nL 87.609555 442.861085 \nL 88.493966 453.551681 \nL 89.378377 466.304098 \nL 91.147198 483.841872 \nL 92.031609 488.214588 \nL 92.91602 493.762702 \nL 93.800431 494.943584 \nL 94.684841 498.80421 \nL 95.569252 499.752896 \nL 96.453663 497.328151 \nL 97.338073 502.870478 \nL 98.222484 501.370682 \nL 99.106895 503.896642 \nL 99.991306 502.458116 \nL 100.875716 504.322256 \nL 101.760127 503.240414 \nL 102.644538 503.174347 \nL 103.528949 505.666174 \nL 104.413359 506.460245 \nL 105.29777 502.776125 \nL 106.182181 503.947681 \nL 107.066592 503.473118 \nL 107.951002 507.13124 \nL 108.835413 502.936914 \nL 110.604234 505.392549 \nL 111.488645 503.979042 \nL 112.373056 506.868252 \nL 113.257467 504.942339 \nL 114.141877 502.039569 \nL 115.026288 506.860744 \nL 115.910699 508.121211 \nL 116.79511 505.095385 \nL 117.67952 504.246229 \nL 118.563931 505.768966 \nL 119.448342 504.768624 \nL 120.332753 504.47178 \nL 121.217163 505.651618 \nL 122.101574 502.40474 \nL 122.985985 505.408918 \nL 123.870395 504.293517 \nL 124.754806 504.560148 \nL 125.639217 506.612155 \nL 126.523628 506.524065 \nL 127.408038 507.792753 \nL 128.292449 504.240146 \nL 129.17686 504.594208 \nL 130.061271 507.2909 \nL 130.945681 505.901874 \nL 131.830092 507.397594 \nL 132.714503 503.463393 \nL 133.598914 504.725371 \nL 134.483324 505.264099 \nL 135.367735 506.928702 \nL 136.252146 506.961524 \nL 137.136556 505.906519 \nL 138.020967 505.518807 \nL 138.905378 507.097832 \nL 139.789789 508.911881 \nL 140.674199 509.275082 \nL 141.55861 506.09714 \nL 142.443021 507.626184 \nL 143.327432 506.465258 \nL 144.211842 506.661177 \nL 145.096253 508.410765 \nL 145.980664 509.052224 \nL 146.865075 508.09057 \nL 147.749485 505.250164 \nL 148.633896 505.334201 \nL 149.518307 507.657206 \nL 150.402717 505.403886 \nL 151.287128 509.485793 \nL 152.171539 507.021575 \nL 153.05595 508.324964 \nL 153.94036 507.870267 \nL 154.824771 507.590075 \nL 155.709182 508.359194 \nL 156.593593 508.324094 \nL 157.478003 509.035837 \nL 158.362414 507.766629 \nL 159.246825 508.685405 \nL 160.131235 508.06325 \nL 161.015646 509.982633 \nL 161.900057 507.096292 \nL 162.784468 507.708318 \nL 163.668878 507.54202 \nL 164.553289 511.253888 \nL 165.4377 507.780782 \nL 166.322111 506.829854 \nL 167.206521 508.000529 \nL 168.090932 510.587664 \nL 168.975343 511.32428 \nL 170.744164 509.961341 \nL 171.628575 507.247598 \nL 172.512986 510.005404 \nL 173.397396 507.804483 \nL 174.281807 511.2476 \nL 175.166218 505.78136 \nL 176.050629 508.258359 \nL 176.935039 510.037792 \nL 177.81945 508.907888 \nL 178.703861 508.905647 \nL 179.588272 510.830001 \nL 180.472682 510.106915 \nL 181.357093 509.720635 \nL 182.241504 509.512485 \nL 183.125915 509.94284 \nL 184.010325 510.203183 \nL 184.894736 509.699917 \nL 185.779147 510.958681 \nL 186.663557 513.017145 \nL 187.547968 512.448131 \nL 188.432379 511.543079 \nL 189.31679 511.914724 \nL 190.2012 511.713092 \nL 191.085611 511.116281 \nL 191.970022 510.691579 \nL 192.854433 513.11659 \nL 193.738843 509.244102 \nL 194.623254 509.551293 \nL 195.507665 514.567625 \nL 196.392076 511.899364 \nL 197.276486 509.635499 \nL 198.160897 512.269511 \nL 199.045308 513.247191 \nL 199.929718 510.892511 \nL 200.814129 512.74056 \nL 201.69854 510.576194 \nL 203.467361 510.117135 \nL 204.351772 511.367449 \nL 205.236183 512.201397 \nL 206.120594 514.993807 \nL 207.005004 510.358923 \nL 207.889415 514.153185 \nL 208.773826 509.663307 \nL 209.658237 512.060032 \nL 210.542647 511.641558 \nL 211.427058 513.170903 \nL 212.311469 512.356115 \nL 213.195879 511.295444 \nL 214.08029 514.076559 \nL 214.964701 511.160561 \nL 215.849112 511.37564 \nL 216.733522 513.310751 \nL 217.617933 513.529049 \nL 218.502344 511.895341 \nL 219.386755 517.800961 \nL 220.271165 514.924925 \nL 221.155576 513.548365 \nL 222.039987 513.9223 \nL 223.808808 513.776055 \nL 224.693219 513.420688 \nL 225.57763 513.620902 \nL 226.46204 514.037576 \nL 227.346451 515.679734 \nL 228.230862 515.298811 \nL 229.115273 515.171581 \nL 229.999683 516.908345 \nL 230.884094 513.221 \nL 231.768505 514.404818 \nL 233.537326 513.789573 \nL 234.421737 515.169871 \nL 235.306148 514.294102 \nL 237.95938 516.806834 \nL 238.843791 516.961052 \nL 240.612612 515.228781 \nL 241.497023 514.953513 \nL 242.381434 514.871499 \nL 243.265844 514.914922 \nL 244.150255 518.459949 \nL 245.034666 513.831431 \nL 245.919077 517.455331 \nL 247.687898 517.850726 \nL 248.572309 516.434096 \nL 249.456719 516.917243 \nL 250.34113 516.610934 \nL 251.225541 517.440478 \nL 252.109952 517.475148 \nL 252.994362 516.921912 \nL 253.878773 515.032681 \nL 254.763184 517.309757 \nL 255.647595 518.499965 \nL 256.532005 517.354454 \nL 257.416416 517.113711 \nL 258.300827 518.364357 \nL 259.185238 516.624748 \nL 260.069648 519.831966 \nL 260.954059 517.410682 \nL 261.83847 516.259203 \nL 262.72288 516.889929 \nL 263.607291 518.449294 \nL 264.491702 517.4562 \nL 265.376113 517.763615 \nL 266.260523 517.413164 \nL 267.144934 516.251967 \nL 268.029345 517.953064 \nL 268.913756 519.175092 \nL 269.798166 515.249777 \nL 270.682577 517.748484 \nL 271.566988 516.467353 \nL 272.451399 519.350136 \nL 273.335809 520.496517 \nL 274.22022 518.904589 \nL 275.104631 520.327247 \nL 275.989041 518.238384 \nL 276.873452 516.747955 \nL 277.757863 520.868676 \nL 278.642274 518.265842 \nL 280.411095 517.030097 \nL 281.295506 520.789888 \nL 282.179917 516.661943 \nL 283.064327 519.607834 \nL 283.948738 516.880102 \nL 284.833149 517.360832 \nL 285.71756 516.537425 \nL 286.60197 519.703782 \nL 287.486381 522.378771 \nL 289.255202 516.56138 \nL 291.908435 521.644529 \nL 292.792845 519.798806 \nL 293.677256 519.763573 \nL 294.561667 517.681674 \nL 295.446078 519.626196 \nL 296.330488 520.025837 \nL 297.214899 519.609398 \nL 298.09931 518.929771 \nL 298.98372 522.200882 \nL 299.868131 519.739135 \nL 300.752542 518.882362 \nL 301.636953 518.971533 \nL 302.521363 520.776376 \nL 303.405774 520.17248 \nL 304.290185 520.555856 \nL 305.174596 518.19391 \nL 306.059006 520.908202 \nL 306.943417 521.338818 \nL 307.827828 520.139622 \nL 308.712239 520.469554 \nL 309.596649 521.012178 \nL 310.48106 518.186311 \nL 311.365471 520.877035 \nL 312.249881 520.850084 \nL 313.134292 520.324813 \nL 314.018703 522.324337 \nL 314.903114 518.519728 \nL 315.787524 521.368336 \nL 316.671935 522.335312 \nL 317.556346 522.482348 \nL 318.440757 520.381361 \nL 319.325167 523.728796 \nL 320.209578 516.997281 \nL 321.9784 523.128857 \nL 322.86281 518.757312 \nL 323.747221 522.363121 \nL 324.631632 520.705344 \nL 325.516042 520.027909 \nL 326.400453 520.61006 \nL 327.284864 521.907735 \nL 328.169275 522.546778 \nL 329.053685 519.27406 \nL 329.938096 521.920558 \nL 330.822507 517.998414 \nL 331.706918 519.899827 \nL 332.591328 521.032418 \nL 333.475739 518.766409 \nL 334.36015 521.838672 \nL 335.244561 519.845635 \nL 336.128971 520.993061 \nL 337.013382 520.436707 \nL 337.897793 520.919727 \nL 338.782203 523.168583 \nL 339.666614 524.192373 \nL 340.551025 522.78978 \nL 341.435436 519.463166 \nL 342.319846 521.03632 \nL 343.204257 520.478209 \nL 344.088668 520.436514 \nL 344.973079 524.297683 \nL 346.7419 523.27794 \nL 347.626311 521.825957 \nL 348.510722 521.756616 \nL 349.395132 521.94554 \nL 350.279543 520.908021 \nL 351.163954 522.530289 \nL 352.048364 519.400674 \nL 352.932775 521.662371 \nL 353.817186 520.287479 \nL 354.701597 520.812544 \nL 355.586007 523.173143 \nL 356.470418 522.430976 \nL 357.354829 523.119507 \nL 358.23924 520.795874 \nL 359.12365 520.956071 \nL 360.008061 520.913983 \nL 360.892472 522.919927 \nL 361.776883 520.442904 \nL 362.661293 522.534269 \nL 363.545704 520.779994 \nL 365.314525 522.681945 \nL 366.198936 522.745276 \nL 367.083347 522.498892 \nL 367.967758 522.586275 \nL 368.852168 521.92481 \nL 369.736579 519.650393 \nL 370.62099 522.991358 \nL 371.505401 519.7549 \nL 372.389811 520.869292 \nL 373.274222 522.500027 \nL 374.158633 520.954168 \nL 375.043043 520.408687 \nL 375.927454 520.925719 \nL 376.811865 522.949645 \nL 377.696276 524.616217 \nL 378.580686 520.033412 \nL 379.465097 522.18647 \nL 380.349508 523.154099 \nL 381.233919 522.81893 \nL 382.118329 523.445845 \nL 383.00274 523.431874 \nL 383.887151 525.444607 \nL 384.771562 521.598617 \nL 385.655972 523.029871 \nL 386.540383 521.397353 \nL 387.424794 521.506843 \nL 388.309204 522.618221 \nL 389.193615 520.426995 \nL 390.962437 523.190799 \nL 391.846847 520.889792 \nL 392.731258 519.55709 \nL 393.615669 522.049347 \nL 394.50008 523.244387 \nL 395.38449 520.507933 \nL 396.268901 521.192277 \nL 397.153312 519.045808 \nL 398.037723 523.756321 \nL 398.922133 521.816045 \nL 399.806544 522.522612 \nL 400.690955 522.406344 \nL 401.575365 523.344189 \nL 402.459776 522.364758 \nL 403.344187 521.875595 \nL 404.228598 521.988806 \nL 405.113008 521.102726 \nL 406.88183 522.962559 \nL 407.766241 522.77436 \nL 408.650651 520.253636 \nL 409.535062 522.088825 \nL 410.419473 522.774613 \nL 411.303884 521.659092 \nL 412.188294 520.180725 \nL 413.072705 523.952868 \nL 413.957116 520.085641 \nL 414.841526 522.427376 \nL 415.725937 523.005251 \nL 416.610348 522.936006 \nL 417.494759 519.875274 \nL 418.379169 522.254942 \nL 419.26358 520.320772 \nL 420.147991 524.454872 \nL 421.032402 522.940059 \nL 421.916812 525.020358 \nL 422.801223 523.426595 \nL 423.685634 522.30156 \nL 424.570045 521.676107 \nL 425.454455 523.853628 \nL 426.338866 523.130971 \nL 427.223277 522.730931 \nL 428.107687 523.657365 \nL 428.992098 521.756091 \nL 429.876509 524.12912 \nL 430.76092 522.847174 \nL 431.64533 520.944087 \nL 432.529741 521.591097 \nL 433.414152 523.167755 \nL 434.298563 521.655147 \nL 435.182973 517.168858 \nL 436.067384 521.892151 \nL 436.951795 523.914821 \nL 437.836205 522.35763 \nL 439.605027 523.73355 \nL 440.489438 522.392265 \nL 441.373848 521.35965 \nL 442.258259 525.064506 \nL 443.14267 521.109612 \nL 444.027081 521.722634 \nL 444.911491 523.249612 \nL 445.795902 521.49228 \nL 446.680313 521.70153 \nL 447.564724 523.68193 \nL 449.333545 521.051487 \nL 450.217956 524.535557 \nL 451.102366 522.938966 \nL 451.986777 523.512933 \nL 452.871188 522.184997 \nL 453.755599 524.57547 \nL 454.640009 522.827259 \nL 455.52442 523.595061 \nL 456.408831 523.174961 \nL 457.293242 522.536848 \nL 458.177652 519.483328 \nL 459.062063 521.816879 \nL 459.946474 521.484748 \nL 460.830885 523.854148 \nL 461.715295 523.954891 \nL 462.599706 523.14252 \nL 463.484117 527.25865 \nL 464.368527 521.136243 \nL 465.252938 520.794345 \nL 466.137349 524.71408 \nL 467.02176 522.381423 \nL 467.90617 523.078446 \nL 468.790581 521.100817 \nL 469.674992 520.17875 \nL 470.559403 523.269671 \nL 471.443813 522.683582 \nL 472.328224 524.932716 \nL 473.212635 526.263672 \nL 474.097046 523.165684 \nL 474.981456 523.092851 \nL 475.865867 522.359177 \nL 476.750278 522.476561 \nL 477.634688 523.015145 \nL 478.519099 521.743799 \nL 479.40351 523.441351 \nL 480.287921 523.073795 \nL 481.172331 523.324836 \nL 482.056742 523.272395 \nL 482.941153 520.95151 \nL 483.825564 522.960813 \nL 484.709974 523.578299 \nL 485.594385 522.427733 \nL 486.478796 521.608626 \nL 487.363207 522.600699 \nL 488.247617 524.801976 \nL 489.132028 521.122652 \nL 490.016439 521.741842 \nL 490.900849 523.45569 \nL 491.78526 523.68158 \nL 492.669671 523.157614 \nL 493.554082 521.746686 \nL 494.438492 523.50711 \nL 495.322903 521.635982 \nL 497.091725 523.73123 \nL 497.976135 523.950839 \nL 499.744957 522.421529 \nL 500.629367 523.540313 \nL 501.513778 523.230017 \nL 502.398189 525.372717 \nL 503.2826 523.21569 \nL 504.16701 524.560551 \nL 505.051421 524.070494 \nL 505.935832 522.698537 \nL 506.820243 520.24945 \nL 507.704653 522.633279 \nL 508.589064 521.858725 \nL 509.473475 523.702056 \nL 510.357886 523.904571 \nL 512.126707 522.434709 \nL 513.011118 522.530838 \nL 513.895528 524.115874 \nL 514.779939 523.947293 \nL 515.66435 521.085149 \nL 516.548761 524.598199 \nL 517.433171 519.481359 \nL 518.317582 522.731414 \nL 519.201993 524.167499 \nL 520.086404 522.813373 \nL 520.970814 520.626036 \nL 521.855225 525.838964 \nL 522.739636 520.12939 \nL 523.624047 524.692963 \nL 524.508457 523.916192 \nL 525.392868 524.644938 \nL 526.277279 525.681031 \nL 528.0461 520.938343 \nL 528.930511 524.864021 \nL 529.814922 522.520969 \nL 530.699332 521.860616 \nL 531.583743 523.822539 \nL 532.468154 524.749668 \nL 533.352565 523.96217 \nL 534.236975 520.310951 \nL 535.121386 521.359095 \nL 536.005797 523.970922 \nL 536.890208 520.51259 \nL 537.774618 524.354213 \nL 539.54344 523.922661 \nL 540.42785 522.612586 \nL 541.312261 521.730637 \nL 542.196672 519.22534 \nL 543.081083 522.538636 \nL 543.965493 524.12555 \nL 545.734315 521.190592 \nL 546.618726 520.99863 \nL 547.503136 524.564912 \nL 548.387547 522.168555 \nL 549.271958 522.159694 \nL 550.156369 523.291096 \nL 551.040779 521.302921 \nL 551.92519 522.723387 \nL 552.809601 522.555138 \nL 553.694011 522.004891 \nL 554.578422 522.239569 \nL 555.462833 520.090727 \nL 556.347244 522.033316 \nL 557.231654 524.642751 \nL 558.116065 525.017338 \nL 559.000476 521.575755 \nL 559.884887 522.286453 \nL 560.769297 524.599788 \nL 561.653708 521.468603 \nL 562.538119 522.642883 \nL 563.42253 521.095858 \nL 564.30694 523.055487 \nL 565.191351 524.479661 \nL 566.075762 523.099242 \nL 566.960172 523.768655 \nL 567.844583 523.326624 \nL 568.728994 524.744752 \nL 569.613405 523.434785 \nL 570.497815 521.90965 \nL 571.382226 524.369905 \nL 572.266637 522.154252 \nL 573.151048 522.620734 \nL 574.035458 523.445071 \nL 574.919869 522.091942 \nL 575.80428 522.481357 \nL 576.68869 521.013597 \nL 577.573101 521.785814 \nL 578.457512 522.366703 \nL 579.341923 522.005561 \nL 580.226333 525.149799 \nL 581.110744 520.682506 \nL 581.995155 522.506913 \nL 582.879566 523.076446 \nL 583.763976 521.940672 \nL 584.648387 522.805466 \nL 585.532798 525.20195 \nL 586.417209 521.279346 \nL 587.301619 523.958715 \nL 588.18603 521.242278 \nL 589.070441 523.700099 \nL 589.954851 520.631805 \nL 590.839262 521.457809 \nL 591.723673 522.950684 \nL 592.608084 521.014171 \nL 593.492494 523.572893 \nL 594.376905 522.303305 \nL 595.261316 522.286109 \nL 596.145727 524.014617 \nL 597.030137 520.394172 \nL 597.914548 524.017419 \nL 598.798959 523.940981 \nL 599.68337 525.590308 \nL 600.56778 523.565361 \nL 601.452191 522.382564 \nL 602.336602 524.927267 \nL 603.221012 523.519474 \nL 604.105423 525.872094 \nL 605.874245 523.11767 \nL 606.758655 522.82953 \nL 607.643066 523.666184 \nL 609.411888 523.401123 \nL 610.296298 522.389674 \nL 612.06512 523.389532 \nL 612.949531 522.960487 \nL 613.833941 523.578825 \nL 614.718352 522.228516 \nL 615.602763 523.876387 \nL 616.487173 521.640409 \nL 617.371584 521.562123 \nL 618.255995 522.038964 \nL 620.024816 524.323958 \nL 620.909227 523.045297 \nL 621.793638 522.866629 \nL 622.678049 524.611657 \nL 623.562459 521.785814 \nL 624.44687 522.484758 \nL 625.331281 522.408295 \nL 626.215692 522.155708 \nL 627.100102 523.868294 \nL 627.984513 521.358291 \nL 628.868924 523.543532 \nL 629.753334 521.307868 \nL 630.637745 521.881877 \nL 631.522156 521.240267 \nL 632.406567 523.697073 \nL 633.290977 522.817341 \nL 634.175388 522.40562 \nL 635.059799 521.712716 \nL 635.94421 522.706933 \nL 636.82862 521.795382 \nL 637.713031 522.524732 \nL 638.597442 522.397526 \nL 639.481852 521.976007 \nL 640.366263 522.709373 \nL 641.250674 521.158628 \nL 642.135085 522.364178 \nL 643.019495 520.970863 \nL 643.903906 524.540256 \nL 644.788317 521.290738 \nL 645.672728 521.92507 \nL 646.557138 524.022131 \nL 647.441549 523.836758 \nL 648.32596 523.028844 \nL 649.210371 525.06218 \nL 650.094781 524.679838 \nL 650.979192 522.571476 \nL 651.863603 522.049377 \nL 652.748013 523.02769 \nL 653.632424 522.388145 \nL 654.516835 523.999323 \nL 655.401246 524.131131 \nL 656.285656 523.442559 \nL 657.170067 519.423186 \nL 658.054478 522.063487 \nL 658.938889 523.407997 \nL 659.823299 520.804209 \nL 660.70771 524.088029 \nL 661.592121 525.044954 \nL 662.476532 523.578674 \nL 663.360942 523.133616 \nL 664.245353 524.069407 \nL 665.129764 523.064088 \nL 666.014174 525.244787 \nL 666.898585 523.370657 \nL 667.782996 523.189077 \nL 668.667407 521.508933 \nL 669.551817 522.567254 \nL 670.436228 523.111099 \nL 671.320639 521.562974 \nL 672.20505 523.308896 \nL 673.08946 523.136914 \nL 673.973871 523.749393 \nL 674.858282 522.487512 \nL 675.742693 523.668298 \nL 676.627103 523.491085 \nL 677.511514 522.029414 \nL 678.395925 525.057457 \nL 679.280335 522.741924 \nL 680.164746 523.922697 \nL 681.049157 522.601176 \nL 681.933568 521.628263 \nL 682.817978 523.464364 \nL 683.702389 523.955459 \nL 684.5868 522.580664 \nL 685.471211 521.884909 \nL 686.355621 523.938511 \nL 687.240032 522.798889 \nL 688.124443 523.385099 \nL 689.008854 522.841605 \nL 690.777675 524.5217 \nL 691.662086 523.905108 \nL 692.546496 524.613898 \nL 693.430907 525.910207 \nL 694.315318 524.444713 \nL 695.199729 522.205195 \nL 696.084139 521.171451 \nL 696.96855 520.769979 \nL 697.852961 521.270673 \nL 698.737372 523.078337 \nL 699.621782 521.114861 \nL 700.506193 519.793508 \nL 701.390604 524.773442 \nL 702.275015 523.834167 \nL 703.159425 523.821319 \nL 704.043836 524.83829 \nL 704.928247 521.692627 \nL 705.812657 525.004002 \nL 706.697068 521.817881 \nL 707.581479 522.685569 \nL 708.46589 523.075486 \nL 709.3503 525.08352 \nL 710.234711 521.346247 \nL 711.119122 523.651651 \nL 712.003533 525.526458 \nL 712.887943 524.021949 \nL 713.772354 524.232026 \nL 714.656765 524.614634 \nL 715.541175 523.975507 \nL 716.425586 522.538624 \nL 717.309997 526.067276 \nL 718.194408 522.122088 \nL 719.078818 522.356223 \nL 719.963229 524.516228 \nL 720.84764 524.559766 \nL 721.732051 526.898433 \nL 722.616461 520.859827 \nL 723.500872 523.35529 \nL 724.385283 523.296308 \nL 725.269694 521.244905 \nL 726.154104 522.100857 \nL 727.038515 522.143561 \nL 728.807336 524.537894 \nL 729.691747 524.292162 \nL 730.576158 521.268837 \nL 732.344979 523.473279 \nL 733.22939 520.346654 \nL 734.113801 522.290132 \nL 734.998212 522.694188 \nL 735.882622 521.329564 \nL 736.767033 523.943663 \nL 737.651444 525.601253 \nL 738.535855 524.475874 \nL 739.420265 522.537307 \nL 740.304676 521.052333 \nL 741.189087 524.742861 \nL 742.073497 522.575916 \nL 742.957908 521.374298 \nL 744.72673 524.75325 \nL 746.495551 521.735016 \nL 747.379962 522.425129 \nL 748.264373 525.433216 \nL 749.148783 524.397509 \nL 750.033194 524.361038 \nL 750.917605 524.672952 \nL 751.802016 522.501145 \nL 752.686426 524.126154 \nL 753.570837 522.57324 \nL 754.455248 523.960938 \nL 755.339658 524.3069 \nL 756.224069 525.085459 \nL 757.10848 522.660823 \nL 757.992891 522.848962 \nL 758.877301 522.051914 \nL 759.761712 524.55265 \nL 760.646123 524.219547 \nL 761.530534 523.290244 \nL 763.299355 522.194969 \nL 765.068177 524.941854 \nL 765.952587 520.675215 \nL 766.836998 524.119643 \nL 767.721409 524.735516 \nL 768.605819 524.145851 \nL 769.49023 523.135066 \nL 770.374641 523.729871 \nL 771.259052 522.122185 \nL 772.143462 521.558758 \nL 773.027873 522.040008 \nL 773.912284 525.12832 \nL 774.796695 524.308863 \nL 775.681105 524.445661 \nL 776.565516 522.004885 \nL 777.449927 523.497603 \nL 778.334337 523.795039 \nL 779.218748 522.986919 \nL 780.103159 522.700839 \nL 780.98757 523.36066 \nL 781.87198 526.143847 \nL 782.756391 522.156807 \nL 783.640802 523.821585 \nL 784.525213 522.196419 \nL 785.409623 523.984766 \nL 786.294034 523.830229 \nL 786.294034 523.830229 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p0e4b88aa24)\" d=\"M 76.112216 124.271154 \nL 77.881037 198.531831 \nL 80.53427 297.25022 \nL 82.303091 355.107672 \nL 84.071913 404.240578 \nL 84.956323 425.463323 \nL 85.840734 443.812242 \nL 86.725145 459.336781 \nL 87.609555 472.222855 \nL 88.493966 482.646777 \nL 89.378377 491.220796 \nL 90.262788 497.727466 \nL 91.147198 502.46406 \nL 92.031609 506.245626 \nL 92.91602 508.757216 \nL 93.800431 510.542622 \nL 94.684841 511.546981 \nL 95.569252 512.333271 \nL 96.453663 512.692263 \nL 98.222484 512.92235 \nL 99.991306 512.827393 \nL 102.644538 512.609174 \nL 104.413359 512.578556 \nL 106.182181 512.584687 \nL 108.835413 512.705273 \nL 110.604234 512.67901 \nL 111.488645 512.745851 \nL 112.373056 512.993117 \nL 113.257467 512.847718 \nL 114.141877 512.832618 \nL 117.67952 513.311132 \nL 125.639217 513.948454 \nL 129.17686 514.092312 \nL 130.945681 514.30408 \nL 131.830092 514.273922 \nL 132.714503 514.61228 \nL 136.252146 515.131867 \nL 137.136556 515.018034 \nL 138.905378 515.20061 \nL 139.789789 515.053592 \nL 140.674199 515.266007 \nL 142.443021 515.168977 \nL 143.327432 515.532402 \nL 145.096253 515.762472 \nL 146.865075 515.866151 \nL 147.749485 515.917281 \nL 149.518307 516.255138 \nL 152.171539 516.665772 \nL 153.05595 516.924201 \nL 154.824771 516.651185 \nL 158.362414 517.28437 \nL 159.246825 517.430373 \nL 160.131235 517.392591 \nL 161.015646 517.544683 \nL 161.900057 517.548005 \nL 163.668878 517.877484 \nL 164.553289 517.756511 \nL 165.4377 517.91785 \nL 166.322111 517.94128 \nL 167.206521 518.203967 \nL 169.859754 518.265486 \nL 171.628575 518.888698 \nL 172.512986 518.555788 \nL 173.397396 518.658725 \nL 174.281807 518.510638 \nL 175.166218 518.956227 \nL 176.050629 519.110656 \nL 177.81945 519.666417 \nL 179.588272 519.451013 \nL 182.241504 519.54411 \nL 183.125915 520.024442 \nL 184.010325 520.355962 \nL 185.779147 520.340137 \nL 186.663557 520.204095 \nL 187.547968 520.494741 \nL 191.085611 521.029767 \nL 191.970022 521.012444 \nL 192.854433 521.45613 \nL 193.738843 521.47869 \nL 195.507665 521.752321 \nL 196.392076 521.86825 \nL 197.276486 521.829231 \nL 198.160897 521.940273 \nL 199.045308 522.201975 \nL 199.929718 522.299893 \nL 201.69854 522.739441 \nL 204.351772 522.932527 \nL 209.658237 523.474946 \nL 211.427058 523.90053 \nL 213.195879 524.122017 \nL 214.964701 524.054367 \nL 216.733522 524.261369 \nL 217.617933 524.325311 \nL 218.502344 524.679615 \nL 222.924398 525.443665 \nL 223.808808 525.666378 \nL 224.693219 525.727722 \nL 226.46204 526.098117 \nL 228.230862 525.909368 \nL 229.115273 526.266915 \nL 229.999683 526.34397 \nL 230.884094 526.603715 \nL 234.421737 526.938328 \nL 235.306148 527.20337 \nL 236.190558 527.109669 \nL 239.728201 527.701895 \nL 240.612612 527.60242 \nL 242.381434 528.011176 \nL 245.919077 528.67865 \nL 247.687898 528.653783 \nL 248.572309 528.714505 \nL 249.456719 528.960986 \nL 250.34113 528.938009 \nL 252.109952 529.42943 \nL 253.878773 529.675036 \nL 256.532005 529.987989 \nL 257.416416 530.139899 \nL 259.185238 530.700136 \nL 261.83847 530.636195 \nL 262.72288 530.730506 \nL 264.491702 531.156923 \nL 266.260523 531.351459 \nL 267.144934 531.229665 \nL 268.913756 531.57895 \nL 269.798166 531.697277 \nL 270.682577 531.66768 \nL 272.451399 531.987108 \nL 274.22022 532.131703 \nL 275.104631 532.190432 \nL 275.989041 532.052565 \nL 277.757863 532.495225 \nL 280.411095 533.009176 \nL 281.295506 532.991708 \nL 282.179917 533.118914 \nL 283.064327 533.073039 \nL 283.948738 533.260344 \nL 284.833149 533.184951 \nL 286.60197 533.525918 \nL 287.486381 533.484785 \nL 290.139613 534.025603 \nL 291.024024 533.917188 \nL 291.908435 533.923198 \nL 295.446078 534.502202 \nL 297.214899 534.492985 \nL 298.09931 534.589555 \nL 298.98372 534.423625 \nL 300.752542 534.457716 \nL 301.636953 534.792969 \nL 304.290185 535.155494 \nL 305.174596 535.012336 \nL 306.943417 535.178839 \nL 308.712239 535.50351 \nL 309.596649 535.329572 \nL 314.018703 535.81366 \nL 315.787524 535.820697 \nL 319.325167 536.247039 \nL 320.209578 536.229447 \nL 321.9784 536.395011 \nL 324.631632 536.582045 \nL 325.516042 536.402474 \nL 326.400453 536.405107 \nL 329.053685 536.756421 \nL 329.938096 536.830851 \nL 330.822507 536.694863 \nL 332.591328 537.049837 \nL 334.36015 537.292241 \nL 337.897793 537.380425 \nL 340.551025 537.156169 \nL 342.319846 537.554928 \nL 343.204257 537.70955 \nL 344.088668 537.531592 \nL 346.7419 537.714778 \nL 350.279543 537.989895 \nL 352.048364 538.09574 \nL 352.932775 538.236144 \nL 353.817186 538.24332 \nL 354.701597 537.831803 \nL 355.586007 538.113513 \nL 356.470418 538.215326 \nL 358.23924 538.236914 \nL 359.12365 538.37402 \nL 360.008061 538.292021 \nL 363.545704 538.414742 \nL 364.430115 538.629464 \nL 372.389811 538.757633 \nL 374.158633 538.960462 \nL 375.043043 538.917423 \nL 376.811865 539.149257 \nL 378.580686 539.055945 \nL 379.465097 539.003997 \nL 380.349508 539.110503 \nL 381.233919 539.066319 \nL 383.00274 539.273095 \nL 388.309204 539.251103 \nL 389.193615 539.449465 \nL 390.962437 539.362492 \nL 391.846847 539.505741 \nL 392.731258 539.402539 \nL 393.615669 539.529364 \nL 396.268901 539.516027 \nL 397.153312 539.599016 \nL 398.037723 539.445811 \nL 398.922133 539.612969 \nL 399.806544 539.518591 \nL 406.88183 539.727506 \nL 407.766241 539.664486 \nL 408.650651 539.731287 \nL 409.535062 539.637468 \nL 412.188294 539.83172 \nL 413.957116 539.800658 \nL 414.841526 539.925046 \nL 415.725937 539.812838 \nL 417.494759 539.983452 \nL 418.379169 539.734207 \nL 419.26358 539.963477 \nL 421.032402 540.030616 \nL 421.916812 539.91524 \nL 422.801223 540.054121 \nL 423.685634 540.013462 \nL 424.570045 540.085002 \nL 425.454455 540.042059 \nL 427.223277 540.21485 \nL 428.107687 540.068331 \nL 430.76092 540.241505 \nL 432.529741 540.281769 \nL 434.298563 540.345614 \nL 438.720616 539.982056 \nL 439.605027 540.193181 \nL 441.373848 540.00664 \nL 442.258259 540.209767 \nL 443.14267 540.2557 \nL 444.027081 540.138877 \nL 444.911491 540.232403 \nL 445.795902 540.133589 \nL 448.449134 540.343614 \nL 451.102366 540.251387 \nL 451.986777 540.332051 \nL 453.755599 540.233203 \nL 458.177652 540.496107 \nL 459.946474 540.35751 \nL 462.599706 540.469624 \nL 464.368527 540.287888 \nL 465.252938 540.443005 \nL 467.90617 540.530201 \nL 468.790581 540.667443 \nL 471.443813 540.608059 \nL 474.097046 540.608956 \nL 475.865867 540.505681 \nL 477.634688 540.65821 \nL 479.40351 540.610523 \nL 486.478796 540.696949 \nL 487.363207 540.817523 \nL 489.132028 540.746581 \nL 490.016439 540.768247 \nL 490.900849 540.597558 \nL 498.860546 540.827344 \nL 502.398189 540.791348 \nL 503.2826 540.69601 \nL 507.704653 540.898693 \nL 512.126707 540.882119 \nL 513.011118 540.950488 \nL 513.895528 540.826091 \nL 517.433171 540.915482 \nL 519.201993 540.757858 \nL 520.970814 540.820292 \nL 521.855225 540.737035 \nL 523.624047 540.845286 \nL 524.508457 540.902997 \nL 527.161689 540.716673 \nL 528.930511 540.902281 \nL 529.814922 540.901846 \nL 530.699332 540.747976 \nL 532.468154 540.912299 \nL 533.352565 540.777479 \nL 535.121386 540.931344 \nL 537.774618 540.829277 \nL 538.659029 540.957443 \nL 541.312261 540.956637 \nL 542.196672 541.005031 \nL 543.081083 540.932361 \nL 543.965493 540.989921 \nL 544.849904 540.893728 \nL 546.618726 540.947743 \nL 550.156369 541.027811 \nL 551.92519 540.971052 \nL 562.538119 541.009277 \nL 566.075762 541.051999 \nL 568.728994 540.964139 \nL 570.497815 541.064635 \nL 571.382226 540.863295 \nL 573.151048 540.993074 \nL 574.035458 540.876907 \nL 575.80428 541.077525 \nL 576.68869 540.967074 \nL 577.573101 541.072303 \nL 580.226333 540.81382 \nL 581.110744 540.833182 \nL 581.995155 540.687242 \nL 582.879566 541.006251 \nL 589.954851 541.064106 \nL 591.723673 541.115849 \nL 593.492494 540.978478 \nL 594.376905 541.084522 \nL 595.261316 540.93925 \nL 596.145727 540.972024 \nL 597.030137 540.890612 \nL 597.914548 541.066293 \nL 600.56778 541.014864 \nL 601.452191 540.959252 \nL 602.336602 541.109332 \nL 612.949531 541.0077 \nL 614.718352 540.975621 \nL 615.602763 540.874367 \nL 617.371584 541.012378 \nL 618.255995 540.932265 \nL 620.024816 541.074743 \nL 621.793638 541.025178 \nL 622.678049 541.130926 \nL 624.44687 541.01535 \nL 628.868924 541.089206 \nL 629.753334 540.944378 \nL 630.637745 541.083891 \nL 635.94421 541.000307 \nL 638.597442 541.122089 \nL 640.366263 540.929676 \nL 643.019495 541.083674 \nL 646.557138 541.048423 \nL 647.441549 540.853033 \nL 650.094781 541.136244 \nL 652.748013 541.146225 \nL 653.632424 540.996852 \nL 655.401246 540.968711 \nL 656.285656 541.086672 \nL 659.823299 541.038837 \nL 661.592121 541.139753 \nL 662.476532 541.036983 \nL 667.782996 541.115557 \nL 669.551817 541.078337 \nL 673.08946 541.061373 \nL 673.973871 541.14612 \nL 674.858282 541.107152 \nL 675.742693 541.183705 \nL 681.049157 540.993542 \nL 689.893264 541.074529 \nL 690.777675 541.017316 \nL 692.546496 541.1 \nL 693.430907 541.123611 \nL 694.315318 540.972242 \nL 695.199729 541.123708 \nL 704.928247 541.080889 \nL 705.812657 540.972855 \nL 710.234711 541.145637 \nL 711.119122 541.003795 \nL 712.003533 541.134329 \nL 720.84764 540.878432 \nL 721.732051 541.060591 \nL 724.385283 541.065952 \nL 726.154104 541.021276 \nL 727.038515 541.132511 \nL 727.922926 541.113596 \nL 728.807336 540.96583 \nL 731.460569 541.143797 \nL 734.998212 541.043183 \nL 736.767033 541.138551 \nL 738.535855 540.889618 \nL 742.073497 541.154824 \nL 743.842319 540.993965 \nL 744.72673 540.998392 \nL 745.61114 541.127166 \nL 747.379962 541.082767 \nL 753.570837 541.112077 \nL 754.455248 541.024791 \nL 757.992891 541.068972 \nL 759.761712 540.980728 \nL 761.530534 540.995985 \nL 763.299355 540.942065 \nL 764.183766 541.157243 \nL 765.068177 541.041637 \nL 771.259052 541.112854 \nL 773.912284 541.105343 \nL 775.681105 541.05829 \nL 776.565516 541.1339 \nL 777.449927 541.080904 \nL 778.334337 541.19268 \nL 779.218748 541.107771 \nL 780.98757 541.144383 \nL 785.409623 541.121536 \nL 786.294034 541.070337 \nL 786.294034 541.070337 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Model Loss -->\n    <g transform=\"translate(398.119687 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"385.994141\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"447.175781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"499.275391\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_21\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_22\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p0e4b88aa24\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 822.640625 603.474375\" width=\"822.640625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-13T20:17:01.469832</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 822.640625 603.474375 \nL 822.640625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \nL 815.440625 22.318125 \nL 34.240625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m3720f7d877\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.749716\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(66.568466 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.190789\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(148.647039 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.631862\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(237.088112 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"335.072936\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(325.529186 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"423.514009\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(413.970259 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"511.955082\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(502.411332 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"600.396155\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(590.852405 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"688.837229\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(679.293479 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"777.278302\" xlink:href=\"#m3720f7d877\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(767.734552 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(409.6125 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mca2b88deec\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca2b88deec\" y=\"486.41205\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2 -->\n      <g transform=\"translate(20.878125 490.211269)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca2b88deec\" y=\"377.128349\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 3 -->\n      <g transform=\"translate(20.878125 380.927568)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca2b88deec\" y=\"267.844647\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 4 -->\n      <g transform=\"translate(20.878125 271.643866)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca2b88deec\" y=\"158.560946\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 5 -->\n      <g transform=\"translate(20.878125 162.360165)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca2b88deec\" y=\"49.277244\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 6 -->\n      <g transform=\"translate(20.878125 53.076463)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- MAE -->\n     <g transform=\"translate(14.798438 305.011875)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"154.6875\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p8a45626e43)\" d=\"M 69.749716 47.027216 \nL 79.478234 386.721715 \nL 80.362645 410.64902 \nL 81.247055 429.448415 \nL 82.131466 443.667841 \nL 83.015877 455.718948 \nL 83.900288 465.771382 \nL 84.784698 473.478738 \nL 85.669109 480.104488 \nL 86.55352 484.355769 \nL 87.437931 483.205924 \nL 88.322341 485.098005 \nL 89.206752 485.853347 \nL 90.091163 480.047609 \nL 90.975573 486.481279 \nL 91.859984 485.454493 \nL 92.744395 485.90452 \nL 93.628806 486.7321 \nL 94.513216 488.607571 \nL 95.397627 482.675908 \nL 96.282038 483.281536 \nL 97.166449 486.063222 \nL 98.050859 491.986157 \nL 98.93527 482.700869 \nL 99.819681 484.11197 \nL 100.704092 484.745503 \nL 101.588502 490.131921 \nL 102.472913 480.444014 \nL 103.357324 484.582997 \nL 104.241734 486.928036 \nL 105.126145 482.732708 \nL 106.010556 488.50249 \nL 106.894967 486.516871 \nL 107.779377 479.793336 \nL 108.663788 487.813016 \nL 109.548199 489.597476 \nL 110.43261 484.536306 \nL 111.31702 485.064472 \nL 112.201431 487.827164 \nL 113.085842 485.865906 \nL 113.970253 488.379066 \nL 114.854663 484.898552 \nL 115.739074 482.134896 \nL 116.623485 486.813809 \nL 117.507895 483.573981 \nL 118.392306 488.529666 \nL 119.276717 488.781581 \nL 120.161128 484.291881 \nL 121.045538 488.503988 \nL 121.929949 484.58826 \nL 122.81436 484.141881 \nL 123.698771 487.672695 \nL 124.583181 486.527892 \nL 125.467592 486.853283 \nL 126.352003 484.497822 \nL 127.236414 483.718275 \nL 128.120824 486.900078 \nL 129.005235 489.274495 \nL 129.889646 488.3197 \nL 130.774056 484.356081 \nL 131.658467 488.744987 \nL 132.542878 490.507052 \nL 133.427289 494.797625 \nL 135.19611 486.367782 \nL 136.080521 488.743032 \nL 136.964932 484.725675 \nL 137.849342 489.693607 \nL 138.733753 488.759043 \nL 139.618164 491.000448 \nL 140.502575 487.155472 \nL 141.386985 486.38378 \nL 142.271396 485.181147 \nL 143.155807 487.277489 \nL 144.040217 485.340241 \nL 144.924628 491.541225 \nL 145.809039 491.520628 \nL 146.69345 491.637108 \nL 147.57786 488.222057 \nL 148.462271 489.928508 \nL 149.346682 488.609278 \nL 150.231093 491.320419 \nL 151.115503 489.794962 \nL 151.999914 488.7592 \nL 152.884325 491.680751 \nL 153.768735 488.298425 \nL 154.653146 493.536667 \nL 155.537557 490.217135 \nL 156.421968 490.337224 \nL 157.306378 491.406949 \nL 158.190789 492.696789 \nL 159.0752 488.601644 \nL 159.959611 488.712079 \nL 160.844021 488.521654 \nL 161.728432 494.747612 \nL 162.612843 495.52781 \nL 163.497254 492.553367 \nL 164.381664 491.014335 \nL 165.266075 490.408772 \nL 166.150486 492.659139 \nL 167.034896 487.885384 \nL 167.919307 493.724239 \nL 168.803718 485.848475 \nL 169.688129 493.024785 \nL 170.572539 492.630126 \nL 171.45695 492.090691 \nL 172.341361 492.089701 \nL 173.225772 496.731016 \nL 174.110182 492.895017 \nL 174.994593 492.324498 \nL 175.879004 491.50291 \nL 176.763415 490.864726 \nL 177.647825 493.644888 \nL 178.532236 491.730946 \nL 180.301057 495.662477 \nL 181.185468 495.720593 \nL 182.069879 492.448795 \nL 182.95429 495.435744 \nL 183.8387 496.324137 \nL 184.723111 491.9366 \nL 186.491933 495.929035 \nL 187.376343 492.761341 \nL 188.260754 492.823834 \nL 189.145165 498.299074 \nL 190.029576 495.240238 \nL 190.913986 493.36977 \nL 192.682808 497.876458 \nL 193.567218 494.562502 \nL 194.451629 494.347807 \nL 195.33604 493.408319 \nL 196.220451 494.240263 \nL 197.104861 493.929711 \nL 197.989272 496.830574 \nL 198.873683 496.546037 \nL 199.758094 500.684864 \nL 200.642504 494.90935 \nL 201.526915 499.715764 \nL 202.411326 491.303861 \nL 203.295737 494.402366 \nL 204.180147 496.333231 \nL 205.064558 496.483518 \nL 205.948969 494.384466 \nL 206.833379 495.309207 \nL 207.71779 499.460045 \nL 208.602201 495.920437 \nL 209.486612 495.631185 \nL 210.371022 498.68933 \nL 212.139844 495.435405 \nL 213.024255 504.3122 \nL 213.908665 498.820584 \nL 214.793076 498.816897 \nL 216.561898 500.128154 \nL 217.446308 498.871352 \nL 218.330719 497.232059 \nL 219.21513 497.623904 \nL 220.09954 500.118201 \nL 220.983951 501.269427 \nL 221.868362 497.952696 \nL 222.752773 502.211155 \nL 223.637183 502.048518 \nL 224.521594 498.166466 \nL 225.406005 499.38085 \nL 226.290416 499.41734 \nL 227.174826 497.621351 \nL 228.059237 500.411648 \nL 228.943648 501.193893 \nL 229.828058 501.393945 \nL 231.59688 503.233251 \nL 232.481291 504.951296 \nL 233.365701 503.585127 \nL 234.250112 500.045689 \nL 235.134523 502.660179 \nL 236.018934 501.296641 \nL 236.903344 503.431427 \nL 237.787755 504.063828 \nL 238.672166 499.305915 \nL 239.556577 504.97287 \nL 240.440987 506.164937 \nL 241.325398 504.519626 \nL 242.209809 501.223517 \nL 243.094219 501.829498 \nL 243.97863 503.735115 \nL 244.863041 504.873482 \nL 245.747452 503.502089 \nL 246.631862 504.414558 \nL 247.516273 502.27585 \nL 248.400684 503.306974 \nL 249.285095 506.107629 \nL 250.169505 503.213697 \nL 251.053916 504.52838 \nL 251.938327 506.950074 \nL 252.822738 506.251806 \nL 253.707148 509.527018 \nL 254.591559 508.070046 \nL 255.47597 504.615613 \nL 256.36038 506.323054 \nL 257.244791 506.344641 \nL 258.129202 506.233984 \nL 259.013613 505.126466 \nL 259.898023 505.720304 \nL 260.782434 503.092761 \nL 262.551256 510.589278 \nL 263.435666 502.979369 \nL 264.320077 505.116135 \nL 265.204488 504.233226 \nL 266.973309 509.944592 \nL 267.85772 506.360235 \nL 268.742131 511.536986 \nL 269.626541 506.533307 \nL 270.510952 504.485832 \nL 271.395363 510.965268 \nL 272.279774 507.766073 \nL 273.164184 505.302717 \nL 274.048595 505.480036 \nL 274.933006 513.022709 \nL 275.817417 506.000659 \nL 276.701827 509.269995 \nL 277.586238 505.327404 \nL 278.470649 502.537771 \nL 279.35506 507.890004 \nL 280.23947 508.496597 \nL 281.123881 512.376669 \nL 282.008292 508.211591 \nL 282.892702 505.005439 \nL 283.777113 509.589602 \nL 284.661524 506.883398 \nL 285.545935 509.140488 \nL 286.430345 508.817715 \nL 287.314756 512.476656 \nL 288.199167 505.66891 \nL 289.083578 512.082596 \nL 289.967988 511.205133 \nL 290.852399 508.7192 \nL 291.73681 509.831486 \nL 292.62122 512.863381 \nL 293.505631 508.68142 \nL 294.390042 508.041359 \nL 295.274453 507.79855 \nL 296.158863 511.105941 \nL 297.043274 511.701655 \nL 297.927685 514.477674 \nL 298.812096 510.169293 \nL 299.696506 512.189097 \nL 300.580917 512.273633 \nL 301.465328 509.97664 \nL 302.349739 512.894869 \nL 303.234149 512.164162 \nL 304.11856 505.91469 \nL 305.002971 512.358443 \nL 305.887381 513.165088 \nL 306.771792 513.319518 \nL 307.656203 516.114583 \nL 308.540614 506.678917 \nL 309.425024 514.354576 \nL 310.309435 514.098414 \nL 311.193846 513.507376 \nL 312.078257 511.476043 \nL 312.962667 515.662668 \nL 313.847078 506.285208 \nL 314.731489 511.577345 \nL 315.6159 514.544701 \nL 316.50031 507.861018 \nL 317.384721 514.228234 \nL 318.269132 512.766182 \nL 319.153542 510.85439 \nL 320.037953 514.089164 \nL 320.922364 511.321222 \nL 321.806775 514.907938 \nL 322.691185 510.50267 \nL 323.575596 513.819414 \nL 324.460007 508.112764 \nL 325.344418 510.80964 \nL 326.228828 510.714343 \nL 327.113239 511.744425 \nL 327.99765 514.232781 \nL 328.882061 511.899532 \nL 329.766471 514.441979 \nL 330.650882 513.114502 \nL 331.535293 512.758118 \nL 332.419703 518.253538 \nL 333.304114 516.68961 \nL 334.188525 514.868581 \nL 335.072936 512.500796 \nL 335.957346 510.708168 \nL 336.841757 513.396224 \nL 337.726168 514.499808 \nL 338.610579 518.763843 \nL 339.494989 518.781991 \nL 340.3794 517.852951 \nL 341.263811 518.032811 \nL 342.148222 515.407196 \nL 343.032632 517.657993 \nL 343.917043 513.786063 \nL 344.801454 515.574041 \nL 345.685864 510.212115 \nL 346.570275 516.836184 \nL 347.454686 513.015831 \nL 348.339097 514.179224 \nL 349.223507 518.129254 \nL 350.107918 517.470434 \nL 350.992329 517.472193 \nL 351.87674 512.989241 \nL 352.76115 512.086557 \nL 353.645561 513.51432 \nL 354.529972 516.115039 \nL 355.414383 513.064306 \nL 356.298793 518.430023 \nL 358.067615 512.207101 \nL 358.952025 516.494469 \nL 359.836436 517.231859 \nL 361.605258 516.136157 \nL 362.489668 516.256259 \nL 363.374079 513.892838 \nL 364.25849 516.150266 \nL 365.142901 514.230214 \nL 366.027311 514.313409 \nL 366.911722 518.606978 \nL 367.796133 514.298245 \nL 368.680543 513.877648 \nL 369.564954 513.085893 \nL 370.449365 517.266656 \nL 371.333776 519.598302 \nL 372.218186 515.442331 \nL 373.102597 516.241134 \nL 373.987008 513.909591 \nL 374.871419 517.159022 \nL 375.755829 514.513552 \nL 376.64024 518.754138 \nL 377.524651 521.67479 \nL 378.409062 515.171682 \nL 379.293472 517.429684 \nL 380.177883 514.161051 \nL 381.946704 516.643779 \nL 382.831115 514.42714 \nL 383.715526 517.091955 \nL 384.599937 519.039443 \nL 385.484347 513.838929 \nL 386.368758 513.181477 \nL 387.253169 516.955426 \nL 388.13758 518.561746 \nL 389.02199 516.146879 \nL 389.906401 514.548141 \nL 390.790812 512.17207 \nL 391.675223 516.702012 \nL 392.559633 517.436849 \nL 393.444044 517.148573 \nL 394.328455 517.063529 \nL 395.212865 517.167099 \nL 396.097276 516.743127 \nL 396.981687 514.45911 \nL 397.866098 517.366799 \nL 398.750508 514.462666 \nL 399.634919 515.563579 \nL 400.51933 517.587722 \nL 401.403741 516.701426 \nL 402.288151 514.705554 \nL 403.172562 516.900684 \nL 404.056973 518.458411 \nL 404.941384 515.941641 \nL 405.825794 514.01148 \nL 406.710205 521.131993 \nL 407.594616 511.58481 \nL 408.479026 518.582629 \nL 409.363437 521.159951 \nL 410.247848 518.531457 \nL 411.132259 512.524923 \nL 412.016669 516.784308 \nL 412.90108 514.284566 \nL 413.785491 521.92793 \nL 414.669902 518.533098 \nL 415.554312 521.407658 \nL 416.438723 518.421347 \nL 417.323134 520.522431 \nL 418.207545 518.438622 \nL 419.091955 518.147741 \nL 420.860777 516.301191 \nL 421.745187 522.113873 \nL 422.629598 516.220811 \nL 423.514009 521.118197 \nL 424.39842 519.639261 \nL 425.28283 513.223817 \nL 426.167241 517.540627 \nL 427.051652 518.373301 \nL 427.936063 516.521045 \nL 428.820473 511.681436 \nL 429.704884 519.052705 \nL 430.589295 517.277208 \nL 431.473705 517.159204 \nL 432.358116 516.04241 \nL 433.242527 519.222794 \nL 434.126938 518.59653 \nL 435.011348 518.189364 \nL 435.895759 523.52913 \nL 436.78017 514.540832 \nL 437.664581 516.339362 \nL 438.548991 520.91309 \nL 439.433402 516.944795 \nL 440.317813 514.999835 \nL 441.202224 516.964753 \nL 442.971045 513.731543 \nL 443.855456 520.274827 \nL 444.739866 518.604073 \nL 446.508688 516.313906 \nL 447.393099 520.637269 \nL 448.277509 517.940706 \nL 449.16192 517.97816 \nL 450.046331 518.661433 \nL 450.930742 516.246175 \nL 451.815152 512.951383 \nL 452.699563 518.709453 \nL 453.583974 516.470277 \nL 454.468385 521.132488 \nL 455.352795 520.503762 \nL 456.237206 518.047428 \nL 457.121617 526.252999 \nL 458.006027 516.375435 \nL 458.890438 516.103914 \nL 459.774849 519.59214 \nL 460.65926 518.909023 \nL 461.54367 519.627419 \nL 462.428081 518.293194 \nL 463.312492 515.364374 \nL 464.196903 519.351468 \nL 465.081313 515.786912 \nL 465.965724 520.71236 \nL 466.850135 521.632619 \nL 467.734546 518.1813 \nL 468.618956 518.63216 \nL 469.503367 516.93677 \nL 470.387778 520.626534 \nL 471.272188 517.836654 \nL 472.156599 517.297818 \nL 473.04101 519.801286 \nL 473.925421 516.864128 \nL 474.809831 519.804869 \nL 475.694242 518.546894 \nL 476.578653 513.75722 \nL 477.463064 520.27015 \nL 478.347474 521.052447 \nL 479.231885 517.248886 \nL 480.116296 517.397088 \nL 481.000707 518.09025 \nL 481.885117 522.292417 \nL 482.769528 514.682195 \nL 484.538349 519.772573 \nL 485.42276 519.194329 \nL 486.307171 520.775232 \nL 487.191582 515.887069 \nL 488.075992 519.852615 \nL 488.960403 518.035065 \nL 489.844814 516.743805 \nL 490.729225 521.729636 \nL 491.613635 522.86889 \nL 492.498046 518.142491 \nL 493.382457 517.036901 \nL 494.266867 519.017166 \nL 495.151278 518.26624 \nL 496.035689 522.011841 \nL 496.9201 517.743676 \nL 497.80451 521.51894 \nL 499.573332 520.876027 \nL 500.457743 512.989762 \nL 501.342153 518.151857 \nL 502.226564 517.612057 \nL 503.110975 518.484779 \nL 503.995386 519.596622 \nL 504.879796 520.186448 \nL 505.764207 518.286732 \nL 506.648618 519.022898 \nL 507.533028 521.283453 \nL 508.417439 520.36464 \nL 509.30185 516.736496 \nL 510.186261 521.439159 \nL 511.070671 513.818242 \nL 511.955082 519.639496 \nL 512.839493 521.216113 \nL 514.608314 515.793869 \nL 515.492725 523.326238 \nL 516.377136 512.279665 \nL 517.261547 520.277433 \nL 518.145957 520.178723 \nL 519.030368 522.703061 \nL 519.914779 520.68865 \nL 520.799189 520.939836 \nL 521.6836 516.010961 \nL 522.568011 522.440333 \nL 523.452422 518.816866 \nL 524.336832 518.081886 \nL 525.221243 519.44412 \nL 526.105654 522.412115 \nL 526.990065 520.114692 \nL 527.874475 516.69073 \nL 528.758886 517.452039 \nL 529.643297 520.137946 \nL 530.527708 515.803718 \nL 531.412118 520.610549 \nL 532.296529 519.979178 \nL 533.18094 522.930198 \nL 534.06535 520.699124 \nL 534.949761 518.881014 \nL 535.834172 516.473599 \nL 537.602993 521.555548 \nL 538.487404 517.645864 \nL 539.371815 517.176895 \nL 540.256226 516.168101 \nL 541.140636 522.56898 \nL 542.025047 518.333124 \nL 542.909458 516.364323 \nL 543.793869 519.792623 \nL 544.678279 518.24038 \nL 545.56269 518.604242 \nL 546.447101 515.916641 \nL 547.331511 518.52661 \nL 548.215922 517.915237 \nL 549.100333 515.710792 \nL 549.984744 516.143322 \nL 550.869154 522.54669 \nL 551.753565 518.872585 \nL 552.637976 517.909687 \nL 553.522387 517.194418 \nL 554.406797 523.084171 \nL 555.291208 517.941201 \nL 556.175619 519.721661 \nL 557.06003 519.421022 \nL 557.94444 519.621283 \nL 558.828851 522.175103 \nL 559.713262 520.789236 \nL 560.597672 522.41033 \nL 561.482083 520.860576 \nL 562.366494 524.766937 \nL 563.250905 522.632308 \nL 564.135315 518.036941 \nL 565.019726 522.658011 \nL 565.904137 519.798264 \nL 566.788548 519.052145 \nL 567.672958 520.503593 \nL 568.557369 518.459765 \nL 569.44178 520.130572 \nL 570.32619 518.6813 \nL 572.095012 520.284833 \nL 572.979423 519.720306 \nL 573.863833 520.111682 \nL 574.748244 513.657845 \nL 575.632655 518.329385 \nL 576.517066 521.488807 \nL 577.401476 519.818482 \nL 578.285887 520.89648 \nL 579.170298 519.573902 \nL 580.054709 516.446423 \nL 580.939119 518.034166 \nL 581.82353 514.655593 \nL 582.707941 519.957044 \nL 583.592351 515.237758 \nL 584.476762 515.765234 \nL 585.361173 519.73491 \nL 586.245584 518.46469 \nL 587.129994 519.477927 \nL 588.898816 518.822168 \nL 589.783227 523.486243 \nL 590.667637 515.694898 \nL 591.552048 522.565294 \nL 592.436459 518.943091 \nL 593.32087 522.19355 \nL 594.20528 520.730586 \nL 595.089691 517.090835 \nL 595.974102 522.750924 \nL 596.858512 518.774148 \nL 597.742923 524.084811 \nL 598.627334 520.849072 \nL 599.511745 519.565342 \nL 600.396155 517.731547 \nL 601.280566 521.506525 \nL 602.164977 518.480662 \nL 603.049388 519.179151 \nL 603.933798 518.630805 \nL 604.818209 520.031432 \nL 605.70262 519.067505 \nL 606.587031 521.080391 \nL 607.471441 521.339367 \nL 608.355852 517.495682 \nL 609.240263 518.339025 \nL 610.124673 516.382093 \nL 611.009084 518.968612 \nL 611.893495 518.194927 \nL 612.777906 519.619029 \nL 613.662316 520.458243 \nL 615.431138 518.343663 \nL 616.315549 519.663245 \nL 617.199959 519.367466 \nL 618.08437 518.801259 \nL 618.968781 519.024982 \nL 619.853192 520.073772 \nL 620.737602 522.566453 \nL 621.622013 517.641018 \nL 622.506424 519.457786 \nL 623.390834 517.649108 \nL 624.275245 516.973182 \nL 625.159656 515.569963 \nL 626.044067 521.767963 \nL 626.928477 518.908255 \nL 627.812888 516.753159 \nL 628.697299 518.613049 \nL 629.58171 519.403956 \nL 630.46612 518.406899 \nL 631.350531 520.299906 \nL 632.234942 519.248706 \nL 633.119352 515.49779 \nL 634.003763 520.294981 \nL 634.888174 518.138087 \nL 635.772585 519.031639 \nL 636.656995 517.932146 \nL 637.541406 520.440239 \nL 638.425817 517.264819 \nL 639.310228 516.289049 \nL 640.194638 523.748802 \nL 641.079049 523.101368 \nL 641.96346 520.137399 \nL 642.847871 524.549154 \nL 643.732281 524.551994 \nL 644.616692 517.604215 \nL 645.501103 518.83642 \nL 646.385513 520.319994 \nL 647.269924 520.045007 \nL 648.154335 520.848968 \nL 649.038746 519.464561 \nL 649.923156 519.670775 \nL 650.807567 514.656661 \nL 651.691978 518.286029 \nL 652.576389 518.83048 \nL 653.460799 517.198652 \nL 654.34521 524.417836 \nL 655.229621 522.890789 \nL 656.114032 521.940501 \nL 656.998442 519.458933 \nL 657.882853 521.944722 \nL 658.767264 520.46877 \nL 659.651674 524.603818 \nL 660.536085 518.782681 \nL 661.420496 522.340072 \nL 662.304907 516.447387 \nL 663.189317 519.42256 \nL 664.073728 518.381769 \nL 664.958139 518.342165 \nL 665.84255 519.107135 \nL 666.72696 520.5621 \nL 667.611371 519.104412 \nL 668.495782 519.151702 \nL 669.380193 522.265228 \nL 670.264603 519.626221 \nL 671.149014 519.285496 \nL 672.033425 523.916546 \nL 672.917835 519.727849 \nL 673.802246 520.036878 \nL 674.686657 517.902 \nL 675.571068 518.452275 \nL 676.455478 520.260315 \nL 677.339889 521.113976 \nL 678.2243 517.787227 \nL 679.108711 518.621308 \nL 679.993121 519.80195 \nL 680.877532 516.905569 \nL 681.761943 520.988181 \nL 682.646354 517.791917 \nL 683.530764 521.151392 \nL 684.415175 520.081875 \nL 685.299586 521.338221 \nL 687.068407 525.058352 \nL 687.952818 522.07849 \nL 688.837229 517.967269 \nL 689.721639 519.127536 \nL 690.60605 518.21366 \nL 691.490461 519.570723 \nL 692.374872 523.10043 \nL 693.259282 518.090641 \nL 694.143693 514.703013 \nL 695.028104 523.993187 \nL 695.912515 521.277786 \nL 696.796925 521.824842 \nL 697.681336 523.27543 \nL 698.565747 520.159963 \nL 699.450157 522.419671 \nL 700.334568 518.79537 \nL 701.218979 519.996401 \nL 702.10339 518.338739 \nL 702.9878 525.214658 \nL 703.872211 517.50553 \nL 705.641033 524.525522 \nL 706.525443 524.623177 \nL 707.409854 522.010994 \nL 708.294265 524.639579 \nL 709.178675 523.789135 \nL 710.063086 520.907345 \nL 710.947497 524.443579 \nL 711.831908 517.793129 \nL 712.716318 518.62364 \nL 713.600729 522.644632 \nL 714.48514 520.084663 \nL 715.369551 523.366988 \nL 716.253961 518.607303 \nL 717.138372 519.408268 \nL 718.022783 519.739209 \nL 718.907194 515.750774 \nL 719.791604 517.647076 \nL 720.676015 518.288908 \nL 721.560426 523.550052 \nL 722.444836 521.85207 \nL 723.329247 520.75939 \nL 724.213658 517.51224 \nL 725.098069 519.301924 \nL 725.982479 520.044225 \nL 726.86689 517.737527 \nL 727.751301 519.463388 \nL 728.635712 522.403582 \nL 729.520122 517.848887 \nL 730.404533 521.01567 \nL 731.288944 521.617195 \nL 732.173355 522.925416 \nL 733.057765 520.726495 \nL 733.942176 516.369782 \nL 734.826587 520.354452 \nL 735.710997 518.026479 \nL 736.595408 518.890029 \nL 737.479819 519.533803 \nL 738.36423 521.173408 \nL 739.24864 519.03758 \nL 740.133051 520.322443 \nL 741.017462 518.124096 \nL 741.901873 524.365934 \nL 742.786283 521.953985 \nL 743.670694 522.375181 \nL 744.555105 522.288495 \nL 745.439516 517.746242 \nL 746.323926 521.986658 \nL 747.208337 517.815588 \nL 748.092748 520.507631 \nL 748.977158 521.622041 \nL 749.861569 523.704221 \nL 750.74598 521.077017 \nL 752.514801 517.542347 \nL 753.399212 523.7148 \nL 755.168034 520.119225 \nL 756.052444 517.678264 \nL 756.936855 519.795502 \nL 757.821266 520.927316 \nL 758.705677 523.90301 \nL 759.590087 517.601922 \nL 760.474498 523.955616 \nL 761.358909 523.977254 \nL 762.243319 520.470242 \nL 763.12773 522.128555 \nL 764.012141 522.08348 \nL 764.896552 519.38147 \nL 765.780962 518.579763 \nL 766.665373 521.190201 \nL 767.549784 522.296247 \nL 768.434195 520.779453 \nL 769.318605 523.022459 \nL 770.203016 518.621113 \nL 771.087427 522.060825 \nL 771.971837 520.793106 \nL 772.856248 521.103841 \nL 773.740659 520.619265 \nL 774.62507 519.880507 \nL 775.50948 526.242291 \nL 776.393891 517.742125 \nL 777.278302 523.682426 \nL 778.162713 518.042868 \nL 779.047123 519.990786 \nL 779.931534 520.349411 \nL 779.931534 520.349411 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p8a45626e43)\" d=\"M 69.749716 100.271671 \nL 75.05618 286.696179 \nL 77.709413 379.787263 \nL 78.593823 406.326947 \nL 79.478234 428.123922 \nL 80.362645 446.109298 \nL 81.247055 459.753893 \nL 82.131466 470.263701 \nL 83.015877 478.328248 \nL 83.900288 484.126639 \nL 84.784698 488.17749 \nL 85.669109 491.230203 \nL 86.55352 492.091759 \nL 87.437931 492.254969 \nL 88.322341 492.28621 \nL 90.091163 491.401191 \nL 91.859984 490.416093 \nL 94.513216 488.740427 \nL 96.282038 488.140921 \nL 97.166449 488.110502 \nL 98.93527 487.712273 \nL 102.472913 487.812912 \nL 103.357324 487.615308 \nL 104.241734 487.552411 \nL 105.126145 487.65387 \nL 106.010556 488.260723 \nL 106.894967 487.766247 \nL 107.779377 487.630056 \nL 108.663788 487.932101 \nL 109.548199 487.936948 \nL 111.31702 488.602178 \nL 113.085842 488.803572 \nL 113.970253 489.128194 \nL 114.854663 489.213864 \nL 115.739074 489.463526 \nL 116.623485 489.414789 \nL 117.507895 489.767877 \nL 118.392306 489.532924 \nL 121.045538 489.489034 \nL 121.929949 489.562835 \nL 122.81436 489.477556 \nL 123.698771 489.811129 \nL 124.583181 489.863292 \nL 125.467592 489.676071 \nL 126.352003 490.504916 \nL 127.236414 490.796774 \nL 128.120824 491.427637 \nL 129.005235 491.587434 \nL 129.889646 491.557496 \nL 130.774056 491.083668 \nL 131.658467 491.33062 \nL 132.542878 491.373012 \nL 133.427289 490.852389 \nL 134.311699 491.337186 \nL 136.080521 490.868517 \nL 136.964932 491.745758 \nL 137.849342 492.067202 \nL 138.733753 492.172218 \nL 139.618164 492.075592 \nL 141.386985 492.238568 \nL 142.271396 492.754683 \nL 144.040217 493.220682 \nL 144.924628 493.686446 \nL 145.809039 493.773353 \nL 146.69345 494.466358 \nL 147.57786 493.784883 \nL 148.462271 493.311888 \nL 151.115503 494.50251 \nL 151.999914 494.626832 \nL 152.884325 494.934063 \nL 153.768735 494.662697 \nL 154.653146 494.98319 \nL 155.537557 494.856171 \nL 156.421968 495.100217 \nL 157.306378 495.556667 \nL 158.190789 495.052497 \nL 159.0752 495.380403 \nL 159.959611 495.312763 \nL 160.844021 495.938194 \nL 162.612843 495.620828 \nL 163.497254 495.706328 \nL 165.266075 497.234313 \nL 166.150486 496.128072 \nL 167.034896 496.296258 \nL 167.919307 495.82643 \nL 168.803718 496.860628 \nL 169.688129 497.17152 \nL 171.45695 498.533168 \nL 173.225772 497.618055 \nL 174.110182 497.676041 \nL 175.879004 497.546898 \nL 176.763415 498.705914 \nL 177.647825 499.53201 \nL 179.416647 499.190125 \nL 180.301057 498.697616 \nL 181.185468 499.355941 \nL 182.069879 499.594308 \nL 182.95429 499.673633 \nL 183.8387 500.260775 \nL 184.723111 500.304235 \nL 185.607522 500.134655 \nL 186.491933 501.239619 \nL 187.376343 501.161024 \nL 188.260754 501.571668 \nL 190.029576 501.875511 \nL 190.913986 501.628117 \nL 191.798397 501.80214 \nL 192.682808 502.402453 \nL 193.567218 502.539621 \nL 194.451629 503.14026 \nL 195.33604 503.583486 \nL 197.104861 503.482717 \nL 198.873683 503.745354 \nL 199.758094 503.757783 \nL 201.526915 504.230686 \nL 202.411326 504.370433 \nL 203.295737 504.357458 \nL 204.180147 504.945303 \nL 205.948969 505.63001 \nL 206.833379 505.699551 \nL 208.602201 505.140027 \nL 209.486612 505.385585 \nL 211.255433 505.462618 \nL 212.139844 506.384844 \nL 213.024255 506.649448 \nL 213.908665 507.102679 \nL 214.793076 507.053969 \nL 215.677487 507.751351 \nL 216.561898 507.920255 \nL 217.446308 508.45926 \nL 218.330719 508.467011 \nL 219.21513 509.036605 \nL 220.09954 509.330717 \nL 221.868362 508.349124 \nL 222.752773 509.330483 \nL 223.637183 509.396689 \nL 224.521594 510.07169 \nL 225.406005 510.201953 \nL 226.290416 510.091036 \nL 227.174826 510.109457 \nL 228.059237 510.451458 \nL 228.943648 511.182504 \nL 229.828058 510.676315 \nL 230.712469 511.035174 \nL 231.59688 511.20895 \nL 233.365701 511.896053 \nL 234.250112 511.435644 \nL 235.134523 512.041038 \nL 236.903344 512.82749 \nL 237.787755 513.215609 \nL 238.672166 513.860725 \nL 239.556577 513.818124 \nL 240.440987 513.506673 \nL 242.209809 513.51501 \nL 243.094219 514.102791 \nL 243.97863 513.896577 \nL 244.863041 514.635204 \nL 246.631862 515.430424 \nL 249.285095 515.693191 \nL 251.053916 516.370224 \nL 251.938327 517.128381 \nL 252.822738 518.066461 \nL 253.707148 517.715875 \nL 254.591559 517.79748 \nL 255.47597 517.211471 \nL 256.36038 517.36121 \nL 258.129202 518.48104 \nL 259.013613 518.740303 \nL 259.898023 518.844589 \nL 260.782434 518.27932 \nL 261.666845 518.798901 \nL 263.435666 519.400517 \nL 264.320077 519.130819 \nL 265.204488 519.688284 \nL 266.088899 519.87996 \nL 266.973309 519.815343 \nL 267.85772 520.083712 \nL 268.742131 520.137099 \nL 269.626541 519.584545 \nL 270.510952 520.297938 \nL 272.279774 521.169956 \nL 273.164184 521.403476 \nL 274.048595 522.140397 \nL 274.933006 521.877395 \nL 275.817417 522.197107 \nL 276.701827 521.864576 \nL 277.586238 522.410512 \nL 278.470649 521.982346 \nL 279.35506 522.628126 \nL 280.23947 522.919632 \nL 281.123881 522.617261 \nL 283.777113 524.249245 \nL 284.661524 523.622095 \nL 285.545935 523.514956 \nL 286.430345 523.939188 \nL 287.314756 524.557336 \nL 288.199167 524.680238 \nL 289.083578 525.29229 \nL 289.967988 524.814332 \nL 290.852399 524.818071 \nL 291.73681 525.053102 \nL 292.62122 524.275535 \nL 293.505631 524.107687 \nL 294.390042 524.187846 \nL 295.274453 525.276982 \nL 296.158863 525.483366 \nL 297.927685 526.300811 \nL 298.812096 525.580865 \nL 299.696506 525.626318 \nL 301.465328 526.302114 \nL 302.349739 527.008446 \nL 303.234149 526.19289 \nL 304.11856 526.488774 \nL 305.002971 526.646212 \nL 306.771792 527.383837 \nL 307.656203 527.512824 \nL 309.425024 527.31268 \nL 310.309435 527.668934 \nL 311.193846 527.896735 \nL 312.078257 528.331558 \nL 312.962667 528.575253 \nL 313.847078 528.368974 \nL 314.731489 528.760701 \nL 316.50031 528.94473 \nL 318.269132 529.229605 \nL 319.153542 528.420341 \nL 320.037953 528.344729 \nL 321.806775 529.08139 \nL 323.575596 529.543805 \nL 324.460007 528.972752 \nL 325.344418 529.658905 \nL 327.99765 530.971412 \nL 328.882061 531.048731 \nL 329.766471 530.93401 \nL 330.650882 530.666553 \nL 331.535293 530.947559 \nL 332.419703 530.602092 \nL 333.304114 530.089624 \nL 334.188525 529.947219 \nL 335.957346 531.225256 \nL 336.841757 531.804582 \nL 337.726168 530.98173 \nL 338.610579 531.272937 \nL 339.494989 531.444355 \nL 340.3794 531.476116 \nL 341.263811 531.733803 \nL 342.148222 532.374632 \nL 343.032632 532.152888 \nL 343.917043 532.31404 \nL 344.801454 532.334702 \nL 345.685864 532.60014 \nL 346.570275 533.262895 \nL 347.454686 533.167207 \nL 348.339097 531.494055 \nL 349.223507 532.382253 \nL 350.107918 532.727394 \nL 350.992329 532.516047 \nL 351.87674 532.702902 \nL 352.76115 533.211761 \nL 353.645561 532.807553 \nL 355.414383 533.156277 \nL 356.298793 533.339172 \nL 357.183204 533.064836 \nL 358.067615 533.925011 \nL 359.836436 533.874477 \nL 360.720847 534.313443 \nL 361.605258 534.307034 \nL 362.489668 534.010472 \nL 364.25849 533.770842 \nL 365.142901 533.662491 \nL 366.027311 533.958531 \nL 366.911722 534.529337 \nL 367.796133 534.718928 \nL 368.680543 534.457151 \nL 369.564954 534.792052 \nL 370.449365 535.568851 \nL 371.333776 534.915763 \nL 372.218186 534.894202 \nL 373.102597 534.626432 \nL 373.987008 535.011594 \nL 374.871419 534.782229 \nL 375.755829 535.432699 \nL 376.64024 535.677019 \nL 377.524651 535.579859 \nL 378.409062 535.300833 \nL 379.293472 535.19643 \nL 380.177883 535.876186 \nL 381.062294 535.476524 \nL 381.946704 535.279572 \nL 382.831115 536.210618 \nL 383.715526 535.62612 \nL 384.599937 535.655354 \nL 385.484347 536.333065 \nL 386.368758 535.74161 \nL 387.253169 536.32745 \nL 388.13758 536.242197 \nL 389.02199 535.871066 \nL 389.906401 536.129521 \nL 390.790812 536.53085 \nL 391.675223 535.787715 \nL 392.559633 536.4975 \nL 393.444044 536.033208 \nL 394.328455 536.162598 \nL 395.212865 536.768904 \nL 396.097276 536.392692 \nL 399.634919 536.857883 \nL 401.403741 536.367002 \nL 402.288151 536.624337 \nL 403.172562 536.228648 \nL 404.056973 536.582648 \nL 404.941384 537.421706 \nL 405.825794 536.961804 \nL 407.594616 536.750327 \nL 408.479026 537.29014 \nL 409.363437 536.757492 \nL 410.247848 536.983443 \nL 411.132259 537.507128 \nL 412.016669 536.438667 \nL 412.90108 537.318683 \nL 414.669902 537.558691 \nL 415.554312 537.040413 \nL 416.438723 537.641469 \nL 417.323134 537.415896 \nL 418.207545 537.745182 \nL 419.091955 537.494817 \nL 420.860777 538.311194 \nL 421.745187 537.547227 \nL 422.629598 537.9768 \nL 423.514009 537.896016 \nL 424.39842 538.342447 \nL 425.28283 538.251058 \nL 427.051652 538.847007 \nL 427.936063 539.034787 \nL 428.820473 538.43381 \nL 429.704884 538.153559 \nL 431.473705 537.846837 \nL 432.358116 537.114931 \nL 433.242527 537.961519 \nL 435.011348 537.183756 \nL 435.895759 537.985894 \nL 436.78017 538.185138 \nL 437.664581 537.667016 \nL 438.548991 538.046094 \nL 439.433402 537.609916 \nL 440.317813 537.908184 \nL 441.202224 538.551176 \nL 442.086634 538.462874 \nL 442.971045 538.22684 \nL 443.855456 537.721016 \nL 445.624277 538.335412 \nL 447.393099 537.883601 \nL 449.16192 538.270912 \nL 450.046331 538.423935 \nL 451.815152 538.997229 \nL 453.583974 538.337535 \nL 456.237206 538.737679 \nL 457.121617 538.189502 \nL 458.006027 537.963642 \nL 458.890438 538.589177 \nL 459.774849 538.591301 \nL 460.65926 538.95429 \nL 461.54367 538.913461 \nL 462.428081 539.651346 \nL 463.312492 539.427088 \nL 464.196903 539.457834 \nL 465.081313 539.255944 \nL 465.965724 539.350173 \nL 466.850135 538.937093 \nL 467.734546 539.166314 \nL 468.618956 538.796799 \nL 469.503367 538.691197 \nL 470.387778 539.197659 \nL 471.272188 539.343477 \nL 472.156599 539.001163 \nL 473.925421 539.120705 \nL 474.809831 538.939803 \nL 475.694242 538.987367 \nL 476.578653 539.196656 \nL 477.463064 538.819102 \nL 480.116296 539.392279 \nL 481.000707 540.050448 \nL 481.885117 539.668999 \nL 482.769528 539.618999 \nL 483.653939 539.704525 \nL 484.538349 538.927987 \nL 486.307171 539.274183 \nL 487.191582 539.022971 \nL 488.075992 539.445523 \nL 488.960403 539.404121 \nL 489.844814 540.031297 \nL 491.613635 540.128353 \nL 492.498046 539.851959 \nL 494.266867 539.997373 \nL 495.151278 539.489126 \nL 496.035689 539.651828 \nL 496.9201 539.224483 \nL 497.80451 539.481466 \nL 498.688921 540.110544 \nL 499.573332 540.198624 \nL 500.457743 540.039283 \nL 501.342153 540.130711 \nL 502.226564 540.025031 \nL 503.110975 540.314179 \nL 503.995386 540.021917 \nL 504.879796 540.176269 \nL 505.764207 540.030854 \nL 506.648618 540.38699 \nL 507.533028 539.767253 \nL 509.30185 540.241915 \nL 510.186261 540.145758 \nL 511.070671 540.189036 \nL 512.839493 539.424783 \nL 513.723904 539.815117 \nL 514.608314 539.694871 \nL 515.492725 539.327088 \nL 516.377136 539.781349 \nL 517.261547 539.796735 \nL 518.145957 540.051216 \nL 519.030368 539.701555 \nL 519.914779 539.228886 \nL 520.799189 539.227153 \nL 521.6836 539.690872 \nL 522.568011 540.028939 \nL 523.452422 540.025239 \nL 524.336832 539.330984 \nL 526.105654 540.045875 \nL 526.990065 539.446995 \nL 527.874475 539.952428 \nL 528.758886 540.135479 \nL 530.527708 539.752037 \nL 531.412118 539.634137 \nL 532.296529 540.218947 \nL 533.18094 540.043778 \nL 534.06535 540.208408 \nL 534.949761 540.223572 \nL 535.834172 540.467058 \nL 536.718583 540.098741 \nL 537.602993 540.356089 \nL 538.487404 539.917891 \nL 539.371815 540.246097 \nL 540.256226 540.16539 \nL 543.793869 540.52864 \nL 544.678279 540.168595 \nL 545.56269 540.275161 \nL 546.447101 540.245276 \nL 547.331511 540.550474 \nL 548.215922 540.475253 \nL 549.100333 539.930646 \nL 549.984744 540.084007 \nL 550.869154 540.38716 \nL 551.753565 539.79495 \nL 552.637976 540.040807 \nL 553.522387 540.168191 \nL 554.406797 539.739113 \nL 555.291208 539.908642 \nL 556.175619 540.370745 \nL 557.06003 540.509138 \nL 557.94444 540.358903 \nL 558.828851 540.597921 \nL 559.713262 540.540717 \nL 560.597672 540.285844 \nL 561.482083 540.282613 \nL 562.366494 540.13247 \nL 563.250905 540.243152 \nL 564.135315 540.613606 \nL 565.019726 539.689426 \nL 566.788548 540.253861 \nL 567.672958 539.742904 \nL 568.557369 540.36018 \nL 569.44178 540.644026 \nL 570.32619 540.108199 \nL 571.210601 540.612199 \nL 572.979423 539.723715 \nL 573.863833 539.479629 \nL 574.748244 539.557925 \nL 575.632655 539.031244 \nL 576.517066 540.322829 \nL 577.401476 540.330425 \nL 578.285887 540.04138 \nL 579.170298 540.082014 \nL 580.054709 540.499132 \nL 580.939119 540.658043 \nL 581.82353 540.421826 \nL 583.592351 540.615795 \nL 584.476762 540.861092 \nL 585.361173 540.975331 \nL 586.245584 540.477715 \nL 587.129994 540.238124 \nL 588.014405 540.763398 \nL 588.898816 540.060284 \nL 589.783227 540.21513 \nL 590.667637 539.841237 \nL 591.552048 540.662525 \nL 592.436459 540.703627 \nL 593.32087 540.288228 \nL 594.20528 540.414791 \nL 595.089691 540.148233 \nL 595.974102 540.90549 \nL 596.858512 540.797478 \nL 597.742923 540.187342 \nL 599.511745 540.600292 \nL 600.396155 540.444078 \nL 601.280566 540.467592 \nL 602.164977 540.686431 \nL 603.049388 540.701204 \nL 603.933798 540.117384 \nL 604.818209 540.263124 \nL 605.70262 540.091693 \nL 606.587031 540.336925 \nL 607.471441 540.345914 \nL 608.355852 540.180424 \nL 609.240263 539.736846 \nL 611.009084 540.341785 \nL 611.893495 539.96595 \nL 612.777906 540.382105 \nL 613.662316 540.628953 \nL 615.431138 540.415703 \nL 616.315549 540.928719 \nL 617.199959 540.536443 \nL 618.08437 540.355216 \nL 618.968781 540.422426 \nL 619.853192 540.754044 \nL 620.737602 540.122399 \nL 621.622013 540.492488 \nL 622.506424 540.701048 \nL 623.390834 540.031375 \nL 624.275245 540.668622 \nL 625.159656 540.55377 \nL 626.044067 540.720889 \nL 628.697299 540.488736 \nL 629.58171 540.271709 \nL 630.46612 540.564661 \nL 631.350531 540.5897 \nL 632.234942 540.826842 \nL 634.003763 539.942175 \nL 634.888174 540.286613 \nL 635.772585 540.188645 \nL 636.656995 540.64159 \nL 637.541406 540.683369 \nL 639.310228 540.311456 \nL 640.194638 540.502741 \nL 641.079049 539.640129 \nL 641.96346 540.116902 \nL 642.847871 540.39172 \nL 643.732281 540.920198 \nL 644.616692 541.006441 \nL 645.501103 540.677142 \nL 646.385513 540.994026 \nL 647.269924 540.266863 \nL 649.038746 540.118283 \nL 649.923156 540.647061 \nL 650.807567 540.542241 \nL 651.691978 540.632144 \nL 652.576389 540.320224 \nL 653.460799 540.427858 \nL 654.34521 540.74073 \nL 655.229621 540.900774 \nL 656.114032 540.420914 \nL 658.767264 540.860389 \nL 659.651674 540.617449 \nL 660.536085 540.793335 \nL 661.420496 540.766069 \nL 662.304907 540.476191 \nL 663.189317 540.579839 \nL 664.073728 540.476269 \nL 664.958139 540.555294 \nL 666.72696 540.50179 \nL 667.611371 540.886991 \nL 668.495782 540.704174 \nL 669.380193 541.157901 \nL 670.264603 540.86345 \nL 671.149014 540.875019 \nL 672.033425 540.737707 \nL 672.917835 540.357379 \nL 673.802246 539.709593 \nL 674.686657 540.180411 \nL 675.571068 540.23557 \nL 676.455478 540.668635 \nL 677.339889 540.615665 \nL 678.2243 540.793739 \nL 679.108711 540.562694 \nL 683.530764 540.536626 \nL 684.415175 540.267579 \nL 685.299586 540.623481 \nL 687.068407 540.74727 \nL 687.952818 540.06582 \nL 688.837229 540.755516 \nL 689.721639 540.762551 \nL 690.60605 540.900709 \nL 691.490461 541.209034 \nL 693.259282 540.560597 \nL 695.028104 540.88535 \nL 696.796925 540.909099 \nL 697.681336 541.178146 \nL 699.450157 540.159072 \nL 700.334568 540.355802 \nL 701.218979 540.822882 \nL 702.10339 541.068231 \nL 702.9878 540.788815 \nL 703.872211 540.960584 \nL 704.756622 540.305021 \nL 705.641033 540.900175 \nL 707.409854 540.574237 \nL 708.294265 540.728145 \nL 710.063086 540.465716 \nL 710.947497 540.232704 \nL 711.831908 539.677923 \nL 714.48514 539.748741 \nL 715.369551 540.539805 \nL 716.253961 540.666824 \nL 717.138372 540.433473 \nL 718.022783 540.549562 \nL 719.791604 540.356297 \nL 720.676015 540.838685 \nL 721.560426 540.765313 \nL 722.444836 540.096969 \nL 723.329247 540.406258 \nL 724.213658 540.54099 \nL 725.098069 540.911691 \nL 725.982479 540.677142 \nL 726.86689 540.134749 \nL 727.751301 540.550657 \nL 728.635712 540.45618 \nL 729.520122 540.897895 \nL 730.404533 540.878289 \nL 731.288944 540.175565 \nL 732.173355 539.786873 \nL 735.710997 540.920459 \nL 736.595408 540.357887 \nL 737.479819 540.191615 \nL 738.36423 540.193869 \nL 739.24864 540.76698 \nL 740.133051 540.527689 \nL 742.786283 540.709437 \nL 743.670694 540.937773 \nL 744.555105 540.586886 \nL 745.439516 540.647009 \nL 746.323926 540.996736 \nL 748.092748 540.309828 \nL 750.74598 540.692632 \nL 751.630391 540.508942 \nL 752.514801 540.0718 \nL 753.399212 540.116185 \nL 754.283623 539.936221 \nL 755.168034 540.184437 \nL 756.052444 540.223259 \nL 756.936855 539.958733 \nL 757.821266 540.897465 \nL 758.705677 540.387316 \nL 759.590087 540.39469 \nL 761.358909 540.013384 \nL 762.243319 540.659932 \nL 765.780962 540.858539 \nL 767.549784 540.670328 \nL 768.434195 540.339049 \nL 769.318605 540.454096 \nL 770.203016 540.792502 \nL 771.087427 540.566524 \nL 771.971837 541.086301 \nL 772.856248 540.684203 \nL 773.740659 540.926934 \nL 774.62507 540.846553 \nL 775.50948 540.886092 \nL 776.393891 540.495276 \nL 778.162713 541.061327 \nL 779.931534 540.532157 \nL 779.931534 540.532157 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 565.918125 \nL 34.240625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 815.440625 565.918125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 22.318125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Model MAE -->\n    <g transform=\"translate(391.845313 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"486.71875\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 41.240625 59.674375 \nL 96.515625 59.674375 \nQ 98.515625 59.674375 98.515625 57.674375 \nL 98.515625 29.318125 \nQ 98.515625 27.318125 96.515625 27.318125 \nL 41.240625 27.318125 \nQ 39.240625 27.318125 39.240625 29.318125 \nL 39.240625 57.674375 \nQ 39.240625 59.674375 41.240625 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 43.240625 35.416562 \nL 63.240625 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_18\">\n     <!-- train -->\n     <g transform=\"translate(71.240625 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 43.240625 50.094687 \nL 63.240625 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_19\">\n     <!-- test -->\n     <g transform=\"translate(71.240625 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8a45626e43\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"34.240625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-13T20:17:01.741119</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m00ffa6d4f5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.553289\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(155.009539 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.994362\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(243.450612 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"341.435436\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(331.891686 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"429.876509\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(420.332759 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"518.317582\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(508.773832 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"606.758655\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(597.214905 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"695.199729\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(685.655979 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"783.640802\" xlink:href=\"#m00ffa6d4f5\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 800 -->\n      <g transform=\"translate(774.097052 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m24a4c9c9f1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m24a4c9c9f1\" y=\"523.355526\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 527.154745)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m24a4c9c9f1\" y=\"460.019815\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 463.819033)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m24a4c9c9f1\" y=\"396.684103\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 400.483322)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m24a4c9c9f1\" y=\"333.348392\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 337.14761)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m24a4c9c9f1\" y=\"270.01268\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 273.811899)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m24a4c9c9f1\" y=\"206.676969\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 210.476187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m24a4c9c9f1\" y=\"143.341257\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 147.140476)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m24a4c9c9f1\" y=\"80.005546\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 83.804764)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- MSE -->\n     <g transform=\"translate(14.798438 304.765781)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p49ef4b9726)\" d=\"M 76.112216 47.027216 \nL 77.881037 129.551756 \nL 80.53427 235.385313 \nL 81.41868 270.590216 \nL 84.071913 359.697617 \nL 85.840734 405.606187 \nL 86.725145 425.638149 \nL 87.609555 442.861085 \nL 88.493966 453.551681 \nL 89.378377 466.304098 \nL 91.147198 483.841872 \nL 92.031609 488.214588 \nL 92.91602 493.762702 \nL 93.800431 494.943584 \nL 94.684841 498.80421 \nL 95.569252 499.752896 \nL 96.453663 497.328145 \nL 97.338073 502.870478 \nL 98.222484 501.370682 \nL 99.106895 503.896642 \nL 99.991306 502.458116 \nL 100.875716 504.322256 \nL 101.760127 503.240414 \nL 102.644538 503.174347 \nL 103.528949 505.666174 \nL 104.413359 506.460245 \nL 105.29777 502.776125 \nL 106.182181 503.947681 \nL 107.066592 503.473124 \nL 107.951002 507.13124 \nL 108.835413 502.936914 \nL 110.604234 505.392549 \nL 111.488645 503.979042 \nL 112.373056 506.868252 \nL 113.257467 504.942339 \nL 114.141877 502.039569 \nL 115.026288 506.860744 \nL 115.910699 508.121211 \nL 116.79511 505.095385 \nL 117.67952 504.246229 \nL 118.563931 505.768966 \nL 119.448342 504.768624 \nL 120.332753 504.47178 \nL 121.217163 505.651618 \nL 122.101574 502.40474 \nL 122.985985 505.408918 \nL 123.870395 504.293517 \nL 124.754806 504.560148 \nL 125.639217 506.612155 \nL 126.523628 506.524065 \nL 127.408038 507.792753 \nL 128.292449 504.240146 \nL 129.17686 504.594208 \nL 130.061271 507.2909 \nL 130.945681 505.901874 \nL 131.830092 507.397594 \nL 132.714503 503.463393 \nL 133.598914 504.725371 \nL 134.483324 505.264099 \nL 135.367735 506.928702 \nL 136.252146 506.961512 \nL 137.136556 505.906519 \nL 138.020967 505.518807 \nL 138.905378 507.097832 \nL 139.789789 508.911881 \nL 140.674199 509.275082 \nL 141.55861 506.09714 \nL 142.443021 507.626184 \nL 143.327432 506.465258 \nL 144.211842 506.661177 \nL 145.096253 508.410765 \nL 145.980664 509.052218 \nL 146.865075 508.09057 \nL 147.749485 505.250164 \nL 148.633896 505.334201 \nL 149.518307 507.657206 \nL 150.402717 505.403886 \nL 151.287128 509.485793 \nL 152.171539 507.021575 \nL 153.05595 508.324964 \nL 153.94036 507.870267 \nL 154.824771 507.590075 \nL 155.709182 508.359194 \nL 156.593593 508.324094 \nL 157.478003 509.035837 \nL 158.362414 507.766629 \nL 159.246825 508.685405 \nL 160.131235 508.06325 \nL 161.015646 509.982633 \nL 161.900057 507.096292 \nL 162.784468 507.708318 \nL 163.668878 507.54202 \nL 164.553289 511.253888 \nL 165.4377 507.780782 \nL 166.322111 506.829854 \nL 167.206521 508.000529 \nL 168.090932 510.587664 \nL 168.975343 511.32428 \nL 170.744164 509.961341 \nL 171.628575 507.247598 \nL 172.512986 510.005404 \nL 173.397396 507.804483 \nL 174.281807 511.2476 \nL 175.166218 505.78136 \nL 176.050629 508.258359 \nL 176.935039 510.037792 \nL 177.81945 508.907888 \nL 178.703861 508.905647 \nL 179.588272 510.830001 \nL 180.472682 510.106915 \nL 181.357093 509.720635 \nL 182.241504 509.512491 \nL 183.125915 509.94284 \nL 184.010325 510.203183 \nL 184.894736 509.699917 \nL 185.779147 510.958681 \nL 186.663557 513.017145 \nL 187.547968 512.448125 \nL 188.432379 511.543079 \nL 189.31679 511.914724 \nL 190.2012 511.713092 \nL 191.085611 511.116281 \nL 191.970022 510.691579 \nL 192.854433 513.11659 \nL 193.738843 509.244102 \nL 194.623254 509.551293 \nL 195.507665 514.567625 \nL 196.392076 511.899364 \nL 197.276486 509.635499 \nL 198.160897 512.269511 \nL 199.045308 513.247191 \nL 199.929718 510.892511 \nL 200.814129 512.74056 \nL 201.69854 510.576194 \nL 203.467361 510.117135 \nL 204.351772 511.367449 \nL 205.236183 512.201397 \nL 206.120594 514.993807 \nL 207.005004 510.358923 \nL 207.889415 514.153185 \nL 208.773826 509.663307 \nL 209.658237 512.060032 \nL 210.542647 511.641558 \nL 211.427058 513.170903 \nL 212.311469 512.356115 \nL 213.195879 511.295444 \nL 214.08029 514.076559 \nL 214.964701 511.160561 \nL 215.849112 511.37564 \nL 216.733522 513.310751 \nL 217.617933 513.529049 \nL 218.502344 511.895341 \nL 219.386755 517.800961 \nL 220.271165 514.924925 \nL 221.155576 513.548365 \nL 222.039987 513.9223 \nL 223.808808 513.776055 \nL 224.693219 513.420688 \nL 225.57763 513.620902 \nL 226.46204 514.037576 \nL 227.346451 515.679734 \nL 228.230862 515.298811 \nL 229.115273 515.171581 \nL 229.999683 516.908345 \nL 230.884094 513.221 \nL 231.768505 514.404818 \nL 233.537326 513.789573 \nL 234.421737 515.169871 \nL 235.306148 514.294102 \nL 237.95938 516.806834 \nL 238.843791 516.961052 \nL 240.612612 515.228781 \nL 241.497023 514.953513 \nL 242.381434 514.871499 \nL 243.265844 514.914922 \nL 244.150255 518.459949 \nL 245.034666 513.831431 \nL 245.919077 517.455331 \nL 247.687898 517.850726 \nL 248.572309 516.434096 \nL 249.456719 516.917243 \nL 250.34113 516.610934 \nL 251.225541 517.440478 \nL 252.109952 517.475148 \nL 252.994362 516.921912 \nL 253.878773 515.032681 \nL 254.763184 517.309757 \nL 255.647595 518.499965 \nL 256.532005 517.354454 \nL 257.416416 517.113711 \nL 258.300827 518.364357 \nL 259.185238 516.624748 \nL 260.069648 519.831966 \nL 260.954059 517.410682 \nL 261.83847 516.259203 \nL 262.72288 516.889929 \nL 263.607291 518.449294 \nL 264.491702 517.4562 \nL 265.376113 517.763615 \nL 266.260523 517.413164 \nL 267.144934 516.251967 \nL 268.029345 517.953064 \nL 268.913756 519.175092 \nL 269.798166 515.249777 \nL 270.682577 517.748484 \nL 271.566988 516.467353 \nL 272.451399 519.350136 \nL 273.335809 520.496517 \nL 274.22022 518.904589 \nL 275.104631 520.327247 \nL 275.989041 518.238384 \nL 276.873452 516.747955 \nL 277.757863 520.868676 \nL 278.642274 518.265842 \nL 280.411095 517.030097 \nL 281.295506 520.789888 \nL 282.179917 516.661943 \nL 283.064327 519.607834 \nL 283.948738 516.880102 \nL 284.833149 517.360832 \nL 285.71756 516.537425 \nL 286.60197 519.703782 \nL 287.486381 522.378771 \nL 289.255202 516.56138 \nL 291.908435 521.644529 \nL 292.792845 519.798806 \nL 293.677256 519.763573 \nL 294.561667 517.681674 \nL 295.446078 519.626196 \nL 296.330488 520.025837 \nL 297.214899 519.609398 \nL 298.09931 518.929771 \nL 298.98372 522.200882 \nL 299.868131 519.739135 \nL 300.752542 518.882362 \nL 301.636953 518.971533 \nL 302.521363 520.776376 \nL 303.405774 520.17248 \nL 304.290185 520.555856 \nL 305.174596 518.19391 \nL 306.059006 520.908202 \nL 306.943417 521.338818 \nL 307.827828 520.139622 \nL 308.712239 520.469554 \nL 309.596649 521.012178 \nL 310.48106 518.186311 \nL 311.365471 520.877035 \nL 312.249881 520.850084 \nL 313.134292 520.324813 \nL 314.018703 522.324337 \nL 314.903114 518.519728 \nL 315.787524 521.368336 \nL 316.671935 522.335312 \nL 317.556346 522.482348 \nL 318.440757 520.381361 \nL 319.325167 523.72879 \nL 320.209578 516.997281 \nL 321.9784 523.128857 \nL 322.86281 518.757312 \nL 323.747221 522.363121 \nL 324.631632 520.705344 \nL 325.516042 520.027909 \nL 326.400453 520.61006 \nL 327.284864 521.907735 \nL 328.169275 522.546778 \nL 329.053685 519.27406 \nL 329.938096 521.920558 \nL 330.822507 517.998414 \nL 331.706918 519.899827 \nL 332.591328 521.032418 \nL 333.475739 518.766409 \nL 334.36015 521.838672 \nL 335.244561 519.845635 \nL 336.128971 520.993061 \nL 337.013382 520.436707 \nL 337.897793 520.919727 \nL 338.782203 523.168583 \nL 339.666614 524.192373 \nL 340.551025 522.78978 \nL 341.435436 519.463166 \nL 342.319846 521.03632 \nL 343.204257 520.478209 \nL 344.088668 520.436514 \nL 344.973079 524.297683 \nL 346.7419 523.27794 \nL 347.626311 521.825957 \nL 348.510722 521.756616 \nL 349.395132 521.94554 \nL 350.279543 520.908021 \nL 351.163954 522.530289 \nL 352.048364 519.400674 \nL 352.932775 521.662371 \nL 353.817186 520.287479 \nL 354.701597 520.812544 \nL 355.586007 523.173143 \nL 356.470418 522.430976 \nL 357.354829 523.119507 \nL 358.23924 520.795868 \nL 359.12365 520.956071 \nL 360.008061 520.913983 \nL 360.892472 522.919927 \nL 361.776883 520.442904 \nL 362.661293 522.534269 \nL 363.545704 520.779994 \nL 365.314525 522.681945 \nL 366.198936 522.745276 \nL 367.083347 522.498892 \nL 367.967758 522.586275 \nL 368.852168 521.924816 \nL 369.736579 519.650393 \nL 370.62099 522.991358 \nL 371.505401 519.7549 \nL 372.389811 520.869292 \nL 373.274222 522.500027 \nL 374.158633 520.954168 \nL 375.043043 520.408687 \nL 375.927454 520.925719 \nL 376.811865 522.949645 \nL 377.696276 524.616217 \nL 378.580686 520.033412 \nL 379.465097 522.18647 \nL 380.349508 523.154099 \nL 381.233919 522.81893 \nL 382.118329 523.445845 \nL 383.00274 523.431874 \nL 383.887151 525.444607 \nL 384.771562 521.598617 \nL 385.655972 523.029871 \nL 386.540383 521.397353 \nL 387.424794 521.506843 \nL 388.309204 522.618221 \nL 389.193615 520.426995 \nL 390.962437 523.190805 \nL 391.846847 520.889792 \nL 392.731258 519.55709 \nL 393.615669 522.049347 \nL 394.50008 523.244387 \nL 395.38449 520.507933 \nL 396.268901 521.192277 \nL 397.153312 519.045808 \nL 398.037723 523.756321 \nL 398.922133 521.816045 \nL 399.806544 522.522612 \nL 400.690955 522.406344 \nL 401.575365 523.344189 \nL 402.459776 522.364758 \nL 403.344187 521.875595 \nL 404.228598 521.988806 \nL 405.113008 521.102726 \nL 406.88183 522.962559 \nL 407.766241 522.77436 \nL 408.650651 520.253636 \nL 409.535062 522.088825 \nL 410.419473 522.774613 \nL 411.303884 521.659092 \nL 412.188294 520.180725 \nL 413.072705 523.952868 \nL 413.957116 520.085641 \nL 414.841526 522.427376 \nL 415.725937 523.005251 \nL 416.610348 522.936006 \nL 417.494759 519.875274 \nL 418.379169 522.254942 \nL 419.26358 520.320772 \nL 420.147991 524.454872 \nL 421.032402 522.940059 \nL 421.916812 525.020358 \nL 422.801223 523.426595 \nL 423.685634 522.30156 \nL 424.570045 521.676107 \nL 425.454455 523.853628 \nL 426.338866 523.130971 \nL 427.223277 522.730931 \nL 428.107687 523.657365 \nL 428.992098 521.756091 \nL 429.876509 524.12912 \nL 430.76092 522.847174 \nL 431.64533 520.944087 \nL 432.529741 521.591097 \nL 433.414152 523.167755 \nL 434.298563 521.655147 \nL 435.182973 517.168858 \nL 436.067384 521.892157 \nL 436.951795 523.914821 \nL 437.836205 522.35763 \nL 439.605027 523.73355 \nL 440.489438 522.392265 \nL 441.373848 521.35965 \nL 442.258259 525.064506 \nL 443.14267 521.109612 \nL 444.027081 521.722634 \nL 444.911491 523.249612 \nL 445.795902 521.49228 \nL 446.680313 521.70153 \nL 447.564724 523.68193 \nL 449.333545 521.051487 \nL 450.217956 524.535557 \nL 451.102366 522.938966 \nL 451.986777 523.512933 \nL 452.871188 522.184997 \nL 453.755599 524.57547 \nL 454.640009 522.827259 \nL 455.52442 523.595061 \nL 456.408831 523.174961 \nL 457.293242 522.536848 \nL 458.177652 519.483328 \nL 459.062063 521.816879 \nL 459.946474 521.484748 \nL 460.830885 523.854148 \nL 461.715295 523.954891 \nL 462.599706 523.14252 \nL 463.484117 527.25865 \nL 464.368527 521.136243 \nL 465.252938 520.794345 \nL 466.137349 524.71408 \nL 467.02176 522.381423 \nL 467.90617 523.078446 \nL 468.790581 521.100817 \nL 469.674992 520.17875 \nL 470.559403 523.269671 \nL 471.443813 522.68357 \nL 472.328224 524.932716 \nL 473.212635 526.263678 \nL 474.097046 523.165684 \nL 474.981456 523.092851 \nL 475.865867 522.359177 \nL 476.750278 522.476561 \nL 477.634688 523.015145 \nL 478.519099 521.743799 \nL 479.40351 523.441351 \nL 480.287921 523.073789 \nL 481.172331 523.324836 \nL 482.056742 523.272395 \nL 482.941153 520.95151 \nL 483.825564 522.960813 \nL 484.709974 523.578299 \nL 485.594385 522.427733 \nL 486.478796 521.608626 \nL 487.363207 522.600699 \nL 488.247617 524.801976 \nL 489.132028 521.122652 \nL 490.016439 521.741842 \nL 490.900849 523.45569 \nL 491.78526 523.68158 \nL 492.669671 523.157614 \nL 493.554082 521.746686 \nL 494.438492 523.50711 \nL 495.322903 521.635988 \nL 497.091725 523.73123 \nL 497.976135 523.950839 \nL 499.744957 522.421529 \nL 500.629367 523.540313 \nL 501.513778 523.230017 \nL 502.398189 525.372717 \nL 503.2826 523.215696 \nL 504.16701 524.560551 \nL 505.051421 524.070494 \nL 505.935832 522.698537 \nL 506.820243 520.24945 \nL 507.704653 522.633279 \nL 508.589064 521.858725 \nL 509.473475 523.702056 \nL 510.357886 523.904571 \nL 512.126707 522.434709 \nL 513.011118 522.530838 \nL 513.895528 524.115874 \nL 514.779939 523.947293 \nL 515.66435 521.085149 \nL 516.548761 524.598199 \nL 517.433171 519.481359 \nL 518.317582 522.731414 \nL 519.201993 524.167499 \nL 520.086404 522.813373 \nL 520.970814 520.626036 \nL 521.855225 525.838964 \nL 522.739636 520.12939 \nL 523.624047 524.692963 \nL 524.508457 523.916192 \nL 525.392868 524.644938 \nL 526.277279 525.681031 \nL 528.0461 520.938343 \nL 528.930511 524.864021 \nL 529.814922 522.520969 \nL 530.699332 521.86061 \nL 531.583743 523.822539 \nL 532.468154 524.749668 \nL 533.352565 523.96217 \nL 534.236975 520.310951 \nL 535.121386 521.359095 \nL 536.005797 523.970922 \nL 536.890208 520.51259 \nL 537.774618 524.354213 \nL 539.54344 523.922661 \nL 540.42785 522.612586 \nL 541.312261 521.730637 \nL 542.196672 519.22534 \nL 543.081083 522.538636 \nL 543.965493 524.12555 \nL 545.734315 521.190592 \nL 546.618726 520.99863 \nL 547.503136 524.564912 \nL 548.387547 522.168555 \nL 549.271958 522.159694 \nL 550.156369 523.291096 \nL 551.040779 521.302921 \nL 551.92519 522.723387 \nL 552.809601 522.555138 \nL 553.694011 522.004891 \nL 554.578422 522.239569 \nL 555.462833 520.090727 \nL 556.347244 522.033316 \nL 557.231654 524.642751 \nL 558.116065 525.017338 \nL 559.000476 521.575755 \nL 559.884887 522.286459 \nL 560.769297 524.599788 \nL 561.653708 521.468603 \nL 562.538119 522.642883 \nL 563.42253 521.095858 \nL 564.30694 523.055487 \nL 565.191351 524.479661 \nL 566.075762 523.099242 \nL 566.960172 523.768655 \nL 567.844583 523.326624 \nL 568.728994 524.744752 \nL 569.613405 523.434785 \nL 570.497815 521.90965 \nL 571.382226 524.369905 \nL 572.266637 522.154252 \nL 573.151048 522.620734 \nL 574.035458 523.445071 \nL 574.919869 522.091942 \nL 575.80428 522.481357 \nL 576.68869 521.013597 \nL 577.573101 521.785814 \nL 578.457512 522.366703 \nL 579.341923 522.005561 \nL 580.226333 525.149799 \nL 581.110744 520.682506 \nL 581.995155 522.506913 \nL 582.879566 523.076446 \nL 583.763976 521.940672 \nL 584.648387 522.805466 \nL 585.532798 525.20195 \nL 586.417209 521.279346 \nL 587.301619 523.958715 \nL 588.18603 521.242272 \nL 589.070441 523.700099 \nL 589.954851 520.631805 \nL 590.839262 521.457809 \nL 591.723673 522.950684 \nL 592.608084 521.014171 \nL 593.492494 523.572893 \nL 594.376905 522.303305 \nL 595.261316 522.286109 \nL 596.145727 524.014617 \nL 597.030137 520.394172 \nL 597.914548 524.017419 \nL 598.798959 523.940981 \nL 599.68337 525.590308 \nL 600.56778 523.565361 \nL 601.452191 522.382564 \nL 602.336602 524.927267 \nL 603.221012 523.519474 \nL 604.105423 525.872094 \nL 605.874245 523.11767 \nL 606.758655 522.82953 \nL 607.643066 523.666184 \nL 609.411888 523.401123 \nL 610.296298 522.389674 \nL 612.06512 523.389532 \nL 612.949531 522.960487 \nL 613.833941 523.578825 \nL 614.718352 522.228516 \nL 615.602763 523.876387 \nL 616.487173 521.640409 \nL 617.371584 521.562123 \nL 618.255995 522.03897 \nL 620.024816 524.323958 \nL 620.909227 523.045297 \nL 621.793638 522.866629 \nL 622.678049 524.611657 \nL 623.562459 521.785814 \nL 624.44687 522.484758 \nL 625.331281 522.408301 \nL 626.215692 522.155708 \nL 627.100102 523.868294 \nL 627.984513 521.358291 \nL 628.868924 523.543532 \nL 629.753334 521.307868 \nL 630.637745 521.881877 \nL 631.522156 521.240267 \nL 632.406567 523.697073 \nL 633.290977 522.817341 \nL 634.175388 522.40562 \nL 635.059799 521.712716 \nL 635.94421 522.706933 \nL 636.82862 521.795388 \nL 637.713031 522.524732 \nL 638.597442 522.397526 \nL 639.481852 521.976007 \nL 640.366263 522.709373 \nL 641.250674 521.158628 \nL 642.135085 522.364178 \nL 643.019495 520.970863 \nL 643.903906 524.540256 \nL 644.788317 521.290738 \nL 645.672728 521.92507 \nL 646.557138 524.022131 \nL 647.441549 523.836758 \nL 648.32596 523.028844 \nL 649.210371 525.06218 \nL 650.094781 524.679838 \nL 650.979192 522.571476 \nL 651.863603 522.049377 \nL 652.748013 523.02769 \nL 653.632424 522.388145 \nL 654.516835 523.999323 \nL 655.401246 524.131131 \nL 656.285656 523.442559 \nL 657.170067 519.423186 \nL 658.054478 522.063487 \nL 658.938889 523.407997 \nL 659.823299 520.804209 \nL 660.70771 524.088023 \nL 661.592121 525.044954 \nL 662.476532 523.578674 \nL 663.360942 523.133616 \nL 664.245353 524.069407 \nL 665.129764 523.064088 \nL 666.014174 525.244787 \nL 666.898585 523.370657 \nL 667.782996 523.189077 \nL 668.667407 521.508933 \nL 669.551817 522.567254 \nL 670.436228 523.111099 \nL 671.320639 521.562974 \nL 672.20505 523.308896 \nL 673.08946 523.136914 \nL 673.973871 523.749393 \nL 674.858282 522.487512 \nL 675.742693 523.668298 \nL 676.627103 523.491085 \nL 677.511514 522.029414 \nL 678.395925 525.057457 \nL 679.280335 522.741924 \nL 680.164746 523.922697 \nL 681.049157 522.601176 \nL 681.933568 521.628263 \nL 682.817978 523.464364 \nL 683.702389 523.955459 \nL 684.5868 522.580664 \nL 685.471211 521.884909 \nL 686.355621 523.938511 \nL 687.240032 522.798889 \nL 688.124443 523.385099 \nL 689.008854 522.841605 \nL 690.777675 524.5217 \nL 691.662086 523.905108 \nL 692.546496 524.613898 \nL 693.430907 525.910207 \nL 694.315318 524.444713 \nL 695.199729 522.205195 \nL 696.084139 521.171451 \nL 696.96855 520.769979 \nL 697.852961 521.270673 \nL 698.737372 523.078337 \nL 699.621782 521.114861 \nL 700.506193 519.793508 \nL 701.390604 524.773442 \nL 702.275015 523.834167 \nL 703.159425 523.821319 \nL 704.043836 524.83829 \nL 704.928247 521.692627 \nL 705.812657 525.004002 \nL 706.697068 521.817881 \nL 707.581479 522.685569 \nL 708.46589 523.075486 \nL 709.3503 525.08352 \nL 710.234711 521.346247 \nL 711.119122 523.651651 \nL 712.003533 525.526458 \nL 712.887943 524.021949 \nL 713.772354 524.232026 \nL 714.656765 524.61464 \nL 715.541175 523.975507 \nL 716.425586 522.538624 \nL 717.309997 526.06727 \nL 718.194408 522.122088 \nL 719.078818 522.356223 \nL 719.963229 524.516228 \nL 720.84764 524.559766 \nL 721.732051 526.898433 \nL 722.616461 520.859827 \nL 723.500872 523.35529 \nL 724.385283 523.296308 \nL 725.269694 521.244911 \nL 726.154104 522.100857 \nL 727.038515 522.143561 \nL 728.807336 524.537894 \nL 729.691747 524.292162 \nL 730.576158 521.268837 \nL 732.344979 523.473279 \nL 733.22939 520.346654 \nL 734.113801 522.290132 \nL 734.998212 522.694188 \nL 735.882622 521.329564 \nL 736.767033 523.943663 \nL 737.651444 525.601253 \nL 738.535855 524.475874 \nL 739.420265 522.537307 \nL 740.304676 521.052333 \nL 741.189087 524.742861 \nL 742.073497 522.575916 \nL 742.957908 521.374298 \nL 744.72673 524.75325 \nL 746.495551 521.735016 \nL 747.379962 522.425129 \nL 748.264373 525.433216 \nL 749.148783 524.397509 \nL 750.033194 524.361038 \nL 750.917605 524.672952 \nL 751.802016 522.501145 \nL 752.686426 524.126154 \nL 753.570837 522.57324 \nL 754.455248 523.960938 \nL 755.339658 524.3069 \nL 756.224069 525.085459 \nL 757.10848 522.660823 \nL 757.992891 522.848962 \nL 758.877301 522.051914 \nL 759.761712 524.55265 \nL 760.646123 524.219547 \nL 761.530534 523.290244 \nL 763.299355 522.194969 \nL 765.068177 524.941854 \nL 765.952587 520.675215 \nL 766.836998 524.119643 \nL 767.721409 524.735516 \nL 768.605819 524.145851 \nL 769.49023 523.135066 \nL 770.374641 523.729871 \nL 771.259052 522.122185 \nL 772.143462 521.558758 \nL 773.027873 522.040008 \nL 773.912284 525.12832 \nL 774.796695 524.308863 \nL 775.681105 524.445661 \nL 776.565516 522.004885 \nL 777.449927 523.497603 \nL 778.334337 523.795039 \nL 779.218748 522.986919 \nL 780.103159 522.700839 \nL 780.98757 523.36066 \nL 781.87198 526.143847 \nL 782.756391 522.156807 \nL 783.640802 523.821585 \nL 784.525213 522.196419 \nL 785.409623 523.984766 \nL 786.294034 523.830229 \nL 786.294034 523.830229 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p49ef4b9726)\" d=\"M 76.112216 124.271154 \nL 77.881037 198.531831 \nL 80.53427 297.25022 \nL 82.303091 355.107672 \nL 84.071913 404.240578 \nL 84.956323 425.463323 \nL 85.840734 443.812242 \nL 86.725145 459.336781 \nL 87.609555 472.222855 \nL 88.493966 482.646777 \nL 89.378377 491.220796 \nL 90.262788 497.727466 \nL 91.147198 502.46406 \nL 92.031609 506.245626 \nL 92.91602 508.757216 \nL 93.800431 510.542622 \nL 94.684841 511.546981 \nL 95.569252 512.333271 \nL 96.453663 512.692263 \nL 98.222484 512.92235 \nL 99.991306 512.827393 \nL 102.644538 512.609174 \nL 104.413359 512.578556 \nL 106.182181 512.584687 \nL 108.835413 512.705273 \nL 110.604234 512.67901 \nL 111.488645 512.745851 \nL 112.373056 512.993117 \nL 113.257467 512.847718 \nL 114.141877 512.832618 \nL 117.67952 513.31112 \nL 125.639217 513.948454 \nL 129.17686 514.092312 \nL 130.945681 514.30408 \nL 131.830092 514.273922 \nL 132.714503 514.61228 \nL 136.252146 515.131867 \nL 137.136556 515.018034 \nL 138.905378 515.20061 \nL 139.789789 515.053592 \nL 140.674199 515.266007 \nL 142.443021 515.168977 \nL 143.327432 515.532402 \nL 145.096253 515.762472 \nL 146.865075 515.866151 \nL 147.749485 515.917281 \nL 149.518307 516.255138 \nL 152.171539 516.665772 \nL 153.05595 516.924201 \nL 154.824771 516.651185 \nL 158.362414 517.28437 \nL 159.246825 517.430373 \nL 160.131235 517.392591 \nL 161.015646 517.544683 \nL 161.900057 517.548005 \nL 163.668878 517.877484 \nL 164.553289 517.756511 \nL 165.4377 517.91785 \nL 166.322111 517.94128 \nL 167.206521 518.203955 \nL 169.859754 518.265486 \nL 171.628575 518.888698 \nL 172.512986 518.555788 \nL 173.397396 518.658725 \nL 174.281807 518.510638 \nL 175.166218 518.956227 \nL 176.050629 519.110656 \nL 177.81945 519.666417 \nL 179.588272 519.451013 \nL 182.241504 519.54411 \nL 183.125915 520.024442 \nL 184.010325 520.355962 \nL 185.779147 520.340137 \nL 186.663557 520.204095 \nL 187.547968 520.494741 \nL 191.085611 521.029767 \nL 191.970022 521.012444 \nL 192.854433 521.45613 \nL 193.738843 521.47869 \nL 195.507665 521.752321 \nL 196.392076 521.86825 \nL 197.276486 521.829231 \nL 198.160897 521.940273 \nL 199.045308 522.201975 \nL 199.929718 522.299893 \nL 201.69854 522.739441 \nL 204.351772 522.932527 \nL 209.658237 523.474946 \nL 211.427058 523.90053 \nL 213.195879 524.122017 \nL 214.964701 524.054367 \nL 216.733522 524.261369 \nL 217.617933 524.325311 \nL 218.502344 524.679615 \nL 222.924398 525.443665 \nL 223.808808 525.666378 \nL 224.693219 525.727722 \nL 226.46204 526.098117 \nL 228.230862 525.909368 \nL 229.115273 526.266915 \nL 229.999683 526.34397 \nL 230.884094 526.603715 \nL 234.421737 526.938328 \nL 235.306148 527.20337 \nL 236.190558 527.109669 \nL 239.728201 527.701895 \nL 240.612612 527.60242 \nL 242.381434 528.011176 \nL 245.919077 528.67865 \nL 247.687898 528.653783 \nL 248.572309 528.714505 \nL 249.456719 528.960986 \nL 250.34113 528.938009 \nL 252.109952 529.42943 \nL 253.878773 529.675036 \nL 256.532005 529.987983 \nL 257.416416 530.139899 \nL 259.185238 530.700136 \nL 261.83847 530.636195 \nL 262.72288 530.730506 \nL 264.491702 531.156923 \nL 266.260523 531.351459 \nL 267.144934 531.229665 \nL 268.913756 531.57895 \nL 269.798166 531.697277 \nL 270.682577 531.66768 \nL 272.451399 531.987102 \nL 274.22022 532.131703 \nL 275.104631 532.190432 \nL 275.989041 532.052565 \nL 277.757863 532.495225 \nL 280.411095 533.009176 \nL 281.295506 532.991708 \nL 282.179917 533.118914 \nL 283.064327 533.073039 \nL 283.948738 533.260344 \nL 284.833149 533.184951 \nL 286.60197 533.525918 \nL 287.486381 533.484785 \nL 290.139613 534.025603 \nL 291.024024 533.917188 \nL 291.908435 533.923198 \nL 295.446078 534.502202 \nL 297.214899 534.492985 \nL 298.09931 534.589555 \nL 298.98372 534.423625 \nL 300.752542 534.457716 \nL 301.636953 534.792969 \nL 304.290185 535.155494 \nL 305.174596 535.012336 \nL 306.943417 535.178839 \nL 308.712239 535.50351 \nL 309.596649 535.329572 \nL 314.018703 535.81366 \nL 315.787524 535.820697 \nL 319.325167 536.247039 \nL 320.209578 536.229447 \nL 321.9784 536.395011 \nL 324.631632 536.582045 \nL 325.516042 536.402474 \nL 326.400453 536.405107 \nL 329.053685 536.756421 \nL 329.938096 536.830851 \nL 330.822507 536.694863 \nL 332.591328 537.049837 \nL 334.36015 537.292241 \nL 337.897793 537.380425 \nL 340.551025 537.156169 \nL 342.319846 537.554928 \nL 343.204257 537.70955 \nL 344.088668 537.531592 \nL 346.7419 537.714778 \nL 350.279543 537.989895 \nL 352.048364 538.09574 \nL 352.932775 538.236144 \nL 353.817186 538.24332 \nL 354.701597 537.831803 \nL 355.586007 538.113513 \nL 356.470418 538.215326 \nL 358.23924 538.236914 \nL 359.12365 538.37402 \nL 360.008061 538.292021 \nL 363.545704 538.414742 \nL 364.430115 538.629464 \nL 372.389811 538.757633 \nL 374.158633 538.960462 \nL 375.043043 538.917423 \nL 376.811865 539.149257 \nL 378.580686 539.055945 \nL 379.465097 539.003997 \nL 380.349508 539.110503 \nL 381.233919 539.066319 \nL 383.00274 539.273095 \nL 388.309204 539.251103 \nL 389.193615 539.449465 \nL 390.962437 539.362492 \nL 391.846847 539.505741 \nL 392.731258 539.402539 \nL 393.615669 539.529364 \nL 396.268901 539.516027 \nL 397.153312 539.599016 \nL 398.037723 539.445811 \nL 398.922133 539.612969 \nL 399.806544 539.518591 \nL 406.88183 539.727506 \nL 407.766241 539.664486 \nL 408.650651 539.731287 \nL 409.535062 539.637468 \nL 412.188294 539.83172 \nL 413.957116 539.800658 \nL 414.841526 539.925046 \nL 415.725937 539.812838 \nL 417.494759 539.983452 \nL 418.379169 539.734207 \nL 419.26358 539.963477 \nL 421.032402 540.030616 \nL 421.916812 539.91524 \nL 422.801223 540.054121 \nL 423.685634 540.013462 \nL 424.570045 540.085002 \nL 425.454455 540.042059 \nL 427.223277 540.21485 \nL 428.107687 540.068331 \nL 430.76092 540.241505 \nL 432.529741 540.281769 \nL 434.298563 540.345614 \nL 438.720616 539.982053 \nL 439.605027 540.193181 \nL 441.373848 540.00664 \nL 442.258259 540.209767 \nL 443.14267 540.2557 \nL 444.027081 540.138877 \nL 444.911491 540.232403 \nL 445.795902 540.133589 \nL 448.449134 540.343614 \nL 451.102366 540.251387 \nL 451.986777 540.332051 \nL 453.755599 540.233203 \nL 458.177652 540.496107 \nL 459.946474 540.35751 \nL 462.599706 540.469624 \nL 464.368527 540.287888 \nL 465.252938 540.443005 \nL 467.90617 540.530207 \nL 468.790581 540.667443 \nL 471.443813 540.608059 \nL 474.097046 540.608956 \nL 475.865867 540.505681 \nL 477.634688 540.65821 \nL 479.40351 540.610523 \nL 486.478796 540.696949 \nL 487.363207 540.817529 \nL 489.132028 540.746581 \nL 490.016439 540.768247 \nL 490.900849 540.597558 \nL 498.860546 540.827344 \nL 502.398189 540.791348 \nL 503.2826 540.69601 \nL 507.704653 540.898693 \nL 512.126707 540.882119 \nL 513.011118 540.950488 \nL 513.895528 540.826091 \nL 517.433171 540.915482 \nL 519.201993 540.757858 \nL 520.970814 540.820298 \nL 521.855225 540.737035 \nL 523.624047 540.845286 \nL 524.508457 540.902997 \nL 527.161689 540.716673 \nL 528.930511 540.902281 \nL 529.814922 540.901846 \nL 530.699332 540.74797 \nL 532.468154 540.912299 \nL 533.352565 540.777479 \nL 535.121386 540.931344 \nL 537.774618 540.829274 \nL 538.659029 540.957443 \nL 541.312261 540.956637 \nL 542.196672 541.005031 \nL 543.081083 540.932361 \nL 543.965493 540.989921 \nL 544.849904 540.893728 \nL 546.618726 540.947743 \nL 550.156369 541.027811 \nL 551.92519 540.971052 \nL 562.538119 541.009277 \nL 566.075762 541.051999 \nL 568.728994 540.964142 \nL 570.497815 541.064635 \nL 571.382226 540.863295 \nL 573.151048 540.993074 \nL 574.035458 540.876907 \nL 575.80428 541.077525 \nL 576.68869 540.967074 \nL 577.573101 541.072303 \nL 580.226333 540.81382 \nL 581.110744 540.833182 \nL 581.995155 540.687242 \nL 582.879566 541.006251 \nL 589.954851 541.064106 \nL 591.723673 541.115849 \nL 593.492494 540.978478 \nL 594.376905 541.084522 \nL 595.261316 540.93925 \nL 596.145727 540.972024 \nL 597.030137 540.890612 \nL 597.914548 541.066293 \nL 600.56778 541.014864 \nL 601.452191 540.959252 \nL 602.336602 541.109332 \nL 612.949531 541.0077 \nL 614.718352 540.975621 \nL 615.602763 540.874367 \nL 617.371584 541.012378 \nL 618.255995 540.932265 \nL 620.024816 541.074743 \nL 621.793638 541.025178 \nL 622.678049 541.130926 \nL 624.44687 541.01535 \nL 628.868924 541.089206 \nL 629.753334 540.944378 \nL 630.637745 541.083891 \nL 635.94421 541.000307 \nL 638.597442 541.122089 \nL 640.366263 540.929676 \nL 643.019495 541.083674 \nL 646.557138 541.048423 \nL 647.441549 540.853033 \nL 650.094781 541.136244 \nL 652.748013 541.146225 \nL 653.632424 540.996852 \nL 655.401246 540.968711 \nL 656.285656 541.086672 \nL 659.823299 541.038837 \nL 661.592121 541.139753 \nL 662.476532 541.036983 \nL 667.782996 541.115557 \nL 669.551817 541.078337 \nL 673.08946 541.061373 \nL 673.973871 541.14612 \nL 674.858282 541.107152 \nL 675.742693 541.183705 \nL 681.049157 540.993542 \nL 689.893264 541.074529 \nL 690.777675 541.01731 \nL 692.546496 541.1 \nL 693.430907 541.123611 \nL 694.315318 540.972235 \nL 695.199729 541.123708 \nL 704.928247 541.080889 \nL 705.812657 540.972855 \nL 710.234711 541.145637 \nL 711.119122 541.003795 \nL 712.003533 541.134329 \nL 720.84764 540.878438 \nL 721.732051 541.060591 \nL 724.385283 541.065955 \nL 726.154104 541.021276 \nL 727.038515 541.132511 \nL 727.922926 541.113596 \nL 728.807336 540.96583 \nL 731.460569 541.143797 \nL 734.998212 541.043183 \nL 736.767033 541.138551 \nL 738.535855 540.889618 \nL 742.073497 541.154824 \nL 743.842319 540.993965 \nL 744.72673 540.998392 \nL 745.61114 541.127166 \nL 747.379962 541.082767 \nL 753.570837 541.112077 \nL 754.455248 541.024791 \nL 757.992891 541.068972 \nL 759.761712 540.980728 \nL 761.530534 540.995985 \nL 763.299355 540.942065 \nL 764.183766 541.157243 \nL 765.068177 541.041637 \nL 771.259052 541.112854 \nL 773.912284 541.105343 \nL 775.681105 541.05829 \nL 776.565516 541.1339 \nL 777.449927 541.080904 \nL 778.334337 541.19268 \nL 779.218748 541.107771 \nL 780.98757 541.144383 \nL 785.409623 541.121536 \nL 786.294034 541.070337 \nL 786.294034 541.070337 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Model MSE -->\n    <g transform=\"translate(398.503125 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"481.787109\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_21\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_22\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p49ef4b9726\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "11/11 [==============================] - 0s 767us/step\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}