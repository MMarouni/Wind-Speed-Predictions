{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1)\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df=pd.read_excel('Preprocessed-data.xlsx')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Date  Temp  Dew Point  Humidity  Wind Speed  Pressure  \\\n",
       "0  2020-01-01   5.7        4.8      94.4        2.47    1029.0   \n",
       "1  2020-01-02   9.3        8.2      92.5        7.42    1021.4   \n",
       "2  2020-01-03   8.2        6.8      91.4        6.81    1021.8   \n",
       "3  2020-01-04   5.6        3.6      87.2        3.94    1033.7   \n",
       "4  2020-01-05   7.8        6.1      89.2        3.33    1033.4   \n",
       "\n",
       "   Wind Direction  Pressure Grad  Wind Gradient  \n",
       "0             128             -8              3  \n",
       "1             188              0              8  \n",
       "2             265             11              7  \n",
       "3             248             -1              2  \n",
       "4             224            -11              5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind Direction</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>94.4</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>128</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>92.5</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>91.4</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>265</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>87.2</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>248</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>224</td>\n",
       "      <td>-11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Date              0\n",
       "Temp              0\n",
       "Dew Point         0\n",
       "Humidity          0\n",
       "Wind Speed        0\n",
       "Pressure          0\n",
       "Wind Direction    0\n",
       "Pressure Grad     0\n",
       "Wind Gradient     0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df['Year']  = df['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "df['Month'] = df['Date'].apply(lambda x: int(str(x)[5:7]))\n",
    "df['Day']=df['Date'].apply(lambda x: int(str(x)[8:10]))\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Date  Temp  Dew Point  Humidity  Wind Speed  Pressure  \\\n",
       "0  2020-01-01   5.7        4.8      94.4        2.47    1029.0   \n",
       "1  2020-01-02   9.3        8.2      92.5        7.42    1021.4   \n",
       "2  2020-01-03   8.2        6.8      91.4        6.81    1021.8   \n",
       "3  2020-01-04   5.6        3.6      87.2        3.94    1033.7   \n",
       "4  2020-01-05   7.8        6.1      89.2        3.33    1033.4   \n",
       "\n",
       "   Wind Direction  Pressure Grad  Wind Gradient  Year  Month  Day  \n",
       "0             128             -8              3  2020      1    1  \n",
       "1             188              0              8  2020      1    2  \n",
       "2             265             11              7  2020      1    3  \n",
       "3             248             -1              2  2020      1    4  \n",
       "4             224            -11              5  2020      1    5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind Direction</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>94.4</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>128</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>92.5</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>91.4</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>265</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>87.2</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>248</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>224</td>\n",
       "      <td>-11</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "del df['Date']\n",
    "#del df['Year']\n",
    "#del df['Month']\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Temp  Dew Point  Humidity  Wind Speed  Pressure  Wind Direction  \\\n",
       "0   5.7        4.8      94.4        2.47    1029.0             128   \n",
       "1   9.3        8.2      92.5        7.42    1021.4             188   \n",
       "2   8.2        6.8      91.4        6.81    1021.8             265   \n",
       "3   5.6        3.6      87.2        3.94    1033.7             248   \n",
       "4   7.8        6.1      89.2        3.33    1033.4             224   \n",
       "\n",
       "   Pressure Grad  Wind Gradient  Year  Month  Day  \n",
       "0             -8              3  2020      1    1  \n",
       "1              0              8  2020      1    2  \n",
       "2             11              7  2020      1    3  \n",
       "3             -1              2  2020      1    4  \n",
       "4            -11              5  2020      1    5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind Direction</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>94.4</td>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>128</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.3</td>\n",
       "      <td>8.2</td>\n",
       "      <td>92.5</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>91.4</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>265</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>87.2</td>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>248</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>89.2</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>224</td>\n",
       "      <td>-11</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X = df.drop(['Wind Speed'], axis=1)\n",
    "#Assign the Target column as the output \n",
    "Y= df['Wind Speed']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "X_norm=(X-X.min())/(X.max()-X.min())\n",
    "X_norm"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Temp  Dew Point  Humidity  Pressure  Wind Direction  Pressure Grad  \\\n",
       "0     0.299639   0.487805  0.923913  0.752055        0.342679       0.449275   \n",
       "1     0.429603   0.606272  0.898098  0.647945        0.529595       0.565217   \n",
       "2     0.389892   0.557491  0.883152  0.653425        0.769470       0.724638   \n",
       "3     0.296029   0.445993  0.826087  0.816438        0.716511       0.550725   \n",
       "4     0.375451   0.533101  0.853261  0.812329        0.641745       0.405797   \n",
       "...        ...        ...       ...       ...             ...            ...   \n",
       "1091  0.198556   0.411150  0.972826  0.706849        0.676012       0.623188   \n",
       "1092  0.259928   0.470383  0.966033  0.767123        0.757009       0.594203   \n",
       "1093  0.415162   0.595819  0.915761  0.795890        0.738318       0.594203   \n",
       "1094  0.346570   0.564460  0.998641  0.831507        0.753894       0.565217   \n",
       "1095  0.397112   0.581882  0.913043  0.836986        0.816199       0.594203   \n",
       "\n",
       "      Wind Gradient  Year  Month       Day  \n",
       "0          0.200000   1.0    0.0  0.000000  \n",
       "1          0.533333   1.0    0.0  0.033333  \n",
       "2          0.466667   1.0    0.0  0.066667  \n",
       "3          0.133333   1.0    0.0  0.100000  \n",
       "4          0.333333   1.0    0.0  0.133333  \n",
       "...             ...   ...    ...       ...  \n",
       "1091       0.066667   0.0    1.0  0.866667  \n",
       "1092       0.200000   0.0    1.0  0.900000  \n",
       "1093       0.266667   0.0    1.0  0.933333  \n",
       "1094       0.200000   0.0    1.0  0.966667  \n",
       "1095       0.200000   0.0    1.0  1.000000  \n",
       "\n",
       "[1096 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "      <th>Dew Point</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Wind Direction</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299639</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.752055</td>\n",
       "      <td>0.342679</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.429603</td>\n",
       "      <td>0.606272</td>\n",
       "      <td>0.898098</td>\n",
       "      <td>0.647945</td>\n",
       "      <td>0.529595</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.389892</td>\n",
       "      <td>0.557491</td>\n",
       "      <td>0.883152</td>\n",
       "      <td>0.653425</td>\n",
       "      <td>0.769470</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.296029</td>\n",
       "      <td>0.445993</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.716511</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375451</td>\n",
       "      <td>0.533101</td>\n",
       "      <td>0.853261</td>\n",
       "      <td>0.812329</td>\n",
       "      <td>0.641745</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.198556</td>\n",
       "      <td>0.411150</td>\n",
       "      <td>0.972826</td>\n",
       "      <td>0.706849</td>\n",
       "      <td>0.676012</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.259928</td>\n",
       "      <td>0.470383</td>\n",
       "      <td>0.966033</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.415162</td>\n",
       "      <td>0.595819</td>\n",
       "      <td>0.915761</td>\n",
       "      <td>0.795890</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.346570</td>\n",
       "      <td>0.564460</td>\n",
       "      <td>0.998641</td>\n",
       "      <td>0.831507</td>\n",
       "      <td>0.753894</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.397112</td>\n",
       "      <td>0.581882</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.836986</td>\n",
       "      <td>0.816199</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_norm, Y, test_size=0.3, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(45, input_dim=x_train.shape[1], activation=\"sigmoid\", kernel_initializer='normal'))\n",
    "model.add(Dropout(0.2)) #dropping a few neurons for generalizing the model\n",
    "\n",
    "model.add(Dense(1, activation=\"linear\", kernel_initializer='normal'))\n",
    "adam = Adam(learning_rate=1e-3, decay=1e-3)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mse','mae'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print('Fit model...')\n",
    "filepath=\"/home/m-marouni/Documents/CE-901/Heathrow/best_weights\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_mae', patience=100, verbose=1, mode='min')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "log = model.fit(x_train, y_train,\n",
    "          validation_split=0.40, batch_size=30, epochs=1000, shuffle=True, callbacks=callbacks_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fit model...\n",
      "Epoch 1/1000\n",
      "16/16 [==============================] - 9s 28ms/step - loss: 42.5869 - mse: 42.5869 - mae: 5.9753 - val_loss: 34.9162 - val_mse: 34.9162 - val_mae: 5.3878\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 5.38780, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 2/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 39.0721 - mse: 39.0721 - mae: 5.7065 - val_loss: 30.6662 - val_mse: 30.6662 - val_mae: 4.9779\n",
      "\n",
      "Epoch 00002: val_mae improved from 5.38780 to 4.97795, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 3/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 34.5265 - mse: 34.5265 - mae: 5.3180 - val_loss: 26.6739 - val_mse: 26.6739 - val_mae: 4.5595\n",
      "\n",
      "Epoch 00003: val_mae improved from 4.97795 to 4.55954, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 4/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 27.4907 - mse: 27.4907 - mae: 4.6639 - val_loss: 22.9274 - val_mse: 22.9274 - val_mae: 4.1311\n",
      "\n",
      "Epoch 00004: val_mae improved from 4.55954 to 4.13108, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 5/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 25.3465 - mse: 25.3465 - mae: 4.3855 - val_loss: 19.3443 - val_mse: 19.3443 - val_mae: 3.6774\n",
      "\n",
      "Epoch 00005: val_mae improved from 4.13108 to 3.67739, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 6/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.2085 - mse: 20.2085 - mae: 3.7742 - val_loss: 16.1356 - val_mse: 16.1356 - val_mae: 3.2312\n",
      "\n",
      "Epoch 00006: val_mae improved from 3.67739 to 3.23117, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 7/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 18.0840 - mse: 18.0840 - mae: 3.4478 - val_loss: 13.3105 - val_mse: 13.3105 - val_mae: 2.8177\n",
      "\n",
      "Epoch 00007: val_mae improved from 3.23117 to 2.81771, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 8/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 15.0356 - mse: 15.0356 - mae: 3.0482 - val_loss: 10.9729 - val_mse: 10.9729 - val_mae: 2.4899\n",
      "\n",
      "Epoch 00008: val_mae improved from 2.81771 to 2.48988, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 9/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.8411 - mse: 12.8411 - mae: 2.6901 - val_loss: 9.1310 - val_mse: 9.1310 - val_mae: 2.2576\n",
      "\n",
      "Epoch 00009: val_mae improved from 2.48988 to 2.25762, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 10/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.2029 - mse: 9.2029 - mae: 2.1602 - val_loss: 7.8497 - val_mse: 7.8497 - val_mae: 2.1133\n",
      "\n",
      "Epoch 00010: val_mae improved from 2.25762 to 2.11327, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 11/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.5168 - mse: 8.5168 - mae: 2.1641 - val_loss: 6.9415 - val_mse: 6.9415 - val_mae: 2.0116\n",
      "\n",
      "Epoch 00011: val_mae improved from 2.11327 to 2.01159, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 12/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.7083 - mse: 7.7083 - mae: 2.0395 - val_loss: 6.3888 - val_mse: 6.3888 - val_mae: 1.9581\n",
      "\n",
      "Epoch 00012: val_mae improved from 2.01159 to 1.95810, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 13/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.5862 - mse: 7.5862 - mae: 2.0225 - val_loss: 6.0934 - val_mse: 6.0934 - val_mae: 1.9433\n",
      "\n",
      "Epoch 00013: val_mae improved from 1.95810 to 1.94334, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 14/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8648 - mse: 5.8648 - mae: 1.8306 - val_loss: 5.9518 - val_mse: 5.9518 - val_mae: 1.9438\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 1.94334\n",
      "Epoch 15/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2267 - mse: 6.2267 - mae: 1.9544 - val_loss: 5.8973 - val_mse: 5.8973 - val_mae: 1.9516\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 1.94334\n",
      "Epoch 16/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3656 - mse: 6.3656 - mae: 1.9872 - val_loss: 5.8853 - val_mse: 5.8853 - val_mae: 1.9611\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 1.94334\n",
      "Epoch 17/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.3255 - mse: 7.3255 - mae: 2.0336 - val_loss: 5.8936 - val_mse: 5.8936 - val_mae: 1.9716\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 1.94334\n",
      "Epoch 18/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7370 - mse: 5.7370 - mae: 1.9032 - val_loss: 5.9009 - val_mse: 5.9009 - val_mae: 1.9772\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 1.94334\n",
      "Epoch 19/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8642 - mse: 6.8642 - mae: 2.0515 - val_loss: 5.9207 - val_mse: 5.9207 - val_mae: 1.9868\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 1.94334\n",
      "Epoch 20/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9507 - mse: 5.9507 - mae: 1.9897 - val_loss: 5.9215 - val_mse: 5.9215 - val_mae: 1.9878\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 1.94334\n",
      "Epoch 21/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.2630 - mse: 7.2630 - mae: 2.0764 - val_loss: 5.9301 - val_mse: 5.9301 - val_mae: 1.9917\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 1.94334\n",
      "Epoch 22/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8590 - mse: 5.8590 - mae: 1.9240 - val_loss: 5.9220 - val_mse: 5.9220 - val_mae: 1.9893\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 1.94334\n",
      "Epoch 23/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8904 - mse: 6.8904 - mae: 2.0798 - val_loss: 5.9445 - val_mse: 5.9445 - val_mae: 1.9986\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 1.94334\n",
      "Epoch 24/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4078 - mse: 6.4078 - mae: 2.0406 - val_loss: 5.9363 - val_mse: 5.9363 - val_mae: 1.9962\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 1.94334\n",
      "Epoch 25/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0434 - mse: 6.0434 - mae: 1.9754 - val_loss: 5.9231 - val_mse: 5.9231 - val_mae: 1.9919\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 1.94334\n",
      "Epoch 26/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0457 - mse: 6.0457 - mae: 1.9821 - val_loss: 5.9186 - val_mse: 5.9186 - val_mae: 1.9909\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 1.94334\n",
      "Epoch 27/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2759 - mse: 6.2759 - mae: 1.9934 - val_loss: 5.9289 - val_mse: 5.9289 - val_mae: 1.9956\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 1.94334\n",
      "Epoch 28/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.0121 - mse: 7.0121 - mae: 2.0852 - val_loss: 5.9337 - val_mse: 5.9337 - val_mae: 1.9980\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 1.94334\n",
      "Epoch 29/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8367 - mse: 5.8367 - mae: 1.9289 - val_loss: 5.9305 - val_mse: 5.9305 - val_mae: 1.9975\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 1.94334\n",
      "Epoch 30/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1005 - mse: 6.1005 - mae: 1.9866 - val_loss: 5.9336 - val_mse: 5.9336 - val_mae: 1.9991\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 1.94334\n",
      "Epoch 31/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7502 - mse: 6.7502 - mae: 2.0652 - val_loss: 5.9168 - val_mse: 5.9168 - val_mae: 1.9940\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 1.94334\n",
      "Epoch 32/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7146 - mse: 6.7146 - mae: 2.0311 - val_loss: 5.8896 - val_mse: 5.8896 - val_mae: 1.9846\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 1.94334\n",
      "Epoch 33/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9139 - mse: 5.9139 - mae: 1.9353 - val_loss: 5.8938 - val_mse: 5.8938 - val_mae: 1.9870\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 1.94334\n",
      "Epoch 34/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2658 - mse: 6.2658 - mae: 1.9706 - val_loss: 5.9012 - val_mse: 5.9012 - val_mae: 1.9906\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 1.94334\n",
      "Epoch 35/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8281 - mse: 6.8281 - mae: 2.0921 - val_loss: 5.8948 - val_mse: 5.8948 - val_mae: 1.9891\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 1.94334\n",
      "Epoch 36/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2265 - mse: 5.2265 - mae: 1.8373 - val_loss: 5.8925 - val_mse: 5.8925 - val_mae: 1.9890\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 1.94334\n",
      "Epoch 37/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4149 - mse: 6.4149 - mae: 2.0495 - val_loss: 5.8934 - val_mse: 5.8934 - val_mae: 1.9902\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 1.94334\n",
      "Epoch 38/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4798 - mse: 6.4798 - mae: 2.0346 - val_loss: 5.8879 - val_mse: 5.8879 - val_mae: 1.9891\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 1.94334\n",
      "Epoch 39/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5168 - mse: 6.5168 - mae: 2.0136 - val_loss: 5.8990 - val_mse: 5.8990 - val_mae: 1.9936\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 1.94334\n",
      "Epoch 40/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0554 - mse: 6.0554 - mae: 1.9466 - val_loss: 5.8853 - val_mse: 5.8853 - val_mae: 1.9898\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 1.94334\n",
      "Epoch 41/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7076 - mse: 6.7076 - mae: 2.0707 - val_loss: 5.8747 - val_mse: 5.8747 - val_mae: 1.9869\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 1.94334\n",
      "Epoch 42/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9167 - mse: 5.9167 - mae: 1.9673 - val_loss: 5.8452 - val_mse: 5.8452 - val_mae: 1.9770\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 1.94334\n",
      "Epoch 43/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0573 - mse: 6.0573 - mae: 1.9336 - val_loss: 5.8624 - val_mse: 5.8624 - val_mae: 1.9841\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 1.94334\n",
      "Epoch 44/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5127 - mse: 6.5127 - mae: 2.0110 - val_loss: 5.8732 - val_mse: 5.8732 - val_mae: 1.9887\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 1.94334\n",
      "Epoch 45/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5411 - mse: 6.5411 - mae: 2.0390 - val_loss: 5.8643 - val_mse: 5.8643 - val_mae: 1.9865\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 1.94334\n",
      "Epoch 46/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.0242 - mse: 7.0242 - mae: 2.1108 - val_loss: 5.8584 - val_mse: 5.8584 - val_mae: 1.9853\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 1.94334\n",
      "Epoch 47/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6215 - mse: 6.6215 - mae: 2.0577 - val_loss: 5.8405 - val_mse: 5.8405 - val_mae: 1.9798\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 1.94334\n",
      "Epoch 48/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3053 - mse: 6.3053 - mae: 1.9715 - val_loss: 5.8336 - val_mse: 5.8336 - val_mae: 1.9783\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 1.94334\n",
      "Epoch 49/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6679 - mse: 6.6679 - mae: 2.0568 - val_loss: 5.8293 - val_mse: 5.8293 - val_mae: 1.9777\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 1.94334\n",
      "Epoch 50/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2418 - mse: 6.2418 - mae: 2.0217 - val_loss: 5.8300 - val_mse: 5.8300 - val_mae: 1.9789\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 1.94334\n",
      "Epoch 51/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5339 - mse: 6.5339 - mae: 2.0094 - val_loss: 5.8141 - val_mse: 5.8141 - val_mae: 1.9743\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 1.94334\n",
      "Epoch 52/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9602 - mse: 6.9602 - mae: 2.1043 - val_loss: 5.8057 - val_mse: 5.8057 - val_mae: 1.9723\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 1.94334\n",
      "Epoch 53/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7204 - mse: 5.7204 - mae: 1.9197 - val_loss: 5.7985 - val_mse: 5.7985 - val_mae: 1.9707\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 1.94334\n",
      "Epoch 54/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4562 - mse: 6.4562 - mae: 1.9350 - val_loss: 5.8017 - val_mse: 5.8017 - val_mae: 1.9729\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 1.94334\n",
      "Epoch 55/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0012 - mse: 6.0012 - mae: 1.9389 - val_loss: 5.7876 - val_mse: 5.7876 - val_mae: 1.9690\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 1.94334\n",
      "Epoch 56/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9787 - mse: 6.9787 - mae: 2.0342 - val_loss: 5.8013 - val_mse: 5.8013 - val_mae: 1.9749\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 1.94334\n",
      "Epoch 57/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1018 - mse: 6.1018 - mae: 2.0047 - val_loss: 5.7948 - val_mse: 5.7948 - val_mae: 1.9736\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 1.94334\n",
      "Epoch 58/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0566 - mse: 6.0566 - mae: 1.9686 - val_loss: 5.7824 - val_mse: 5.7824 - val_mae: 1.9704\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 1.94334\n",
      "Epoch 59/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2892 - mse: 6.2892 - mae: 2.0166 - val_loss: 5.7739 - val_mse: 5.7739 - val_mae: 1.9685\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 1.94334\n",
      "Epoch 60/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4154 - mse: 6.4154 - mae: 2.0298 - val_loss: 5.7588 - val_mse: 5.7588 - val_mae: 1.9642\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 1.94334\n",
      "Epoch 61/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7803 - mse: 6.7803 - mae: 2.0050 - val_loss: 5.7601 - val_mse: 5.7601 - val_mae: 1.9658\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 1.94334\n",
      "Epoch 62/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7090 - mse: 5.7090 - mae: 1.9242 - val_loss: 5.7357 - val_mse: 5.7357 - val_mae: 1.9584\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 1.94334\n",
      "Epoch 63/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3765 - mse: 6.3765 - mae: 1.9949 - val_loss: 5.7281 - val_mse: 5.7281 - val_mae: 1.9568\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 1.94334\n",
      "Epoch 64/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0886 - mse: 6.0886 - mae: 1.9482 - val_loss: 5.7441 - val_mse: 5.7441 - val_mae: 1.9638\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 1.94334\n",
      "Epoch 65/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6482 - mse: 6.6482 - mae: 2.0337 - val_loss: 5.7188 - val_mse: 5.7188 - val_mae: 1.9564\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 1.94334\n",
      "Epoch 66/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9526 - mse: 6.9526 - mae: 2.1001 - val_loss: 5.7116 - val_mse: 5.7116 - val_mae: 1.9551\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 1.94334\n",
      "Epoch 67/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8247 - mse: 5.8247 - mae: 1.9052 - val_loss: 5.6878 - val_mse: 5.6878 - val_mae: 1.9474\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 1.94334\n",
      "Epoch 68/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7593 - mse: 5.7593 - mae: 1.9079 - val_loss: 5.6815 - val_mse: 5.6815 - val_mae: 1.9466\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 1.94334\n",
      "Epoch 69/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9860 - mse: 5.9860 - mae: 1.9626 - val_loss: 5.6869 - val_mse: 5.6869 - val_mae: 1.9504\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 1.94334\n",
      "Epoch 70/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4619 - mse: 6.4619 - mae: 2.0020 - val_loss: 5.7189 - val_mse: 5.7189 - val_mae: 1.9630\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 1.94334\n",
      "Epoch 71/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4385 - mse: 6.4385 - mae: 1.9375 - val_loss: 5.6993 - val_mse: 5.6993 - val_mae: 1.9575\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 1.94334\n",
      "Epoch 72/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0684 - mse: 6.0684 - mae: 1.9782 - val_loss: 5.6952 - val_mse: 5.6952 - val_mae: 1.9574\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 1.94334\n",
      "Epoch 73/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9366 - mse: 5.9366 - mae: 1.9502 - val_loss: 5.7019 - val_mse: 5.7019 - val_mae: 1.9610\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 1.94334\n",
      "Epoch 74/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8501 - mse: 5.8501 - mae: 1.9214 - val_loss: 5.6497 - val_mse: 5.6497 - val_mae: 1.9439\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 1.94334\n",
      "Epoch 75/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4843 - mse: 6.4843 - mae: 2.0015 - val_loss: 5.6650 - val_mse: 5.6650 - val_mae: 1.9510\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 1.94334\n",
      "Epoch 76/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0064 - mse: 6.0064 - mae: 1.9391 - val_loss: 5.6634 - val_mse: 5.6634 - val_mae: 1.9519\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 1.94334\n",
      "Epoch 77/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5663 - mse: 6.5663 - mae: 2.0417 - val_loss: 5.6242 - val_mse: 5.6242 - val_mae: 1.9394\n",
      "\n",
      "Epoch 00077: val_mae improved from 1.94334 to 1.93941, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 78/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.9000 - mse: 5.9000 - mae: 1.9353 - val_loss: 5.6169 - val_mse: 5.6169 - val_mae: 1.9384\n",
      "\n",
      "Epoch 00078: val_mae improved from 1.93941 to 1.93843, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 79/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3964 - mse: 6.3964 - mae: 1.9697 - val_loss: 5.6199 - val_mse: 5.6199 - val_mae: 1.9413\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 1.93843\n",
      "Epoch 80/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1765 - mse: 6.1765 - mae: 1.9497 - val_loss: 5.6329 - val_mse: 5.6329 - val_mae: 1.9475\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 1.93843\n",
      "Epoch 81/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3713 - mse: 6.3713 - mae: 1.9390 - val_loss: 5.6160 - val_mse: 5.6160 - val_mae: 1.9434\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 1.93843\n",
      "Epoch 82/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5450 - mse: 6.5450 - mae: 2.0261 - val_loss: 5.6104 - val_mse: 5.6104 - val_mae: 1.9430\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 1.93843\n",
      "Epoch 83/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6929 - mse: 5.6929 - mae: 1.8613 - val_loss: 5.5795 - val_mse: 5.5795 - val_mae: 1.9336\n",
      "\n",
      "Epoch 00083: val_mae improved from 1.93843 to 1.93356, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 84/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6191 - mse: 6.6191 - mae: 2.0460 - val_loss: 5.5736 - val_mse: 5.5736 - val_mae: 1.9334\n",
      "\n",
      "Epoch 00084: val_mae improved from 1.93356 to 1.93342, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 85/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.4331 - mse: 6.4331 - mae: 1.9368 - val_loss: 5.5530 - val_mse: 5.5530 - val_mae: 1.9275\n",
      "\n",
      "Epoch 00085: val_mae improved from 1.93342 to 1.92750, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 86/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.7657 - mse: 5.7657 - mae: 1.9518 - val_loss: 5.5163 - val_mse: 5.5163 - val_mae: 1.9149\n",
      "\n",
      "Epoch 00086: val_mae improved from 1.92750 to 1.91492, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 87/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3253 - mse: 6.3253 - mae: 1.9845 - val_loss: 5.5202 - val_mse: 5.5202 - val_mae: 1.9191\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 1.91492\n",
      "Epoch 88/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5366 - mse: 5.5366 - mae: 1.9008 - val_loss: 5.5069 - val_mse: 5.5069 - val_mae: 1.9159\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 1.91492\n",
      "Epoch 89/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2248 - mse: 6.2248 - mae: 2.0142 - val_loss: 5.5353 - val_mse: 5.5353 - val_mae: 1.9293\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 1.91492\n",
      "Epoch 90/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2033 - mse: 6.2033 - mae: 1.9539 - val_loss: 5.5314 - val_mse: 5.5314 - val_mae: 1.9297\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 1.91492\n",
      "Epoch 91/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3667 - mse: 6.3667 - mae: 1.9918 - val_loss: 5.5083 - val_mse: 5.5083 - val_mae: 1.9235\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 1.91492\n",
      "Epoch 92/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.9289 - mse: 6.9289 - mae: 2.0270 - val_loss: 5.4961 - val_mse: 5.4961 - val_mae: 1.9210\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 1.91492\n",
      "Epoch 93/1000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.5397 - mse: 6.5397 - mae: 2.0067 - val_loss: 5.4664 - val_mse: 5.4664 - val_mae: 1.9118\n",
      "\n",
      "Epoch 00093: val_mae improved from 1.91492 to 1.91183, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 94/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.6227 - mse: 6.6227 - mae: 2.0215 - val_loss: 5.4625 - val_mse: 5.4625 - val_mae: 1.9126\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 1.91183\n",
      "Epoch 95/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.3970 - mse: 5.3970 - mae: 1.8447 - val_loss: 5.4418 - val_mse: 5.4418 - val_mae: 1.9069\n",
      "\n",
      "Epoch 00095: val_mae improved from 1.91183 to 1.90687, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 96/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0220 - mse: 6.0220 - mae: 1.9785 - val_loss: 5.4583 - val_mse: 5.4583 - val_mae: 1.9153\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 1.90687\n",
      "Epoch 97/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3741 - mse: 5.3741 - mae: 1.8494 - val_loss: 5.4375 - val_mse: 5.4375 - val_mae: 1.9099\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 1.90687\n",
      "Epoch 98/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4065 - mse: 5.4065 - mae: 1.8334 - val_loss: 5.4262 - val_mse: 5.4262 - val_mae: 1.9077\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 1.90687\n",
      "Epoch 99/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2472 - mse: 6.2472 - mae: 1.9579 - val_loss: 5.4120 - val_mse: 5.4120 - val_mae: 1.9046\n",
      "\n",
      "Epoch 00099: val_mae improved from 1.90687 to 1.90458, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 100/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6489 - mse: 5.6489 - mae: 1.8817 - val_loss: 5.3917 - val_mse: 5.3917 - val_mae: 1.8993\n",
      "\n",
      "Epoch 00100: val_mae improved from 1.90458 to 1.89931, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 101/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.8556 - mse: 6.8556 - mae: 2.0728 - val_loss: 5.4203 - val_mse: 5.4203 - val_mae: 1.9114\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 1.89931\n",
      "Epoch 102/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6850 - mse: 5.6850 - mae: 1.8993 - val_loss: 5.3819 - val_mse: 5.3819 - val_mae: 1.9004\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 1.89931\n",
      "Epoch 103/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9092 - mse: 5.9092 - mae: 1.9168 - val_loss: 5.3725 - val_mse: 5.3725 - val_mae: 1.8992\n",
      "\n",
      "Epoch 00103: val_mae improved from 1.89931 to 1.89923, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 104/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2026 - mse: 6.2026 - mae: 1.9536 - val_loss: 5.3441 - val_mse: 5.3441 - val_mae: 1.8911\n",
      "\n",
      "Epoch 00104: val_mae improved from 1.89923 to 1.89107, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 105/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1768 - mse: 6.1768 - mae: 1.9689 - val_loss: 5.3480 - val_mse: 5.3480 - val_mae: 1.8949\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 1.89107\n",
      "Epoch 106/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 6.5186 - mse: 6.5186 - mae: 2.0164 - val_loss: 5.3578 - val_mse: 5.3578 - val_mae: 1.9001\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 1.89107\n",
      "Epoch 107/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.2278 - mse: 7.2278 - mae: 2.0956 - val_loss: 5.3255 - val_mse: 5.3255 - val_mae: 1.8913\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 1.89107\n",
      "Epoch 108/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0787 - mse: 6.0787 - mae: 1.9243 - val_loss: 5.2888 - val_mse: 5.2888 - val_mae: 1.8801\n",
      "\n",
      "Epoch 00108: val_mae improved from 1.89107 to 1.88014, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 109/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.2434 - mse: 5.2434 - mae: 1.8280 - val_loss: 5.2641 - val_mse: 5.2641 - val_mae: 1.8728\n",
      "\n",
      "Epoch 00109: val_mae improved from 1.88014 to 1.87281, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 110/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0967 - mse: 6.0967 - mae: 1.9053 - val_loss: 5.3043 - val_mse: 5.3043 - val_mae: 1.8902\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 1.87281\n",
      "Epoch 111/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9540 - mse: 4.9540 - mae: 1.7394 - val_loss: 5.2653 - val_mse: 5.2653 - val_mae: 1.8789\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 1.87281\n",
      "Epoch 112/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3669 - mse: 6.3669 - mae: 1.9908 - val_loss: 5.2720 - val_mse: 5.2720 - val_mae: 1.8836\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 1.87281\n",
      "Epoch 113/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5520 - mse: 5.5520 - mae: 1.8777 - val_loss: 5.2231 - val_mse: 5.2231 - val_mae: 1.8685\n",
      "\n",
      "Epoch 00113: val_mae improved from 1.87281 to 1.86847, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 114/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 6.2664 - mse: 6.2664 - mae: 1.9292 - val_loss: 5.2183 - val_mse: 5.2183 - val_mae: 1.8690\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 1.86847\n",
      "Epoch 115/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.4699 - mse: 5.4699 - mae: 1.8707 - val_loss: 5.1903 - val_mse: 5.1903 - val_mae: 1.8606\n",
      "\n",
      "Epoch 00115: val_mae improved from 1.86847 to 1.86062, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 116/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7541 - mse: 5.7541 - mae: 1.9398 - val_loss: 5.1748 - val_mse: 5.1748 - val_mae: 1.8574\n",
      "\n",
      "Epoch 00116: val_mae improved from 1.86062 to 1.85744, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 117/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8792 - mse: 5.8792 - mae: 1.9513 - val_loss: 5.1857 - val_mse: 5.1857 - val_mae: 1.8641\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 1.85744\n",
      "Epoch 118/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.3103 - mse: 5.3103 - mae: 1.8331 - val_loss: 5.1655 - val_mse: 5.1655 - val_mae: 1.8589\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 1.85744\n",
      "Epoch 119/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.7305 - mse: 6.7305 - mae: 1.9900 - val_loss: 5.1440 - val_mse: 5.1440 - val_mae: 1.8537\n",
      "\n",
      "Epoch 00119: val_mae improved from 1.85744 to 1.85365, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 120/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.9914 - mse: 5.9914 - mae: 1.8610 - val_loss: 5.1444 - val_mse: 5.1444 - val_mae: 1.8563\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 1.85365\n",
      "Epoch 121/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7263 - mse: 5.7263 - mae: 1.9111 - val_loss: 5.1465 - val_mse: 5.1465 - val_mae: 1.8590\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 1.85365\n",
      "Epoch 122/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.6303 - mse: 6.6303 - mae: 1.9959 - val_loss: 5.1091 - val_mse: 5.1091 - val_mae: 1.8484\n",
      "\n",
      "Epoch 00122: val_mae improved from 1.85365 to 1.84843, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 123/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6716 - mse: 5.6716 - mae: 1.8987 - val_loss: 5.0871 - val_mse: 5.0871 - val_mae: 1.8426\n",
      "\n",
      "Epoch 00123: val_mae improved from 1.84843 to 1.84257, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 124/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8470 - mse: 5.8470 - mae: 1.9011 - val_loss: 5.0844 - val_mse: 5.0844 - val_mae: 1.8436\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 1.84257\n",
      "Epoch 125/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8663 - mse: 5.8663 - mae: 1.9364 - val_loss: 5.0875 - val_mse: 5.0875 - val_mae: 1.8467\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 1.84257\n",
      "Epoch 126/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2028 - mse: 6.2028 - mae: 1.9831 - val_loss: 5.0775 - val_mse: 5.0775 - val_mae: 1.8455\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 1.84257\n",
      "Epoch 127/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8397 - mse: 5.8397 - mae: 1.9350 - val_loss: 5.0323 - val_mse: 5.0323 - val_mae: 1.8313\n",
      "\n",
      "Epoch 00127: val_mae improved from 1.84257 to 1.83129, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 128/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2908 - mse: 5.2908 - mae: 1.8254 - val_loss: 5.0326 - val_mse: 5.0326 - val_mae: 1.8341\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 1.83129\n",
      "Epoch 129/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6285 - mse: 5.6285 - mae: 1.9059 - val_loss: 5.0319 - val_mse: 5.0319 - val_mae: 1.8358\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 1.83129\n",
      "Epoch 130/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1529 - mse: 5.1529 - mae: 1.8085 - val_loss: 5.0028 - val_mse: 5.0028 - val_mae: 1.8277\n",
      "\n",
      "Epoch 00130: val_mae improved from 1.83129 to 1.82772, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 131/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4836 - mse: 5.4836 - mae: 1.8570 - val_loss: 4.9913 - val_mse: 4.9913 - val_mae: 1.8259\n",
      "\n",
      "Epoch 00131: val_mae improved from 1.82772 to 1.82591, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 132/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.8468 - mse: 5.8468 - mae: 1.9195 - val_loss: 5.0213 - val_mse: 5.0213 - val_mae: 1.8375\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 1.82591\n",
      "Epoch 133/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9185 - mse: 5.9185 - mae: 1.9175 - val_loss: 4.9613 - val_mse: 4.9613 - val_mae: 1.8198\n",
      "\n",
      "Epoch 00133: val_mae improved from 1.82591 to 1.81980, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 134/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8072 - mse: 5.8072 - mae: 1.8643 - val_loss: 4.9624 - val_mse: 4.9624 - val_mae: 1.8224\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 1.81980\n",
      "Epoch 135/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5978 - mse: 5.5978 - mae: 1.8779 - val_loss: 4.9332 - val_mse: 4.9332 - val_mae: 1.8140\n",
      "\n",
      "Epoch 00135: val_mae improved from 1.81980 to 1.81402, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 136/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6518 - mse: 5.6518 - mae: 1.8434 - val_loss: 4.9310 - val_mse: 4.9310 - val_mae: 1.8154\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 1.81402\n",
      "Epoch 137/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1746 - mse: 5.1746 - mae: 1.8411 - val_loss: 4.9109 - val_mse: 4.9109 - val_mae: 1.8100\n",
      "\n",
      "Epoch 00137: val_mae improved from 1.81402 to 1.81003, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 138/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1347 - mse: 6.1347 - mae: 1.9110 - val_loss: 4.9362 - val_mse: 4.9362 - val_mae: 1.8201\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 1.81003\n",
      "Epoch 139/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2026 - mse: 6.2026 - mae: 1.9221 - val_loss: 4.8977 - val_mse: 4.8977 - val_mae: 1.8096\n",
      "\n",
      "Epoch 00139: val_mae improved from 1.81003 to 1.80957, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 140/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4427 - mse: 5.4427 - mae: 1.8603 - val_loss: 4.8626 - val_mse: 4.8626 - val_mae: 1.7987\n",
      "\n",
      "Epoch 00140: val_mae improved from 1.80957 to 1.79872, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 141/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4503 - mse: 5.4503 - mae: 1.8217 - val_loss: 4.8663 - val_mse: 4.8663 - val_mae: 1.8027\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 1.79872\n",
      "Epoch 142/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6035 - mse: 5.6035 - mae: 1.8942 - val_loss: 4.8309 - val_mse: 4.8309 - val_mae: 1.7914\n",
      "\n",
      "Epoch 00142: val_mae improved from 1.79872 to 1.79135, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 143/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.3588 - mse: 5.3588 - mae: 1.8291 - val_loss: 4.8230 - val_mse: 4.8230 - val_mae: 1.7912\n",
      "\n",
      "Epoch 00143: val_mae improved from 1.79135 to 1.79118, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 144/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9001 - mse: 5.9001 - mae: 1.8364 - val_loss: 4.8280 - val_mse: 4.8280 - val_mae: 1.7956\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 1.79118\n",
      "Epoch 145/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8589 - mse: 4.8589 - mae: 1.7381 - val_loss: 4.8168 - val_mse: 4.8168 - val_mae: 1.7939\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 1.79118\n",
      "Epoch 146/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3727 - mse: 5.3727 - mae: 1.8073 - val_loss: 4.7891 - val_mse: 4.7891 - val_mae: 1.7856\n",
      "\n",
      "Epoch 00146: val_mae improved from 1.79118 to 1.78558, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 147/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6086 - mse: 5.6086 - mae: 1.8630 - val_loss: 4.7673 - val_mse: 4.7673 - val_mae: 1.7794\n",
      "\n",
      "Epoch 00147: val_mae improved from 1.78558 to 1.77945, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 148/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5399 - mse: 5.5399 - mae: 1.8630 - val_loss: 4.7573 - val_mse: 4.7573 - val_mae: 1.7781\n",
      "\n",
      "Epoch 00148: val_mae improved from 1.77945 to 1.77809, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 149/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3082 - mse: 5.3082 - mae: 1.8560 - val_loss: 4.7635 - val_mse: 4.7635 - val_mae: 1.7827\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 1.77809\n",
      "Epoch 150/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7850 - mse: 4.7850 - mae: 1.7317 - val_loss: 4.7415 - val_mse: 4.7415 - val_mae: 1.7766\n",
      "\n",
      "Epoch 00150: val_mae improved from 1.77809 to 1.77656, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 151/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2685 - mse: 5.2685 - mae: 1.7677 - val_loss: 4.7331 - val_mse: 4.7331 - val_mae: 1.7756\n",
      "\n",
      "Epoch 00151: val_mae improved from 1.77656 to 1.77557, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 152/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6161 - mse: 5.6161 - mae: 1.8735 - val_loss: 4.7440 - val_mse: 4.7440 - val_mae: 1.7815\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 1.77557\n",
      "Epoch 153/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.1945 - mse: 5.1945 - mae: 1.8099 - val_loss: 4.7245 - val_mse: 4.7245 - val_mae: 1.7766\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 1.77557\n",
      "Epoch 154/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.7901 - mse: 5.7901 - mae: 1.8848 - val_loss: 4.6898 - val_mse: 4.6898 - val_mae: 1.7660\n",
      "\n",
      "Epoch 00154: val_mae improved from 1.77557 to 1.76603, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 155/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0272 - mse: 5.0272 - mae: 1.7584 - val_loss: 4.6754 - val_mse: 4.6754 - val_mae: 1.7628\n",
      "\n",
      "Epoch 00155: val_mae improved from 1.76603 to 1.76279, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 156/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3770 - mse: 5.3770 - mae: 1.8014 - val_loss: 4.6807 - val_mse: 4.6807 - val_mae: 1.7668\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 1.76279\n",
      "Epoch 157/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1556 - mse: 5.1556 - mae: 1.8219 - val_loss: 4.6830 - val_mse: 4.6830 - val_mae: 1.7693\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 1.76279\n",
      "Epoch 158/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3926 - mse: 5.3926 - mae: 1.8358 - val_loss: 4.6640 - val_mse: 4.6640 - val_mae: 1.7644\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 1.76279\n",
      "Epoch 159/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3631 - mse: 5.3631 - mae: 1.8020 - val_loss: 4.6427 - val_mse: 4.6427 - val_mae: 1.7590\n",
      "\n",
      "Epoch 00159: val_mae improved from 1.76279 to 1.75896, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 160/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2232 - mse: 5.2232 - mae: 1.8159 - val_loss: 4.6205 - val_mse: 4.6205 - val_mae: 1.7526\n",
      "\n",
      "Epoch 00160: val_mae improved from 1.75896 to 1.75264, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 161/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.8342 - mse: 5.8342 - mae: 1.9157 - val_loss: 4.6123 - val_mse: 4.6123 - val_mae: 1.7516\n",
      "\n",
      "Epoch 00161: val_mae improved from 1.75264 to 1.75158, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 162/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5930 - mse: 5.5930 - mae: 1.8483 - val_loss: 4.5946 - val_mse: 4.5946 - val_mae: 1.7467\n",
      "\n",
      "Epoch 00162: val_mae improved from 1.75158 to 1.74673, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 163/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9609 - mse: 5.9609 - mae: 1.8939 - val_loss: 4.5875 - val_mse: 4.5875 - val_mae: 1.7459\n",
      "\n",
      "Epoch 00163: val_mae improved from 1.74673 to 1.74594, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 164/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6899 - mse: 5.6899 - mae: 1.7955 - val_loss: 4.5880 - val_mse: 4.5880 - val_mae: 1.7480\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 1.74594\n",
      "Epoch 165/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.0221 - mse: 6.0221 - mae: 1.8640 - val_loss: 4.5935 - val_mse: 4.5935 - val_mae: 1.7519\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 1.74594\n",
      "Epoch 166/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5118 - mse: 5.5118 - mae: 1.8585 - val_loss: 4.5499 - val_mse: 4.5499 - val_mae: 1.7378\n",
      "\n",
      "Epoch 00166: val_mae improved from 1.74594 to 1.73779, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 167/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9605 - mse: 5.9605 - mae: 1.8764 - val_loss: 4.5427 - val_mse: 4.5427 - val_mae: 1.7369\n",
      "\n",
      "Epoch 00167: val_mae improved from 1.73779 to 1.73694, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 168/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9697 - mse: 4.9697 - mae: 1.7740 - val_loss: 4.5299 - val_mse: 4.5299 - val_mae: 1.7338\n",
      "\n",
      "Epoch 00168: val_mae improved from 1.73694 to 1.73382, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 169/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2249 - mse: 5.2249 - mae: 1.7749 - val_loss: 4.5334 - val_mse: 4.5334 - val_mae: 1.7371\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 1.73382\n",
      "Epoch 170/1000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.7311 - mse: 4.7311 - mae: 1.6885 - val_loss: 4.5105 - val_mse: 4.5105 - val_mae: 1.7301\n",
      "\n",
      "Epoch 00170: val_mae improved from 1.73382 to 1.73007, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 171/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8868 - mse: 4.8868 - mae: 1.7445 - val_loss: 4.4926 - val_mse: 4.4926 - val_mae: 1.7244\n",
      "\n",
      "Epoch 00171: val_mae improved from 1.73007 to 1.72443, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 172/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3993 - mse: 5.3993 - mae: 1.8279 - val_loss: 4.4900 - val_mse: 4.4900 - val_mae: 1.7256\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 1.72443\n",
      "Epoch 173/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0887 - mse: 6.0887 - mae: 1.8624 - val_loss: 4.5118 - val_mse: 4.5118 - val_mae: 1.7352\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 1.72443\n",
      "Epoch 174/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5792 - mse: 5.5792 - mae: 1.8732 - val_loss: 4.4695 - val_mse: 4.4695 - val_mae: 1.7215\n",
      "\n",
      "Epoch 00174: val_mae improved from 1.72443 to 1.72146, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 175/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4907 - mse: 5.4907 - mae: 1.8301 - val_loss: 4.4688 - val_mse: 4.4688 - val_mae: 1.7232\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 1.72146\n",
      "Epoch 176/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2346 - mse: 5.2346 - mae: 1.8169 - val_loss: 4.4493 - val_mse: 4.4493 - val_mae: 1.7173\n",
      "\n",
      "Epoch 00176: val_mae improved from 1.72146 to 1.71728, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 177/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1678 - mse: 5.1678 - mae: 1.7908 - val_loss: 4.4483 - val_mse: 4.4483 - val_mae: 1.7187\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 1.71728\n",
      "Epoch 178/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.5099 - mse: 5.5099 - mae: 1.8697 - val_loss: 4.4405 - val_mse: 4.4405 - val_mae: 1.7176\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 1.71728\n",
      "Epoch 179/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8178 - mse: 4.8178 - mae: 1.7067 - val_loss: 4.4385 - val_mse: 4.4385 - val_mae: 1.7181\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 1.71728\n",
      "Epoch 180/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8461 - mse: 5.8461 - mae: 1.8562 - val_loss: 4.4184 - val_mse: 4.4184 - val_mae: 1.7120\n",
      "\n",
      "Epoch 00180: val_mae improved from 1.71728 to 1.71202, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 181/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0500 - mse: 5.0500 - mae: 1.7203 - val_loss: 4.4058 - val_mse: 4.4058 - val_mae: 1.7084\n",
      "\n",
      "Epoch 00181: val_mae improved from 1.71202 to 1.70844, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 182/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8506 - mse: 4.8506 - mae: 1.7382 - val_loss: 4.4196 - val_mse: 4.4196 - val_mae: 1.7147\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 1.70844\n",
      "Epoch 183/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0903 - mse: 5.0903 - mae: 1.8056 - val_loss: 4.4014 - val_mse: 4.4014 - val_mae: 1.7101\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 1.70844\n",
      "Epoch 184/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5016 - mse: 5.5016 - mae: 1.8176 - val_loss: 4.3881 - val_mse: 4.3881 - val_mae: 1.7067\n",
      "\n",
      "Epoch 00184: val_mae improved from 1.70844 to 1.70671, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 185/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9504 - mse: 4.9504 - mae: 1.7905 - val_loss: 4.3634 - val_mse: 4.3634 - val_mae: 1.6987\n",
      "\n",
      "Epoch 00185: val_mae improved from 1.70671 to 1.69875, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 186/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3443 - mse: 5.3443 - mae: 1.8462 - val_loss: 4.3694 - val_mse: 4.3694 - val_mae: 1.7025\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 1.69875\n",
      "Epoch 187/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8466 - mse: 4.8466 - mae: 1.7162 - val_loss: 4.4036 - val_mse: 4.4036 - val_mae: 1.7143\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 1.69875\n",
      "Epoch 188/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6107 - mse: 5.6107 - mae: 1.8025 - val_loss: 4.3423 - val_mse: 4.3423 - val_mae: 1.6951\n",
      "\n",
      "Epoch 00188: val_mae improved from 1.69875 to 1.69512, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 189/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3567 - mse: 5.3567 - mae: 1.7356 - val_loss: 4.3309 - val_mse: 4.3309 - val_mae: 1.6922\n",
      "\n",
      "Epoch 00189: val_mae improved from 1.69512 to 1.69215, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 190/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5026 - mse: 5.5026 - mae: 1.8569 - val_loss: 4.3166 - val_mse: 4.3166 - val_mae: 1.6881\n",
      "\n",
      "Epoch 00190: val_mae improved from 1.69215 to 1.68811, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 191/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3470 - mse: 5.3470 - mae: 1.8184 - val_loss: 4.3088 - val_mse: 4.3088 - val_mae: 1.6865\n",
      "\n",
      "Epoch 00191: val_mae improved from 1.68811 to 1.68649, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 192/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8954 - mse: 4.8954 - mae: 1.7001 - val_loss: 4.2965 - val_mse: 4.2965 - val_mae: 1.6831\n",
      "\n",
      "Epoch 00192: val_mae improved from 1.68649 to 1.68308, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 193/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9021 - mse: 4.9021 - mae: 1.7289 - val_loss: 4.3047 - val_mse: 4.3047 - val_mae: 1.6871\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 1.68308\n",
      "Epoch 194/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8443 - mse: 4.8443 - mae: 1.7392 - val_loss: 4.3006 - val_mse: 4.3006 - val_mae: 1.6866\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 1.68308\n",
      "Epoch 195/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6786 - mse: 5.6786 - mae: 1.8495 - val_loss: 4.2966 - val_mse: 4.2966 - val_mae: 1.6860\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 1.68308\n",
      "Epoch 196/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6488 - mse: 4.6488 - mae: 1.7160 - val_loss: 4.2907 - val_mse: 4.2907 - val_mae: 1.6849\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 1.68308\n",
      "Epoch 197/1000\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.8736 - mse: 4.8736 - mae: 1.7333 - val_loss: 4.2727 - val_mse: 4.2727 - val_mae: 1.6798\n",
      "\n",
      "Epoch 00197: val_mae improved from 1.68308 to 1.67975, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 198/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.4320 - mse: 5.4320 - mae: 1.8639 - val_loss: 4.2861 - val_mse: 4.2861 - val_mae: 1.6850\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 1.67975\n",
      "Epoch 199/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1798 - mse: 5.1798 - mae: 1.7941 - val_loss: 4.2551 - val_mse: 4.2551 - val_mae: 1.6759\n",
      "\n",
      "Epoch 00199: val_mae improved from 1.67975 to 1.67595, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 200/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4957 - mse: 5.4957 - mae: 1.7914 - val_loss: 4.2654 - val_mse: 4.2654 - val_mae: 1.6800\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 1.67595\n",
      "Epoch 201/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0212 - mse: 5.0212 - mae: 1.7690 - val_loss: 4.2405 - val_mse: 4.2405 - val_mae: 1.6729\n",
      "\n",
      "Epoch 00201: val_mae improved from 1.67595 to 1.67293, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 202/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8448 - mse: 5.8448 - mae: 1.8989 - val_loss: 4.2346 - val_mse: 4.2346 - val_mae: 1.6718\n",
      "\n",
      "Epoch 00202: val_mae improved from 1.67293 to 1.67181, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 203/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7049 - mse: 5.7049 - mae: 1.8049 - val_loss: 4.2343 - val_mse: 4.2343 - val_mae: 1.6724\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 1.67181\n",
      "Epoch 204/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6878 - mse: 4.6878 - mae: 1.7099 - val_loss: 4.2348 - val_mse: 4.2348 - val_mae: 1.6731\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 1.67181\n",
      "Epoch 205/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5332 - mse: 5.5332 - mae: 1.8336 - val_loss: 4.2096 - val_mse: 4.2096 - val_mae: 1.6661\n",
      "\n",
      "Epoch 00205: val_mae improved from 1.67181 to 1.66611, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 206/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2862 - mse: 5.2862 - mae: 1.7988 - val_loss: 4.1943 - val_mse: 4.1943 - val_mae: 1.6619\n",
      "\n",
      "Epoch 00206: val_mae improved from 1.66611 to 1.66194, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 207/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0113 - mse: 5.0113 - mae: 1.7335 - val_loss: 4.1855 - val_mse: 4.1855 - val_mae: 1.6598\n",
      "\n",
      "Epoch 00207: val_mae improved from 1.66194 to 1.65976, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 208/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.1444 - mse: 5.1444 - mae: 1.7265 - val_loss: 4.1758 - val_mse: 4.1758 - val_mae: 1.6572\n",
      "\n",
      "Epoch 00208: val_mae improved from 1.65976 to 1.65719, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 209/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9308 - mse: 4.9308 - mae: 1.6846 - val_loss: 4.1742 - val_mse: 4.1742 - val_mae: 1.6576\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 1.65719\n",
      "Epoch 210/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.6806 - mse: 4.6806 - mae: 1.7043 - val_loss: 4.1576 - val_mse: 4.1576 - val_mae: 1.6526\n",
      "\n",
      "Epoch 00210: val_mae improved from 1.65719 to 1.65263, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 211/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6226 - mse: 5.6226 - mae: 1.8483 - val_loss: 4.1704 - val_mse: 4.1704 - val_mae: 1.6576\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 1.65263\n",
      "Epoch 212/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4087 - mse: 5.4087 - mae: 1.8385 - val_loss: 4.1701 - val_mse: 4.1701 - val_mae: 1.6580\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 1.65263\n",
      "Epoch 213/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3716 - mse: 5.3716 - mae: 1.7570 - val_loss: 4.1511 - val_mse: 4.1511 - val_mae: 1.6525\n",
      "\n",
      "Epoch 00213: val_mae improved from 1.65263 to 1.65252, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 214/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4510 - mse: 5.4510 - mae: 1.8076 - val_loss: 4.1314 - val_mse: 4.1314 - val_mae: 1.6465\n",
      "\n",
      "Epoch 00214: val_mae improved from 1.65252 to 1.64645, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 215/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9121 - mse: 4.9121 - mae: 1.7381 - val_loss: 4.1366 - val_mse: 4.1366 - val_mae: 1.6491\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 1.64645\n",
      "Epoch 216/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2459 - mse: 5.2459 - mae: 1.7634 - val_loss: 4.1381 - val_mse: 4.1381 - val_mae: 1.6502\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 1.64645\n",
      "Epoch 217/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0901 - mse: 5.0901 - mae: 1.7121 - val_loss: 4.1451 - val_mse: 4.1451 - val_mae: 1.6528\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 1.64645\n",
      "Epoch 218/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2550 - mse: 5.2550 - mae: 1.8285 - val_loss: 4.1161 - val_mse: 4.1161 - val_mae: 1.6441\n",
      "\n",
      "Epoch 00218: val_mae improved from 1.64645 to 1.64412, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 219/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6969 - mse: 4.6969 - mae: 1.6768 - val_loss: 4.1225 - val_mse: 4.1225 - val_mae: 1.6470\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 1.64412\n",
      "Epoch 220/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6199 - mse: 4.6199 - mae: 1.6712 - val_loss: 4.1141 - val_mse: 4.1141 - val_mae: 1.6447\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 1.64412\n",
      "Epoch 221/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3304 - mse: 5.3304 - mae: 1.7657 - val_loss: 4.1275 - val_mse: 4.1275 - val_mae: 1.6490\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 1.64412\n",
      "Epoch 222/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9714 - mse: 4.9714 - mae: 1.6793 - val_loss: 4.1067 - val_mse: 4.1067 - val_mae: 1.6434\n",
      "\n",
      "Epoch 00222: val_mae improved from 1.64412 to 1.64341, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 223/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9166 - mse: 4.9166 - mae: 1.7310 - val_loss: 4.0984 - val_mse: 4.0984 - val_mae: 1.6413\n",
      "\n",
      "Epoch 00223: val_mae improved from 1.64341 to 1.64134, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 224/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.8266 - mse: 4.8266 - mae: 1.7171 - val_loss: 4.0786 - val_mse: 4.0786 - val_mae: 1.6352\n",
      "\n",
      "Epoch 00224: val_mae improved from 1.64134 to 1.63524, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 225/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5529 - mse: 5.5529 - mae: 1.8426 - val_loss: 4.0782 - val_mse: 4.0782 - val_mae: 1.6358\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 1.63524\n",
      "Epoch 226/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8711 - mse: 4.8711 - mae: 1.7475 - val_loss: 4.0879 - val_mse: 4.0879 - val_mae: 1.6393\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 1.63524\n",
      "Epoch 227/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4279 - mse: 5.4279 - mae: 1.8097 - val_loss: 4.1017 - val_mse: 4.1017 - val_mae: 1.6431\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 1.63524\n",
      "Epoch 228/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5053 - mse: 5.5053 - mae: 1.7661 - val_loss: 4.0602 - val_mse: 4.0602 - val_mae: 1.6310\n",
      "\n",
      "Epoch 00228: val_mae improved from 1.63524 to 1.63104, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 229/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6969 - mse: 4.6969 - mae: 1.7016 - val_loss: 4.0609 - val_mse: 4.0609 - val_mae: 1.6319\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 1.63104\n",
      "Epoch 230/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9101 - mse: 4.9101 - mae: 1.6932 - val_loss: 4.0612 - val_mse: 4.0612 - val_mae: 1.6325\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 1.63104\n",
      "Epoch 231/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9203 - mse: 4.9203 - mae: 1.7236 - val_loss: 4.0639 - val_mse: 4.0639 - val_mae: 1.6335\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 1.63104\n",
      "Epoch 232/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9120 - mse: 4.9121 - mae: 1.7139 - val_loss: 4.0488 - val_mse: 4.0488 - val_mae: 1.6290\n",
      "\n",
      "Epoch 00232: val_mae improved from 1.63104 to 1.62900, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 233/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0524 - mse: 5.0524 - mae: 1.6986 - val_loss: 4.0424 - val_mse: 4.0424 - val_mae: 1.6272\n",
      "\n",
      "Epoch 00233: val_mae improved from 1.62900 to 1.62720, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 234/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6496 - mse: 5.6496 - mae: 1.8313 - val_loss: 4.0296 - val_mse: 4.0296 - val_mae: 1.6232\n",
      "\n",
      "Epoch 00234: val_mae improved from 1.62720 to 1.62317, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 235/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9746 - mse: 4.9746 - mae: 1.7131 - val_loss: 4.0540 - val_mse: 4.0540 - val_mae: 1.6316\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 1.62317\n",
      "Epoch 236/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3751 - mse: 5.3751 - mae: 1.7922 - val_loss: 4.0294 - val_mse: 4.0294 - val_mae: 1.6244\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 1.62317\n",
      "Epoch 237/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6846 - mse: 4.6846 - mae: 1.6654 - val_loss: 4.0404 - val_mse: 4.0404 - val_mae: 1.6280\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 1.62317\n",
      "Epoch 238/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1649 - mse: 5.1649 - mae: 1.7814 - val_loss: 4.0192 - val_mse: 4.0192 - val_mae: 1.6220\n",
      "\n",
      "Epoch 00238: val_mae improved from 1.62317 to 1.62196, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 239/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8241 - mse: 4.8241 - mae: 1.7154 - val_loss: 4.0158 - val_mse: 4.0158 - val_mae: 1.6213\n",
      "\n",
      "Epoch 00239: val_mae improved from 1.62196 to 1.62127, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 240/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1531 - mse: 5.1531 - mae: 1.8059 - val_loss: 4.0201 - val_mse: 4.0201 - val_mae: 1.6230\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 1.62127\n",
      "Epoch 241/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6914 - mse: 4.6914 - mae: 1.6681 - val_loss: 4.0036 - val_mse: 4.0036 - val_mae: 1.6182\n",
      "\n",
      "Epoch 00241: val_mae improved from 1.62127 to 1.61818, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 242/1000\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.3975 - mse: 5.3975 - mae: 1.8012 - val_loss: 3.9869 - val_mse: 3.9869 - val_mae: 1.6126\n",
      "\n",
      "Epoch 00242: val_mae improved from 1.61818 to 1.61261, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 243/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.5948 - mse: 4.5948 - mae: 1.6611 - val_loss: 3.9824 - val_mse: 3.9824 - val_mae: 1.6116\n",
      "\n",
      "Epoch 00243: val_mae improved from 1.61261 to 1.61160, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 244/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8805 - mse: 4.8805 - mae: 1.6942 - val_loss: 4.0037 - val_mse: 4.0037 - val_mae: 1.6189\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 1.61160\n",
      "Epoch 245/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9322 - mse: 4.9322 - mae: 1.7042 - val_loss: 4.0020 - val_mse: 4.0020 - val_mae: 1.6187\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 1.61160\n",
      "Epoch 246/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0182 - mse: 5.0182 - mae: 1.7244 - val_loss: 3.9806 - val_mse: 3.9806 - val_mae: 1.6124\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 1.61160\n",
      "Epoch 247/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1492 - mse: 5.1492 - mae: 1.7745 - val_loss: 3.9626 - val_mse: 3.9626 - val_mae: 1.6062\n",
      "\n",
      "Epoch 00247: val_mae improved from 1.61160 to 1.60615, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 248/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7796 - mse: 5.7796 - mae: 1.8102 - val_loss: 3.9658 - val_mse: 3.9658 - val_mae: 1.6078\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 1.60615\n",
      "Epoch 249/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6553 - mse: 4.6553 - mae: 1.6688 - val_loss: 3.9594 - val_mse: 3.9594 - val_mae: 1.6059\n",
      "\n",
      "Epoch 00249: val_mae improved from 1.60615 to 1.60594, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 250/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8050 - mse: 4.8050 - mae: 1.6979 - val_loss: 3.9814 - val_mse: 3.9814 - val_mae: 1.6131\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 1.60594\n",
      "Epoch 251/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4124 - mse: 5.4124 - mae: 1.8092 - val_loss: 3.9756 - val_mse: 3.9756 - val_mae: 1.6115\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 1.60594\n",
      "Epoch 252/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6219 - mse: 4.6219 - mae: 1.6727 - val_loss: 3.9647 - val_mse: 3.9647 - val_mae: 1.6086\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 1.60594\n",
      "Epoch 253/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7990 - mse: 4.7990 - mae: 1.7034 - val_loss: 3.9936 - val_mse: 3.9936 - val_mae: 1.6160\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 1.60594\n",
      "Epoch 254/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7182 - mse: 4.7182 - mae: 1.6896 - val_loss: 3.9634 - val_mse: 3.9634 - val_mae: 1.6083\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 1.60594\n",
      "Epoch 255/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7514 - mse: 4.7514 - mae: 1.6971 - val_loss: 3.9627 - val_mse: 3.9627 - val_mae: 1.6080\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 1.60594\n",
      "Epoch 256/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0487 - mse: 5.0487 - mae: 1.7460 - val_loss: 3.9411 - val_mse: 3.9411 - val_mae: 1.6016\n",
      "\n",
      "Epoch 00256: val_mae improved from 1.60594 to 1.60163, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 257/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7351 - mse: 4.7351 - mae: 1.7183 - val_loss: 3.9454 - val_mse: 3.9454 - val_mae: 1.6031\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 1.60163\n",
      "Epoch 258/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0853 - mse: 5.0853 - mae: 1.7921 - val_loss: 3.9349 - val_mse: 3.9349 - val_mae: 1.6001\n",
      "\n",
      "Epoch 00258: val_mae improved from 1.60163 to 1.60011, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 259/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8056 - mse: 4.8056 - mae: 1.7028 - val_loss: 3.9255 - val_mse: 3.9255 - val_mae: 1.5974\n",
      "\n",
      "Epoch 00259: val_mae improved from 1.60011 to 1.59740, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 260/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7529 - mse: 5.7529 - mae: 1.8146 - val_loss: 3.9373 - val_mse: 3.9373 - val_mae: 1.6009\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 1.59740\n",
      "Epoch 261/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4022 - mse: 4.4022 - mae: 1.6058 - val_loss: 3.9342 - val_mse: 3.9342 - val_mae: 1.6001\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 1.59740\n",
      "Epoch 262/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4245 - mse: 5.4245 - mae: 1.8153 - val_loss: 3.9286 - val_mse: 3.9286 - val_mae: 1.5986\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 1.59740\n",
      "Epoch 263/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7425 - mse: 4.7425 - mae: 1.6217 - val_loss: 3.9253 - val_mse: 3.9253 - val_mae: 1.5976\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 1.59740\n",
      "Epoch 264/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6536 - mse: 4.6536 - mae: 1.6814 - val_loss: 3.9078 - val_mse: 3.9078 - val_mae: 1.5925\n",
      "\n",
      "Epoch 00264: val_mae improved from 1.59740 to 1.59249, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 265/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6181 - mse: 5.6181 - mae: 1.8165 - val_loss: 3.9249 - val_mse: 3.9249 - val_mae: 1.5974\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 1.59249\n",
      "Epoch 266/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9142 - mse: 4.9142 - mae: 1.7219 - val_loss: 3.8975 - val_mse: 3.8975 - val_mae: 1.5894\n",
      "\n",
      "Epoch 00266: val_mae improved from 1.59249 to 1.58941, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 267/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3087 - mse: 5.3087 - mae: 1.7356 - val_loss: 3.8950 - val_mse: 3.8950 - val_mae: 1.5888\n",
      "\n",
      "Epoch 00267: val_mae improved from 1.58941 to 1.58875, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 268/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0051 - mse: 5.0051 - mae: 1.6749 - val_loss: 3.8997 - val_mse: 3.8997 - val_mae: 1.5902\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 1.58875\n",
      "Epoch 269/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9470 - mse: 4.9470 - mae: 1.7349 - val_loss: 3.8972 - val_mse: 3.8972 - val_mae: 1.5896\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 1.58875\n",
      "Epoch 270/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9099 - mse: 4.9099 - mae: 1.6776 - val_loss: 3.8919 - val_mse: 3.8919 - val_mae: 1.5880\n",
      "\n",
      "Epoch 00270: val_mae improved from 1.58875 to 1.58804, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 271/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6206 - mse: 4.6206 - mae: 1.6671 - val_loss: 3.8940 - val_mse: 3.8940 - val_mae: 1.5884\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 1.58804\n",
      "Epoch 272/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0088 - mse: 5.0088 - mae: 1.7232 - val_loss: 3.9045 - val_mse: 3.9045 - val_mae: 1.5917\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 1.58804\n",
      "Epoch 273/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1422 - mse: 5.1422 - mae: 1.7686 - val_loss: 3.9015 - val_mse: 3.9015 - val_mae: 1.5909\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 1.58804\n",
      "Epoch 274/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5412 - mse: 5.5412 - mae: 1.7936 - val_loss: 3.8921 - val_mse: 3.8921 - val_mae: 1.5882\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 1.58804\n",
      "Epoch 275/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2142 - mse: 5.2142 - mae: 1.7617 - val_loss: 3.8694 - val_mse: 3.8694 - val_mae: 1.5814\n",
      "\n",
      "Epoch 00275: val_mae improved from 1.58804 to 1.58139, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 276/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5027 - mse: 5.5027 - mae: 1.7717 - val_loss: 3.8693 - val_mse: 3.8693 - val_mae: 1.5813\n",
      "\n",
      "Epoch 00276: val_mae improved from 1.58139 to 1.58130, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 277/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9857 - mse: 4.9857 - mae: 1.7299 - val_loss: 3.8858 - val_mse: 3.8858 - val_mae: 1.5863\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 1.58130\n",
      "Epoch 278/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3198 - mse: 5.3198 - mae: 1.7402 - val_loss: 3.8684 - val_mse: 3.8684 - val_mae: 1.5812\n",
      "\n",
      "Epoch 00278: val_mae improved from 1.58130 to 1.58120, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 279/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2136 - mse: 4.2136 - mae: 1.6112 - val_loss: 3.8766 - val_mse: 3.8766 - val_mae: 1.5838\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 1.58120\n",
      "Epoch 280/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6886 - mse: 4.6886 - mae: 1.7039 - val_loss: 3.8769 - val_mse: 3.8769 - val_mae: 1.5839\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 1.58120\n",
      "Epoch 281/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7383 - mse: 4.7383 - mae: 1.6695 - val_loss: 3.8639 - val_mse: 3.8639 - val_mae: 1.5803\n",
      "\n",
      "Epoch 00281: val_mae improved from 1.58120 to 1.58031, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 282/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7816 - mse: 4.7816 - mae: 1.6998 - val_loss: 3.8564 - val_mse: 3.8564 - val_mae: 1.5781\n",
      "\n",
      "Epoch 00282: val_mae improved from 1.58031 to 1.57809, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 283/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6806 - mse: 5.6806 - mae: 1.8538 - val_loss: 3.8757 - val_mse: 3.8757 - val_mae: 1.5838\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 1.57809\n",
      "Epoch 284/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1272 - mse: 5.1272 - mae: 1.7467 - val_loss: 3.8683 - val_mse: 3.8683 - val_mae: 1.5817\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 1.57809\n",
      "Epoch 285/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6944 - mse: 4.6944 - mae: 1.6636 - val_loss: 3.8496 - val_mse: 3.8496 - val_mae: 1.5764\n",
      "\n",
      "Epoch 00285: val_mae improved from 1.57809 to 1.57642, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 286/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8155 - mse: 4.8155 - mae: 1.6914 - val_loss: 3.8412 - val_mse: 3.8412 - val_mae: 1.5738\n",
      "\n",
      "Epoch 00286: val_mae improved from 1.57642 to 1.57381, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 287/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5734 - mse: 4.5734 - mae: 1.6602 - val_loss: 3.8467 - val_mse: 3.8467 - val_mae: 1.5756\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 1.57381\n",
      "Epoch 288/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7337 - mse: 4.7337 - mae: 1.7156 - val_loss: 3.8469 - val_mse: 3.8469 - val_mae: 1.5756\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 1.57381\n",
      "Epoch 289/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.5840 - mse: 5.5840 - mae: 1.8316 - val_loss: 3.8840 - val_mse: 3.8840 - val_mae: 1.5855\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 1.57381\n",
      "Epoch 290/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4699 - mse: 5.4699 - mae: 1.8069 - val_loss: 3.8388 - val_mse: 3.8388 - val_mae: 1.5735\n",
      "\n",
      "Epoch 00290: val_mae improved from 1.57381 to 1.57347, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 291/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5347 - mse: 5.5347 - mae: 1.7882 - val_loss: 3.8254 - val_mse: 3.8254 - val_mae: 1.5692\n",
      "\n",
      "Epoch 00291: val_mae improved from 1.57347 to 1.56916, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 292/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3947 - mse: 4.3947 - mae: 1.6433 - val_loss: 3.8345 - val_mse: 3.8345 - val_mae: 1.5723\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 1.56916\n",
      "Epoch 293/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2236 - mse: 5.2236 - mae: 1.7375 - val_loss: 3.8261 - val_mse: 3.8261 - val_mae: 1.5698\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 1.56916\n",
      "Epoch 294/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0634 - mse: 5.0634 - mae: 1.6787 - val_loss: 3.8209 - val_mse: 3.8209 - val_mae: 1.5680\n",
      "\n",
      "Epoch 00294: val_mae improved from 1.56916 to 1.56803, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 295/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9328 - mse: 4.9328 - mae: 1.6803 - val_loss: 3.8305 - val_mse: 3.8305 - val_mae: 1.5709\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 1.56803\n",
      "Epoch 296/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2617 - mse: 5.2617 - mae: 1.7551 - val_loss: 3.8411 - val_mse: 3.8411 - val_mae: 1.5737\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 1.56803\n",
      "Epoch 297/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7265 - mse: 4.7265 - mae: 1.6786 - val_loss: 3.8358 - val_mse: 3.8358 - val_mae: 1.5721\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 1.56803\n",
      "Epoch 298/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8172 - mse: 4.8172 - mae: 1.6620 - val_loss: 3.8392 - val_mse: 3.8392 - val_mae: 1.5730\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 1.56803\n",
      "Epoch 299/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8863 - mse: 4.8863 - mae: 1.7197 - val_loss: 3.8456 - val_mse: 3.8456 - val_mae: 1.5748\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 1.56803\n",
      "Epoch 300/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.4529 - mse: 4.4529 - mae: 1.6282 - val_loss: 3.8446 - val_mse: 3.8446 - val_mae: 1.5744\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 1.56803\n",
      "Epoch 301/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4184 - mse: 5.4184 - mae: 1.7906 - val_loss: 3.8276 - val_mse: 3.8276 - val_mae: 1.5698\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 1.56803\n",
      "Epoch 302/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5538 - mse: 4.5538 - mae: 1.6284 - val_loss: 3.8100 - val_mse: 3.8100 - val_mae: 1.5651\n",
      "\n",
      "Epoch 00302: val_mae improved from 1.56803 to 1.56508, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 303/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.0650 - mse: 5.0650 - mae: 1.6995 - val_loss: 3.8022 - val_mse: 3.8022 - val_mae: 1.5622\n",
      "\n",
      "Epoch 00303: val_mae improved from 1.56508 to 1.56225, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 304/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3947 - mse: 4.3947 - mae: 1.5976 - val_loss: 3.8241 - val_mse: 3.8241 - val_mae: 1.5688\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 1.56225\n",
      "Epoch 305/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1135 - mse: 5.1135 - mae: 1.7349 - val_loss: 3.8064 - val_mse: 3.8064 - val_mae: 1.5640\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 1.56225\n",
      "Epoch 306/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8570 - mse: 4.8570 - mae: 1.7089 - val_loss: 3.7998 - val_mse: 3.7998 - val_mae: 1.5618\n",
      "\n",
      "Epoch 00306: val_mae improved from 1.56225 to 1.56185, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 307/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0791 - mse: 5.0791 - mae: 1.7017 - val_loss: 3.8032 - val_mse: 3.8032 - val_mae: 1.5629\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 1.56185\n",
      "Epoch 308/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6206 - mse: 5.6206 - mae: 1.7734 - val_loss: 3.8033 - val_mse: 3.8033 - val_mae: 1.5630\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 1.56185\n",
      "Epoch 309/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1216 - mse: 5.1216 - mae: 1.7347 - val_loss: 3.7922 - val_mse: 3.7922 - val_mae: 1.5594\n",
      "\n",
      "Epoch 00309: val_mae improved from 1.56185 to 1.55935, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 310/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0296 - mse: 5.0296 - mae: 1.7325 - val_loss: 3.7995 - val_mse: 3.7995 - val_mae: 1.5619\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 1.55935\n",
      "Epoch 311/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9063 - mse: 4.9063 - mae: 1.6914 - val_loss: 3.7963 - val_mse: 3.7963 - val_mae: 1.5608\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 1.55935\n",
      "Epoch 312/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9888 - mse: 4.9888 - mae: 1.7355 - val_loss: 3.7963 - val_mse: 3.7963 - val_mae: 1.5609\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 1.55935\n",
      "Epoch 313/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1011 - mse: 5.1011 - mae: 1.7191 - val_loss: 3.7929 - val_mse: 3.7929 - val_mae: 1.5598\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 1.55935\n",
      "Epoch 314/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0727 - mse: 5.0727 - mae: 1.7328 - val_loss: 3.7919 - val_mse: 3.7919 - val_mae: 1.5594\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 1.55935\n",
      "Epoch 315/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3961 - mse: 4.3961 - mae: 1.5842 - val_loss: 3.7900 - val_mse: 3.7900 - val_mae: 1.5590\n",
      "\n",
      "Epoch 00315: val_mae improved from 1.55935 to 1.55897, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 316/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8138 - mse: 4.8138 - mae: 1.6922 - val_loss: 3.8422 - val_mse: 3.8422 - val_mae: 1.5719\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 1.55897\n",
      "Epoch 317/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1124 - mse: 5.1124 - mae: 1.7474 - val_loss: 3.7863 - val_mse: 3.7863 - val_mae: 1.5576\n",
      "\n",
      "Epoch 00317: val_mae improved from 1.55897 to 1.55756, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 318/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2492 - mse: 5.2492 - mae: 1.6751 - val_loss: 3.7927 - val_mse: 3.7927 - val_mae: 1.5595\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 1.55756\n",
      "Epoch 319/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1645 - mse: 5.1645 - mae: 1.7533 - val_loss: 3.8056 - val_mse: 3.8056 - val_mae: 1.5626\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 1.55756\n",
      "Epoch 320/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6242 - mse: 4.6242 - mae: 1.6545 - val_loss: 3.7928 - val_mse: 3.7928 - val_mae: 1.5594\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 1.55756\n",
      "Epoch 321/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7217 - mse: 4.7217 - mae: 1.6863 - val_loss: 3.7779 - val_mse: 3.7779 - val_mae: 1.5547\n",
      "\n",
      "Epoch 00321: val_mae improved from 1.55756 to 1.55475, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 322/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1231 - mse: 5.1231 - mae: 1.6959 - val_loss: 3.7888 - val_mse: 3.7888 - val_mae: 1.5582\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 1.55475\n",
      "Epoch 323/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5474 - mse: 5.5474 - mae: 1.8054 - val_loss: 3.7833 - val_mse: 3.7833 - val_mae: 1.5567\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 1.55475\n",
      "Epoch 324/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6191 - mse: 4.6191 - mae: 1.6909 - val_loss: 3.7886 - val_mse: 3.7886 - val_mae: 1.5581\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 1.55475\n",
      "Epoch 325/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3035 - mse: 5.3035 - mae: 1.7684 - val_loss: 3.7847 - val_mse: 3.7847 - val_mae: 1.5569\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 1.55475\n",
      "Epoch 326/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0979 - mse: 5.0979 - mae: 1.6941 - val_loss: 3.7952 - val_mse: 3.7952 - val_mae: 1.5592\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 1.55475\n",
      "Epoch 327/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5724 - mse: 4.5724 - mae: 1.6132 - val_loss: 3.7637 - val_mse: 3.7637 - val_mae: 1.5500\n",
      "\n",
      "Epoch 00327: val_mae improved from 1.55475 to 1.55004, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 328/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3481 - mse: 5.3481 - mae: 1.7726 - val_loss: 3.7745 - val_mse: 3.7745 - val_mae: 1.5539\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 1.55004\n",
      "Epoch 329/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3707 - mse: 4.3707 - mae: 1.5743 - val_loss: 3.7897 - val_mse: 3.7897 - val_mae: 1.5577\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 1.55004\n",
      "Epoch 330/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6190 - mse: 4.6190 - mae: 1.6667 - val_loss: 3.7662 - val_mse: 3.7662 - val_mae: 1.5515\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 1.55004\n",
      "Epoch 331/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3877 - mse: 4.3877 - mae: 1.5613 - val_loss: 3.7609 - val_mse: 3.7609 - val_mae: 1.5496\n",
      "\n",
      "Epoch 00331: val_mae improved from 1.55004 to 1.54960, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 332/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8127 - mse: 4.8127 - mae: 1.6935 - val_loss: 3.7699 - val_mse: 3.7699 - val_mae: 1.5525\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 1.54960\n",
      "Epoch 333/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7066 - mse: 4.7066 - mae: 1.6737 - val_loss: 3.7765 - val_mse: 3.7765 - val_mae: 1.5541\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 1.54960\n",
      "Epoch 334/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7860 - mse: 4.7860 - mae: 1.6746 - val_loss: 3.7695 - val_mse: 3.7695 - val_mae: 1.5522\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 1.54960\n",
      "Epoch 335/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4726 - mse: 4.4726 - mae: 1.6483 - val_loss: 3.7747 - val_mse: 3.7747 - val_mae: 1.5534\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 1.54960\n",
      "Epoch 336/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5556 - mse: 4.5556 - mae: 1.6233 - val_loss: 3.7811 - val_mse: 3.7811 - val_mae: 1.5546\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 1.54960\n",
      "Epoch 337/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7550 - mse: 4.7550 - mae: 1.6774 - val_loss: 3.7717 - val_mse: 3.7717 - val_mae: 1.5522\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 1.54960\n",
      "Epoch 338/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0489 - mse: 5.0489 - mae: 1.7232 - val_loss: 3.7701 - val_mse: 3.7701 - val_mae: 1.5518\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 1.54960\n",
      "Epoch 339/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0477 - mse: 5.0477 - mae: 1.6961 - val_loss: 3.7751 - val_mse: 3.7751 - val_mae: 1.5527\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 1.54960\n",
      "Epoch 340/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9361 - mse: 4.9361 - mae: 1.6847 - val_loss: 3.7671 - val_mse: 3.7671 - val_mae: 1.5507\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 1.54960\n",
      "Epoch 341/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9300 - mse: 4.9300 - mae: 1.6721 - val_loss: 3.7541 - val_mse: 3.7541 - val_mae: 1.5468\n",
      "\n",
      "Epoch 00341: val_mae improved from 1.54960 to 1.54675, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 342/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.5188 - mse: 4.5188 - mae: 1.6528 - val_loss: 3.7773 - val_mse: 3.7773 - val_mae: 1.5528\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 1.54675\n",
      "Epoch 343/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1619 - mse: 5.1619 - mae: 1.7161 - val_loss: 3.7589 - val_mse: 3.7589 - val_mae: 1.5482\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 1.54675\n",
      "Epoch 344/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4151 - mse: 4.4151 - mae: 1.6387 - val_loss: 3.7598 - val_mse: 3.7598 - val_mae: 1.5484\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 1.54675\n",
      "Epoch 345/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7747 - mse: 4.7747 - mae: 1.6151 - val_loss: 3.7518 - val_mse: 3.7518 - val_mae: 1.5459\n",
      "\n",
      "Epoch 00345: val_mae improved from 1.54675 to 1.54586, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 346/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3250 - mse: 5.3250 - mae: 1.7435 - val_loss: 3.7726 - val_mse: 3.7726 - val_mae: 1.5511\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 1.54586\n",
      "Epoch 347/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1060 - mse: 5.1060 - mae: 1.7461 - val_loss: 3.7570 - val_mse: 3.7570 - val_mae: 1.5472\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 1.54586\n",
      "Epoch 348/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7587 - mse: 4.7587 - mae: 1.6701 - val_loss: 3.7480 - val_mse: 3.7480 - val_mae: 1.5443\n",
      "\n",
      "Epoch 00348: val_mae improved from 1.54586 to 1.54431, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 349/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1626 - mse: 5.1626 - mae: 1.6819 - val_loss: 3.7524 - val_mse: 3.7524 - val_mae: 1.5457\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 1.54431\n",
      "Epoch 350/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0029 - mse: 5.0029 - mae: 1.7170 - val_loss: 3.7618 - val_mse: 3.7618 - val_mae: 1.5479\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 1.54431\n",
      "Epoch 351/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1323 - mse: 5.1323 - mae: 1.6843 - val_loss: 3.7599 - val_mse: 3.7599 - val_mae: 1.5474\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 1.54431\n",
      "Epoch 352/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4711 - mse: 4.4711 - mae: 1.6273 - val_loss: 3.7435 - val_mse: 3.7435 - val_mae: 1.5427\n",
      "\n",
      "Epoch 00352: val_mae improved from 1.54431 to 1.54269, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 353/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9797 - mse: 4.9797 - mae: 1.6673 - val_loss: 3.7577 - val_mse: 3.7577 - val_mae: 1.5468\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 1.54269\n",
      "Epoch 354/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0087 - mse: 5.0087 - mae: 1.6746 - val_loss: 3.7706 - val_mse: 3.7706 - val_mae: 1.5497\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 1.54269\n",
      "Epoch 355/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9250 - mse: 4.9250 - mae: 1.6770 - val_loss: 3.7336 - val_mse: 3.7336 - val_mae: 1.5391\n",
      "\n",
      "Epoch 00355: val_mae improved from 1.54269 to 1.53914, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 356/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.0381 - mse: 5.0381 - mae: 1.7097 - val_loss: 3.7633 - val_mse: 3.7633 - val_mae: 1.5480\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 1.53914\n",
      "Epoch 357/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8061 - mse: 4.8061 - mae: 1.6724 - val_loss: 3.7562 - val_mse: 3.7562 - val_mae: 1.5461\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 1.53914\n",
      "Epoch 358/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6538 - mse: 4.6538 - mae: 1.6889 - val_loss: 3.7334 - val_mse: 3.7334 - val_mae: 1.5391\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 1.53914\n",
      "Epoch 359/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4034 - mse: 4.4034 - mae: 1.6267 - val_loss: 3.7621 - val_mse: 3.7621 - val_mae: 1.5473\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 1.53914\n",
      "Epoch 360/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6817 - mse: 4.6817 - mae: 1.6860 - val_loss: 3.7380 - val_mse: 3.7380 - val_mae: 1.5409\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 1.53914\n",
      "Epoch 361/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6093 - mse: 4.6093 - mae: 1.6508 - val_loss: 3.7423 - val_mse: 3.7423 - val_mae: 1.5423\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 1.53914\n",
      "Epoch 362/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2949 - mse: 5.2949 - mae: 1.7542 - val_loss: 3.7414 - val_mse: 3.7414 - val_mae: 1.5419\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 1.53914\n",
      "Epoch 363/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5301 - mse: 5.5301 - mae: 1.7022 - val_loss: 3.7285 - val_mse: 3.7285 - val_mae: 1.5375\n",
      "\n",
      "Epoch 00363: val_mae improved from 1.53914 to 1.53749, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 364/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9691 - mse: 4.9691 - mae: 1.7519 - val_loss: 3.7335 - val_mse: 3.7335 - val_mae: 1.5394\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 1.53749\n",
      "Epoch 365/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8157 - mse: 4.8157 - mae: 1.6525 - val_loss: 3.7697 - val_mse: 3.7697 - val_mae: 1.5486\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 1.53749\n",
      "Epoch 366/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1515 - mse: 5.1515 - mae: 1.7271 - val_loss: 3.7308 - val_mse: 3.7308 - val_mae: 1.5384\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 1.53749\n",
      "Epoch 367/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2336 - mse: 5.2336 - mae: 1.7309 - val_loss: 3.7450 - val_mse: 3.7450 - val_mae: 1.5424\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 1.53749\n",
      "Epoch 368/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5636 - mse: 5.5636 - mae: 1.7050 - val_loss: 3.7298 - val_mse: 3.7298 - val_mae: 1.5379\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 1.53749\n",
      "Epoch 369/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7928 - mse: 4.7928 - mae: 1.6322 - val_loss: 3.7255 - val_mse: 3.7255 - val_mae: 1.5362\n",
      "\n",
      "Epoch 00369: val_mae improved from 1.53749 to 1.53625, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 370/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.0704 - mse: 5.0704 - mae: 1.7001 - val_loss: 3.7447 - val_mse: 3.7447 - val_mae: 1.5423\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 1.53625\n",
      "Epoch 371/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8070 - mse: 4.8070 - mae: 1.6460 - val_loss: 3.7415 - val_mse: 3.7415 - val_mae: 1.5414\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 1.53625\n",
      "Epoch 372/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3296 - mse: 5.3296 - mae: 1.6961 - val_loss: 3.7286 - val_mse: 3.7286 - val_mae: 1.5375\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 1.53625\n",
      "Epoch 373/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9925 - mse: 4.9925 - mae: 1.6917 - val_loss: 3.7277 - val_mse: 3.7277 - val_mae: 1.5372\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 1.53625\n",
      "Epoch 374/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4580 - mse: 4.4580 - mae: 1.6319 - val_loss: 3.7306 - val_mse: 3.7306 - val_mae: 1.5380\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 1.53625\n",
      "Epoch 375/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4241 - mse: 4.4241 - mae: 1.5799 - val_loss: 3.7364 - val_mse: 3.7364 - val_mae: 1.5396\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 1.53625\n",
      "Epoch 376/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6091 - mse: 4.6091 - mae: 1.6381 - val_loss: 3.7557 - val_mse: 3.7557 - val_mae: 1.5441\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 1.53625\n",
      "Epoch 377/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7657 - mse: 4.7657 - mae: 1.6497 - val_loss: 3.7409 - val_mse: 3.7409 - val_mae: 1.5405\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 1.53625\n",
      "Epoch 378/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7890 - mse: 5.7890 - mae: 1.7631 - val_loss: 3.7506 - val_mse: 3.7506 - val_mae: 1.5428\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 1.53625\n",
      "Epoch 379/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4424 - mse: 5.4424 - mae: 1.7821 - val_loss: 3.7252 - val_mse: 3.7252 - val_mae: 1.5360\n",
      "\n",
      "Epoch 00379: val_mae improved from 1.53625 to 1.53598, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 380/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3244 - mse: 5.3244 - mae: 1.7590 - val_loss: 3.7104 - val_mse: 3.7104 - val_mae: 1.5282\n",
      "\n",
      "Epoch 00380: val_mae improved from 1.53598 to 1.52825, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 381/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5955 - mse: 4.5955 - mae: 1.6482 - val_loss: 3.7280 - val_mse: 3.7280 - val_mae: 1.5369\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 1.52825\n",
      "Epoch 382/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1231 - mse: 5.1231 - mae: 1.7245 - val_loss: 3.7381 - val_mse: 3.7381 - val_mae: 1.5394\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 1.52825\n",
      "Epoch 383/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5710 - mse: 4.5710 - mae: 1.6344 - val_loss: 3.7398 - val_mse: 3.7398 - val_mae: 1.5396\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 1.52825\n",
      "Epoch 384/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8080 - mse: 4.8080 - mae: 1.6650 - val_loss: 3.7214 - val_mse: 3.7214 - val_mae: 1.5346\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 1.52825\n",
      "Epoch 385/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7051 - mse: 4.7051 - mae: 1.6886 - val_loss: 3.7317 - val_mse: 3.7317 - val_mae: 1.5375\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 1.52825\n",
      "Epoch 386/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4385 - mse: 4.4385 - mae: 1.6193 - val_loss: 3.7267 - val_mse: 3.7267 - val_mae: 1.5360\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 1.52825\n",
      "Epoch 387/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2526 - mse: 4.2526 - mae: 1.6200 - val_loss: 3.7163 - val_mse: 3.7163 - val_mae: 1.5327\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 1.52825\n",
      "Epoch 388/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0836 - mse: 5.0836 - mae: 1.6843 - val_loss: 3.7561 - val_mse: 3.7561 - val_mae: 1.5433\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 1.52825\n",
      "Epoch 389/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8423 - mse: 4.8423 - mae: 1.6680 - val_loss: 3.7204 - val_mse: 3.7204 - val_mae: 1.5338\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 1.52825\n",
      "Epoch 390/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8996 - mse: 5.8996 - mae: 1.7869 - val_loss: 3.7179 - val_mse: 3.7179 - val_mae: 1.5328\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 1.52825\n",
      "Epoch 391/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6125 - mse: 4.6125 - mae: 1.6639 - val_loss: 3.7088 - val_mse: 3.7088 - val_mae: 1.5296\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 1.52825\n",
      "Epoch 392/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6223 - mse: 4.6223 - mae: 1.6589 - val_loss: 3.7395 - val_mse: 3.7395 - val_mae: 1.5387\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 1.52825\n",
      "Epoch 393/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7768 - mse: 4.7768 - mae: 1.6802 - val_loss: 3.7229 - val_mse: 3.7229 - val_mae: 1.5345\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 1.52825\n",
      "Epoch 394/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.6085 - mse: 4.6085 - mae: 1.6566 - val_loss: 3.7305 - val_mse: 3.7305 - val_mae: 1.5363\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 1.52825\n",
      "Epoch 395/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8660 - mse: 4.8660 - mae: 1.7062 - val_loss: 3.7172 - val_mse: 3.7172 - val_mae: 1.5328\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 1.52825\n",
      "Epoch 396/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8158 - mse: 4.8158 - mae: 1.6595 - val_loss: 3.7239 - val_mse: 3.7239 - val_mae: 1.5346\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 1.52825\n",
      "Epoch 397/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7479 - mse: 4.7479 - mae: 1.6526 - val_loss: 3.7109 - val_mse: 3.7109 - val_mae: 1.5308\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 1.52825\n",
      "Epoch 398/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9055 - mse: 4.9055 - mae: 1.6643 - val_loss: 3.7055 - val_mse: 3.7055 - val_mae: 1.5288\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 1.52825\n",
      "Epoch 399/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4142 - mse: 5.4142 - mae: 1.7762 - val_loss: 3.7238 - val_mse: 3.7238 - val_mae: 1.5341\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 1.52825\n",
      "Epoch 400/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1451 - mse: 5.1451 - mae: 1.6735 - val_loss: 3.7058 - val_mse: 3.7058 - val_mae: 1.5287\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 1.52825\n",
      "Epoch 401/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6081 - mse: 4.6081 - mae: 1.6108 - val_loss: 3.7128 - val_mse: 3.7128 - val_mae: 1.5310\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 1.52825\n",
      "Epoch 402/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9364 - mse: 4.9364 - mae: 1.6562 - val_loss: 3.7093 - val_mse: 3.7093 - val_mae: 1.5297\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 1.52825\n",
      "Epoch 403/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0538 - mse: 5.0538 - mae: 1.7079 - val_loss: 3.7237 - val_mse: 3.7237 - val_mae: 1.5337\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 1.52825\n",
      "Epoch 404/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8861 - mse: 4.8861 - mae: 1.7014 - val_loss: 3.6995 - val_mse: 3.6995 - val_mae: 1.5261\n",
      "\n",
      "Epoch 00404: val_mae improved from 1.52825 to 1.52613, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 405/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5249 - mse: 4.5249 - mae: 1.6305 - val_loss: 3.6997 - val_mse: 3.6997 - val_mae: 1.5261\n",
      "\n",
      "Epoch 00405: val_mae improved from 1.52613 to 1.52605, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 406/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6654 - mse: 4.6654 - mae: 1.6413 - val_loss: 3.6954 - val_mse: 3.6954 - val_mae: 1.5242\n",
      "\n",
      "Epoch 00406: val_mae improved from 1.52605 to 1.52418, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 407/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.5094 - mse: 4.5094 - mae: 1.6289 - val_loss: 3.7096 - val_mse: 3.7096 - val_mae: 1.5293\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 1.52418\n",
      "Epoch 408/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7949 - mse: 4.7949 - mae: 1.6831 - val_loss: 3.7074 - val_mse: 3.7074 - val_mae: 1.5286\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 1.52418\n",
      "Epoch 409/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1715 - mse: 5.1715 - mae: 1.7327 - val_loss: 3.7188 - val_mse: 3.7188 - val_mae: 1.5318\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 1.52418\n",
      "Epoch 410/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.1916 - mse: 4.1916 - mae: 1.5968 - val_loss: 3.7210 - val_mse: 3.7210 - val_mae: 1.5323\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 1.52418\n",
      "Epoch 411/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9535 - mse: 4.9535 - mae: 1.7216 - val_loss: 3.7369 - val_mse: 3.7369 - val_mae: 1.5366\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 1.52418\n",
      "Epoch 412/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7344 - mse: 4.7344 - mae: 1.6246 - val_loss: 3.6991 - val_mse: 3.6991 - val_mae: 1.5256\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 1.52418\n",
      "Epoch 413/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8866 - mse: 4.8866 - mae: 1.6858 - val_loss: 3.7177 - val_mse: 3.7177 - val_mae: 1.5314\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 1.52418\n",
      "Epoch 414/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8914 - mse: 4.8914 - mae: 1.7029 - val_loss: 3.7258 - val_mse: 3.7258 - val_mae: 1.5335\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 1.52418\n",
      "Epoch 415/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5479 - mse: 4.5479 - mae: 1.6671 - val_loss: 3.7055 - val_mse: 3.7055 - val_mae: 1.5276\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 1.52418\n",
      "Epoch 416/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0022 - mse: 5.0022 - mae: 1.7067 - val_loss: 3.7182 - val_mse: 3.7182 - val_mae: 1.5314\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 1.52418\n",
      "Epoch 417/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2839 - mse: 5.2839 - mae: 1.7179 - val_loss: 3.7299 - val_mse: 3.7299 - val_mae: 1.5344\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 1.52418\n",
      "Epoch 418/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5616 - mse: 4.5616 - mae: 1.6225 - val_loss: 3.7029 - val_mse: 3.7029 - val_mae: 1.5266\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 1.52418\n",
      "Epoch 419/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0860 - mse: 5.0860 - mae: 1.7377 - val_loss: 3.7201 - val_mse: 3.7201 - val_mae: 1.5315\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 1.52418\n",
      "Epoch 420/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4610 - mse: 4.4610 - mae: 1.5921 - val_loss: 3.7131 - val_mse: 3.7131 - val_mae: 1.5294\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 1.52418\n",
      "Epoch 421/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9557 - mse: 4.9557 - mae: 1.6475 - val_loss: 3.7013 - val_mse: 3.7013 - val_mae: 1.5259\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 1.52418\n",
      "Epoch 422/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6332 - mse: 5.6332 - mae: 1.7532 - val_loss: 3.7044 - val_mse: 3.7044 - val_mae: 1.5268\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 1.52418\n",
      "Epoch 423/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7034 - mse: 4.7034 - mae: 1.6604 - val_loss: 3.7079 - val_mse: 3.7079 - val_mae: 1.5276\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 1.52418\n",
      "Epoch 424/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7856 - mse: 4.7856 - mae: 1.6955 - val_loss: 3.7084 - val_mse: 3.7084 - val_mae: 1.5278\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 1.52418\n",
      "Epoch 425/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9324 - mse: 5.9324 - mae: 1.8400 - val_loss: 3.7037 - val_mse: 3.7037 - val_mae: 1.5263\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 1.52418\n",
      "Epoch 426/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5241 - mse: 4.5241 - mae: 1.6466 - val_loss: 3.7063 - val_mse: 3.7063 - val_mae: 1.5271\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 1.52418\n",
      "Epoch 427/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1569 - mse: 5.1569 - mae: 1.7338 - val_loss: 3.7204 - val_mse: 3.7204 - val_mae: 1.5312\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 1.52418\n",
      "Epoch 428/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2436 - mse: 5.2436 - mae: 1.7300 - val_loss: 3.7090 - val_mse: 3.7090 - val_mae: 1.5279\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 1.52418\n",
      "Epoch 429/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1091 - mse: 5.1091 - mae: 1.7531 - val_loss: 3.7015 - val_mse: 3.7015 - val_mae: 1.5256\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 1.52418\n",
      "Epoch 430/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4793 - mse: 5.4793 - mae: 1.7781 - val_loss: 3.7267 - val_mse: 3.7267 - val_mae: 1.5328\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 1.52418\n",
      "Epoch 431/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6190 - mse: 5.6190 - mae: 1.7250 - val_loss: 3.7168 - val_mse: 3.7168 - val_mae: 1.5300\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 1.52418\n",
      "Epoch 432/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1527 - mse: 5.1527 - mae: 1.7091 - val_loss: 3.7158 - val_mse: 3.7158 - val_mae: 1.5295\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 1.52418\n",
      "Epoch 433/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3369 - mse: 4.3369 - mae: 1.6200 - val_loss: 3.7002 - val_mse: 3.7002 - val_mae: 1.5250\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 1.52418\n",
      "Epoch 434/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6592 - mse: 4.6592 - mae: 1.6096 - val_loss: 3.7040 - val_mse: 3.7040 - val_mae: 1.5260\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 1.52418\n",
      "Epoch 435/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1067 - mse: 5.1067 - mae: 1.7187 - val_loss: 3.7102 - val_mse: 3.7102 - val_mae: 1.5278\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 1.52418\n",
      "Epoch 436/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3904 - mse: 4.3904 - mae: 1.6382 - val_loss: 3.7049 - val_mse: 3.7049 - val_mae: 1.5262\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 1.52418\n",
      "Epoch 437/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6770 - mse: 4.6770 - mae: 1.6383 - val_loss: 3.7031 - val_mse: 3.7031 - val_mae: 1.5256\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 1.52418\n",
      "Epoch 438/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8578 - mse: 4.8578 - mae: 1.6185 - val_loss: 3.7049 - val_mse: 3.7049 - val_mae: 1.5260\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 1.52418\n",
      "Epoch 439/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6965 - mse: 5.6965 - mae: 1.8211 - val_loss: 3.7060 - val_mse: 3.7060 - val_mae: 1.5263\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 1.52418\n",
      "Epoch 440/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9679 - mse: 4.9679 - mae: 1.7042 - val_loss: 3.7316 - val_mse: 3.7316 - val_mae: 1.5333\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 1.52418\n",
      "Epoch 441/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4434 - mse: 4.4434 - mae: 1.6356 - val_loss: 3.7077 - val_mse: 3.7077 - val_mae: 1.5267\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 1.52418\n",
      "Epoch 442/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2486 - mse: 4.2486 - mae: 1.5411 - val_loss: 3.7128 - val_mse: 3.7128 - val_mae: 1.5281\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 1.52418\n",
      "Epoch 443/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9945 - mse: 4.9945 - mae: 1.7140 - val_loss: 3.7037 - val_mse: 3.7037 - val_mae: 1.5255\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 1.52418\n",
      "Epoch 444/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7874 - mse: 5.7874 - mae: 1.7802 - val_loss: 3.7051 - val_mse: 3.7051 - val_mae: 1.5259\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 1.52418\n",
      "Epoch 445/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3948 - mse: 4.3948 - mae: 1.5874 - val_loss: 3.6834 - val_mse: 3.6834 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00445: val_mae improved from 1.52418 to 1.51770, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 446/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8146 - mse: 4.8146 - mae: 1.6805 - val_loss: 3.6993 - val_mse: 3.6993 - val_mae: 1.5242\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 1.51770\n",
      "Epoch 447/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0018 - mse: 5.0018 - mae: 1.6355 - val_loss: 3.6937 - val_mse: 3.6937 - val_mae: 1.5225\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 1.51770\n",
      "Epoch 448/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6343 - mse: 4.6343 - mae: 1.6143 - val_loss: 3.7026 - val_mse: 3.7026 - val_mae: 1.5251\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 1.51770\n",
      "Epoch 449/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5627 - mse: 5.5627 - mae: 1.7433 - val_loss: 3.6917 - val_mse: 3.6917 - val_mae: 1.5216\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 1.51770\n",
      "Epoch 450/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1617 - mse: 5.1617 - mae: 1.7366 - val_loss: 3.7165 - val_mse: 3.7165 - val_mae: 1.5288\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 1.51770\n",
      "Epoch 451/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0444 - mse: 5.0444 - mae: 1.6572 - val_loss: 3.6902 - val_mse: 3.6902 - val_mae: 1.5210\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 1.51770\n",
      "Epoch 452/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5078 - mse: 5.5078 - mae: 1.7452 - val_loss: 3.7031 - val_mse: 3.7031 - val_mae: 1.5249\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 1.51770\n",
      "Epoch 453/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7267 - mse: 4.7267 - mae: 1.6201 - val_loss: 3.6955 - val_mse: 3.6955 - val_mae: 1.5224\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 1.51770\n",
      "Epoch 454/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6127 - mse: 4.6127 - mae: 1.6237 - val_loss: 3.6931 - val_mse: 3.6931 - val_mae: 1.5216\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 1.51770\n",
      "Epoch 455/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9014 - mse: 4.9014 - mae: 1.7291 - val_loss: 3.6892 - val_mse: 3.6892 - val_mae: 1.5203\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 1.51770\n",
      "Epoch 456/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3899 - mse: 4.3899 - mae: 1.6755 - val_loss: 3.7036 - val_mse: 3.7036 - val_mae: 1.5245\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 1.51770\n",
      "Epoch 457/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7085 - mse: 4.7085 - mae: 1.6839 - val_loss: 3.7094 - val_mse: 3.7094 - val_mae: 1.5260\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 1.51770\n",
      "Epoch 458/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1292 - mse: 5.1292 - mae: 1.7363 - val_loss: 3.7035 - val_mse: 3.7035 - val_mae: 1.5243\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 1.51770\n",
      "Epoch 459/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4421 - mse: 5.4421 - mae: 1.7492 - val_loss: 3.6996 - val_mse: 3.6996 - val_mae: 1.5231\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 1.51770\n",
      "Epoch 460/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6401 - mse: 5.6401 - mae: 1.7501 - val_loss: 3.7001 - val_mse: 3.7001 - val_mae: 1.5232\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 1.51770\n",
      "Epoch 461/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9789 - mse: 4.9789 - mae: 1.7080 - val_loss: 3.6944 - val_mse: 3.6944 - val_mae: 1.5215\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 1.51770\n",
      "Epoch 462/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4783 - mse: 5.4783 - mae: 1.7903 - val_loss: 3.7174 - val_mse: 3.7174 - val_mae: 1.5279\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 1.51770\n",
      "Epoch 463/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1436 - mse: 5.1436 - mae: 1.6818 - val_loss: 3.7023 - val_mse: 3.7023 - val_mae: 1.5235\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 1.51770\n",
      "Epoch 464/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6171 - mse: 4.6171 - mae: 1.6445 - val_loss: 3.6926 - val_mse: 3.6926 - val_mae: 1.5206\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 1.51770\n",
      "Epoch 465/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6417 - mse: 4.6417 - mae: 1.6432 - val_loss: 3.6952 - val_mse: 3.6952 - val_mae: 1.5213\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 1.51770\n",
      "Epoch 466/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2500 - mse: 5.2500 - mae: 1.7127 - val_loss: 3.6863 - val_mse: 3.6863 - val_mae: 1.5182\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 1.51770\n",
      "Epoch 467/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3931 - mse: 5.3931 - mae: 1.7327 - val_loss: 3.6859 - val_mse: 3.6859 - val_mae: 1.5180\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 1.51770\n",
      "Epoch 468/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4468 - mse: 4.4468 - mae: 1.6053 - val_loss: 3.6878 - val_mse: 3.6878 - val_mae: 1.5188\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 1.51770\n",
      "Epoch 469/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7793 - mse: 4.7793 - mae: 1.6866 - val_loss: 3.6928 - val_mse: 3.6928 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 1.51770\n",
      "Epoch 470/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2524 - mse: 5.2524 - mae: 1.7001 - val_loss: 3.7081 - val_mse: 3.7081 - val_mae: 1.5251\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 1.51770\n",
      "Epoch 471/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7451 - mse: 4.7451 - mae: 1.6201 - val_loss: 3.6925 - val_mse: 3.6925 - val_mae: 1.5206\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 1.51770\n",
      "Epoch 472/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7695 - mse: 4.7695 - mae: 1.6385 - val_loss: 3.6894 - val_mse: 3.6894 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 1.51770\n",
      "Epoch 473/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3470 - mse: 6.3470 - mae: 1.8126 - val_loss: 3.7022 - val_mse: 3.7022 - val_mae: 1.5235\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 1.51770\n",
      "Epoch 474/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8659 - mse: 4.8659 - mae: 1.6796 - val_loss: 3.6994 - val_mse: 3.6994 - val_mae: 1.5226\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 1.51770\n",
      "Epoch 475/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8195 - mse: 4.8195 - mae: 1.6805 - val_loss: 3.7073 - val_mse: 3.7073 - val_mae: 1.5247\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 1.51770\n",
      "Epoch 476/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4271 - mse: 5.4271 - mae: 1.6886 - val_loss: 3.6872 - val_mse: 3.6872 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 1.51770\n",
      "Epoch 477/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.5022 - mse: 4.5022 - mae: 1.6007 - val_loss: 3.6878 - val_mse: 3.6878 - val_mae: 1.5186\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 1.51770\n",
      "Epoch 478/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2195 - mse: 5.2195 - mae: 1.7319 - val_loss: 3.6932 - val_mse: 3.6932 - val_mae: 1.5204\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 1.51770\n",
      "Epoch 479/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3205 - mse: 4.3205 - mae: 1.5750 - val_loss: 3.7088 - val_mse: 3.7088 - val_mae: 1.5249\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 1.51770\n",
      "Epoch 480/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8187 - mse: 4.8187 - mae: 1.6555 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5191\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 1.51770\n",
      "Epoch 481/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5150 - mse: 4.5150 - mae: 1.6476 - val_loss: 3.6866 - val_mse: 3.6866 - val_mae: 1.5181\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 1.51770\n",
      "Epoch 482/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7618 - mse: 4.7618 - mae: 1.6485 - val_loss: 3.7014 - val_mse: 3.7014 - val_mae: 1.5228\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 1.51770\n",
      "Epoch 483/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0296 - mse: 5.0296 - mae: 1.6900 - val_loss: 3.6930 - val_mse: 3.6930 - val_mae: 1.5202\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 1.51770\n",
      "Epoch 484/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6914 - mse: 4.6914 - mae: 1.6551 - val_loss: 3.7090 - val_mse: 3.7090 - val_mae: 1.5249\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 1.51770\n",
      "Epoch 485/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7096 - mse: 4.7096 - mae: 1.6290 - val_loss: 3.6955 - val_mse: 3.6955 - val_mae: 1.5208\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 1.51770\n",
      "Epoch 486/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6820 - mse: 4.6820 - mae: 1.6346 - val_loss: 3.6864 - val_mse: 3.6864 - val_mae: 1.5175\n",
      "\n",
      "Epoch 00486: val_mae improved from 1.51770 to 1.51749, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 487/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9914 - mse: 4.9914 - mae: 1.6800 - val_loss: 3.6880 - val_mse: 3.6880 - val_mae: 1.5179\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 1.51749\n",
      "Epoch 488/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4818 - mse: 5.4818 - mae: 1.7605 - val_loss: 3.6961 - val_mse: 3.6961 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 1.51749\n",
      "Epoch 489/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7782 - mse: 4.7782 - mae: 1.6703 - val_loss: 3.6962 - val_mse: 3.6962 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 1.51749\n",
      "Epoch 490/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6377 - mse: 4.6377 - mae: 1.6394 - val_loss: 3.7040 - val_mse: 3.7040 - val_mae: 1.5230\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 1.51749\n",
      "Epoch 491/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4210 - mse: 4.4210 - mae: 1.6098 - val_loss: 3.6842 - val_mse: 3.6842 - val_mae: 1.5164\n",
      "\n",
      "Epoch 00491: val_mae improved from 1.51749 to 1.51637, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 492/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1933 - mse: 5.1933 - mae: 1.7347 - val_loss: 3.6860 - val_mse: 3.6860 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 1.51637\n",
      "Epoch 493/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0979 - mse: 5.0979 - mae: 1.6741 - val_loss: 3.6863 - val_mse: 3.6863 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 1.51637\n",
      "Epoch 494/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8585 - mse: 4.8585 - mae: 1.6850 - val_loss: 3.6898 - val_mse: 3.6898 - val_mae: 1.5187\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 1.51637\n",
      "Epoch 495/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8653 - mse: 4.8653 - mae: 1.6544 - val_loss: 3.6849 - val_mse: 3.6849 - val_mae: 1.5168\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 1.51637\n",
      "Epoch 496/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0021 - mse: 5.0021 - mae: 1.6987 - val_loss: 3.7019 - val_mse: 3.7019 - val_mae: 1.5223\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 1.51637\n",
      "Epoch 497/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1494 - mse: 5.1494 - mae: 1.6955 - val_loss: 3.6846 - val_mse: 3.6846 - val_mae: 1.5168\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 1.51637\n",
      "Epoch 498/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0939 - mse: 5.0939 - mae: 1.6287 - val_loss: 3.6883 - val_mse: 3.6883 - val_mae: 1.5180\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 1.51637\n",
      "Epoch 499/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7502 - mse: 4.7502 - mae: 1.6786 - val_loss: 3.6997 - val_mse: 3.6997 - val_mae: 1.5216\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 1.51637\n",
      "Epoch 500/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9598 - mse: 4.9598 - mae: 1.6807 - val_loss: 3.6983 - val_mse: 3.6983 - val_mae: 1.5209\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 1.51637\n",
      "Epoch 501/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3027 - mse: 4.3027 - mae: 1.6109 - val_loss: 3.6977 - val_mse: 3.6977 - val_mae: 1.5207\n",
      "\n",
      "Epoch 00501: val_mae did not improve from 1.51637\n",
      "Epoch 502/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9804 - mse: 4.9804 - mae: 1.6368 - val_loss: 3.7144 - val_mse: 3.7144 - val_mae: 1.5251\n",
      "\n",
      "Epoch 00502: val_mae did not improve from 1.51637\n",
      "Epoch 503/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5764 - mse: 4.5764 - mae: 1.6054 - val_loss: 3.6941 - val_mse: 3.6941 - val_mae: 1.5195\n",
      "\n",
      "Epoch 00503: val_mae did not improve from 1.51637\n",
      "Epoch 504/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5061 - mse: 4.5061 - mae: 1.6247 - val_loss: 3.7014 - val_mse: 3.7014 - val_mae: 1.5217\n",
      "\n",
      "Epoch 00504: val_mae did not improve from 1.51637\n",
      "Epoch 505/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7868 - mse: 4.7868 - mae: 1.6387 - val_loss: 3.7110 - val_mse: 3.7110 - val_mae: 1.5240\n",
      "\n",
      "Epoch 00505: val_mae did not improve from 1.51637\n",
      "Epoch 506/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6062 - mse: 4.6062 - mae: 1.6192 - val_loss: 3.6881 - val_mse: 3.6881 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00506: val_mae did not improve from 1.51637\n",
      "Epoch 507/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2883 - mse: 5.2883 - mae: 1.7072 - val_loss: 3.6976 - val_mse: 3.6976 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00507: val_mae did not improve from 1.51637\n",
      "Epoch 508/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8315 - mse: 4.8315 - mae: 1.7088 - val_loss: 3.6888 - val_mse: 3.6888 - val_mae: 1.5176\n",
      "\n",
      "Epoch 00508: val_mae did not improve from 1.51637\n",
      "Epoch 509/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5349 - mse: 4.5349 - mae: 1.6574 - val_loss: 3.6947 - val_mse: 3.6947 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00509: val_mae did not improve from 1.51637\n",
      "Epoch 510/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6125 - mse: 4.6125 - mae: 1.6368 - val_loss: 3.7122 - val_mse: 3.7122 - val_mae: 1.5242\n",
      "\n",
      "Epoch 00510: val_mae did not improve from 1.51637\n",
      "Epoch 511/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2932 - mse: 5.2932 - mae: 1.7064 - val_loss: 3.6990 - val_mse: 3.6990 - val_mae: 1.5206\n",
      "\n",
      "Epoch 00511: val_mae did not improve from 1.51637\n",
      "Epoch 512/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8430 - mse: 4.8430 - mae: 1.7083 - val_loss: 3.6929 - val_mse: 3.6929 - val_mae: 1.5187\n",
      "\n",
      "Epoch 00512: val_mae did not improve from 1.51637\n",
      "Epoch 513/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5619 - mse: 4.5619 - mae: 1.6183 - val_loss: 3.6825 - val_mse: 3.6825 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00513: val_mae improved from 1.51637 to 1.51519, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 514/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9843 - mse: 4.9843 - mae: 1.7054 - val_loss: 3.6796 - val_mse: 3.6796 - val_mae: 1.5141\n",
      "\n",
      "Epoch 00514: val_mae improved from 1.51519 to 1.51413, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 515/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6372 - mse: 4.6372 - mae: 1.6360 - val_loss: 3.7140 - val_mse: 3.7140 - val_mae: 1.5242\n",
      "\n",
      "Epoch 00515: val_mae did not improve from 1.51413\n",
      "Epoch 516/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3977 - mse: 5.3977 - mae: 1.7238 - val_loss: 3.6928 - val_mse: 3.6928 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00516: val_mae did not improve from 1.51413\n",
      "Epoch 517/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.1460 - mse: 4.1460 - mae: 1.5817 - val_loss: 3.6829 - val_mse: 3.6829 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00517: val_mae did not improve from 1.51413\n",
      "Epoch 518/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1570 - mse: 5.1570 - mae: 1.6976 - val_loss: 3.7164 - val_mse: 3.7164 - val_mae: 1.5244\n",
      "\n",
      "Epoch 00518: val_mae did not improve from 1.51413\n",
      "Epoch 519/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2496 - mse: 5.2496 - mae: 1.7050 - val_loss: 3.6979 - val_mse: 3.6979 - val_mae: 1.5196\n",
      "\n",
      "Epoch 00519: val_mae did not improve from 1.51413\n",
      "Epoch 520/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8884 - mse: 4.8884 - mae: 1.6783 - val_loss: 3.6826 - val_mse: 3.6826 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00520: val_mae did not improve from 1.51413\n",
      "Epoch 521/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3461 - mse: 5.3461 - mae: 1.7616 - val_loss: 3.6865 - val_mse: 3.6865 - val_mae: 1.5162\n",
      "\n",
      "Epoch 00521: val_mae did not improve from 1.51413\n",
      "Epoch 522/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3294 - mse: 5.3294 - mae: 1.6906 - val_loss: 3.6901 - val_mse: 3.6901 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00522: val_mae did not improve from 1.51413\n",
      "Epoch 523/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5811 - mse: 4.5811 - mae: 1.6085 - val_loss: 3.7007 - val_mse: 3.7007 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00523: val_mae did not improve from 1.51413\n",
      "Epoch 524/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2910 - mse: 5.2910 - mae: 1.7475 - val_loss: 3.6867 - val_mse: 3.6867 - val_mae: 1.5162\n",
      "\n",
      "Epoch 00524: val_mae did not improve from 1.51413\n",
      "Epoch 525/1000\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.4581 - mse: 5.4581 - mae: 1.7101 - val_loss: 3.6955 - val_mse: 3.6955 - val_mae: 1.5189\n",
      "\n",
      "Epoch 00525: val_mae did not improve from 1.51413\n",
      "Epoch 526/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5721 - mse: 4.5721 - mae: 1.6111 - val_loss: 3.6849 - val_mse: 3.6849 - val_mae: 1.5157\n",
      "\n",
      "Epoch 00526: val_mae did not improve from 1.51413\n",
      "Epoch 527/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.2559 - mse: 5.2559 - mae: 1.7285 - val_loss: 3.6936 - val_mse: 3.6936 - val_mae: 1.5186\n",
      "\n",
      "Epoch 00527: val_mae did not improve from 1.51413\n",
      "Epoch 528/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7529 - mse: 4.7529 - mae: 1.6265 - val_loss: 3.6818 - val_mse: 3.6818 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00528: val_mae did not improve from 1.51413\n",
      "Epoch 529/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3342 - mse: 5.3342 - mae: 1.7059 - val_loss: 3.6977 - val_mse: 3.6977 - val_mae: 1.5200\n",
      "\n",
      "Epoch 00529: val_mae did not improve from 1.51413\n",
      "Epoch 530/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8239 - mse: 4.8239 - mae: 1.6529 - val_loss: 3.6870 - val_mse: 3.6870 - val_mae: 1.5165\n",
      "\n",
      "Epoch 00530: val_mae did not improve from 1.51413\n",
      "Epoch 531/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7640 - mse: 4.7640 - mae: 1.6629 - val_loss: 3.7076 - val_mse: 3.7076 - val_mae: 1.5225\n",
      "\n",
      "Epoch 00531: val_mae did not improve from 1.51413\n",
      "Epoch 532/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8901 - mse: 4.8901 - mae: 1.6920 - val_loss: 3.6974 - val_mse: 3.6974 - val_mae: 1.5198\n",
      "\n",
      "Epoch 00532: val_mae did not improve from 1.51413\n",
      "Epoch 533/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4016 - mse: 5.4016 - mae: 1.7105 - val_loss: 3.6981 - val_mse: 3.6981 - val_mae: 1.5199\n",
      "\n",
      "Epoch 00533: val_mae did not improve from 1.51413\n",
      "Epoch 534/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4318 - mse: 4.4318 - mae: 1.6035 - val_loss: 3.6854 - val_mse: 3.6854 - val_mae: 1.5160\n",
      "\n",
      "Epoch 00534: val_mae did not improve from 1.51413\n",
      "Epoch 535/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4198 - mse: 5.4198 - mae: 1.7130 - val_loss: 3.6804 - val_mse: 3.6804 - val_mae: 1.5143\n",
      "\n",
      "Epoch 00535: val_mae did not improve from 1.51413\n",
      "Epoch 536/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3972 - mse: 4.3972 - mae: 1.6079 - val_loss: 3.6764 - val_mse: 3.6764 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00536: val_mae improved from 1.51413 to 1.51274, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 537/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.4250 - mse: 4.4250 - mae: 1.6123 - val_loss: 3.6771 - val_mse: 3.6771 - val_mae: 1.5129\n",
      "\n",
      "Epoch 00537: val_mae did not improve from 1.51274\n",
      "Epoch 538/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8575 - mse: 4.8575 - mae: 1.6264 - val_loss: 3.6869 - val_mse: 3.6869 - val_mae: 1.5163\n",
      "\n",
      "Epoch 00538: val_mae did not improve from 1.51274\n",
      "Epoch 539/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3722 - mse: 5.3722 - mae: 1.7084 - val_loss: 3.6875 - val_mse: 3.6875 - val_mae: 1.5166\n",
      "\n",
      "Epoch 00539: val_mae did not improve from 1.51274\n",
      "Epoch 540/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7556 - mse: 4.7556 - mae: 1.6337 - val_loss: 3.6950 - val_mse: 3.6950 - val_mae: 1.5190\n",
      "\n",
      "Epoch 00540: val_mae did not improve from 1.51274\n",
      "Epoch 541/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9272 - mse: 4.9272 - mae: 1.6741 - val_loss: 3.6862 - val_mse: 3.6862 - val_mae: 1.5162\n",
      "\n",
      "Epoch 00541: val_mae did not improve from 1.51274\n",
      "Epoch 542/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5266 - mse: 4.5266 - mae: 1.6511 - val_loss: 3.6985 - val_mse: 3.6985 - val_mae: 1.5200\n",
      "\n",
      "Epoch 00542: val_mae did not improve from 1.51274\n",
      "Epoch 543/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0654 - mse: 5.0654 - mae: 1.7331 - val_loss: 3.7110 - val_mse: 3.7110 - val_mae: 1.5231\n",
      "\n",
      "Epoch 00543: val_mae did not improve from 1.51274\n",
      "Epoch 544/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8432 - mse: 4.8432 - mae: 1.6578 - val_loss: 3.6902 - val_mse: 3.6902 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00544: val_mae did not improve from 1.51274\n",
      "Epoch 545/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3117 - mse: 4.3117 - mae: 1.5635 - val_loss: 3.6747 - val_mse: 3.6747 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00545: val_mae improved from 1.51274 to 1.51152, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 546/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.7102 - mse: 4.7102 - mae: 1.6793 - val_loss: 3.6865 - val_mse: 3.6865 - val_mae: 1.5160\n",
      "\n",
      "Epoch 00546: val_mae did not improve from 1.51152\n",
      "Epoch 547/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9810 - mse: 4.9810 - mae: 1.6704 - val_loss: 3.6889 - val_mse: 3.6889 - val_mae: 1.5166\n",
      "\n",
      "Epoch 00547: val_mae did not improve from 1.51152\n",
      "Epoch 548/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2767 - mse: 5.2767 - mae: 1.7014 - val_loss: 3.6882 - val_mse: 3.6882 - val_mae: 1.5163\n",
      "\n",
      "Epoch 00548: val_mae did not improve from 1.51152\n",
      "Epoch 549/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9539 - mse: 4.9539 - mae: 1.6952 - val_loss: 3.6972 - val_mse: 3.6972 - val_mae: 1.5192\n",
      "\n",
      "Epoch 00549: val_mae did not improve from 1.51152\n",
      "Epoch 550/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9419 - mse: 4.9419 - mae: 1.6423 - val_loss: 3.6847 - val_mse: 3.6847 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00550: val_mae did not improve from 1.51152\n",
      "Epoch 551/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9649 - mse: 4.9649 - mae: 1.6646 - val_loss: 3.6825 - val_mse: 3.6825 - val_mae: 1.5143\n",
      "\n",
      "Epoch 00551: val_mae did not improve from 1.51152\n",
      "Epoch 552/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4198 - mse: 4.4198 - mae: 1.5810 - val_loss: 3.6921 - val_mse: 3.6921 - val_mae: 1.5175\n",
      "\n",
      "Epoch 00552: val_mae did not improve from 1.51152\n",
      "Epoch 553/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6419 - mse: 5.6419 - mae: 1.7794 - val_loss: 3.7133 - val_mse: 3.7133 - val_mae: 1.5230\n",
      "\n",
      "Epoch 00553: val_mae did not improve from 1.51152\n",
      "Epoch 554/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3549 - mse: 5.3549 - mae: 1.7630 - val_loss: 3.6779 - val_mse: 3.6779 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00554: val_mae did not improve from 1.51152\n",
      "Epoch 555/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4654 - mse: 4.4654 - mae: 1.6283 - val_loss: 3.6856 - val_mse: 3.6856 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00555: val_mae did not improve from 1.51152\n",
      "Epoch 556/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8113 - mse: 4.8113 - mae: 1.6502 - val_loss: 3.7032 - val_mse: 3.7032 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00556: val_mae did not improve from 1.51152\n",
      "Epoch 557/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9518 - mse: 5.9518 - mae: 1.7878 - val_loss: 3.6937 - val_mse: 3.6937 - val_mae: 1.5178\n",
      "\n",
      "Epoch 00557: val_mae did not improve from 1.51152\n",
      "Epoch 558/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9277 - mse: 4.9277 - mae: 1.6568 - val_loss: 3.6892 - val_mse: 3.6892 - val_mae: 1.5164\n",
      "\n",
      "Epoch 00558: val_mae did not improve from 1.51152\n",
      "Epoch 559/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9768 - mse: 4.9768 - mae: 1.6652 - val_loss: 3.6708 - val_mse: 3.6708 - val_mae: 1.5096\n",
      "\n",
      "Epoch 00559: val_mae improved from 1.51152 to 1.50957, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 560/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0731 - mse: 5.0731 - mae: 1.6588 - val_loss: 3.6695 - val_mse: 3.6695 - val_mae: 1.5087\n",
      "\n",
      "Epoch 00560: val_mae improved from 1.50957 to 1.50869, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 561/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7960 - mse: 4.7960 - mae: 1.6563 - val_loss: 3.6982 - val_mse: 3.6982 - val_mae: 1.5188\n",
      "\n",
      "Epoch 00561: val_mae did not improve from 1.50869\n",
      "Epoch 562/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0707 - mse: 5.0707 - mae: 1.6635 - val_loss: 3.6970 - val_mse: 3.6970 - val_mae: 1.5183\n",
      "\n",
      "Epoch 00562: val_mae did not improve from 1.50869\n",
      "Epoch 563/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5323 - mse: 4.5323 - mae: 1.6278 - val_loss: 3.6837 - val_mse: 3.6837 - val_mae: 1.5141\n",
      "\n",
      "Epoch 00563: val_mae did not improve from 1.50869\n",
      "Epoch 564/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4486 - mse: 5.4486 - mae: 1.7571 - val_loss: 3.6927 - val_mse: 3.6927 - val_mae: 1.5171\n",
      "\n",
      "Epoch 00564: val_mae did not improve from 1.50869\n",
      "Epoch 565/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8754 - mse: 4.8754 - mae: 1.6478 - val_loss: 3.6722 - val_mse: 3.6722 - val_mae: 1.5101\n",
      "\n",
      "Epoch 00565: val_mae did not improve from 1.50869\n",
      "Epoch 566/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3651 - mse: 4.3651 - mae: 1.6011 - val_loss: 3.6770 - val_mse: 3.6770 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00566: val_mae did not improve from 1.50869\n",
      "Epoch 567/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6413 - mse: 4.6413 - mae: 1.6219 - val_loss: 3.7036 - val_mse: 3.7036 - val_mae: 1.5199\n",
      "\n",
      "Epoch 00567: val_mae did not improve from 1.50869\n",
      "Epoch 568/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4671 - mse: 4.4671 - mae: 1.6516 - val_loss: 3.6770 - val_mse: 3.6770 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00568: val_mae did not improve from 1.50869\n",
      "Epoch 569/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3450 - mse: 4.3450 - mae: 1.5573 - val_loss: 3.6909 - val_mse: 3.6909 - val_mae: 1.5160\n",
      "\n",
      "Epoch 00569: val_mae did not improve from 1.50869\n",
      "Epoch 570/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.1098 - mse: 4.1098 - mae: 1.5624 - val_loss: 3.6932 - val_mse: 3.6932 - val_mae: 1.5166\n",
      "\n",
      "Epoch 00570: val_mae did not improve from 1.50869\n",
      "Epoch 571/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9468 - mse: 4.9468 - mae: 1.6293 - val_loss: 3.6999 - val_mse: 3.6999 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00571: val_mae did not improve from 1.50869\n",
      "Epoch 572/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5476 - mse: 4.5476 - mae: 1.6541 - val_loss: 3.7012 - val_mse: 3.7012 - val_mae: 1.5187\n",
      "\n",
      "Epoch 00572: val_mae did not improve from 1.50869\n",
      "Epoch 573/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7391 - mse: 4.7391 - mae: 1.6552 - val_loss: 3.7192 - val_mse: 3.7192 - val_mae: 1.5232\n",
      "\n",
      "Epoch 00573: val_mae did not improve from 1.50869\n",
      "Epoch 574/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4554 - mse: 4.4554 - mae: 1.5937 - val_loss: 3.6810 - val_mse: 3.6810 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00574: val_mae did not improve from 1.50869\n",
      "Epoch 575/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5857 - mse: 4.5857 - mae: 1.5986 - val_loss: 3.6955 - val_mse: 3.6955 - val_mae: 1.5170\n",
      "\n",
      "Epoch 00575: val_mae did not improve from 1.50869\n",
      "Epoch 576/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6266 - mse: 4.6266 - mae: 1.6178 - val_loss: 3.7019 - val_mse: 3.7019 - val_mae: 1.5190\n",
      "\n",
      "Epoch 00576: val_mae did not improve from 1.50869\n",
      "Epoch 577/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4554 - mse: 4.4554 - mae: 1.6279 - val_loss: 3.6955 - val_mse: 3.6955 - val_mae: 1.5170\n",
      "\n",
      "Epoch 00577: val_mae did not improve from 1.50869\n",
      "Epoch 578/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2825 - mse: 5.2825 - mae: 1.6954 - val_loss: 3.6887 - val_mse: 3.6887 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00578: val_mae did not improve from 1.50869\n",
      "Epoch 579/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4128 - mse: 5.4128 - mae: 1.7591 - val_loss: 3.6957 - val_mse: 3.6957 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00579: val_mae did not improve from 1.50869\n",
      "Epoch 580/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7588 - mse: 4.7588 - mae: 1.6308 - val_loss: 3.7010 - val_mse: 3.7010 - val_mae: 1.5186\n",
      "\n",
      "Epoch 00580: val_mae did not improve from 1.50869\n",
      "Epoch 581/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7734 - mse: 4.7734 - mae: 1.6779 - val_loss: 3.6965 - val_mse: 3.6965 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00581: val_mae did not improve from 1.50869\n",
      "Epoch 582/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4092 - mse: 5.4092 - mae: 1.7166 - val_loss: 3.6943 - val_mse: 3.6943 - val_mae: 1.5168\n",
      "\n",
      "Epoch 00582: val_mae did not improve from 1.50869\n",
      "Epoch 583/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0996 - mse: 5.0996 - mae: 1.6815 - val_loss: 3.6818 - val_mse: 3.6818 - val_mae: 1.5129\n",
      "\n",
      "Epoch 00583: val_mae did not improve from 1.50869\n",
      "Epoch 584/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9432 - mse: 4.9432 - mae: 1.6817 - val_loss: 3.6776 - val_mse: 3.6776 - val_mae: 1.5114\n",
      "\n",
      "Epoch 00584: val_mae did not improve from 1.50869\n",
      "Epoch 585/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0853 - mse: 5.0853 - mae: 1.7194 - val_loss: 3.6941 - val_mse: 3.6941 - val_mae: 1.5171\n",
      "\n",
      "Epoch 00585: val_mae did not improve from 1.50869\n",
      "Epoch 586/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0921 - mse: 5.0921 - mae: 1.6959 - val_loss: 3.7072 - val_mse: 3.7072 - val_mae: 1.5207\n",
      "\n",
      "Epoch 00586: val_mae did not improve from 1.50869\n",
      "Epoch 587/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6393 - mse: 4.6393 - mae: 1.6606 - val_loss: 3.6816 - val_mse: 3.6816 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00587: val_mae did not improve from 1.50869\n",
      "Epoch 588/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4063 - mse: 4.4063 - mae: 1.5741 - val_loss: 3.7157 - val_mse: 3.7157 - val_mae: 1.5227\n",
      "\n",
      "Epoch 00588: val_mae did not improve from 1.50869\n",
      "Epoch 589/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6509 - mse: 4.6509 - mae: 1.6106 - val_loss: 3.6875 - val_mse: 3.6875 - val_mae: 1.5150\n",
      "\n",
      "Epoch 00589: val_mae did not improve from 1.50869\n",
      "Epoch 590/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1468 - mse: 5.1468 - mae: 1.7200 - val_loss: 3.7051 - val_mse: 3.7051 - val_mae: 1.5202\n",
      "\n",
      "Epoch 00590: val_mae did not improve from 1.50869\n",
      "Epoch 591/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6560 - mse: 4.6560 - mae: 1.6720 - val_loss: 3.6778 - val_mse: 3.6778 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00591: val_mae did not improve from 1.50869\n",
      "Epoch 592/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9605 - mse: 4.9605 - mae: 1.6653 - val_loss: 3.6769 - val_mse: 3.6769 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00592: val_mae did not improve from 1.50869\n",
      "Epoch 593/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6538 - mse: 4.6538 - mae: 1.6099 - val_loss: 3.6935 - val_mse: 3.6935 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00593: val_mae did not improve from 1.50869\n",
      "Epoch 594/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5792 - mse: 4.5792 - mae: 1.6192 - val_loss: 3.6770 - val_mse: 3.6770 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00594: val_mae did not improve from 1.50869\n",
      "Epoch 595/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8643 - mse: 4.8643 - mae: 1.6815 - val_loss: 3.6939 - val_mse: 3.6939 - val_mae: 1.5171\n",
      "\n",
      "Epoch 00595: val_mae did not improve from 1.50869\n",
      "Epoch 596/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0741 - mse: 5.0741 - mae: 1.6578 - val_loss: 3.6811 - val_mse: 3.6811 - val_mae: 1.5129\n",
      "\n",
      "Epoch 00596: val_mae did not improve from 1.50869\n",
      "Epoch 597/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6006 - mse: 4.6006 - mae: 1.6572 - val_loss: 3.6826 - val_mse: 3.6826 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00597: val_mae did not improve from 1.50869\n",
      "Epoch 598/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5607 - mse: 4.5607 - mae: 1.6438 - val_loss: 3.6866 - val_mse: 3.6866 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00598: val_mae did not improve from 1.50869\n",
      "Epoch 599/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9809 - mse: 4.9809 - mae: 1.6530 - val_loss: 3.6875 - val_mse: 3.6875 - val_mae: 1.5149\n",
      "\n",
      "Epoch 00599: val_mae did not improve from 1.50869\n",
      "Epoch 600/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0547 - mse: 5.0547 - mae: 1.7223 - val_loss: 3.6849 - val_mse: 3.6849 - val_mae: 1.5140\n",
      "\n",
      "Epoch 00600: val_mae did not improve from 1.50869\n",
      "Epoch 601/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3755 - mse: 5.3755 - mae: 1.7816 - val_loss: 3.6948 - val_mse: 3.6948 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00601: val_mae did not improve from 1.50869\n",
      "Epoch 602/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5867 - mse: 4.5867 - mae: 1.5937 - val_loss: 3.6810 - val_mse: 3.6810 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00602: val_mae did not improve from 1.50869\n",
      "Epoch 603/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4660 - mse: 4.4660 - mae: 1.6117 - val_loss: 3.6824 - val_mse: 3.6824 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00603: val_mae did not improve from 1.50869\n",
      "Epoch 604/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6338 - mse: 4.6338 - mae: 1.6221 - val_loss: 3.6867 - val_mse: 3.6867 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00604: val_mae did not improve from 1.50869\n",
      "Epoch 605/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8411 - mse: 4.8411 - mae: 1.6850 - val_loss: 3.7042 - val_mse: 3.7042 - val_mae: 1.5197\n",
      "\n",
      "Epoch 00605: val_mae did not improve from 1.50869\n",
      "Epoch 606/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4698 - mse: 5.4698 - mae: 1.7475 - val_loss: 3.7014 - val_mse: 3.7014 - val_mae: 1.5190\n",
      "\n",
      "Epoch 00606: val_mae did not improve from 1.50869\n",
      "Epoch 607/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3366 - mse: 5.3366 - mae: 1.7156 - val_loss: 3.6968 - val_mse: 3.6968 - val_mae: 1.5176\n",
      "\n",
      "Epoch 00607: val_mae did not improve from 1.50869\n",
      "Epoch 608/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.5999 - mse: 4.5999 - mae: 1.6473 - val_loss: 3.6868 - val_mse: 3.6868 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00608: val_mae did not improve from 1.50869\n",
      "Epoch 609/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0784 - mse: 5.0784 - mae: 1.6875 - val_loss: 3.6945 - val_mse: 3.6945 - val_mae: 1.5171\n",
      "\n",
      "Epoch 00609: val_mae did not improve from 1.50869\n",
      "Epoch 610/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2243 - mse: 5.2243 - mae: 1.7315 - val_loss: 3.6928 - val_mse: 3.6928 - val_mae: 1.5166\n",
      "\n",
      "Epoch 00610: val_mae did not improve from 1.50869\n",
      "Epoch 611/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2483 - mse: 5.2483 - mae: 1.7551 - val_loss: 3.6999 - val_mse: 3.6999 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00611: val_mae did not improve from 1.50869\n",
      "Epoch 612/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2685 - mse: 5.2685 - mae: 1.7009 - val_loss: 3.6942 - val_mse: 3.6942 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00612: val_mae did not improve from 1.50869\n",
      "Epoch 613/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6003 - mse: 4.6003 - mae: 1.6364 - val_loss: 3.6839 - val_mse: 3.6839 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00613: val_mae did not improve from 1.50869\n",
      "Epoch 614/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2687 - mse: 4.2687 - mae: 1.5778 - val_loss: 3.7075 - val_mse: 3.7075 - val_mae: 1.5204\n",
      "\n",
      "Epoch 00614: val_mae did not improve from 1.50869\n",
      "Epoch 615/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3753 - mse: 4.3753 - mae: 1.6423 - val_loss: 3.6939 - val_mse: 3.6939 - val_mae: 1.5165\n",
      "\n",
      "Epoch 00615: val_mae did not improve from 1.50869\n",
      "Epoch 616/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7142 - mse: 4.7142 - mae: 1.6261 - val_loss: 3.6857 - val_mse: 3.6857 - val_mae: 1.5137\n",
      "\n",
      "Epoch 00616: val_mae did not improve from 1.50869\n",
      "Epoch 617/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9828 - mse: 4.9828 - mae: 1.6484 - val_loss: 3.6971 - val_mse: 3.6971 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00617: val_mae did not improve from 1.50869\n",
      "Epoch 618/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8903 - mse: 5.8903 - mae: 1.7902 - val_loss: 3.7112 - val_mse: 3.7112 - val_mae: 1.5210\n",
      "\n",
      "Epoch 00618: val_mae did not improve from 1.50869\n",
      "Epoch 619/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6144 - mse: 5.6144 - mae: 1.7641 - val_loss: 3.6802 - val_mse: 3.6802 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00619: val_mae did not improve from 1.50869\n",
      "Epoch 620/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4815 - mse: 5.4815 - mae: 1.6755 - val_loss: 3.6976 - val_mse: 3.6976 - val_mae: 1.5175\n",
      "\n",
      "Epoch 00620: val_mae did not improve from 1.50869\n",
      "Epoch 621/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5018 - mse: 4.5018 - mae: 1.6342 - val_loss: 3.7014 - val_mse: 3.7014 - val_mae: 1.5183\n",
      "\n",
      "Epoch 00621: val_mae did not improve from 1.50869\n",
      "Epoch 622/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0027 - mse: 5.0027 - mae: 1.6773 - val_loss: 3.6987 - val_mse: 3.6987 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00622: val_mae did not improve from 1.50869\n",
      "Epoch 623/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.3168 - mse: 5.3168 - mae: 1.7546 - val_loss: 3.6819 - val_mse: 3.6819 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00623: val_mae did not improve from 1.50869\n",
      "Epoch 624/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8144 - mse: 5.8144 - mae: 1.8170 - val_loss: 3.7002 - val_mse: 3.7002 - val_mae: 1.5181\n",
      "\n",
      "Epoch 00624: val_mae did not improve from 1.50869\n",
      "Epoch 625/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8774 - mse: 4.8774 - mae: 1.6458 - val_loss: 3.6914 - val_mse: 3.6914 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00625: val_mae did not improve from 1.50869\n",
      "Epoch 626/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9260 - mse: 4.9260 - mae: 1.6496 - val_loss: 3.6826 - val_mse: 3.6826 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00626: val_mae did not improve from 1.50869\n",
      "Epoch 627/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1622 - mse: 5.1622 - mae: 1.6943 - val_loss: 3.7201 - val_mse: 3.7201 - val_mae: 1.5228\n",
      "\n",
      "Epoch 00627: val_mae did not improve from 1.50869\n",
      "Epoch 628/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.7754 - mse: 4.7754 - mae: 1.6316 - val_loss: 3.6847 - val_mse: 3.6847 - val_mae: 1.5129\n",
      "\n",
      "Epoch 00628: val_mae did not improve from 1.50869\n",
      "Epoch 629/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2671 - mse: 5.2671 - mae: 1.7497 - val_loss: 3.6843 - val_mse: 3.6843 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00629: val_mae did not improve from 1.50869\n",
      "Epoch 630/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0323 - mse: 5.0323 - mae: 1.6872 - val_loss: 3.6816 - val_mse: 3.6816 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00630: val_mae did not improve from 1.50869\n",
      "Epoch 631/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4351 - mse: 4.4351 - mae: 1.6200 - val_loss: 3.6839 - val_mse: 3.6839 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00631: val_mae did not improve from 1.50869\n",
      "Epoch 632/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8239 - mse: 4.8239 - mae: 1.6528 - val_loss: 3.6944 - val_mse: 3.6944 - val_mae: 1.5159\n",
      "\n",
      "Epoch 00632: val_mae did not improve from 1.50869\n",
      "Epoch 633/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9983 - mse: 4.9983 - mae: 1.6808 - val_loss: 3.6991 - val_mse: 3.6991 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00633: val_mae did not improve from 1.50869\n",
      "Epoch 634/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2712 - mse: 4.2712 - mae: 1.5649 - val_loss: 3.7058 - val_mse: 3.7058 - val_mae: 1.5190\n",
      "\n",
      "Epoch 00634: val_mae did not improve from 1.50869\n",
      "Epoch 635/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1072 - mse: 5.1072 - mae: 1.7042 - val_loss: 3.6867 - val_mse: 3.6867 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00635: val_mae did not improve from 1.50869\n",
      "Epoch 636/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7457 - mse: 4.7457 - mae: 1.6520 - val_loss: 3.6873 - val_mse: 3.6873 - val_mae: 1.5136\n",
      "\n",
      "Epoch 00636: val_mae did not improve from 1.50869\n",
      "Epoch 637/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6598 - mse: 4.6598 - mae: 1.6361 - val_loss: 3.6929 - val_mse: 3.6929 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00637: val_mae did not improve from 1.50869\n",
      "Epoch 638/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5136 - mse: 5.5136 - mae: 1.7148 - val_loss: 3.7097 - val_mse: 3.7097 - val_mae: 1.5201\n",
      "\n",
      "Epoch 00638: val_mae did not improve from 1.50869\n",
      "Epoch 639/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6120 - mse: 4.6120 - mae: 1.6383 - val_loss: 3.6979 - val_mse: 3.6979 - val_mae: 1.5168\n",
      "\n",
      "Epoch 00639: val_mae did not improve from 1.50869\n",
      "Epoch 640/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0848 - mse: 5.0848 - mae: 1.7125 - val_loss: 3.6914 - val_mse: 3.6914 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00640: val_mae did not improve from 1.50869\n",
      "Epoch 641/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9698 - mse: 4.9698 - mae: 1.6854 - val_loss: 3.7095 - val_mse: 3.7095 - val_mae: 1.5197\n",
      "\n",
      "Epoch 00641: val_mae did not improve from 1.50869\n",
      "Epoch 642/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9702 - mse: 4.9702 - mae: 1.7423 - val_loss: 3.6844 - val_mse: 3.6844 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00642: val_mae did not improve from 1.50869\n",
      "Epoch 643/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9691 - mse: 5.9691 - mae: 1.8078 - val_loss: 3.6866 - val_mse: 3.6866 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00643: val_mae did not improve from 1.50869\n",
      "Epoch 644/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2035 - mse: 5.2035 - mae: 1.6782 - val_loss: 3.6896 - val_mse: 3.6896 - val_mae: 1.5141\n",
      "\n",
      "Epoch 00644: val_mae did not improve from 1.50869\n",
      "Epoch 645/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3411 - mse: 4.3411 - mae: 1.5711 - val_loss: 3.7067 - val_mse: 3.7067 - val_mae: 1.5191\n",
      "\n",
      "Epoch 00645: val_mae did not improve from 1.50869\n",
      "Epoch 646/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5583 - mse: 4.5583 - mae: 1.6078 - val_loss: 3.6925 - val_mse: 3.6925 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00646: val_mae did not improve from 1.50869\n",
      "Epoch 647/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7106 - mse: 5.7106 - mae: 1.8111 - val_loss: 3.7089 - val_mse: 3.7089 - val_mae: 1.5196\n",
      "\n",
      "Epoch 00647: val_mae did not improve from 1.50869\n",
      "Epoch 648/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6361 - mse: 5.6361 - mae: 1.8051 - val_loss: 3.6851 - val_mse: 3.6851 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00648: val_mae did not improve from 1.50869\n",
      "Epoch 649/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6085 - mse: 5.6085 - mae: 1.7077 - val_loss: 3.6808 - val_mse: 3.6808 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00649: val_mae did not improve from 1.50869\n",
      "Epoch 650/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6488 - mse: 4.6488 - mae: 1.6206 - val_loss: 3.6748 - val_mse: 3.6748 - val_mae: 1.5085\n",
      "\n",
      "Epoch 00650: val_mae improved from 1.50869 to 1.50854, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 651/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7658 - mse: 4.7658 - mae: 1.6432 - val_loss: 3.6760 - val_mse: 3.6760 - val_mae: 1.5091\n",
      "\n",
      "Epoch 00651: val_mae did not improve from 1.50854\n",
      "Epoch 652/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5299 - mse: 5.5299 - mae: 1.7184 - val_loss: 3.6971 - val_mse: 3.6971 - val_mae: 1.5163\n",
      "\n",
      "Epoch 00652: val_mae did not improve from 1.50854\n",
      "Epoch 653/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8490 - mse: 4.8490 - mae: 1.6667 - val_loss: 3.6818 - val_mse: 3.6818 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00653: val_mae did not improve from 1.50854\n",
      "Epoch 654/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6602 - mse: 4.6602 - mae: 1.6427 - val_loss: 3.7108 - val_mse: 3.7108 - val_mae: 1.5199\n",
      "\n",
      "Epoch 00654: val_mae did not improve from 1.50854\n",
      "Epoch 655/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1169 - mse: 5.1169 - mae: 1.6929 - val_loss: 3.6990 - val_mse: 3.6990 - val_mae: 1.5170\n",
      "\n",
      "Epoch 00655: val_mae did not improve from 1.50854\n",
      "Epoch 656/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3672 - mse: 4.3672 - mae: 1.5619 - val_loss: 3.7057 - val_mse: 3.7057 - val_mae: 1.5189\n",
      "\n",
      "Epoch 00656: val_mae did not improve from 1.50854\n",
      "Epoch 657/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1718 - mse: 5.1718 - mae: 1.7323 - val_loss: 3.6853 - val_mse: 3.6853 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00657: val_mae did not improve from 1.50854\n",
      "Epoch 658/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6667 - mse: 5.6667 - mae: 1.8280 - val_loss: 3.6874 - val_mse: 3.6874 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00658: val_mae did not improve from 1.50854\n",
      "Epoch 659/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7485 - mse: 4.7485 - mae: 1.6737 - val_loss: 3.6941 - val_mse: 3.6941 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00659: val_mae did not improve from 1.50854\n",
      "Epoch 660/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9930 - mse: 4.9930 - mae: 1.7157 - val_loss: 3.6985 - val_mse: 3.6985 - val_mae: 1.5165\n",
      "\n",
      "Epoch 00660: val_mae did not improve from 1.50854\n",
      "Epoch 661/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9844 - mse: 4.9844 - mae: 1.7003 - val_loss: 3.6891 - val_mse: 3.6891 - val_mae: 1.5137\n",
      "\n",
      "Epoch 00661: val_mae did not improve from 1.50854\n",
      "Epoch 662/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2738 - mse: 5.2738 - mae: 1.7205 - val_loss: 3.6824 - val_mse: 3.6824 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00662: val_mae did not improve from 1.50854\n",
      "Epoch 663/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3888 - mse: 4.3888 - mae: 1.5808 - val_loss: 3.6817 - val_mse: 3.6817 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00663: val_mae did not improve from 1.50854\n",
      "Epoch 664/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5819 - mse: 4.5819 - mae: 1.6197 - val_loss: 3.7009 - val_mse: 3.7009 - val_mae: 1.5173\n",
      "\n",
      "Epoch 00664: val_mae did not improve from 1.50854\n",
      "Epoch 665/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5944 - mse: 4.5944 - mae: 1.6264 - val_loss: 3.6894 - val_mse: 3.6894 - val_mae: 1.5138\n",
      "\n",
      "Epoch 00665: val_mae did not improve from 1.50854\n",
      "Epoch 666/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1204 - mse: 5.1204 - mae: 1.6777 - val_loss: 3.6882 - val_mse: 3.6882 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00666: val_mae did not improve from 1.50854\n",
      "Epoch 667/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7124 - mse: 4.7124 - mae: 1.6523 - val_loss: 3.6869 - val_mse: 3.6869 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00667: val_mae did not improve from 1.50854\n",
      "Epoch 668/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3623 - mse: 4.3623 - mae: 1.6212 - val_loss: 3.6971 - val_mse: 3.6971 - val_mae: 1.5159\n",
      "\n",
      "Epoch 00668: val_mae did not improve from 1.50854\n",
      "Epoch 669/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3619 - mse: 5.3619 - mae: 1.7128 - val_loss: 3.6900 - val_mse: 3.6900 - val_mae: 1.5134\n",
      "\n",
      "Epoch 00669: val_mae did not improve from 1.50854\n",
      "Epoch 670/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6045 - mse: 4.6045 - mae: 1.6420 - val_loss: 3.6853 - val_mse: 3.6853 - val_mae: 1.5119\n",
      "\n",
      "Epoch 00670: val_mae did not improve from 1.50854\n",
      "Epoch 671/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9118 - mse: 4.9118 - mae: 1.6506 - val_loss: 3.7095 - val_mse: 3.7095 - val_mae: 1.5190\n",
      "\n",
      "Epoch 00671: val_mae did not improve from 1.50854\n",
      "Epoch 672/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4390 - mse: 4.4390 - mae: 1.6061 - val_loss: 3.6901 - val_mse: 3.6901 - val_mae: 1.5136\n",
      "\n",
      "Epoch 00672: val_mae did not improve from 1.50854\n",
      "Epoch 673/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9481 - mse: 4.9481 - mae: 1.7025 - val_loss: 3.6938 - val_mse: 3.6938 - val_mae: 1.5148\n",
      "\n",
      "Epoch 00673: val_mae did not improve from 1.50854\n",
      "Epoch 674/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3548 - mse: 4.3548 - mae: 1.5795 - val_loss: 3.6977 - val_mse: 3.6977 - val_mae: 1.5160\n",
      "\n",
      "Epoch 00674: val_mae did not improve from 1.50854\n",
      "Epoch 675/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8250 - mse: 4.8250 - mae: 1.6723 - val_loss: 3.6904 - val_mse: 3.6904 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00675: val_mae did not improve from 1.50854\n",
      "Epoch 676/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8409 - mse: 4.8409 - mae: 1.7028 - val_loss: 3.6821 - val_mse: 3.6821 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00676: val_mae did not improve from 1.50854\n",
      "Epoch 677/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7123 - mse: 4.7123 - mae: 1.6065 - val_loss: 3.6843 - val_mse: 3.6843 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00677: val_mae did not improve from 1.50854\n",
      "Epoch 678/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0183 - mse: 5.0183 - mae: 1.6854 - val_loss: 3.7050 - val_mse: 3.7050 - val_mae: 1.5180\n",
      "\n",
      "Epoch 00678: val_mae did not improve from 1.50854\n",
      "Epoch 679/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6028 - mse: 5.6028 - mae: 1.7464 - val_loss: 3.6799 - val_mse: 3.6799 - val_mae: 1.5103\n",
      "\n",
      "Epoch 00679: val_mae did not improve from 1.50854\n",
      "Epoch 680/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3370 - mse: 5.3370 - mae: 1.7397 - val_loss: 3.6904 - val_mse: 3.6904 - val_mae: 1.5135\n",
      "\n",
      "Epoch 00680: val_mae did not improve from 1.50854\n",
      "Epoch 681/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7819 - mse: 4.7819 - mae: 1.6744 - val_loss: 3.6818 - val_mse: 3.6818 - val_mae: 1.5108\n",
      "\n",
      "Epoch 00681: val_mae did not improve from 1.50854\n",
      "Epoch 682/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4269 - mse: 4.4269 - mae: 1.6334 - val_loss: 3.6932 - val_mse: 3.6932 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00682: val_mae did not improve from 1.50854\n",
      "Epoch 683/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9889 - mse: 4.9889 - mae: 1.6889 - val_loss: 3.7149 - val_mse: 3.7149 - val_mae: 1.5201\n",
      "\n",
      "Epoch 00683: val_mae did not improve from 1.50854\n",
      "Epoch 684/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7741 - mse: 4.7741 - mae: 1.6944 - val_loss: 3.7145 - val_mse: 3.7145 - val_mae: 1.5201\n",
      "\n",
      "Epoch 00684: val_mae did not improve from 1.50854\n",
      "Epoch 685/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9103 - mse: 4.9103 - mae: 1.6442 - val_loss: 3.6931 - val_mse: 3.6931 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00685: val_mae did not improve from 1.50854\n",
      "Epoch 686/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1187 - mse: 5.1187 - mae: 1.7235 - val_loss: 3.6885 - val_mse: 3.6885 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00686: val_mae did not improve from 1.50854\n",
      "Epoch 687/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4465 - mse: 4.4465 - mae: 1.6032 - val_loss: 3.6836 - val_mse: 3.6836 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00687: val_mae did not improve from 1.50854\n",
      "Epoch 688/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7104 - mse: 4.7104 - mae: 1.6538 - val_loss: 3.7069 - val_mse: 3.7069 - val_mae: 1.5184\n",
      "\n",
      "Epoch 00688: val_mae did not improve from 1.50854\n",
      "Epoch 689/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8053 - mse: 4.8053 - mae: 1.6603 - val_loss: 3.6877 - val_mse: 3.6877 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00689: val_mae did not improve from 1.50854\n",
      "Epoch 690/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4714 - mse: 5.4714 - mae: 1.7164 - val_loss: 3.7135 - val_mse: 3.7135 - val_mae: 1.5198\n",
      "\n",
      "Epoch 00690: val_mae did not improve from 1.50854\n",
      "Epoch 691/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2737 - mse: 4.2737 - mae: 1.6215 - val_loss: 3.6931 - val_mse: 3.6931 - val_mae: 1.5143\n",
      "\n",
      "Epoch 00691: val_mae did not improve from 1.50854\n",
      "Epoch 692/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2559 - mse: 5.2559 - mae: 1.6822 - val_loss: 3.6910 - val_mse: 3.6910 - val_mae: 1.5136\n",
      "\n",
      "Epoch 00692: val_mae did not improve from 1.50854\n",
      "Epoch 693/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2500 - mse: 5.2500 - mae: 1.6794 - val_loss: 3.6866 - val_mse: 3.6866 - val_mae: 1.5122\n",
      "\n",
      "Epoch 00693: val_mae did not improve from 1.50854\n",
      "Epoch 694/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6223 - mse: 4.6223 - mae: 1.6419 - val_loss: 3.6865 - val_mse: 3.6865 - val_mae: 1.5122\n",
      "\n",
      "Epoch 00694: val_mae did not improve from 1.50854\n",
      "Epoch 695/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2615 - mse: 4.2615 - mae: 1.5973 - val_loss: 3.6876 - val_mse: 3.6876 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00695: val_mae did not improve from 1.50854\n",
      "Epoch 696/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0173 - mse: 5.0173 - mae: 1.7033 - val_loss: 3.6905 - val_mse: 3.6905 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00696: val_mae did not improve from 1.50854\n",
      "Epoch 697/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0442 - mse: 5.0442 - mae: 1.7105 - val_loss: 3.6905 - val_mse: 3.6905 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00697: val_mae did not improve from 1.50854\n",
      "Epoch 698/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1356 - mse: 5.1356 - mae: 1.7024 - val_loss: 3.7014 - val_mse: 3.7014 - val_mae: 1.5164\n",
      "\n",
      "Epoch 00698: val_mae did not improve from 1.50854\n",
      "Epoch 699/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4607 - mse: 4.4607 - mae: 1.6140 - val_loss: 3.6955 - val_mse: 3.6955 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00699: val_mae did not improve from 1.50854\n",
      "Epoch 700/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3512 - mse: 4.3512 - mae: 1.6128 - val_loss: 3.7050 - val_mse: 3.7050 - val_mae: 1.5173\n",
      "\n",
      "Epoch 00700: val_mae did not improve from 1.50854\n",
      "Epoch 701/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1034 - mse: 5.1034 - mae: 1.6955 - val_loss: 3.6797 - val_mse: 3.6797 - val_mae: 1.5093\n",
      "\n",
      "Epoch 00701: val_mae did not improve from 1.50854\n",
      "Epoch 702/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9100 - mse: 4.9100 - mae: 1.6927 - val_loss: 3.6926 - val_mse: 3.6926 - val_mae: 1.5136\n",
      "\n",
      "Epoch 00702: val_mae did not improve from 1.50854\n",
      "Epoch 703/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5511 - mse: 4.5511 - mae: 1.6158 - val_loss: 3.6991 - val_mse: 3.6991 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00703: val_mae did not improve from 1.50854\n",
      "Epoch 704/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7176 - mse: 4.7176 - mae: 1.6958 - val_loss: 3.6812 - val_mse: 3.6812 - val_mae: 1.5098\n",
      "\n",
      "Epoch 00704: val_mae did not improve from 1.50854\n",
      "Epoch 705/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6788 - mse: 4.6788 - mae: 1.6337 - val_loss: 3.6857 - val_mse: 3.6857 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00705: val_mae did not improve from 1.50854\n",
      "Epoch 706/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0129 - mse: 5.0129 - mae: 1.6943 - val_loss: 3.7075 - val_mse: 3.7075 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00706: val_mae did not improve from 1.50854\n",
      "Epoch 707/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6931 - mse: 5.6931 - mae: 1.8489 - val_loss: 3.6872 - val_mse: 3.6872 - val_mae: 1.5119\n",
      "\n",
      "Epoch 00707: val_mae did not improve from 1.50854\n",
      "Epoch 708/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8278 - mse: 4.8278 - mae: 1.6549 - val_loss: 3.6801 - val_mse: 3.6801 - val_mae: 1.5093\n",
      "\n",
      "Epoch 00708: val_mae did not improve from 1.50854\n",
      "Epoch 709/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7252 - mse: 4.7252 - mae: 1.6905 - val_loss: 3.6828 - val_mse: 3.6828 - val_mae: 1.5101\n",
      "\n",
      "Epoch 00709: val_mae did not improve from 1.50854\n",
      "Epoch 710/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5950 - mse: 4.5950 - mae: 1.5823 - val_loss: 3.6844 - val_mse: 3.6844 - val_mae: 1.5106\n",
      "\n",
      "Epoch 00710: val_mae did not improve from 1.50854\n",
      "Epoch 711/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3777 - mse: 5.3777 - mae: 1.7651 - val_loss: 3.6826 - val_mse: 3.6826 - val_mae: 1.5099\n",
      "\n",
      "Epoch 00711: val_mae did not improve from 1.50854\n",
      "Epoch 712/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6123 - mse: 4.6123 - mae: 1.6518 - val_loss: 3.7205 - val_mse: 3.7205 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00712: val_mae did not improve from 1.50854\n",
      "Epoch 713/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5836 - mse: 5.5836 - mae: 1.7446 - val_loss: 3.7309 - val_mse: 3.7309 - val_mae: 1.5229\n",
      "\n",
      "Epoch 00713: val_mae did not improve from 1.50854\n",
      "Epoch 714/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1208 - mse: 5.1208 - mae: 1.7426 - val_loss: 3.6984 - val_mse: 3.6984 - val_mae: 1.5148\n",
      "\n",
      "Epoch 00714: val_mae did not improve from 1.50854\n",
      "Epoch 715/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6910 - mse: 4.6910 - mae: 1.6365 - val_loss: 3.6798 - val_mse: 3.6798 - val_mae: 1.5086\n",
      "\n",
      "Epoch 00715: val_mae did not improve from 1.50854\n",
      "Epoch 716/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8673 - mse: 4.8673 - mae: 1.6892 - val_loss: 3.6840 - val_mse: 3.6840 - val_mae: 1.5101\n",
      "\n",
      "Epoch 00716: val_mae did not improve from 1.50854\n",
      "Epoch 717/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1877 - mse: 5.1877 - mae: 1.6889 - val_loss: 3.7015 - val_mse: 3.7015 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00717: val_mae did not improve from 1.50854\n",
      "Epoch 718/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0170 - mse: 5.0170 - mae: 1.7084 - val_loss: 3.6981 - val_mse: 3.6981 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00718: val_mae did not improve from 1.50854\n",
      "Epoch 719/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6968 - mse: 5.6968 - mae: 1.7297 - val_loss: 3.7319 - val_mse: 3.7319 - val_mae: 1.5227\n",
      "\n",
      "Epoch 00719: val_mae did not improve from 1.50854\n",
      "Epoch 720/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9390 - mse: 4.9390 - mae: 1.7019 - val_loss: 3.6882 - val_mse: 3.6882 - val_mae: 1.5112\n",
      "\n",
      "Epoch 00720: val_mae did not improve from 1.50854\n",
      "Epoch 721/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0451 - mse: 5.0451 - mae: 1.6463 - val_loss: 3.6869 - val_mse: 3.6869 - val_mae: 1.5109\n",
      "\n",
      "Epoch 00721: val_mae did not improve from 1.50854\n",
      "Epoch 722/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8417 - mse: 4.8417 - mae: 1.6656 - val_loss: 3.6984 - val_mse: 3.6984 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00722: val_mae did not improve from 1.50854\n",
      "Epoch 723/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.1466 - mse: 4.1466 - mae: 1.5314 - val_loss: 3.7020 - val_mse: 3.7020 - val_mae: 1.5157\n",
      "\n",
      "Epoch 00723: val_mae did not improve from 1.50854\n",
      "Epoch 724/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8192 - mse: 4.8192 - mae: 1.6681 - val_loss: 3.6912 - val_mse: 3.6912 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00724: val_mae did not improve from 1.50854\n",
      "Epoch 725/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3002 - mse: 4.3002 - mae: 1.5849 - val_loss: 3.6907 - val_mse: 3.6907 - val_mae: 1.5122\n",
      "\n",
      "Epoch 00725: val_mae did not improve from 1.50854\n",
      "Epoch 726/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7526 - mse: 4.7526 - mae: 1.6650 - val_loss: 3.6916 - val_mse: 3.6916 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00726: val_mae did not improve from 1.50854\n",
      "Epoch 727/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0574 - mse: 5.0574 - mae: 1.6935 - val_loss: 3.7073 - val_mse: 3.7073 - val_mae: 1.5170\n",
      "\n",
      "Epoch 00727: val_mae did not improve from 1.50854\n",
      "Epoch 728/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3451 - mse: 4.3451 - mae: 1.5565 - val_loss: 3.7061 - val_mse: 3.7061 - val_mae: 1.5166\n",
      "\n",
      "Epoch 00728: val_mae did not improve from 1.50854\n",
      "Epoch 729/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2336 - mse: 5.2336 - mae: 1.6598 - val_loss: 3.7063 - val_mse: 3.7063 - val_mae: 1.5166\n",
      "\n",
      "Epoch 00729: val_mae did not improve from 1.50854\n",
      "Epoch 730/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7171 - mse: 4.7171 - mae: 1.6865 - val_loss: 3.6935 - val_mse: 3.6935 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00730: val_mae did not improve from 1.50854\n",
      "Epoch 731/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5148 - mse: 4.5148 - mae: 1.5734 - val_loss: 3.6899 - val_mse: 3.6899 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00731: val_mae did not improve from 1.50854\n",
      "Epoch 732/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7845 - mse: 4.7845 - mae: 1.6426 - val_loss: 3.6949 - val_mse: 3.6949 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00732: val_mae did not improve from 1.50854\n",
      "Epoch 733/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0637 - mse: 5.0637 - mae: 1.6572 - val_loss: 3.7098 - val_mse: 3.7098 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00733: val_mae did not improve from 1.50854\n",
      "Epoch 734/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4196 - mse: 4.4196 - mae: 1.6128 - val_loss: 3.7018 - val_mse: 3.7018 - val_mae: 1.5150\n",
      "\n",
      "Epoch 00734: val_mae did not improve from 1.50854\n",
      "Epoch 735/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7832 - mse: 4.7832 - mae: 1.6559 - val_loss: 3.6916 - val_mse: 3.6916 - val_mae: 1.5121\n",
      "\n",
      "Epoch 00735: val_mae did not improve from 1.50854\n",
      "Epoch 736/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9026 - mse: 4.9026 - mae: 1.6669 - val_loss: 3.6963 - val_mse: 3.6963 - val_mae: 1.5135\n",
      "\n",
      "Epoch 00736: val_mae did not improve from 1.50854\n",
      "Epoch 737/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9890 - mse: 4.9890 - mae: 1.6987 - val_loss: 3.6886 - val_mse: 3.6886 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00737: val_mae did not improve from 1.50854\n",
      "Epoch 738/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0710 - mse: 5.0710 - mae: 1.6330 - val_loss: 3.6996 - val_mse: 3.6996 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00738: val_mae did not improve from 1.50854\n",
      "Epoch 739/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6019 - mse: 4.6019 - mae: 1.6398 - val_loss: 3.7388 - val_mse: 3.7388 - val_mae: 1.5240\n",
      "\n",
      "Epoch 00739: val_mae did not improve from 1.50854\n",
      "Epoch 740/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3824 - mse: 4.3824 - mae: 1.6112 - val_loss: 3.7071 - val_mse: 3.7071 - val_mae: 1.5165\n",
      "\n",
      "Epoch 00740: val_mae did not improve from 1.50854\n",
      "Epoch 741/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6189 - mse: 4.6189 - mae: 1.6584 - val_loss: 3.7031 - val_mse: 3.7031 - val_mae: 1.5156\n",
      "\n",
      "Epoch 00741: val_mae did not improve from 1.50854\n",
      "Epoch 742/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.6380 - mse: 4.6380 - mae: 1.6567 - val_loss: 3.6886 - val_mse: 3.6886 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00742: val_mae did not improve from 1.50854\n",
      "Epoch 743/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0331 - mse: 5.0331 - mae: 1.6544 - val_loss: 3.6997 - val_mse: 3.6997 - val_mae: 1.5144\n",
      "\n",
      "Epoch 00743: val_mae did not improve from 1.50854\n",
      "Epoch 744/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.7442 - mse: 5.7442 - mae: 1.7932 - val_loss: 3.7215 - val_mse: 3.7215 - val_mae: 1.5199\n",
      "\n",
      "Epoch 00744: val_mae did not improve from 1.50854\n",
      "Epoch 745/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0195 - mse: 5.0195 - mae: 1.7328 - val_loss: 3.6956 - val_mse: 3.6956 - val_mae: 1.5129\n",
      "\n",
      "Epoch 00745: val_mae did not improve from 1.50854\n",
      "Epoch 746/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6039 - mse: 5.6039 - mae: 1.7145 - val_loss: 3.7049 - val_mse: 3.7049 - val_mae: 1.5156\n",
      "\n",
      "Epoch 00746: val_mae did not improve from 1.50854\n",
      "Epoch 747/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7223 - mse: 4.7223 - mae: 1.5897 - val_loss: 3.6880 - val_mse: 3.6880 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00747: val_mae did not improve from 1.50854\n",
      "Epoch 748/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8123 - mse: 4.8123 - mae: 1.6737 - val_loss: 3.7002 - val_mse: 3.7002 - val_mae: 1.5143\n",
      "\n",
      "Epoch 00748: val_mae did not improve from 1.50854\n",
      "Epoch 749/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3137 - mse: 5.3137 - mae: 1.7356 - val_loss: 3.7159 - val_mse: 3.7159 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00749: val_mae did not improve from 1.50854\n",
      "Epoch 750/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8531 - mse: 4.8531 - mae: 1.6690 - val_loss: 3.6982 - val_mse: 3.6982 - val_mae: 1.5135\n",
      "\n",
      "Epoch 00750: val_mae did not improve from 1.50854\n",
      "Epoch 00750: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def show_info(model, X, y, log, weights = None):\n",
    "    '''\n",
    "    Show metrics about the evaluation model and plots about loss, rmse and rmspe\n",
    "    '''\n",
    "    if (log != None):\n",
    "        # summarize history for loss\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['loss'])\n",
    "        plt.plot(log.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MAE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mae'])\n",
    "        plt.plot(log.history['val_mae'])\n",
    "        plt.title('Model MAE')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MSE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mse'])\n",
    "        plt.plot(log.history['val_mse'])\n",
    "        plt.title('Model MSE')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    if (weights != None):\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    predictions = model.predict(X, verbose=1)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    mae= mean_absolute_error(y, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "show_info(model, x_test, y_test, log, weights='/home/m-marouni/Documents/CE-901/Heathrow/best_weights')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-06T20:07:31.507787</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m4a8f4eef9e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#m4a8f4eef9e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.929548\" xlink:href=\"#m4a8f4eef9e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(161.385798 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"265.74688\" xlink:href=\"#m4a8f4eef9e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(256.20313 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.564213\" xlink:href=\"#m4a8f4eef9e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(351.020463 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"455.381545\" xlink:href=\"#m4a8f4eef9e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(445.837795 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"550.198877\" xlink:href=\"#m4a8f4eef9e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(540.655127 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"645.016209\" xlink:href=\"#m4a8f4eef9e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(635.472459 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"739.833541\" xlink:href=\"#m4a8f4eef9e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(730.289791 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m05bf2cac85\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m05bf2cac85\" y=\"523.883008\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 527.682227)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m05bf2cac85\" y=\"458.773164\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 462.572382)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m05bf2cac85\" y=\"393.66332\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 397.462538)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m05bf2cac85\" y=\"328.553475\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 332.352694)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m05bf2cac85\" y=\"263.443631\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 267.24285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m05bf2cac85\" y=\"198.333787\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 202.133006)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m05bf2cac85\" y=\"133.223943\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 137.023161)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m05bf2cac85\" y=\"68.114098\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 71.913317)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 303.775937)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p51b44bd8d1)\" d=\"M 76.112216 47.027216 \nL 78.008563 165.828838 \nL 80.853083 319.718899 \nL 82.749429 401.961817 \nL 83.697602 431.592255 \nL 84.645776 454.985778 \nL 85.593949 474.973492 \nL 86.542122 488.564456 \nL 89.386642 502.43661 \nL 90.334816 502.79058 \nL 91.282989 504.114037 \nL 92.231162 504.22955 \nL 93.179336 502.678781 \nL 94.127509 503.928886 \nL 95.075682 503.602765 \nL 96.023856 503.801731 \nL 96.972029 504.967563 \nL 97.920202 504.801413 \nL 98.868376 505.35661 \nL 99.816549 503.786382 \nL 100.764722 505.075904 \nL 101.712896 504.73773 \nL 102.661069 506.38637 \nL 103.609242 505.477016 \nL 104.557416 503.81125 \nL 105.505589 504.818632 \nL 106.453762 504.504363 \nL 107.401936 504.399525 \nL 108.350109 506.094841 \nL 109.298282 506.075244 \nL 110.246456 503.419415 \nL 111.194629 505.172869 \nL 112.142802 501.996428 \nL 113.090975 504.37134 \nL 114.039149 506.274869 \nL 115.935495 505.945376 \nL 116.883669 506.505745 \nL 117.831842 504.480967 \nL 118.780015 507.095057 \nL 119.728189 504.90537 \nL 120.676362 507.516107 \nL 121.624535 504.142327 \nL 122.572709 507.875219 \nL 123.520882 503.857106 \nL 124.469055 505.429198 \nL 125.417229 507.589092 \nL 126.365402 507.103222 \nL 127.313575 507.172835 \nL 128.261749 507.063867 \nL 129.209922 506.442373 \nL 130.158095 508.169499 \nL 131.106269 506.766669 \nL 132.054442 507.633415 \nL 133.002615 507.91812 \nL 133.950789 509.25094 \nL 134.898962 507.235451 \nL 135.847135 503.385108 \nL 136.795309 503.136485 \nL 137.743482 508.517509 \nL 138.691655 508.101259 \nL 139.639828 503.459881 \nL 140.588002 506.594011 \nL 141.536175 505.315709 \nL 142.484348 507.224671 \nL 143.432522 506.573899 \nL 144.380695 507.773696 \nL 145.328868 506.136152 \nL 146.277042 509.15698 \nL 147.225215 505.881866 \nL 148.173388 509.685384 \nL 149.121562 508.441879 \nL 150.069735 506.062819 \nL 151.966082 508.991743 \nL 152.914255 507.543944 \nL 153.862428 509.425435 \nL 154.810602 508.650284 \nL 155.758775 508.922123 \nL 156.706948 509.467398 \nL 157.655122 513.269395 \nL 158.603295 508.874591 \nL 159.551468 508.822426 \nL 160.499642 510.475916 \nL 161.447815 507.481366 \nL 162.395988 507.273855 \nL 163.344162 508.852243 \nL 164.292335 511.213222 \nL 165.240508 510.099629 \nL 166.188681 511.207167 \nL 167.136855 510.353182 \nL 168.085028 511.145881 \nL 169.033201 510.485839 \nL 169.981375 510.738591 \nL 170.929548 508.306733 \nL 171.877721 511.971763 \nL 172.825895 508.579671 \nL 173.774068 508.903396 \nL 174.722241 510.520462 \nL 175.670415 510.944971 \nL 176.618588 508.181887 \nL 177.566761 510.384999 \nL 178.514935 511.946255 \nL 179.463108 511.692298 \nL 180.411281 511.890551 \nL 181.359455 510.334169 \nL 182.307628 512.548432 \nL 183.255801 513.537614 \nL 184.203975 514.782963 \nL 185.152148 508.975909 \nL 186.100321 510.379199 \nL 187.048495 513.965011 \nL 187.996668 509.118848 \nL 188.944841 513.343013 \nL 189.893015 511.808308 \nL 190.841188 512.195766 \nL 191.789361 512.33665 \nL 192.737535 514.524772 \nL 193.685708 510.872731 \nL 194.633881 513.041219 \nL 195.582054 511.083378 \nL 196.530228 515.243468 \nL 197.478401 513.942402 \nL 198.426574 513.801264 \nL 199.374748 513.2762 \nL 200.322921 514.915427 \nL 201.271094 513.571691 \nL 202.219268 513.290444 \nL 203.167441 513.626861 \nL 204.115614 517.198478 \nL 206.011961 516.116479 \nL 206.960134 513.724435 \nL 207.908308 518.459593 \nL 208.856481 515.204361 \nL 209.804654 516.943919 \nL 210.752828 516.059918 \nL 211.701001 516.649739 \nL 212.649174 517.770658 \nL 213.597348 515.517946 \nL 214.545521 516.276078 \nL 215.493694 516.190234 \nL 216.441868 515.402266 \nL 217.390041 518.573708 \nL 218.338214 519.61169 \nL 219.286388 516.682338 \nL 220.234561 518.09452 \nL 221.182734 517.572778 \nL 222.130907 515.598171 \nL 223.079081 520.178256 \nL 224.027254 516.240213 \nL 224.975427 517.404219 \nL 225.923601 517.067454 \nL 226.871774 517.823506 \nL 227.819947 519.301426 \nL 228.768121 516.103439 \nL 229.716294 520.268801 \nL 230.664467 517.907823 \nL 231.612641 517.655164 \nL 232.560814 517.530002 \nL 233.508987 517.222893 \nL 234.457161 520.030616 \nL 235.405334 516.218486 \nL 236.353507 520.304536 \nL 237.301681 520.698309 \nL 238.249854 518.333164 \nL 239.198027 516.806016 \nL 240.146201 517.110218 \nL 241.094374 519.291256 \nL 242.042547 520.568626 \nL 242.990721 518.351693 \nL 243.938894 518.463877 \nL 244.887067 521.540552 \nL 245.835241 519.247585 \nL 246.783414 520.914314 \nL 247.731587 519.038572 \nL 248.679761 522.072073 \nL 250.576107 519.937128 \nL 251.52428 520.636445 \nL 252.472454 523.081076 \nL 253.420627 520.902634 \nL 254.3688 521.867835 \nL 255.316974 519.980612 \nL 256.265147 520.280375 \nL 257.21332 520.323307 \nL 258.161494 523.274273 \nL 259.109667 521.311775 \nL 260.05784 520.594786 \nL 261.006014 522.023591 \nL 261.954187 519.980966 \nL 262.90236 519.933191 \nL 263.850534 520.836361 \nL 264.798707 522.36672 \nL 266.695054 519.588119 \nL 267.643227 521.007733 \nL 268.5914 522.010365 \nL 269.539574 522.108547 \nL 270.487747 520.155996 \nL 271.43592 521.920993 \nL 272.384094 523.164983 \nL 273.332267 523.477301 \nL 274.28044 522.012606 \nL 275.228614 524.357925 \nL 276.176787 523.080262 \nL 277.12496 523.524933 \nL 278.073133 521.414856 \nL 279.021307 523.743378 \nL 279.96948 520.529954 \nL 280.917653 522.62172 \nL 281.865827 521.837173 \nL 282.814 523.997875 \nL 283.762173 523.614037 \nL 284.710347 523.061795 \nL 285.65852 520.146589 \nL 286.606693 524.396007 \nL 287.554867 521.854622 \nL 288.50304 520.575314 \nL 289.451213 523.672126 \nL 290.399387 520.7035 \nL 291.34756 522.640615 \nL 292.295733 522.379014 \nL 293.243907 522.873273 \nL 294.19208 523.559358 \nL 295.140253 522.376673 \nL 296.088427 524.316825 \nL 297.0366 519.423229 \nL 297.984773 523.333113 \nL 298.932947 524.023203 \nL 299.88112 523.469359 \nL 300.829293 525.69388 \nL 301.777467 523.028731 \nL 302.72564 524.138641 \nL 303.673813 524.789332 \nL 304.621987 521.898913 \nL 305.57016 524.390449 \nL 306.518333 523.833625 \nL 307.466506 524.121013 \nL 308.41468 524.636365 \nL 309.362853 521.145252 \nL 310.311026 523.601941 \nL 311.2592 524.574364 \nL 313.155546 522.906959 \nL 314.10372 522.695599 \nL 315.051893 523.899736 \nL 316.94824 524.114176 \nL 317.896413 525.372714 \nL 318.844586 524.78162 \nL 319.79276 524.492717 \nL 320.740933 523.026098 \nL 321.689106 524.30273 \nL 322.63728 526.354028 \nL 323.585453 523.475873 \nL 324.533626 525.327559 \nL 325.4818 523.628784 \nL 326.429973 521.616809 \nL 327.378146 524.265784 \nL 328.32632 523.352834 \nL 329.274493 523.749761 \nL 330.222666 521.635158 \nL 333.067186 524.099056 \nL 334.015359 524.34137 \nL 334.963533 522.427714 \nL 335.911706 521.871548 \nL 336.859879 523.736467 \nL 337.808053 522.330606 \nL 338.756226 523.972032 \nL 339.704399 527.765844 \nL 340.652573 525.838683 \nL 341.600746 524.588428 \nL 342.548919 524.530644 \nL 343.497093 521.444102 \nL 344.445266 522.900942 \nL 345.393439 525.567843 \nL 346.341613 524.158809 \nL 347.289786 526.264198 \nL 348.237959 524.359079 \nL 349.186133 526.633381 \nL 350.134306 523.752282 \nL 351.082479 523.567678 \nL 352.030653 525.406636 \nL 352.978826 525.471374 \nL 353.926999 525.309999 \nL 354.875173 524.40144 \nL 355.823346 519.764508 \nL 356.771519 524.708207 \nL 357.719693 522.832676 \nL 358.667866 525.621709 \nL 359.616039 524.172265 \nL 360.564213 524.489718 \nL 361.512386 524.134431 \nL 362.460559 522.913205 \nL 363.408732 525.276897 \nL 364.356906 524.020222 \nL 365.305079 523.408122 \nL 366.253252 525.029125 \nL 367.201426 524.12888 \nL 368.149599 524.01921 \nL 369.097772 526.090286 \nL 370.045946 525.219641 \nL 370.994119 522.74536 \nL 371.942292 523.411047 \nL 372.890466 527.043441 \nL 373.838639 523.061336 \nL 375.734986 525.180193 \nL 376.683159 525.929656 \nL 377.631332 522.817432 \nL 378.579506 524.699793 \nL 379.527679 522.840183 \nL 380.475852 522.992232 \nL 381.424026 524.504 \nL 382.372199 525.385493 \nL 383.320372 522.559589 \nL 384.268546 524.553718 \nL 385.216719 526.070106 \nL 387.113066 524.983294 \nL 388.061239 524.038397 \nL 389.009412 526.269445 \nL 389.957585 525.456074 \nL 390.905759 525.604304 \nL 391.853932 524.821869 \nL 392.802105 522.181277 \nL 393.750279 526.601478 \nL 394.698452 525.624131 \nL 395.646625 523.820877 \nL 396.594799 524.78257 \nL 397.542972 527.069825 \nL 398.491145 522.718157 \nL 399.439319 525.685926 \nL 400.387492 524.339241 \nL 401.335665 525.196747 \nL 402.283839 523.909851 \nL 403.232012 523.022372 \nL 404.180185 526.664596 \nL 405.128359 527.287252 \nL 406.076532 524.416709 \nL 407.024705 525.313036 \nL 407.972879 525.28356 \nL 408.921052 526.317387 \nL 409.869225 525.684039 \nL 410.817399 523.699819 \nL 411.765572 522.610921 \nL 412.713745 526.961981 \nL 413.661919 525.056167 \nL 414.610092 524.18228 \nL 415.558265 524.147322 \nL 416.506439 525.390249 \nL 417.454612 525.655705 \nL 418.402785 524.118808 \nL 420.299132 525.250327 \nL 421.247305 524.027345 \nL 422.195478 526.169511 \nL 423.143652 525.158205 \nL 424.091825 525.37413 \nL 425.039998 524.094226 \nL 425.988172 526.071167 \nL 426.936345 524.411828 \nL 427.884518 522.996591 \nL 428.832692 524.250248 \nL 429.780865 525.895151 \nL 430.729038 525.542521 \nL 431.677212 527.678168 \nL 432.625385 525.321108 \nL 433.573558 525.270098 \nL 434.521732 522.714481 \nL 435.469905 523.182667 \nL 436.418078 525.699674 \nL 437.366252 526.224719 \nL 438.314425 525.22064 \nL 439.262598 523.805118 \nL 440.210772 523.750395 \nL 441.158945 523.97354 \nL 442.107118 524.064054 \nL 443.055292 526.806318 \nL 444.003465 526.210095 \nL 444.951638 522.940545 \nL 445.899811 523.766899 \nL 446.847985 526.951487 \nL 447.796158 525.399731 \nL 448.744331 524.562386 \nL 449.692505 525.539125 \nL 450.640678 525.230942 \nL 452.537025 523.880568 \nL 453.485198 522.641211 \nL 454.433371 525.797273 \nL 455.381545 525.826755 \nL 456.329718 526.111056 \nL 457.277891 525.166042 \nL 458.226065 523.806459 \nL 459.174238 526.97086 \nL 460.122411 522.797978 \nL 461.070585 524.260475 \nL 462.018758 525.02535 \nL 462.966931 526.459886 \nL 463.915105 523.956483 \nL 464.863278 525.718935 \nL 465.811451 526.164792 \nL 466.759625 525.526725 \nL 467.707798 525.12596 \nL 468.655971 524.990143 \nL 469.604145 525.875082 \nL 470.552318 525.271576 \nL 471.500491 526.728974 \nL 472.448665 525.814646 \nL 473.396838 527.196433 \nL 474.345011 525.863278 \nL 475.293184 523.817226 \nL 476.241358 524.865925 \nL 477.189531 525.626304 \nL 478.137704 524.664772 \nL 479.085878 526.656648 \nL 480.034051 524.332385 \nL 480.982224 523.01276 \nL 481.930398 525.385617 \nL 482.878571 524.285977 \nL 483.826744 522.381933 \nL 484.774918 528.32224 \nL 485.723091 524.993589 \nL 486.671264 526.106343 \nL 487.619438 526.326949 \nL 488.567611 527.399306 \nL 489.515784 527.002565 \nL 490.463958 525.090225 \nL 491.412131 527.463144 \nL 492.360304 524.967528 \nL 494.256651 527.62994 \nL 495.204824 524.045072 \nL 496.152998 525.553729 \nL 497.101171 523.250759 \nL 498.049344 526.50242 \nL 499.945691 526.065324 \nL 500.893864 526.974554 \nL 501.842037 525.016737 \nL 502.790211 524.564845 \nL 503.738384 525.524949 \nL 504.686557 524.712665 \nL 505.634731 526.023604 \nL 506.582904 524.893109 \nL 507.531077 525.945583 \nL 508.479251 524.545795 \nL 509.427424 522.145431 \nL 510.375597 522.999379 \nL 512.271944 525.343536 \nL 513.220117 525.472057 \nL 514.168291 524.558903 \nL 516.064637 524.419795 \nL 517.012811 525.735371 \nL 517.960984 523.397026 \nL 518.909157 526.282677 \nL 519.857331 526.703379 \nL 520.805504 525.670347 \nL 521.753677 523.297304 \nL 522.701851 526.686279 \nL 523.650024 521.874044 \nL 524.598197 525.454715 \nL 525.546371 526.523916 \nL 527.442717 526.63905 \nL 528.390891 524.474648 \nL 529.339064 525.29352 \nL 530.287237 524.809444 \nL 531.23541 523.337553 \nL 532.183584 526.368322 \nL 533.131757 525.164036 \nL 534.07993 527.281998 \nL 535.028104 527.26917 \nL 535.976277 525.324877 \nL 536.92445 526.128753 \nL 537.872624 523.70968 \nL 538.820797 524.386705 \nL 539.76897 524.615389 \nL 540.717144 526.132274 \nL 541.665317 521.84355 \nL 542.61349 525.956996 \nL 543.561664 525.535331 \nL 544.509837 526.556752 \nL 545.45801 526.05696 \nL 546.406184 526.518788 \nL 547.354357 525.912059 \nL 548.30253 526.162873 \nL 549.250704 523.675286 \nL 550.198877 526.416538 \nL 551.14705 527.520326 \nL 552.095224 526.136005 \nL 553.043397 525.44527 \nL 553.99157 525.34068 \nL 554.939744 525.472716 \nL 555.887917 523.880201 \nL 556.83609 526.594983 \nL 557.784263 526.328073 \nL 558.732437 528.676124 \nL 559.68061 524.501895 \nL 560.628783 526.914591 \nL 561.576957 526.027751 \nL 562.52513 525.31827 \nL 563.473303 525.161229 \nL 564.421477 523.491415 \nL 565.36965 527.46664 \nL 566.317823 524.880529 \nL 567.265997 527.277813 \nL 569.162343 524.423942 \nL 570.110517 524.832233 \nL 571.05869 526.423933 \nL 572.006863 524.136753 \nL 572.955037 525.222901 \nL 573.90321 524.698309 \nL 574.851383 524.485092 \nL 575.799557 522.725391 \nL 576.74773 523.363198 \nL 577.695903 526.604191 \nL 578.644077 523.901506 \nL 579.59225 523.94404 \nL 580.540423 526.215324 \nL 581.488597 524.739813 \nL 582.43677 524.737807 \nL 583.384943 525.4637 \nL 584.333117 524.916655 \nL 585.28129 525.854988 \nL 586.229463 525.678935 \nL 587.177636 527.275802 \nL 588.12581 527.332859 \nL 589.073983 526.291985 \nL 590.022156 527.190081 \nL 590.97033 526.36723 \nL 591.918503 525.071926 \nL 592.866676 523.971082 \nL 593.81485 523.971231 \nL 594.763023 524.961474 \nL 595.711196 526.33858 \nL 596.65937 525.165849 \nL 597.607543 527.309562 \nL 598.555716 525.732683 \nL 599.50389 526.475782 \nL 600.452063 522.676039 \nL 601.400236 524.194227 \nL 602.34841 526.20953 \nL 603.296583 522.758406 \nL 604.244756 524.02573 \nL 605.19293 526.256442 \nL 606.141103 527.380765 \nL 607.089276 525.721096 \nL 608.03745 525.279108 \nL 608.985623 524.975905 \nL 609.933796 524.93478 \nL 610.88197 521.926706 \nL 611.830143 527.115004 \nL 612.778316 525.072274 \nL 613.726489 526.704311 \nL 614.674663 525.357892 \nL 615.622836 524.692311 \nL 616.571009 527.515098 \nL 617.519183 526.452521 \nL 618.467356 526.671532 \nL 619.415529 524.378043 \nL 620.363703 526.618094 \nL 621.311876 527.954795 \nL 622.260049 525.078011 \nL 623.208223 526.157341 \nL 624.156396 524.992415 \nL 625.104569 526.237088 \nL 626.052743 525.602156 \nL 627.000916 527.181953 \nL 627.949089 527.166492 \nL 628.897263 524.261506 \nL 629.845436 524.268435 \nL 630.793609 526.855266 \nL 631.741783 525.352341 \nL 632.689956 526.453459 \nL 633.638129 524.463993 \nL 634.586303 527.031761 \nL 635.534476 526.911089 \nL 636.482649 524.01564 \nL 637.430823 523.428266 \nL 638.378996 525.825444 \nL 639.327169 525.355365 \nL 640.275343 525.679624 \nL 642.171689 524.074238 \nL 643.119862 525.759731 \nL 644.068036 522.620198 \nL 645.016209 522.864313 \nL 645.964382 527.264016 \nL 646.912556 526.352371 \nL 648.808902 525.593003 \nL 649.757076 525.245658 \nL 650.705249 524.588664 \nL 651.653422 525.93404 \nL 652.601596 525.426642 \nL 653.549769 525.501763 \nL 654.497942 525.2192 \nL 656.394289 525.798446 \nL 657.342462 527.081685 \nL 658.290636 525.213158 \nL 659.238809 524.389779 \nL 660.186982 526.362448 \nL 661.135156 526.019083 \nL 662.083329 524.657284 \nL 663.031502 526.597566 \nL 664.927849 524.337483 \nL 665.876022 526.020915 \nL 666.824196 523.953509 \nL 667.772369 528.299625 \nL 668.720542 525.22467 \nL 669.668715 525.337929 \nL 670.616889 523.991827 \nL 671.565062 525.271346 \nL 672.513235 525.94056 \nL 673.461409 525.861279 \nL 674.409582 526.038947 \nL 675.357755 524.206422 \nL 676.305929 525.996698 \nL 677.254102 525.193493 \nL 678.202275 525.730447 \nL 679.150449 524.18505 \nL 680.098622 526.357419 \nL 681.046795 525.360401 \nL 681.994969 524.1736 \nL 682.943142 523.80441 \nL 683.891315 522.772067 \nL 684.839489 523.082255 \nL 685.787662 525.300996 \nL 686.735835 526.091192 \nL 687.684009 525.816186 \nL 688.632182 523.239234 \nL 689.580355 525.608433 \nL 690.528529 526.676095 \nL 691.476702 527.438338 \nL 692.424875 524.874916 \nL 693.373049 523.989449 \nL 694.321222 524.654763 \nL 695.269395 524.373423 \nL 696.217569 523.474246 \nL 697.165742 526.215044 \nL 698.113915 526.502699 \nL 699.062088 523.027271 \nL 700.010262 524.768307 \nL 700.958435 525.137857 \nL 701.906608 526.15169 \nL 702.854782 524.919828 \nL 703.802955 525.412584 \nL 704.751128 525.113076 \nL 705.699302 526.119551 \nL 706.647475 526.76122 \nL 707.595648 525.307367 \nL 708.543822 525.217356 \nL 709.491995 524.521963 \nL 710.440168 525.818297 \nL 711.388342 525.250892 \nL 712.336515 526.713103 \nL 713.284688 525.637965 \nL 714.232862 526.028646 \nL 715.181035 523.435543 \nL 716.129208 524.442825 \nL 717.077382 525.252631 \nL 718.025555 527.217961 \nL 718.973728 522.565705 \nL 719.921902 526.23379 \nL 720.870075 524.60076 \nL 721.818248 524.701228 \nL 722.766422 525.309496 \nL 724.662768 523.544095 \nL 725.610941 524.3111 \nL 726.559115 527.208157 \nL 727.507288 525.151319 \nL 728.455461 523.974348 \nL 729.403635 525.63622 \nL 730.351808 524.896928 \nL 731.299981 527.03617 \nL 732.248155 524.160858 \nL 733.196328 526.634865 \nL 734.144501 524.485583 \nL 735.092675 525.633432 \nL 736.040848 526.096253 \nL 736.989021 524.546031 \nL 737.937195 525.81587 \nL 738.885368 526.447889 \nL 739.833541 526.520489 \nL 740.781715 525.227893 \nL 741.729888 526.713886 \nL 742.678061 523.687866 \nL 743.626235 527.065931 \nL 744.574408 524.736217 \nL 745.522581 525.031236 \nL 746.470755 523.892546 \nL 747.418928 524.641177 \nL 748.367101 524.690672 \nL 749.315275 525.755912 \nL 750.263448 524.414945 \nL 751.211621 525.234947 \nL 752.159794 526.854434 \nL 753.107968 525.701338 \nL 754.056141 525.021674 \nL 755.004314 525.149463 \nL 755.952488 523.202083 \nL 756.900661 526.20252 \nL 757.848834 524.618097 \nL 758.797008 525.969489 \nL 759.745181 525.264596 \nL 760.693354 526.597305 \nL 761.641528 526.615623 \nL 762.589701 525.385337 \nL 764.486048 524.114294 \nL 765.434221 524.378912 \nL 766.382394 526.324689 \nL 767.330568 525.344039 \nL 768.278741 526.472391 \nL 769.226914 525.251544 \nL 770.175088 529.322499 \nL 771.123261 524.795504 \nL 772.071434 523.764142 \nL 773.019608 526.970084 \nL 773.967781 526.964936 \nL 774.915954 525.080657 \nL 775.864128 524.45959 \nL 776.812301 527.102281 \nL 777.760474 524.659519 \nL 778.708648 525.135622 \nL 779.656821 525.443842 \nL 780.604994 525.339866 \nL 781.553167 523.229684 \nL 782.501341 523.048085 \nL 783.449514 526.425498 \nL 784.397687 525.838167 \nL 785.345861 524.110575 \nL 786.294034 524.870495 \nL 786.294034 524.870495 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p51b44bd8d1)\" d=\"M 76.112216 134.315548 \nL 78.008563 241.645777 \nL 79.904909 337.092362 \nL 80.853083 378.875186 \nL 81.801256 415.663313 \nL 82.749429 446.104271 \nL 83.697602 470.088664 \nL 84.645776 486.77372 \nL 85.593949 498.600909 \nL 86.542122 505.798605 \nL 87.490296 509.644235 \nL 88.438469 511.489327 \nL 89.386642 512.19887 \nL 90.334816 512.354986 \nL 92.231162 512.151263 \nL 93.179336 511.893773 \nL 96.023856 511.876307 \nL 96.972029 511.583703 \nL 99.816549 511.920685 \nL 101.712896 511.724599 \nL 104.557416 511.94392 \nL 105.505589 512.29881 \nL 108.350109 512.230538 \nL 111.194629 512.320617 \nL 112.142802 512.175629 \nL 114.039149 512.4922 \nL 114.987322 512.877466 \nL 116.883669 512.511735 \nL 121.624535 513.083611 \nL 122.572709 513.075408 \nL 124.469055 513.390608 \nL 125.417229 513.484611 \nL 126.365402 513.442965 \nL 127.313575 513.626551 \nL 128.261749 513.447951 \nL 131.106269 513.80561 \nL 132.054442 514.002323 \nL 133.002615 513.985135 \nL 133.950789 514.302433 \nL 134.898962 514.401305 \nL 135.847135 514.193745 \nL 136.795309 514.523238 \nL 137.743482 514.617204 \nL 138.691655 514.926617 \nL 139.639828 515.008742 \nL 140.588002 514.937943 \nL 141.536175 514.522083 \nL 142.484348 514.776474 \nL 143.432522 514.830645 \nL 144.380695 514.743372 \nL 145.328868 515.422707 \nL 146.277042 515.22335 \nL 147.225215 515.244381 \nL 148.173388 515.754479 \nL 149.121562 515.850091 \nL 150.069735 515.810531 \nL 151.017908 515.641177 \nL 151.966082 515.861075 \nL 152.914255 515.934886 \nL 153.862428 516.336861 \nL 154.810602 516.413497 \nL 155.758775 516.6823 \nL 156.706948 517.159999 \nL 157.655122 517.108585 \nL 158.603295 517.281746 \nL 159.551468 516.912649 \nL 160.499642 516.963206 \nL 161.447815 517.264453 \nL 162.395988 517.422729 \nL 163.344162 517.809665 \nL 164.292335 517.86016 \nL 165.240508 518.129807 \nL 166.188681 517.914988 \nL 167.136855 518.186101 \nL 169.033201 518.517377 \nL 169.981375 518.781834 \nL 170.929548 518.410483 \nL 171.877721 518.910349 \nL 172.825895 519.032015 \nL 173.774068 519.401888 \nL 175.670415 519.224033 \nL 178.514935 520.443489 \nL 179.463108 519.920934 \nL 180.411281 520.428487 \nL 181.359455 520.340941 \nL 182.307628 520.97827 \nL 183.255801 521.040357 \nL 184.203975 521.404418 \nL 185.152148 521.607272 \nL 186.100321 521.464916 \nL 187.996668 522.007235 \nL 189.893015 521.974984 \nL 190.841188 522.46194 \nL 191.789361 522.749142 \nL 194.633881 522.87382 \nL 195.582054 523.462405 \nL 197.478401 523.467763 \nL 198.426574 523.846752 \nL 199.374748 523.995919 \nL 200.322921 523.606027 \nL 201.271094 524.386674 \nL 202.219268 524.372982 \nL 203.167441 524.753212 \nL 204.115614 524.782018 \nL 205.063788 525.043022 \nL 206.011961 524.713193 \nL 207.908308 525.671614 \nL 208.856481 525.623584 \nL 209.804654 526.084505 \nL 210.752828 526.18732 \nL 211.701001 526.123084 \nL 212.649174 526.268277 \nL 214.545521 526.913498 \nL 215.493694 527.043596 \nL 216.441868 526.962887 \nL 217.390041 527.249107 \nL 218.338214 527.358535 \nL 219.286388 527.216993 \nL 220.234561 527.470738 \nL 221.182734 527.921985 \nL 222.130907 528.109594 \nL 224.027254 528.010704 \nL 228.768121 529.161789 \nL 230.664467 529.247763 \nL 231.612641 529.176126 \nL 232.560814 529.744189 \nL 235.405334 529.958549 \nL 237.301681 530.490498 \nL 238.249854 530.524606 \nL 239.198027 530.239808 \nL 240.146201 530.791702 \nL 241.094374 530.800693 \nL 242.042547 531.053613 \nL 244.887067 531.194739 \nL 246.783414 531.620719 \nL 247.731587 531.441207 \nL 250.576107 532.172173 \nL 251.52428 532.09481 \nL 252.472454 531.649953 \nL 253.420627 532.448117 \nL 257.21332 533.043433 \nL 258.161494 532.937054 \nL 261.006014 533.11905 \nL 261.954187 533.354391 \nL 262.90236 533.179231 \nL 263.850534 533.58261 \nL 264.798707 533.449127 \nL 265.74688 533.77274 \nL 267.643227 533.853909 \nL 268.5914 533.847731 \nL 269.539574 534.175374 \nL 271.43592 534.48948 \nL 274.28044 534.852765 \nL 275.228614 534.686416 \nL 276.176787 534.689571 \nL 278.073133 535.193765 \nL 280.917653 535.015047 \nL 281.865827 535.393147 \nL 282.814 535.309755 \nL 283.762173 535.419301 \nL 284.710347 535.245072 \nL 285.65852 535.515925 \nL 286.606693 535.623433 \nL 287.554867 535.881966 \nL 288.50304 535.886276 \nL 290.399387 535.580887 \nL 291.34756 536.12153 \nL 294.19208 536.073413 \nL 296.088427 536.352561 \nL 297.0366 536.519264 \nL 297.984773 536.201897 \nL 298.932947 536.522424 \nL 299.88112 536.379373 \nL 300.829293 536.654615 \nL 303.673813 536.858456 \nL 304.621987 537.075706 \nL 305.57016 537.134608 \nL 306.518333 536.856494 \nL 307.466506 536.878897 \nL 309.362853 537.391524 \nL 310.311026 537.350405 \nL 311.2592 537.433204 \nL 312.207373 537.146897 \nL 314.10372 537.364112 \nL 315.051893 536.987862 \nL 316.000066 537.381135 \nL 316.94824 537.39026 \nL 317.896413 537.672214 \nL 318.844586 537.616324 \nL 320.740933 537.87576 \nL 321.689106 537.721976 \nL 324.533626 537.877974 \nL 325.4818 538.10582 \nL 326.429973 537.882796 \nL 327.378146 538.239309 \nL 331.17084 538.312638 \nL 334.963533 538.309559 \nL 335.911706 538.605115 \nL 336.859879 538.607015 \nL 337.808053 538.392 \nL 338.756226 538.618086 \nL 340.652573 538.507935 \nL 342.548919 538.774553 \nL 343.497093 538.523393 \nL 344.445266 538.620269 \nL 345.393439 538.863716 \nL 346.341613 538.973109 \nL 348.237959 538.899181 \nL 349.186133 538.415565 \nL 350.134306 539.004532 \nL 351.082479 539.178912 \nL 352.030653 539.059683 \nL 353.926999 539.236945 \nL 355.823346 538.974016 \nL 356.771519 539.043272 \nL 359.616039 538.928548 \nL 362.460559 539.48133 \nL 363.408732 539.195538 \nL 364.356906 539.426153 \nL 365.305079 539.512349 \nL 367.201426 539.466328 \nL 368.149599 539.611292 \nL 369.097772 539.516341 \nL 373.838639 539.639693 \nL 374.786812 538.959526 \nL 375.734986 539.68812 \nL 377.631332 539.436306 \nL 379.527679 539.797222 \nL 380.475852 539.654589 \nL 381.424026 539.726904 \nL 382.372199 539.658411 \nL 383.320372 539.70903 \nL 384.268546 539.571657 \nL 385.216719 539.982099 \nL 387.113066 539.643639 \nL 388.061239 539.949664 \nL 389.009412 540.018095 \nL 390.905759 539.814772 \nL 391.853932 539.906528 \nL 393.750279 539.75506 \nL 395.646625 539.898164 \nL 396.594799 539.833549 \nL 398.491145 540.107615 \nL 399.439319 539.805378 \nL 400.387492 540.044522 \nL 401.335665 540.033249 \nL 402.283839 540.136479 \nL 403.232012 539.866409 \nL 405.128359 540.186297 \nL 407.972879 540.031525 \nL 408.921052 540.244702 \nL 410.817399 539.892392 \nL 411.765572 540.374105 \nL 412.713745 539.987054 \nL 413.661919 540.080347 \nL 414.610092 540.376008 \nL 415.558265 540.003285 \nL 416.506439 540.316839 \nL 418.402785 540.272703 \nL 419.350958 540.440884 \nL 420.299132 540.375291 \nL 421.247305 539.903821 \nL 422.195478 540.410268 \nL 423.143652 540.226149 \nL 424.091825 540.42373 \nL 425.039998 540.478876 \nL 425.988172 540.229998 \nL 426.936345 540.27102 \nL 427.884518 540.439446 \nL 429.780865 540.41246 \nL 430.729038 540.338116 \nL 431.677212 540.086789 \nL 432.625385 540.278751 \nL 433.573558 540.152754 \nL 434.521732 540.483492 \nL 435.469905 540.67597 \nL 437.366252 540.315011 \nL 438.314425 540.292763 \nL 439.262598 540.532425 \nL 440.210772 540.399057 \nL 442.107118 540.599002 \nL 443.055292 540.081486 \nL 444.003465 540.54553 \nL 445.899811 540.69738 \nL 446.847985 540.297634 \nL 447.796158 540.512763 \nL 448.744331 540.415022 \nL 449.692505 540.588064 \nL 450.640678 540.500217 \nL 452.537025 540.739734 \nL 453.485198 540.501931 \nL 454.433371 540.736368 \nL 455.381545 540.645172 \nL 456.329718 540.690938 \nL 457.277891 540.502465 \nL 458.226065 540.81754 \nL 460.122411 540.872058 \nL 461.070585 540.686315 \nL 462.018758 540.715496 \nL 462.966931 540.566282 \nL 463.915105 540.538274 \nL 464.863278 540.331295 \nL 465.811451 540.823278 \nL 466.759625 540.580765 \nL 467.707798 540.476116 \nL 468.655971 540.740314 \nL 470.552318 540.422318 \nL 471.500491 540.774354 \nL 472.448665 540.550348 \nL 475.293184 540.754326 \nL 477.189531 540.702381 \nL 479.085878 540.729473 \nL 480.034051 540.545508 \nL 481.930398 540.792293 \nL 482.878571 540.464492 \nL 483.826744 540.59291 \nL 484.774918 540.60631 \nL 485.723091 540.808922 \nL 488.567611 540.74717 \nL 491.412131 540.733149 \nL 492.360304 540.400203 \nL 493.308478 540.711307 \nL 494.256651 540.644883 \nL 495.204824 540.763631 \nL 496.152998 540.745108 \nL 497.101171 541.027575 \nL 498.049344 540.820446 \nL 498.997518 540.893515 \nL 499.945691 540.777161 \nL 500.893864 540.919603 \nL 501.842037 540.597127 \nL 502.790211 540.939386 \nL 503.738384 540.771364 \nL 505.634731 540.901966 \nL 506.582904 540.952004 \nL 508.479251 540.689267 \nL 512.271944 540.884549 \nL 513.220117 540.585611 \nL 515.116464 540.908253 \nL 516.064637 540.873732 \nL 517.012811 540.990418 \nL 519.857331 540.905732 \nL 520.805504 540.706632 \nL 521.753677 540.908852 \nL 522.701851 540.950132 \nL 523.650024 540.783352 \nL 524.598197 540.819965 \nL 525.546371 540.716508 \nL 526.494544 540.97862 \nL 528.390891 540.900417 \nL 529.339064 540.696737 \nL 530.287237 540.955608 \nL 531.23541 540.986702 \nL 532.183584 540.793283 \nL 533.131757 540.903049 \nL 534.07993 540.693819 \nL 535.976277 540.988735 \nL 538.820797 540.861487 \nL 539.76897 540.758862 \nL 540.717144 541.017826 \nL 545.45801 540.787422 \nL 546.406184 541.011847 \nL 550.198877 540.84207 \nL 551.14705 540.623901 \nL 552.095224 540.888373 \nL 553.99157 540.668854 \nL 554.939744 540.966093 \nL 555.887917 540.843244 \nL 556.83609 540.957204 \nL 557.784263 540.880991 \nL 558.732437 540.653188 \nL 560.628783 540.903437 \nL 562.52513 541.077113 \nL 563.473303 540.62922 \nL 564.421477 540.905499 \nL 565.36965 541.03439 \nL 566.317823 540.598555 \nL 568.21417 541.037522 \nL 571.05869 540.802846 \nL 572.006863 540.98446 \nL 572.955037 540.870636 \nL 573.90321 541.007981 \nL 574.851383 540.895058 \nL 575.799557 541.048876 \nL 576.74773 540.841366 \nL 577.695903 540.981383 \nL 578.644077 540.712189 \nL 579.59225 540.845181 \nL 580.540423 540.836913 \nL 582.43677 541.066539 \nL 584.333117 541.109504 \nL 585.28129 540.982451 \nL 588.12581 540.991734 \nL 590.022156 540.668354 \nL 591.918503 541.14056 \nL 592.866676 540.98709 \nL 595.711196 540.847587 \nL 596.65937 541.011204 \nL 597.607543 541.039571 \nL 598.555716 540.914804 \nL 599.50389 540.638788 \nL 600.452063 541.09915 \nL 601.400236 540.999117 \nL 602.34841 540.769253 \nL 606.141103 541.209034 \nL 607.089276 540.835364 \nL 608.03745 540.850611 \nL 608.985623 541.024396 \nL 609.933796 540.906393 \nL 610.88197 541.173548 \nL 611.830143 541.11069 \nL 612.778316 540.764416 \nL 613.726489 541.110721 \nL 614.674663 540.930206 \nL 617.519183 540.795472 \nL 618.467356 540.56115 \nL 619.415529 541.058622 \nL 621.311876 540.786847 \nL 623.208223 540.958486 \nL 625.104569 540.798301 \nL 628.897263 541.103208 \nL 630.793609 540.718035 \nL 631.741783 541.051611 \nL 632.689956 540.606664 \nL 633.638129 540.97397 \nL 634.586303 540.744835 \nL 635.534476 541.1001 \nL 636.482649 541.112761 \nL 637.430823 540.896647 \nL 638.378996 541.11156 \nL 639.327169 540.890596 \nL 640.275343 541.057156 \nL 645.016209 540.87923 \nL 645.964382 541.059581 \nL 647.860729 540.98487 \nL 648.808902 540.757173 \nL 650.705249 540.853496 \nL 651.653422 540.984081 \nL 652.601596 540.883186 \nL 653.549769 540.904952 \nL 654.497942 540.812234 \nL 656.394289 541.021772 \nL 657.342462 540.713453 \nL 659.238809 540.998316 \nL 661.135156 540.665395 \nL 662.083329 541.069572 \nL 663.031502 540.842359 \nL 664.927849 540.827882 \nL 665.876022 541.047044 \nL 666.824196 540.809561 \nL 668.720542 541.037507 \nL 669.668715 540.54965 \nL 670.616889 541.010599 \nL 673.461409 541.021741 \nL 675.357755 540.82357 \nL 676.305929 540.735887 \nL 677.254102 540.984569 \nL 679.150449 540.903742 \nL 680.098622 540.685616 \nL 681.994969 540.923636 \nL 682.943142 540.687479 \nL 683.891315 541.014998 \nL 685.787662 540.946987 \nL 686.735835 540.724316 \nL 687.684009 540.908982 \nL 688.632182 540.695986 \nL 689.580355 541.005274 \nL 692.424875 541.124174 \nL 693.373049 540.849137 \nL 694.321222 541.047935 \nL 695.269395 540.671235 \nL 696.217569 540.824445 \nL 697.165742 540.73684 \nL 698.113915 541.003178 \nL 701.906608 540.953236 \nL 703.802955 541.050099 \nL 704.751128 540.800114 \nL 705.699302 540.949613 \nL 707.595648 540.981629 \nL 708.543822 540.849472 \nL 710.440168 541.003591 \nL 711.388342 540.688432 \nL 712.336515 540.939837 \nL 714.232862 540.842039 \nL 717.077382 541.015703 \nL 718.025555 540.746279 \nL 718.973728 541.072919 \nL 719.921902 540.936347 \nL 720.870075 541.048991 \nL 721.818248 540.900653 \nL 722.766422 540.617909 \nL 723.714595 540.622306 \nL 724.662768 540.901537 \nL 726.559115 541.025628 \nL 727.507288 540.721696 \nL 728.455461 540.97225 \nL 729.403635 540.635991 \nL 730.351808 540.90077 \nL 735.092675 540.934764 \nL 736.040848 540.935813 \nL 736.989021 540.793957 \nL 737.937195 540.869615 \nL 738.885368 540.746216 \nL 739.833541 541.075465 \nL 741.729888 540.823237 \nL 742.678061 541.056768 \nL 743.626235 540.997379 \nL 744.574408 540.7144 \nL 745.522581 540.97776 \nL 746.470755 541.071099 \nL 749.315275 541.037687 \nL 750.263448 540.544347 \nL 751.211621 540.409492 \nL 752.159794 540.832458 \nL 753.107968 541.074198 \nL 754.056141 541.019742 \nL 755.004314 540.791896 \nL 755.952488 540.836606 \nL 756.900661 540.395739 \nL 757.848834 540.965084 \nL 758.797008 540.982554 \nL 760.693354 540.784926 \nL 761.641528 540.925583 \nL 763.537874 540.921336 \nL 764.486048 540.716927 \nL 766.382394 540.729554 \nL 767.330568 540.896359 \nL 768.278741 540.943041 \nL 769.226914 540.878361 \nL 770.175088 540.683819 \nL 772.071434 540.920942 \nL 773.019608 540.859416 \nL 773.967781 540.95988 \nL 774.915954 540.816152 \nL 775.864128 540.306013 \nL 776.812301 540.719516 \nL 777.760474 540.771116 \nL 778.708648 540.959936 \nL 779.656821 540.81532 \nL 780.604994 540.531885 \nL 781.553167 540.868268 \nL 782.501341 540.748303 \nL 783.449514 540.968335 \nL 785.345861 540.604423 \nL 786.294034 540.834914 \nL 786.294034 540.834914 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_19\">\n    <!-- Model Loss -->\n    <g transform=\"translate(398.119687 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"385.994141\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"447.175781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"499.275391\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_20\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_22\"/>\n    <g id=\"text_21\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p51b44bd8d1\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 822.640625 603.474375\" width=\"822.640625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-06T20:07:31.829226</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 822.640625 603.474375 \nL 822.640625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \nL 815.440625 22.318125 \nL 34.240625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"maa1d9e7030\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.749716\" xlink:href=\"#maa1d9e7030\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(66.568466 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.567048\" xlink:href=\"#maa1d9e7030\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(155.023298 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"259.38438\" xlink:href=\"#maa1d9e7030\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(249.84063 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"354.201713\" xlink:href=\"#maa1d9e7030\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(344.657963 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"449.019045\" xlink:href=\"#maa1d9e7030\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(439.475295 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"543.836377\" xlink:href=\"#maa1d9e7030\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(534.292627 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"638.653709\" xlink:href=\"#maa1d9e7030\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(629.109959 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"733.471041\" xlink:href=\"#maa1d9e7030\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(723.927291 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- epoch -->\n     <g transform=\"translate(409.6125 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma779ca4a4c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#ma779ca4a4c\" y=\"486.388044\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2 -->\n      <g transform=\"translate(20.878125 490.187263)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#ma779ca4a4c\" y=\"374.839883\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 3 -->\n      <g transform=\"translate(20.878125 378.639102)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#ma779ca4a4c\" y=\"263.291722\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 4 -->\n      <g transform=\"translate(20.878125 267.090941)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#ma779ca4a4c\" y=\"151.743561\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 5 -->\n      <g transform=\"translate(20.878125 155.542779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#ma779ca4a4c\" y=\"40.1954\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 6 -->\n      <g transform=\"translate(20.878125 43.994618)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- MAE -->\n     <g transform=\"translate(14.798438 305.011875)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"154.6875\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p7b81d1b57e)\" d=\"M 69.749716 47.027216 \nL 72.594236 184.515439 \nL 75.438756 334.398866 \nL 76.386929 382.638486 \nL 77.335102 419.619846 \nL 78.283276 445.419353 \nL 79.231449 465.885281 \nL 80.179622 479.759468 \nL 81.127796 481.872346 \nL 82.075969 487.02424 \nL 83.024142 486.74898 \nL 83.972316 483.115244 \nL 84.920489 488.042249 \nL 85.868662 487.575411 \nL 86.816836 483.587761 \nL 87.765009 483.818101 \nL 88.713182 483.751454 \nL 89.661356 486.270999 \nL 90.609529 484.712071 \nL 91.557702 485.336791 \nL 92.505876 484.887545 \nL 93.454049 483.657998 \nL 94.402222 485.005601 \nL 95.350396 484.895311 \nL 96.298569 487.806483 \nL 97.246742 486.093822 \nL 98.194916 482.019497 \nL 100.091262 484.798079 \nL 101.039436 484.434923 \nL 101.987609 486.120497 \nL 102.935782 487.013921 \nL 103.883956 481.353182 \nL 104.832129 485.047303 \nL 105.780302 482.968572 \nL 106.728475 485.758164 \nL 107.676649 487.537646 \nL 108.624822 486.396727 \nL 109.572995 486.440809 \nL 110.521169 487.526649 \nL 111.469342 482.544273 \nL 112.417515 487.586448 \nL 113.365689 483.582441 \nL 114.313862 489.554437 \nL 115.262035 484.440242 \nL 116.210209 488.059869 \nL 117.158382 484.176843 \nL 118.106555 484.183412 \nL 119.054729 486.405225 \nL 120.002902 489.755961 \nL 120.951075 486.65732 \nL 121.899249 488.666571 \nL 122.847422 485.685772 \nL 123.795595 489.62631 \nL 124.743769 486.779804 \nL 125.691942 488.579817 \nL 126.640115 488.665254 \nL 127.588289 491.965154 \nL 128.536462 489.307035 \nL 129.484635 482.46861 \nL 130.432809 484.745714 \nL 131.380982 490.566036 \nL 132.329155 489.772291 \nL 133.277328 484.643109 \nL 134.225502 489.254802 \nL 135.173675 489.05232 \nL 136.121848 488.443849 \nL 137.070022 488.274132 \nL 138.018195 487.605584 \nL 138.966368 487.60807 \nL 139.914542 491.800557 \nL 140.862715 485.924012 \nL 141.810888 493.937903 \nL 142.759062 488.790531 \nL 143.707235 485.811993 \nL 144.655408 487.41717 \nL 145.603582 491.728152 \nL 146.551755 489.879376 \nL 147.499928 493.021181 \nL 148.448102 490.785819 \nL 149.396275 490.560106 \nL 150.344448 490.65755 \nL 151.292622 495.744552 \nL 152.240795 491.251806 \nL 153.188968 488.511109 \nL 154.137142 492.644488 \nL 155.085315 487.606634 \nL 156.033488 490.223212 \nL 156.981662 489.645937 \nL 157.929835 493.658468 \nL 158.878008 494.588287 \nL 159.826181 493.494655 \nL 160.774355 494.487066 \nL 161.722528 493.762947 \nL 163.618875 496.666938 \nL 164.567048 490.538125 \nL 165.515221 492.077293 \nL 166.463395 490.843304 \nL 167.411568 492.321583 \nL 168.359741 493.174888 \nL 169.307915 493.565026 \nL 170.256088 489.276264 \nL 171.204261 494.342043 \nL 172.152435 496.828796 \nL 173.100608 495.769298 \nL 174.048781 495.071694 \nL 174.996955 490.37644 \nL 175.945128 493.948222 \nL 176.893301 498.141347 \nL 177.841475 499.988301 \nL 178.789648 490.688334 \nL 179.737821 493.458539 \nL 180.685995 497.768031 \nL 181.634168 495.455542 \nL 182.582341 498.346409 \nL 183.530515 496.842852 \nL 184.478688 495.621509 \nL 185.426861 495.879761 \nL 186.375035 500.404807 \nL 187.323208 493.291534 \nL 188.271381 496.419377 \nL 189.219554 494.585043 \nL 190.167728 500.557158 \nL 191.115901 497.761887 \nL 192.064074 498.03623 \nL 193.012248 500.068897 \nL 193.960421 500.592449 \nL 194.908594 499.921919 \nL 195.856768 496.426545 \nL 196.804941 498.40436 \nL 197.753114 503.52398 \nL 198.701288 500.422799 \nL 199.649461 501.657279 \nL 200.597634 495.777011 \nL 201.545808 505.375335 \nL 202.493981 501.526923 \nL 203.442154 503.325234 \nL 204.390328 501.552867 \nL 205.338501 504.6718 \nL 206.286674 505.300004 \nL 207.234848 500.715678 \nL 208.183021 502.067031 \nL 209.131194 502.559134 \nL 210.079368 501.520195 \nL 211.027541 505.587764 \nL 211.975714 507.242195 \nL 212.923888 501.490594 \nL 213.872061 504.501791 \nL 214.820234 502.791629 \nL 215.768407 504.06829 \nL 216.716581 509.484499 \nL 217.664754 500.117832 \nL 218.612927 500.958705 \nL 219.561101 504.76249 \nL 220.509274 503.117314 \nL 221.457447 505.228078 \nL 222.405621 502.946067 \nL 223.353794 509.776221 \nL 224.301967 509.281365 \nL 225.250141 505.710048 \nL 226.198314 505.405587 \nL 227.146487 504.298138 \nL 228.094661 509.52648 \nL 229.042834 503.465152 \nL 229.991007 509.123949 \nL 230.939181 509.028007 \nL 231.887354 504.058104 \nL 232.835527 504.888045 \nL 233.783701 502.885962 \nL 234.731874 511.699822 \nL 235.680047 509.816513 \nL 236.628221 505.375654 \nL 237.576394 503.429208 \nL 238.524567 512.224891 \nL 239.472741 508.574094 \nL 240.420914 510.54284 \nL 241.369087 507.55022 \nL 242.317261 511.77772 \nL 243.265434 510.074579 \nL 244.213607 506.95272 \nL 246.109954 513.277952 \nL 247.058127 509.480045 \nL 248.0063 513.656428 \nL 248.954474 507.696879 \nL 249.902647 509.043724 \nL 250.85082 512.483675 \nL 251.798994 513.955876 \nL 252.747167 511.288714 \nL 253.69534 510.040059 \nL 254.643514 511.762879 \nL 255.591687 509.297296 \nL 256.53986 507.697956 \nL 257.488034 510.394293 \nL 258.436207 512.50644 \nL 260.332554 506.923998 \nL 261.280727 512.913426 \nL 262.2289 511.263063 \nL 263.177074 512.482797 \nL 264.125247 512.321657 \nL 266.021594 513.341116 \nL 266.969767 517.268223 \nL 267.91794 513.868391 \nL 268.866114 515.682699 \nL 269.814287 515.737406 \nL 270.76246 516.146572 \nL 271.710633 512.699428 \nL 272.658807 514.107548 \nL 273.60698 512.268666 \nL 274.555153 516.773061 \nL 275.503327 511.800339 \nL 276.4515 515.778681 \nL 277.399673 517.091564 \nL 278.347847 511.131976 \nL 280.244193 516.360357 \nL 281.192367 513.064805 \nL 282.14054 512.89078 \nL 283.088713 516.278285 \nL 284.036887 508.465054 \nL 284.98506 513.864867 \nL 285.933233 513.997019 \nL 286.881407 515.492251 \nL 287.82958 517.589918 \nL 288.777753 514.646845 \nL 289.725927 517.821681 \nL 290.6741 508.773225 \nL 291.622273 516.526869 \nL 292.570447 515.188934 \nL 293.51862 514.887198 \nL 294.466793 519.047399 \nL 295.414967 516.227528 \nL 296.36314 515.887801 \nL 297.311313 519.7806 \nL 298.259487 514.898555 \nL 299.20766 518.888506 \nL 300.155833 519.15584 \nL 301.104006 516.31476 \nL 302.05218 515.826433 \nL 303.000353 512.024935 \nL 303.948526 517.243237 \nL 304.8967 519.128181 \nL 305.844873 516.260692 \nL 306.793046 514.529799 \nL 307.74122 515.34038 \nL 310.58574 518.683138 \nL 311.533913 519.1487 \nL 312.482086 516.950211 \nL 313.43026 518.51245 \nL 314.378433 518.501387 \nL 315.326606 518.35248 \nL 316.27478 519.497056 \nL 317.222953 515.143655 \nL 318.171126 520.387954 \nL 320.067473 514.603029 \nL 321.015646 521.893226 \nL 321.96382 519.308058 \nL 322.911993 519.540912 \nL 323.860166 513.007174 \nL 324.80834 515.253307 \nL 325.756513 516.944919 \nL 326.704686 519.644553 \nL 327.652859 518.182085 \nL 328.601033 515.455976 \nL 329.549206 516.217794 \nL 330.497379 519.719604 \nL 331.445553 514.781842 \nL 333.341899 522.452722 \nL 334.290073 519.621056 \nL 335.238246 518.970711 \nL 336.186419 519.024859 \nL 337.134593 512.922388 \nL 338.082766 517.949511 \nL 339.030939 518.899011 \nL 339.979113 518.384222 \nL 340.927286 520.868502 \nL 341.875459 518.484233 \nL 342.823633 522.987058 \nL 343.771806 517.261933 \nL 344.719979 519.137144 \nL 345.668153 519.859136 \nL 346.616326 519.122197 \nL 347.564499 521.180928 \nL 348.512673 519.902034 \nL 349.460846 512.683192 \nL 350.409019 519.803472 \nL 351.357193 518.080545 \nL 352.305366 520.261015 \nL 353.253539 518.568779 \nL 354.201713 520.356359 \nL 355.149886 519.763353 \nL 356.098059 516.543146 \nL 357.046232 520.696139 \nL 357.994406 518.088803 \nL 358.942579 520.322118 \nL 359.890752 520.769209 \nL 360.838926 519.043369 \nL 361.787099 519.146412 \nL 362.735272 522.032971 \nL 363.683446 519.231557 \nL 364.631619 516.822927 \nL 365.579792 519.393894 \nL 366.527966 522.806265 \nL 367.476139 521.415125 \nL 368.424312 518.550761 \nL 369.372486 518.833281 \nL 370.320659 522.75834 \nL 371.268832 516.096347 \nL 372.217006 520.062695 \nL 373.165179 517.080354 \nL 374.113352 518.348731 \nL 375.061526 520.924857 \nL 376.009699 520.085354 \nL 376.957872 518.070864 \nL 377.906046 521.239212 \nL 378.854219 523.366345 \nL 379.802392 519.702863 \nL 380.750566 521.84961 \nL 381.698739 521.172351 \nL 382.646912 525.1162 \nL 383.595085 521.022953 \nL 384.543259 520.775339 \nL 385.491432 521.375911 \nL 386.439605 515.86602 \nL 387.387779 523.176482 \nL 388.335952 521.508089 \nL 389.284125 517.575516 \nL 390.232299 520.644557 \nL 391.180472 524.263493 \nL 392.128645 516.979519 \nL 393.076819 521.157059 \nL 394.024992 519.684007 \nL 394.973165 520.368659 \nL 395.921339 522.369173 \nL 396.869512 516.177476 \nL 397.817685 523.302982 \nL 398.765859 524.343624 \nL 400.662205 521.472212 \nL 401.610379 522.902552 \nL 402.558552 523.360415 \nL 403.506725 523.034012 \nL 404.454899 520.18732 \nL 405.403072 520.111005 \nL 406.351245 523.417195 \nL 407.299419 522.172795 \nL 408.247592 518.986682 \nL 409.195765 520.699343 \nL 410.143939 520.702349 \nL 411.092112 522.036774 \nL 412.040285 520.368726 \nL 412.988458 521.406814 \nL 413.936632 521.339023 \nL 414.884805 519.580605 \nL 415.832978 522.098302 \nL 416.781152 519.599753 \nL 417.729325 525.677438 \nL 418.677498 522.704791 \nL 419.625672 523.261308 \nL 420.573845 522.290106 \nL 421.522018 518.649881 \nL 422.470192 521.088829 \nL 423.418365 522.042199 \nL 424.366538 521.558859 \nL 425.314712 526.935827 \nL 426.262885 522.235213 \nL 427.211058 523.788716 \nL 428.159232 516.573012 \nL 429.107405 521.17053 \nL 430.055578 523.599904 \nL 431.003752 521.342175 \nL 431.951925 521.509724 \nL 432.900098 519.295252 \nL 433.848272 518.992852 \nL 434.796445 519.825333 \nL 435.744618 520.162174 \nL 436.692792 524.248254 \nL 437.640965 524.481161 \nL 438.589138 519.475847 \nL 439.537311 521.099521 \nL 440.485485 524.560601 \nL 441.433658 521.749732 \nL 442.381831 521.014044 \nL 443.330005 523.250071 \nL 444.278178 522.436871 \nL 446.174525 520.066605 \nL 447.122698 518.748642 \nL 448.070871 523.046392 \nL 449.967218 523.46387 \nL 450.915391 522.18181 \nL 451.863565 520.244034 \nL 452.811738 524.392985 \nL 453.759911 518.246805 \nL 454.708085 520.151802 \nL 455.656258 520.522486 \nL 456.604431 525.254787 \nL 457.552605 520.12554 \nL 458.500778 524.077008 \nL 459.448951 523.812678 \nL 460.397125 523.70292 \nL 461.345298 520.727694 \nL 462.293471 521.893492 \nL 463.241645 522.667385 \nL 464.189818 523.206947 \nL 465.137991 524.226007 \nL 466.086165 522.203831 \nL 467.034338 524.856538 \nL 467.982511 522.49066 \nL 468.930684 521.77412 \nL 469.878858 521.515017 \nL 470.827031 521.993463 \nL 471.775204 520.611965 \nL 472.723378 525.379266 \nL 474.619724 518.542303 \nL 475.567898 521.780556 \nL 476.516071 518.203295 \nL 477.464244 518.293413 \nL 478.412418 525.92924 \nL 479.360591 522.286476 \nL 480.308764 524.212324 \nL 481.256938 523.352143 \nL 482.205111 524.184625 \nL 483.153284 524.712432 \nL 484.101458 522.867008 \nL 485.049631 525.272247 \nL 485.997804 521.475882 \nL 486.945978 523.825298 \nL 487.894151 527.401375 \nL 488.842324 520.004212 \nL 489.790498 521.158469 \nL 490.738671 522.02337 \nL 491.686844 524.326231 \nL 492.635018 527.739519 \nL 495.479537 522.390476 \nL 496.427711 522.003144 \nL 497.375884 521.488501 \nL 498.324057 523.138385 \nL 499.272231 526.306466 \nL 500.220404 522.261676 \nL 501.168577 521.21743 \nL 502.116751 519.769271 \nL 503.064924 516.741878 \nL 504.013097 519.791185 \nL 504.961271 520.755725 \nL 505.909444 524.110903 \nL 506.857617 522.927751 \nL 507.805791 522.864481 \nL 508.753964 522.285984 \nL 509.702137 520.753597 \nL 510.650311 523.315455 \nL 511.598484 520.10796 \nL 512.546657 526.295535 \nL 513.494831 522.833272 \nL 514.443004 525.784137 \nL 515.391177 521.005773 \nL 516.339351 526.476635 \nL 517.287524 520.721284 \nL 518.235697 522.932445 \nL 519.183871 523.509799 \nL 520.132044 525.112211 \nL 521.080217 525.754085 \nL 522.028391 519.787608 \nL 522.976564 520.75635 \nL 523.924737 522.951248 \nL 524.87291 522.015099 \nL 525.821084 523.5665 \nL 526.769257 524.753668 \nL 527.71743 525.467535 \nL 528.665604 526.941465 \nL 529.613777 523.041805 \nL 530.56195 524.37853 \nL 531.510124 519.273285 \nL 532.458297 522.334865 \nL 533.40647 522.037691 \nL 534.354644 523.662083 \nL 535.302817 519.182223 \nL 536.25099 524.919409 \nL 537.199164 524.013219 \nL 538.147337 524.753429 \nL 539.09551 525.322725 \nL 540.043684 526.100633 \nL 540.991857 524.419686 \nL 541.94003 524.300128 \nL 542.888204 520.531408 \nL 543.836377 522.991859 \nL 544.78455 527.830315 \nL 545.732724 525.15092 \nL 546.680897 524.247124 \nL 547.62907 521.860208 \nL 548.577244 524.703935 \nL 549.525417 519.944879 \nL 550.47359 526.207732 \nL 551.421763 523.022656 \nL 552.369937 527.509671 \nL 553.31811 522.506391 \nL 554.266283 524.855581 \nL 555.214457 526.285469 \nL 556.16263 525.263763 \nL 557.110803 523.314764 \nL 558.058977 519.736067 \nL 559.00715 526.983632 \nL 559.955323 522.026109 \nL 560.903497 525.491936 \nL 561.85167 522.231557 \nL 562.799843 522.560352 \nL 563.748017 523.389284 \nL 564.69619 525.154816 \nL 565.644363 521.208082 \nL 566.592537 523.487446 \nL 568.488883 522.252221 \nL 569.437057 519.31593 \nL 570.38523 521.956124 \nL 571.333403 524.234664 \nL 572.281577 521.278346 \nL 573.22975 519.166678 \nL 574.177923 523.554852 \nL 575.126097 521.761102 \nL 576.07427 522.951527 \nL 577.022443 523.605023 \nL 577.970617 524.620613 \nL 578.91879 523.921399 \nL 579.866963 524.058644 \nL 580.815136 525.373349 \nL 581.76331 525.909387 \nL 582.711483 524.259557 \nL 583.659656 524.738496 \nL 584.60783 526.182134 \nL 585.556003 523.762387 \nL 586.504176 519.134365 \nL 587.45235 521.511865 \nL 588.400523 523.09691 \nL 589.348696 524.218574 \nL 590.29687 522.244947 \nL 591.245043 525.731226 \nL 592.193216 523.998951 \nL 593.14139 523.083559 \nL 594.089563 519.995489 \nL 595.037736 523.364949 \nL 595.98591 523.23051 \nL 596.934083 518.930034 \nL 597.882256 519.552241 \nL 598.83043 525.862965 \nL 599.778603 526.059623 \nL 600.726776 522.889348 \nL 601.67495 522.846649 \nL 602.623123 523.002005 \nL 603.571296 522.316076 \nL 604.51947 519.182715 \nL 605.467643 526.250563 \nL 606.415816 521.840249 \nL 607.363989 524.883851 \nL 608.312163 523.201628 \nL 609.260336 522.320025 \nL 610.208509 528.750202 \nL 611.156683 525.292393 \nL 612.104856 525.921913 \nL 613.053029 520.781216 \nL 614.001203 526.881693 \nL 614.949376 528.377072 \nL 615.897549 521.98228 \nL 617.793896 521.724759 \nL 618.742069 524.883572 \nL 619.690243 522.477096 \nL 620.638416 527.181673 \nL 621.586589 526.402608 \nL 622.534763 521.671981 \nL 623.482936 521.882282 \nL 624.431109 524.058697 \nL 625.379283 523.725433 \nL 626.327456 525.258484 \nL 627.275629 522.807288 \nL 628.223803 526.511954 \nL 629.171976 526.313328 \nL 630.120149 522.719126 \nL 631.068323 521.372839 \nL 632.016496 524.349781 \nL 632.964669 521.094654 \nL 633.912843 522.948456 \nL 634.861016 523.147254 \nL 635.809189 520.457022 \nL 636.757362 523.297477 \nL 637.705536 519.35815 \nL 638.653709 519.793911 \nL 639.601882 526.151589 \nL 640.550056 525.841929 \nL 641.498229 524.81235 \nL 642.446402 522.727091 \nL 643.394576 524.733828 \nL 644.342749 521.625799 \nL 645.290922 524.360592 \nL 648.135442 524.216818 \nL 649.083616 523.60634 \nL 651.928136 524.272788 \nL 652.876309 522.823073 \nL 653.824482 526.833396 \nL 654.772656 524.584988 \nL 655.720829 521.269863 \nL 656.669002 526.114117 \nL 657.617176 522.471963 \nL 658.565349 522.256875 \nL 659.513522 522.656986 \nL 660.461696 522.274747 \nL 661.409869 531.063368 \nL 662.358042 524.429526 \nL 663.306215 523.131005 \nL 664.254389 522.08898 \nL 666.150735 524.077061 \nL 667.098909 521.933092 \nL 668.047082 524.244757 \nL 668.995255 522.521763 \nL 669.943429 524.649162 \nL 670.891602 523.625276 \nL 671.839775 523.151829 \nL 672.787949 520.915881 \nL 673.736122 525.837727 \nL 674.684295 523.023002 \nL 675.632469 522.984572 \nL 676.580642 521.489645 \nL 677.528815 516.548598 \nL 678.476989 520.758212 \nL 679.425162 523.394709 \nL 680.373335 522.959426 \nL 681.321509 524.366017 \nL 682.269682 519.112277 \nL 683.217855 522.05672 \nL 684.166029 526.63485 \nL 685.114202 528.107982 \nL 686.062375 523.615209 \nL 687.010549 522.266915 \nL 687.958722 522.82024 \nL 689.855069 520.433777 \nL 690.803242 524.144506 \nL 691.751415 523.95346 \nL 692.699588 520.162853 \nL 693.647762 523.652855 \nL 694.595935 521.792963 \nL 695.544108 524.329648 \nL 696.492282 523.140778 \nL 697.440455 524.455297 \nL 698.388628 524.669734 \nL 699.336802 525.889973 \nL 700.284975 526.451436 \nL 701.233148 521.120012 \nL 702.181322 521.830714 \nL 703.129495 520.572804 \nL 704.077668 524.226286 \nL 705.025842 521.010254 \nL 705.974015 525.228804 \nL 706.922188 521.352627 \nL 707.870362 525.007439 \nL 708.818535 520.177932 \nL 709.766708 520.872491 \nL 710.714882 523.006779 \nL 711.663055 524.786048 \nL 712.611228 521.085651 \nL 713.559402 523.714649 \nL 714.507575 522.77092 \nL 715.455748 522.225958 \nL 716.403922 524.176926 \nL 717.352095 520.562817 \nL 718.300268 520.289392 \nL 719.248441 520.237279 \nL 720.196615 527.505868 \nL 721.144788 523.604811 \nL 722.092961 520.785831 \nL 723.041135 524.054322 \nL 723.989308 522.045523 \nL 724.937481 528.130468 \nL 725.885655 523.174395 \nL 726.833828 526.231667 \nL 727.782001 521.1517 \nL 728.730175 522.079858 \nL 729.678348 525.426645 \nL 730.626521 521.992599 \nL 731.574695 524.650027 \nL 732.522868 526.8218 \nL 733.471041 523.449854 \nL 734.419215 522.31843 \nL 735.367388 525.076813 \nL 736.315561 521.107087 \nL 737.263735 524.379966 \nL 738.211908 524.438316 \nL 739.160081 523.812519 \nL 740.108255 521.71198 \nL 741.056428 520.66195 \nL 742.004601 526.110659 \nL 742.952775 522.819004 \nL 743.900948 521.904862 \nL 744.849121 523.522445 \nL 745.797294 524.754852 \nL 746.745468 525.781983 \nL 747.693641 523.332835 \nL 748.641814 523.692654 \nL 749.589988 521.569723 \nL 750.538161 524.016091 \nL 751.486334 519.198233 \nL 752.434508 524.183614 \nL 753.382681 523.515092 \nL 754.330854 524.88328 \nL 755.279028 525.290438 \nL 756.227201 523.668187 \nL 757.175374 523.943713 \nL 758.123548 524.539723 \nL 759.071721 522.812448 \nL 760.019894 523.842478 \nL 760.968068 522.363881 \nL 761.916241 526.535996 \nL 762.864414 524.99599 \nL 763.812588 528.52659 \nL 764.760761 521.990618 \nL 765.708934 520.632523 \nL 766.657108 526.501834 \nL 767.605281 523.321439 \nL 768.553454 525.153114 \nL 769.501628 521.960645 \nL 770.449801 524.628073 \nL 771.397974 521.417891 \nL 772.346148 523.643799 \nL 773.294321 522.8177 \nL 774.242494 523.129874 \nL 775.190667 519.652372 \nL 776.138841 518.870554 \nL 777.087014 526.115633 \nL 778.035187 523.10667 \nL 778.983361 520.981744 \nL 779.931534 521.244597 \nL 779.931534 521.244597 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p7b81d1b57e)\" d=\"M 69.749716 108.484894 \nL 72.594236 248.669866 \nL 75.438756 395.174139 \nL 76.386929 431.742476 \nL 77.335102 457.650757 \nL 78.283276 473.752447 \nL 79.231449 485.0952 \nL 80.179622 491.06197 \nL 81.127796 492.708462 \nL 82.075969 492.661774 \nL 83.024142 491.791461 \nL 84.920489 489.557482 \nL 85.868662 488.9357 \nL 86.816836 487.857081 \nL 87.765009 487.747854 \nL 88.713182 487.311388 \nL 89.661356 487.577805 \nL 90.609529 486.547455 \nL 91.557702 486.810428 \nL 92.505876 487.296255 \nL 93.454049 487.399165 \nL 94.402222 486.880573 \nL 95.350396 486.61623 \nL 96.298569 486.667892 \nL 97.246742 486.485316 \nL 98.194916 487.056766 \nL 99.143089 488.102394 \nL 100.091262 487.836802 \nL 101.039436 487.4383 \nL 101.987609 487.608137 \nL 102.935782 487.612924 \nL 103.883956 487.477554 \nL 104.832129 487.6069 \nL 105.780302 487.097284 \nL 107.676649 487.849873 \nL 108.624822 488.953399 \nL 109.572995 488.162459 \nL 110.521169 487.651633 \nL 111.469342 487.899034 \nL 112.417515 488.028752 \nL 113.365689 488.638619 \nL 115.262035 488.872098 \nL 116.210209 488.740213 \nL 117.158382 489.256425 \nL 119.054729 489.65107 \nL 120.002902 489.412418 \nL 120.951075 489.849523 \nL 121.899249 489.189604 \nL 122.847422 489.331955 \nL 123.795595 489.689527 \nL 124.743769 489.904708 \nL 125.691942 490.377916 \nL 126.640115 490.197534 \nL 127.588289 491.031611 \nL 128.536462 491.202312 \nL 129.484635 490.425973 \nL 130.432809 491.256952 \nL 131.380982 491.397826 \nL 132.329155 492.249935 \nL 133.277328 492.342899 \nL 134.225502 491.926299 \nL 135.173675 490.51504 \nL 136.121848 491.132074 \nL 137.070022 491.138377 \nL 138.018195 490.736724 \nL 138.966368 492.646283 \nL 139.914542 491.859465 \nL 140.862715 491.753563 \nL 141.810888 493.147256 \nL 142.759062 493.256455 \nL 143.707235 492.931516 \nL 144.655408 492.244656 \nL 145.603582 492.702625 \nL 146.551755 492.74519 \nL 147.499928 493.799794 \nL 148.448102 493.814435 \nL 149.396275 494.475444 \nL 150.344448 495.879057 \nL 151.292622 495.411275 \nL 152.240795 495.764618 \nL 153.188968 494.278893 \nL 154.137142 494.232311 \nL 155.085315 494.921605 \nL 156.033488 495.202636 \nL 156.981662 496.222733 \nL 157.929835 496.140567 \nL 158.878008 496.776204 \nL 159.826181 495.837076 \nL 160.774355 496.436757 \nL 162.670701 497.031744 \nL 163.618875 497.619882 \nL 164.567048 496.275444 \nL 165.515221 497.496999 \nL 166.463395 497.628752 \nL 167.411568 498.538998 \nL 168.359741 498.109818 \nL 169.307915 497.534552 \nL 170.256088 498.514503 \nL 171.204261 499.758505 \nL 172.152435 500.575628 \nL 173.100608 498.638703 \nL 174.048781 499.898196 \nL 174.996955 499.372343 \nL 175.945128 501.059859 \nL 176.893301 501.003398 \nL 177.841475 501.935877 \nL 178.789648 502.290882 \nL 179.737821 501.54441 \nL 181.634168 502.712934 \nL 183.530515 502.1132 \nL 184.478688 503.295488 \nL 185.426861 503.949104 \nL 186.375035 503.829598 \nL 187.323208 503.48567 \nL 188.271381 503.62705 \nL 189.219554 505.20752 \nL 190.167728 504.889774 \nL 191.115901 504.701161 \nL 192.064074 505.605516 \nL 193.012248 505.807254 \nL 193.960421 504.511259 \nL 194.908594 506.488582 \nL 195.856768 506.201873 \nL 196.804941 507.133913 \nL 197.753114 506.981004 \nL 198.701288 507.57905 \nL 199.649461 506.45071 \nL 201.545808 508.840085 \nL 202.493981 508.394164 \nL 203.442154 509.662075 \nL 204.390328 509.681396 \nL 205.338501 509.189599 \nL 206.286674 509.383318 \nL 207.234848 510.306476 \nL 208.183021 510.990357 \nL 209.131194 511.142069 \nL 210.079368 510.623503 \nL 211.027541 511.31265 \nL 211.975714 511.422315 \nL 212.923888 510.763394 \nL 213.872061 511.309432 \nL 214.820234 512.487172 \nL 215.768407 512.848148 \nL 216.716581 512.400099 \nL 217.664754 512.120185 \nL 219.561101 513.276011 \nL 220.509274 513.980663 \nL 221.457447 514.099237 \nL 222.405621 514.639478 \nL 223.353794 514.728093 \nL 224.301967 514.495186 \nL 225.250141 514.064517 \nL 226.198314 515.637381 \nL 227.146487 515.732206 \nL 228.094661 516.079592 \nL 229.042834 515.711675 \nL 229.991007 516.498532 \nL 230.939181 517.127521 \nL 231.887354 516.991327 \nL 232.835527 515.927215 \nL 233.783701 517.458577 \nL 234.731874 517.264579 \nL 235.680047 517.924604 \nL 236.628221 517.766483 \nL 237.576394 517.892783 \nL 238.524567 517.837093 \nL 239.472741 518.511918 \nL 240.420914 518.910806 \nL 241.369087 518.21755 \nL 243.265434 519.104538 \nL 244.213607 519.992178 \nL 245.16178 519.576163 \nL 246.109954 518.256512 \nL 247.058127 520.396704 \nL 248.0063 520.728106 \nL 248.954474 521.179027 \nL 249.902647 521.3599 \nL 250.85082 521.73948 \nL 251.798994 521.287934 \nL 254.643514 521.533327 \nL 255.591687 522.111014 \nL 256.53986 521.523687 \nL 257.488034 522.535592 \nL 258.436207 522.08039 \nL 259.38438 522.87234 \nL 260.332554 522.996952 \nL 262.2289 522.850998 \nL 263.177074 523.633214 \nL 264.125247 524.097645 \nL 266.021594 524.628139 \nL 266.969767 524.583699 \nL 267.91794 525.136652 \nL 268.866114 524.58548 \nL 269.814287 524.534152 \nL 271.710633 525.825453 \nL 272.658807 525.530486 \nL 273.60698 525.406566 \nL 274.555153 525.123088 \nL 275.503327 526.086258 \nL 276.4515 525.766372 \nL 277.399673 526.018467 \nL 278.347847 525.540951 \nL 279.29602 526.165538 \nL 280.244193 526.39548 \nL 281.192367 527.076502 \nL 282.14054 527.018844 \nL 284.036887 526.196229 \nL 284.98506 527.545189 \nL 287.82958 527.272349 \nL 288.777753 527.772444 \nL 289.725927 527.973384 \nL 290.6741 528.423095 \nL 291.622273 527.481759 \nL 292.570447 528.287486 \nL 293.51862 527.880234 \nL 294.466793 528.557467 \nL 295.414967 528.63422 \nL 296.36314 528.438999 \nL 298.259487 529.600329 \nL 299.20766 529.713611 \nL 300.155833 528.896422 \nL 301.104006 528.926514 \nL 303.000353 530.320885 \nL 303.948526 530.141567 \nL 304.8967 530.344355 \nL 305.844873 529.544998 \nL 306.793046 529.722321 \nL 307.74122 530.05016 \nL 308.689393 529.220205 \nL 309.637566 530.083989 \nL 310.58574 530.115969 \nL 311.533913 530.825993 \nL 312.482086 530.665465 \nL 314.378433 531.297885 \nL 315.326606 530.902561 \nL 318.171126 531.276449 \nL 319.1193 531.844495 \nL 320.067473 531.300345 \nL 321.015646 532.189115 \nL 321.96382 532.261826 \nL 322.911993 532.097069 \nL 325.756513 532.300043 \nL 326.704686 531.937205 \nL 327.652859 532.018972 \nL 328.601033 532.318221 \nL 329.549206 533.082725 \nL 330.497379 533.092818 \nL 331.445553 532.540942 \nL 332.393726 533.104852 \nL 333.341899 532.819845 \nL 334.290073 532.798862 \nL 335.238246 533.203693 \nL 336.186419 533.451068 \nL 337.134593 532.815736 \nL 338.082766 533.049468 \nL 339.030939 533.637753 \nL 339.979113 533.928983 \nL 340.927286 533.730357 \nL 341.875459 533.724559 \nL 342.823633 532.627416 \nL 343.771806 533.966495 \nL 344.719979 534.447203 \nL 345.668153 534.095482 \nL 347.564499 534.57353 \nL 349.460846 533.943344 \nL 350.409019 534.118766 \nL 353.253539 533.861378 \nL 355.149886 534.903017 \nL 356.098059 535.218276 \nL 357.046232 534.490739 \nL 357.994406 535.028121 \nL 358.942579 535.263368 \nL 359.890752 535.140751 \nL 360.838926 535.1374 \nL 361.787099 535.541288 \nL 362.735272 535.260283 \nL 363.683446 535.38157 \nL 364.631619 535.373698 \nL 366.527966 535.532139 \nL 367.476139 535.584013 \nL 368.424312 534.136478 \nL 369.372486 535.741749 \nL 370.320659 535.526993 \nL 371.268832 535.182027 \nL 372.217006 535.533043 \nL 373.165179 536.055186 \nL 374.113352 535.664583 \nL 375.061526 535.834858 \nL 376.009699 535.685526 \nL 376.957872 535.811681 \nL 377.906046 535.560596 \nL 378.854219 536.579828 \nL 380.750566 535.730073 \nL 381.698739 536.419765 \nL 382.646912 536.629641 \nL 383.595085 536.308146 \nL 384.543259 536.129533 \nL 385.491432 536.339461 \nL 387.387779 536.075797 \nL 388.335952 536.341177 \nL 389.284125 536.384527 \nL 390.232299 536.286617 \nL 391.180472 536.511944 \nL 392.128645 536.946735 \nL 393.076819 536.267907 \nL 394.024992 536.781433 \nL 394.973165 536.763667 \nL 395.921339 537.046228 \nL 396.869512 536.457424 \nL 398.765859 537.219522 \nL 400.662205 536.821206 \nL 401.610379 536.879715 \nL 402.558552 537.400542 \nL 403.506725 536.94724 \nL 404.454899 536.617607 \nL 405.403072 537.796317 \nL 406.351245 536.807084 \nL 407.299419 537.014805 \nL 408.247592 537.796065 \nL 409.195765 536.889117 \nL 410.143939 537.600112 \nL 411.092112 537.446751 \nL 412.040285 537.485021 \nL 412.988458 537.979983 \nL 413.936632 537.764789 \nL 414.884805 536.738482 \nL 415.832978 537.881515 \nL 416.781152 537.427762 \nL 417.729325 537.933282 \nL 418.677498 538.118797 \nL 419.625672 537.442429 \nL 420.573845 537.54756 \nL 421.522018 537.97388 \nL 422.470192 538.016804 \nL 424.366538 537.745587 \nL 425.314712 537.238178 \nL 426.262885 537.649884 \nL 427.211058 537.390967 \nL 429.107405 539.011131 \nL 430.055578 538.044304 \nL 431.003752 537.765892 \nL 431.951925 537.742728 \nL 432.900098 538.302756 \nL 433.848272 537.982829 \nL 434.796445 538.142307 \nL 435.744618 538.515477 \nL 436.692792 537.337152 \nL 437.640965 538.391331 \nL 438.589138 538.499054 \nL 439.537311 538.859884 \nL 440.485485 537.847247 \nL 441.433658 538.318234 \nL 442.381831 538.113052 \nL 443.330005 538.499879 \nL 444.278178 538.299418 \nL 445.226351 538.725831 \nL 446.174525 538.944736 \nL 447.122698 538.355534 \nL 448.070871 538.958366 \nL 449.019045 538.70534 \nL 449.967218 538.845948 \nL 450.915391 538.402408 \nL 451.863565 539.247083 \nL 452.811738 539.256205 \nL 453.759911 539.464591 \nL 454.708085 538.897556 \nL 455.656258 538.977049 \nL 456.604431 538.610329 \nL 457.552605 538.563069 \nL 458.500778 538.078173 \nL 459.448951 539.303717 \nL 460.397125 538.661192 \nL 461.345298 538.42742 \nL 462.293471 539.088509 \nL 464.189818 538.322622 \nL 465.137991 539.193906 \nL 466.086165 538.651578 \nL 467.034338 538.886001 \nL 467.982511 539.277215 \nL 470.827031 539.059893 \nL 471.775204 539.226498 \nL 472.723378 539.141886 \nL 473.671551 538.681909 \nL 475.567898 539.303518 \nL 476.516071 538.502512 \nL 477.464244 538.813808 \nL 478.412418 538.866453 \nL 479.360591 539.377918 \nL 481.256938 539.062566 \nL 483.153284 539.307919 \nL 485.049631 539.231232 \nL 485.997804 538.450891 \nL 486.945978 539.180595 \nL 487.894151 539.022992 \nL 488.842324 539.319821 \nL 489.790498 539.278226 \nL 490.738671 540.187594 \nL 491.686844 539.463315 \nL 492.635018 539.657805 \nL 493.583191 539.36103 \nL 494.531364 539.747684 \nL 495.479537 538.953818 \nL 496.427711 539.823201 \nL 497.375884 539.38946 \nL 498.324057 539.66734 \nL 500.220404 539.900553 \nL 501.168577 539.433209 \nL 502.116751 539.266205 \nL 504.013097 539.587594 \nL 504.961271 539.57657 \nL 505.909444 539.759346 \nL 506.857617 539.052859 \nL 507.805791 539.5408 \nL 508.753964 539.86687 \nL 509.702137 539.785701 \nL 510.650311 540.127529 \nL 511.598484 540.15314 \nL 513.494831 539.873452 \nL 514.443004 539.361854 \nL 515.391177 539.868412 \nL 516.339351 539.994673 \nL 517.287524 539.536824 \nL 518.235697 539.641702 \nL 519.183871 539.40922 \nL 520.132044 540.097224 \nL 521.080217 540.084777 \nL 522.028391 539.887734 \nL 522.976564 539.385165 \nL 523.924737 540.03035 \nL 524.87291 540.14314 \nL 525.821084 539.622248 \nL 526.769257 539.912268 \nL 527.71743 539.387625 \nL 529.613777 540.210758 \nL 530.56195 540.16185 \nL 531.510124 539.870394 \nL 532.458297 539.870447 \nL 533.40647 539.598405 \nL 534.354644 540.335982 \nL 537.199164 540.076519 \nL 538.147337 540.288443 \nL 539.09551 539.669095 \nL 540.043684 540.286821 \nL 540.991857 540.151039 \nL 541.94003 539.756686 \nL 543.836377 539.852149 \nL 544.78455 539.361482 \nL 545.732724 539.983357 \nL 547.62907 539.488381 \nL 548.577244 540.22113 \nL 549.525417 539.878651 \nL 550.47359 540.196716 \nL 551.421763 539.993888 \nL 552.369937 539.462663 \nL 553.31811 539.861471 \nL 554.266283 540.078541 \nL 555.214457 540.467814 \nL 556.16263 540.586455 \nL 557.110803 539.467038 \nL 558.058977 540.098155 \nL 559.00715 540.470606 \nL 559.955323 539.438555 \nL 561.85167 540.498877 \nL 563.748017 540.221849 \nL 564.69619 539.872415 \nL 565.644363 540.354399 \nL 566.592537 540.053847 \nL 567.54071 540.411711 \nL 568.488883 540.084139 \nL 569.437057 540.494675 \nL 570.38523 539.934182 \nL 571.333403 540.317073 \nL 572.281577 539.651702 \nL 573.22975 539.953051 \nL 574.177923 539.943052 \nL 575.126097 540.382124 \nL 577.022443 540.74056 \nL 577.970617 540.719191 \nL 578.91879 540.33908 \nL 579.866963 540.307724 \nL 580.815136 540.040802 \nL 581.76331 540.35521 \nL 583.659656 539.585945 \nL 585.556003 540.877459 \nL 586.504176 540.38017 \nL 588.400523 540.339865 \nL 589.348696 540.015404 \nL 590.29687 540.494688 \nL 591.245043 540.56236 \nL 592.193216 540.209389 \nL 593.14139 539.594163 \nL 594.089563 540.757595 \nL 595.037736 540.455873 \nL 595.98591 539.877096 \nL 596.934083 540.177581 \nL 597.882256 540.334679 \nL 598.83043 541.09413 \nL 599.778603 541.19305 \nL 600.726776 540.066586 \nL 601.67495 540.117409 \nL 602.623123 540.588822 \nL 603.571296 540.253577 \nL 604.51947 541.040501 \nL 605.467643 540.844361 \nL 606.415816 539.946669 \nL 607.363989 540.875079 \nL 608.312163 540.373069 \nL 609.260336 540.306528 \nL 610.208509 540.100362 \nL 611.156683 540.078035 \nL 612.104856 539.572009 \nL 613.053029 540.783192 \nL 614.001203 540.266715 \nL 614.949376 540.046693 \nL 616.845723 540.497241 \nL 618.742069 540.088301 \nL 620.638416 540.289866 \nL 621.586589 540.726399 \nL 622.534763 540.891222 \nL 623.482936 540.254308 \nL 624.431109 539.857428 \nL 625.379283 540.710548 \nL 626.327456 539.630027 \nL 627.275629 540.487228 \nL 628.223803 539.911949 \nL 629.171976 540.847274 \nL 630.120149 540.882605 \nL 631.068323 540.248204 \nL 632.016496 540.865079 \nL 632.964669 540.249122 \nL 633.912843 540.724736 \nL 637.705536 540.598768 \nL 638.653709 540.244893 \nL 639.601882 540.744776 \nL 640.550056 540.694551 \nL 641.498229 540.532932 \nL 642.446402 539.962692 \nL 644.342749 540.194708 \nL 645.290922 540.542958 \nL 646.239096 540.256635 \nL 647.187269 540.309998 \nL 648.135442 540.095216 \nL 649.083616 540.299919 \nL 650.031789 540.677716 \nL 650.979962 539.887986 \nL 651.928136 540.321793 \nL 652.876309 540.63047 \nL 654.772656 539.815076 \nL 655.720829 540.849002 \nL 656.669002 540.215226 \nL 657.617176 540.118447 \nL 658.565349 540.190027 \nL 659.513522 540.79262 \nL 660.461696 540.144616 \nL 662.358042 540.775014 \nL 663.306215 539.620173 \nL 664.254389 540.718978 \nL 667.098909 540.768791 \nL 668.047082 540.392523 \nL 669.943429 540.042664 \nL 670.891602 540.678939 \nL 671.839775 540.650443 \nL 672.787949 540.435647 \nL 673.736122 539.922507 \nL 674.684295 540.290105 \nL 675.632469 540.52889 \nL 676.580642 539.969062 \nL 677.528815 540.789376 \nL 679.425162 540.587572 \nL 680.373335 540.029446 \nL 681.321509 540.473758 \nL 682.269682 539.976163 \nL 683.217855 540.761238 \nL 684.166029 540.923509 \nL 685.114202 541.209034 \nL 686.062375 541.14724 \nL 687.010549 540.349173 \nL 687.958722 540.896501 \nL 688.906895 539.938903 \nL 689.855069 540.264374 \nL 690.803242 540.056919 \nL 691.751415 540.751518 \nL 692.699588 540.695468 \nL 694.595935 540.31666 \nL 696.492282 540.879108 \nL 697.440455 540.897339 \nL 698.388628 540.231024 \nL 699.336802 540.617877 \nL 701.233148 540.735933 \nL 702.181322 540.385103 \nL 704.077668 540.835771 \nL 705.025842 540.04035 \nL 705.974015 540.64785 \nL 707.870362 540.378973 \nL 709.766708 540.926421 \nL 710.714882 540.846103 \nL 711.663055 540.156531 \nL 712.611228 541.012882 \nL 713.559402 540.659778 \nL 714.507575 540.96122 \nL 715.455748 540.541163 \nL 716.403922 539.923398 \nL 717.352095 539.922946 \nL 718.300268 540.549594 \nL 720.196615 540.896248 \nL 721.144788 540.111146 \nL 722.092961 540.744696 \nL 723.041135 539.957639 \nL 723.989308 540.570631 \nL 727.782001 540.784097 \nL 729.678348 540.686293 \nL 730.626521 540.330636 \nL 731.574695 540.538158 \nL 732.522868 540.234893 \nL 733.471041 541.124594 \nL 734.419215 540.641986 \nL 735.367388 540.436684 \nL 736.315561 541.073093 \nL 737.263735 540.918602 \nL 738.211908 540.192966 \nL 739.160081 540.839494 \nL 740.108255 541.122148 \nL 742.004601 540.983055 \nL 742.952775 541.054184 \nL 743.900948 539.878625 \nL 744.849121 539.611237 \nL 745.797294 540.508119 \nL 746.745468 541.202625 \nL 747.693641 541.035474 \nL 748.641814 540.428812 \nL 749.589988 540.550777 \nL 750.538161 539.634136 \nL 751.486334 540.908123 \nL 752.434508 540.945915 \nL 753.382681 540.527241 \nL 754.330854 540.413227 \nL 755.279028 540.770121 \nL 757.175374 540.753007 \nL 758.123548 540.265917 \nL 760.019894 540.307285 \nL 760.968068 540.743951 \nL 761.916241 540.87097 \nL 762.864414 540.692131 \nL 763.812588 540.247872 \nL 765.708934 540.81758 \nL 766.657108 540.657238 \nL 767.605281 540.932484 \nL 768.553454 540.552918 \nL 769.501628 539.479392 \nL 770.449801 540.316926 \nL 771.397974 540.421685 \nL 772.346148 540.924905 \nL 773.294321 540.55986 \nL 774.242494 539.938384 \nL 775.190667 540.723912 \nL 776.138841 540.420062 \nL 777.087014 541.004531 \nL 778.983361 540.098487 \nL 779.931534 540.651999 \nL 779.931534 540.651999 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 565.918125 \nL 34.240625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 815.440625 565.918125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 22.318125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Model MAE -->\n    <g transform=\"translate(391.845313 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"486.71875\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 41.240625 59.674375 \nL 96.515625 59.674375 \nQ 98.515625 59.674375 98.515625 57.674375 \nL 98.515625 29.318125 \nQ 98.515625 27.318125 96.515625 27.318125 \nL 41.240625 27.318125 \nQ 39.240625 27.318125 39.240625 29.318125 \nL 39.240625 57.674375 \nQ 39.240625 59.674375 41.240625 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 43.240625 35.416562 \nL 63.240625 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_17\">\n     <!-- train -->\n     <g transform=\"translate(71.240625 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 43.240625 50.094687 \nL 63.240625 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_18\">\n     <!-- test -->\n     <g transform=\"translate(71.240625 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7b81d1b57e\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"34.240625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-06T20:07:32.160309</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m15e56d4c3e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#m15e56d4c3e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.929548\" xlink:href=\"#m15e56d4c3e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(161.385798 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"265.74688\" xlink:href=\"#m15e56d4c3e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(256.20313 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.564213\" xlink:href=\"#m15e56d4c3e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(351.020463 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"455.381545\" xlink:href=\"#m15e56d4c3e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(445.837795 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"550.198877\" xlink:href=\"#m15e56d4c3e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(540.655127 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"645.016209\" xlink:href=\"#m15e56d4c3e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(635.472459 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"739.833541\" xlink:href=\"#m15e56d4c3e\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(730.289791 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mc3f99fc5fd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mc3f99fc5fd\" y=\"523.883008\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 527.682227)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mc3f99fc5fd\" y=\"458.773164\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 462.572382)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mc3f99fc5fd\" y=\"393.66332\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 397.462538)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mc3f99fc5fd\" y=\"328.553475\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 332.352694)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mc3f99fc5fd\" y=\"263.443631\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 267.24285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mc3f99fc5fd\" y=\"198.333787\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 202.133006)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mc3f99fc5fd\" y=\"133.223943\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 137.023161)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mc3f99fc5fd\" y=\"68.114098\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 71.913317)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- MSE -->\n     <g transform=\"translate(14.798438 304.765781)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p70b95cfae3)\" d=\"M 76.112216 47.027216 \nL 78.008563 165.828838 \nL 80.853083 319.718899 \nL 82.749429 401.961817 \nL 83.697602 431.592255 \nL 84.645776 454.985778 \nL 85.593949 474.973492 \nL 86.542122 488.564456 \nL 89.386642 502.43661 \nL 90.334816 502.79058 \nL 91.282989 504.114037 \nL 92.231162 504.22955 \nL 93.179336 502.678781 \nL 94.127509 503.928886 \nL 95.075682 503.602765 \nL 96.023856 503.801731 \nL 96.972029 504.967563 \nL 97.920202 504.801413 \nL 98.868376 505.35661 \nL 99.816549 503.786382 \nL 100.764722 505.075904 \nL 101.712896 504.73773 \nL 102.661069 506.38637 \nL 103.609242 505.477016 \nL 104.557416 503.81125 \nL 105.505589 504.818632 \nL 106.453762 504.504363 \nL 107.401936 504.399525 \nL 108.350109 506.094841 \nL 109.298282 506.07525 \nL 110.246456 503.419415 \nL 111.194629 505.172869 \nL 112.142802 501.996428 \nL 113.090975 504.37134 \nL 114.039149 506.274869 \nL 115.935495 505.945376 \nL 116.883669 506.505745 \nL 117.831842 504.480967 \nL 118.780015 507.095057 \nL 119.728189 504.90537 \nL 120.676362 507.516107 \nL 121.624535 504.142327 \nL 122.572709 507.875219 \nL 123.520882 503.857106 \nL 124.469055 505.429198 \nL 125.417229 507.589092 \nL 126.365402 507.103222 \nL 127.313575 507.172835 \nL 128.261749 507.063867 \nL 129.209922 506.442373 \nL 130.158095 508.169499 \nL 131.106269 506.766669 \nL 132.054442 507.633415 \nL 133.002615 507.91812 \nL 133.950789 509.25094 \nL 134.898962 507.235451 \nL 135.847135 503.385108 \nL 136.795309 503.136485 \nL 137.743482 508.517522 \nL 138.691655 508.101259 \nL 139.639828 503.459881 \nL 140.588002 506.594011 \nL 141.536175 505.315709 \nL 142.484348 507.224671 \nL 143.432522 506.573899 \nL 144.380695 507.773696 \nL 145.328868 506.136152 \nL 146.277042 509.15698 \nL 147.225215 505.881866 \nL 148.173388 509.685384 \nL 149.121562 508.441879 \nL 150.069735 506.062819 \nL 151.966082 508.991737 \nL 152.914255 507.543944 \nL 153.862428 509.425435 \nL 154.810602 508.650284 \nL 155.758775 508.922123 \nL 156.706948 509.467398 \nL 157.655122 513.269395 \nL 158.603295 508.874591 \nL 159.551468 508.822426 \nL 160.499642 510.475916 \nL 161.447815 507.481366 \nL 162.395988 507.273855 \nL 163.344162 508.852243 \nL 164.292335 511.213222 \nL 165.240508 510.099629 \nL 166.188681 511.207167 \nL 167.136855 510.353182 \nL 168.085028 511.145881 \nL 169.033201 510.485839 \nL 169.981375 510.738591 \nL 170.929548 508.306733 \nL 171.877721 511.971763 \nL 172.825895 508.579671 \nL 173.774068 508.903396 \nL 174.722241 510.520462 \nL 175.670415 510.944971 \nL 176.618588 508.181887 \nL 177.566761 510.384999 \nL 178.514935 511.946255 \nL 179.463108 511.692298 \nL 180.411281 511.890551 \nL 181.359455 510.334169 \nL 182.307628 512.548432 \nL 183.255801 513.537614 \nL 184.203975 514.782963 \nL 185.152148 508.975909 \nL 186.100321 510.379199 \nL 187.048495 513.965011 \nL 187.996668 509.118848 \nL 188.944841 513.343013 \nL 189.893015 511.808308 \nL 190.841188 512.195766 \nL 191.789361 512.33665 \nL 192.737535 514.524772 \nL 193.685708 510.872731 \nL 194.633881 513.041219 \nL 195.582054 511.083378 \nL 196.530228 515.243468 \nL 197.478401 513.942402 \nL 198.426574 513.801264 \nL 199.374748 513.2762 \nL 200.322921 514.915427 \nL 201.271094 513.571691 \nL 202.219268 513.290444 \nL 203.167441 513.626861 \nL 204.115614 517.198478 \nL 206.011961 516.116479 \nL 206.960134 513.724435 \nL 207.908308 518.459593 \nL 208.856481 515.204361 \nL 209.804654 516.943919 \nL 210.752828 516.059918 \nL 211.701001 516.649739 \nL 212.649174 517.770658 \nL 213.597348 515.517946 \nL 214.545521 516.276078 \nL 215.493694 516.190234 \nL 216.441868 515.402266 \nL 217.390041 518.573708 \nL 218.338214 519.61169 \nL 219.286388 516.682338 \nL 220.234561 518.09452 \nL 221.182734 517.572778 \nL 222.130907 515.598171 \nL 223.079081 520.178256 \nL 224.027254 516.240213 \nL 224.975427 517.404219 \nL 225.923601 517.067454 \nL 226.871774 517.823506 \nL 227.819947 519.301426 \nL 228.768121 516.103439 \nL 229.716294 520.268801 \nL 230.664467 517.907823 \nL 231.612641 517.655164 \nL 232.560814 517.530002 \nL 233.508987 517.222893 \nL 234.457161 520.030616 \nL 235.405334 516.218486 \nL 236.353507 520.304536 \nL 237.301681 520.698309 \nL 238.249854 518.333164 \nL 239.198027 516.806016 \nL 240.146201 517.110218 \nL 241.094374 519.291256 \nL 242.042547 520.568626 \nL 242.990721 518.351693 \nL 243.938894 518.463877 \nL 244.887067 521.540552 \nL 245.835241 519.247585 \nL 246.783414 520.914314 \nL 247.731587 519.038572 \nL 248.679761 522.072073 \nL 250.576107 519.937128 \nL 251.52428 520.636445 \nL 252.472454 523.081076 \nL 253.420627 520.902634 \nL 254.3688 521.867835 \nL 255.316974 519.980612 \nL 256.265147 520.280375 \nL 257.21332 520.323307 \nL 258.161494 523.274273 \nL 259.109667 521.311775 \nL 260.05784 520.594786 \nL 261.006014 522.023591 \nL 261.954187 519.980966 \nL 262.90236 519.933191 \nL 263.850534 520.836361 \nL 264.798707 522.36672 \nL 266.695054 519.588119 \nL 267.643227 521.007733 \nL 268.5914 522.010365 \nL 269.539574 522.108547 \nL 270.487747 520.155996 \nL 271.43592 521.920993 \nL 272.384094 523.164983 \nL 273.332267 523.477301 \nL 274.28044 522.012606 \nL 275.228614 524.357925 \nL 276.176787 523.080262 \nL 277.12496 523.524933 \nL 278.073133 521.414856 \nL 279.021307 523.743378 \nL 279.96948 520.529954 \nL 280.917653 522.62172 \nL 281.865827 521.837173 \nL 282.814 523.997869 \nL 283.762173 523.614043 \nL 284.710347 523.061795 \nL 285.65852 520.146589 \nL 286.606693 524.396007 \nL 287.554867 521.854622 \nL 288.50304 520.575314 \nL 289.451213 523.672126 \nL 290.399387 520.7035 \nL 291.34756 522.640615 \nL 292.295733 522.379014 \nL 293.243907 522.873273 \nL 294.19208 523.559358 \nL 295.140253 522.376667 \nL 296.088427 524.316825 \nL 297.0366 519.423229 \nL 297.984773 523.333113 \nL 298.932947 524.023203 \nL 299.88112 523.469359 \nL 300.829293 525.69388 \nL 301.777467 523.028731 \nL 302.72564 524.138641 \nL 303.673813 524.789332 \nL 304.621987 521.898913 \nL 305.57016 524.390449 \nL 306.518333 523.833625 \nL 307.466506 524.121013 \nL 308.41468 524.636365 \nL 309.362853 521.145252 \nL 310.311026 523.601941 \nL 311.2592 524.574364 \nL 313.155546 522.906959 \nL 314.10372 522.695599 \nL 315.051893 523.899736 \nL 316.94824 524.114176 \nL 317.896413 525.372714 \nL 318.844586 524.78162 \nL 319.79276 524.492717 \nL 320.740933 523.026098 \nL 321.689106 524.30273 \nL 322.63728 526.354028 \nL 323.585453 523.475873 \nL 324.533626 525.327559 \nL 325.4818 523.628784 \nL 326.429973 521.616809 \nL 327.378146 524.265784 \nL 328.32632 523.352834 \nL 329.274493 523.749761 \nL 330.222666 521.635158 \nL 333.067186 524.099056 \nL 334.015359 524.34137 \nL 334.963533 522.427714 \nL 335.911706 521.871548 \nL 336.859879 523.736467 \nL 337.808053 522.330606 \nL 338.756226 523.972038 \nL 339.704399 527.765844 \nL 340.652573 525.838683 \nL 341.600746 524.588428 \nL 342.548919 524.530644 \nL 343.497093 521.444102 \nL 344.445266 522.900942 \nL 345.393439 525.567843 \nL 346.341613 524.158809 \nL 347.289786 526.264198 \nL 348.237959 524.359079 \nL 349.186133 526.633381 \nL 350.134306 523.752282 \nL 351.082479 523.567678 \nL 352.030653 525.406636 \nL 352.978826 525.471374 \nL 353.926999 525.309999 \nL 354.875173 524.40144 \nL 355.823346 519.764508 \nL 356.771519 524.708207 \nL 357.719693 522.832676 \nL 358.667866 525.621709 \nL 359.616039 524.172265 \nL 360.564213 524.489718 \nL 361.512386 524.134431 \nL 362.460559 522.913205 \nL 363.408732 525.276897 \nL 364.356906 524.020222 \nL 365.305079 523.408122 \nL 366.253252 525.029125 \nL 367.201426 524.12888 \nL 368.149599 524.01921 \nL 369.097772 526.090286 \nL 370.045946 525.219641 \nL 370.994119 522.74536 \nL 371.942292 523.411047 \nL 372.890466 527.043441 \nL 373.838639 523.061336 \nL 375.734986 525.180193 \nL 376.683159 525.929656 \nL 377.631332 522.817432 \nL 378.579506 524.699793 \nL 379.527679 522.840183 \nL 380.475852 522.992232 \nL 381.424026 524.504 \nL 382.372199 525.385493 \nL 383.320372 522.559589 \nL 384.268546 524.553718 \nL 385.216719 526.070106 \nL 387.113066 524.983294 \nL 388.061239 524.038397 \nL 389.009412 526.269445 \nL 389.957585 525.456074 \nL 390.905759 525.604304 \nL 391.853932 524.821869 \nL 392.802105 522.181277 \nL 393.750279 526.601478 \nL 394.698452 525.624131 \nL 395.646625 523.820877 \nL 396.594799 524.78257 \nL 397.542972 527.069825 \nL 398.491145 522.718157 \nL 399.439319 525.685926 \nL 400.387492 524.339241 \nL 401.335665 525.196747 \nL 402.283839 523.909851 \nL 403.232012 523.022372 \nL 404.180185 526.664596 \nL 405.128359 527.287252 \nL 406.076532 524.416709 \nL 407.024705 525.313036 \nL 407.972879 525.28356 \nL 408.921052 526.317387 \nL 409.869225 525.684039 \nL 410.817399 523.699819 \nL 411.765572 522.610921 \nL 412.713745 526.961981 \nL 413.661919 525.056167 \nL 414.610092 524.18228 \nL 415.558265 524.147322 \nL 416.506439 525.390249 \nL 417.454612 525.655705 \nL 418.402785 524.118808 \nL 420.299132 525.250327 \nL 421.247305 524.027345 \nL 422.195478 526.169511 \nL 423.143652 525.158205 \nL 424.091825 525.37413 \nL 425.039998 524.094226 \nL 425.988172 526.071167 \nL 426.936345 524.411828 \nL 427.884518 522.996585 \nL 428.832692 524.250248 \nL 429.780865 525.895151 \nL 430.729038 525.542521 \nL 431.677212 527.678168 \nL 432.625385 525.321108 \nL 433.573558 525.270098 \nL 434.521732 522.714481 \nL 435.469905 523.182667 \nL 436.418078 525.699674 \nL 437.366252 526.224719 \nL 438.314425 525.22064 \nL 439.262598 523.805118 \nL 440.210772 523.750395 \nL 441.158945 523.97354 \nL 442.107118 524.064054 \nL 443.055292 526.806318 \nL 444.003465 526.210095 \nL 444.951638 522.940545 \nL 445.899811 523.766899 \nL 446.847985 526.951487 \nL 447.796158 525.399731 \nL 448.744331 524.562386 \nL 449.692505 525.539125 \nL 450.640678 525.230942 \nL 452.537025 523.880568 \nL 453.485198 522.641211 \nL 454.433371 525.797273 \nL 455.381545 525.826755 \nL 456.329718 526.111056 \nL 457.277891 525.166042 \nL 458.226065 523.806459 \nL 459.174238 526.97086 \nL 460.122411 522.797978 \nL 461.070585 524.260475 \nL 462.018758 525.02535 \nL 462.966931 526.459886 \nL 463.915105 523.956483 \nL 464.863278 525.718935 \nL 465.811451 526.164792 \nL 466.759625 525.526725 \nL 467.707798 525.12596 \nL 468.655971 524.990143 \nL 469.604145 525.875082 \nL 470.552318 525.271576 \nL 471.500491 526.728974 \nL 472.448665 525.814646 \nL 473.396838 527.196433 \nL 474.345011 525.863278 \nL 475.293184 523.817226 \nL 476.241358 524.865925 \nL 477.189531 525.626304 \nL 478.137704 524.664772 \nL 479.085878 526.656648 \nL 480.034051 524.332385 \nL 480.982224 523.01276 \nL 481.930398 525.385617 \nL 482.878571 524.285977 \nL 483.826744 522.381933 \nL 484.774918 528.32224 \nL 485.723091 524.993589 \nL 486.671264 526.106343 \nL 487.619438 526.326949 \nL 488.567611 527.399306 \nL 489.515784 527.002565 \nL 490.463958 525.090225 \nL 491.412131 527.463144 \nL 492.360304 524.967528 \nL 494.256651 527.62994 \nL 495.204824 524.045072 \nL 496.152998 525.553729 \nL 497.101171 523.250759 \nL 498.049344 526.50242 \nL 499.945691 526.065324 \nL 500.893864 526.974554 \nL 501.842037 525.016737 \nL 502.790211 524.564845 \nL 503.738384 525.524949 \nL 504.686557 524.712665 \nL 505.634731 526.023604 \nL 506.582904 524.893109 \nL 507.531077 525.945583 \nL 508.479251 524.545795 \nL 509.427424 522.145431 \nL 510.375597 522.999379 \nL 512.271944 525.343536 \nL 513.220117 525.472057 \nL 514.168291 524.558909 \nL 516.064637 524.419795 \nL 517.012811 525.735371 \nL 517.960984 523.397026 \nL 518.909157 526.282677 \nL 519.857331 526.703379 \nL 520.805504 525.670347 \nL 521.753677 523.297304 \nL 522.701851 526.686279 \nL 523.650024 521.874044 \nL 524.598197 525.454715 \nL 525.546371 526.523916 \nL 527.442717 526.63905 \nL 528.390891 524.474648 \nL 529.339064 525.29352 \nL 530.287237 524.809444 \nL 531.23541 523.337553 \nL 532.183584 526.368322 \nL 533.131757 525.164036 \nL 534.07993 527.281998 \nL 535.028104 527.26917 \nL 535.976277 525.324877 \nL 536.92445 526.128753 \nL 537.872624 523.70968 \nL 538.820797 524.386705 \nL 539.76897 524.615389 \nL 540.717144 526.132274 \nL 541.665317 521.84355 \nL 542.61349 525.956996 \nL 543.561664 525.535331 \nL 544.509837 526.556758 \nL 545.45801 526.05696 \nL 546.406184 526.518788 \nL 547.354357 525.912059 \nL 548.30253 526.162873 \nL 549.250704 523.675286 \nL 550.198877 526.416532 \nL 551.14705 527.520326 \nL 552.095224 526.136005 \nL 553.043397 525.44527 \nL 553.99157 525.34068 \nL 554.939744 525.472716 \nL 555.887917 523.880201 \nL 556.83609 526.594983 \nL 557.784263 526.328073 \nL 558.732437 528.676124 \nL 559.68061 524.501895 \nL 560.628783 526.914591 \nL 561.576957 526.027751 \nL 562.52513 525.31827 \nL 563.473303 525.161229 \nL 564.421477 523.491415 \nL 565.36965 527.46664 \nL 566.317823 524.880529 \nL 567.265997 527.277813 \nL 569.162343 524.423942 \nL 570.110517 524.832233 \nL 571.05869 526.423933 \nL 572.006863 524.136753 \nL 572.955037 525.222901 \nL 573.90321 524.698309 \nL 574.851383 524.485092 \nL 575.799557 522.725391 \nL 576.74773 523.363198 \nL 577.695903 526.604191 \nL 578.644077 523.901506 \nL 579.59225 523.94404 \nL 580.540423 526.215324 \nL 581.488597 524.739813 \nL 582.43677 524.737807 \nL 583.384943 525.4637 \nL 584.333117 524.916655 \nL 585.28129 525.854988 \nL 586.229463 525.678935 \nL 587.177636 527.275802 \nL 588.12581 527.332859 \nL 589.073983 526.291985 \nL 590.022156 527.190081 \nL 590.97033 526.36723 \nL 591.918503 525.071926 \nL 592.866676 523.971082 \nL 593.81485 523.971231 \nL 594.763023 524.961474 \nL 595.711196 526.33858 \nL 596.65937 525.165849 \nL 597.607543 527.309562 \nL 598.555716 525.732683 \nL 599.50389 526.475782 \nL 600.452063 522.676039 \nL 601.400236 524.194227 \nL 602.34841 526.20953 \nL 603.296583 522.758406 \nL 604.244756 524.02573 \nL 605.19293 526.256442 \nL 606.141103 527.380765 \nL 607.089276 525.721096 \nL 608.03745 525.279108 \nL 608.985623 524.975905 \nL 609.933796 524.93478 \nL 610.88197 521.926706 \nL 611.830143 527.115004 \nL 612.778316 525.072274 \nL 613.726489 526.704311 \nL 614.674663 525.357892 \nL 615.622836 524.692311 \nL 616.571009 527.515098 \nL 617.519183 526.452521 \nL 618.467356 526.671532 \nL 619.415529 524.378043 \nL 620.363703 526.618094 \nL 621.311876 527.954795 \nL 622.260049 525.078011 \nL 623.208223 526.157341 \nL 624.156396 524.992415 \nL 625.104569 526.237088 \nL 626.052743 525.602156 \nL 627.000916 527.181953 \nL 627.949089 527.166492 \nL 628.897263 524.261506 \nL 629.845436 524.268435 \nL 630.793609 526.855266 \nL 631.741783 525.352341 \nL 632.689956 526.453459 \nL 633.638129 524.463993 \nL 634.586303 527.031761 \nL 635.534476 526.911089 \nL 636.482649 524.01564 \nL 637.430823 523.428266 \nL 638.378996 525.825444 \nL 639.327169 525.355365 \nL 640.275343 525.679624 \nL 642.171689 524.074238 \nL 643.119862 525.759731 \nL 644.068036 522.620198 \nL 645.016209 522.864313 \nL 645.964382 527.264016 \nL 646.912556 526.352371 \nL 648.808902 525.593009 \nL 649.757076 525.245658 \nL 650.705249 524.588664 \nL 651.653422 525.93404 \nL 652.601596 525.426636 \nL 653.549769 525.501763 \nL 654.497942 525.2192 \nL 656.394289 525.798446 \nL 657.342462 527.081685 \nL 658.290636 525.213158 \nL 659.238809 524.389779 \nL 660.186982 526.362448 \nL 661.135156 526.019083 \nL 662.083329 524.657284 \nL 663.031502 526.597566 \nL 664.927849 524.337483 \nL 665.876022 526.020915 \nL 666.824196 523.953509 \nL 667.772369 528.299625 \nL 668.720542 525.22467 \nL 669.668715 525.337929 \nL 670.616889 523.991833 \nL 671.565062 525.271346 \nL 672.513235 525.94056 \nL 673.461409 525.861279 \nL 674.409582 526.038947 \nL 675.357755 524.206422 \nL 676.305929 525.996698 \nL 677.254102 525.193493 \nL 678.202275 525.730447 \nL 679.150449 524.18505 \nL 680.098622 526.357419 \nL 681.046795 525.360401 \nL 681.994969 524.1736 \nL 682.943142 523.80441 \nL 683.891315 522.772067 \nL 684.839489 523.082255 \nL 685.787662 525.300996 \nL 686.735835 526.091192 \nL 687.684009 525.816186 \nL 688.632182 523.239234 \nL 689.580355 525.608433 \nL 690.528529 526.676095 \nL 691.476702 527.438338 \nL 692.424875 524.874916 \nL 693.373049 523.989449 \nL 694.321222 524.654763 \nL 695.269395 524.373436 \nL 696.217569 523.474246 \nL 697.165742 526.215044 \nL 698.113915 526.502699 \nL 699.062088 523.027271 \nL 700.010262 524.768307 \nL 700.958435 525.137857 \nL 701.906608 526.15169 \nL 702.854782 524.919828 \nL 703.802955 525.41259 \nL 704.751128 525.113076 \nL 705.699302 526.119551 \nL 706.647475 526.76122 \nL 707.595648 525.307367 \nL 708.543822 525.217356 \nL 709.491995 524.521963 \nL 710.440168 525.818297 \nL 711.388342 525.250892 \nL 712.336515 526.713103 \nL 713.284688 525.637965 \nL 714.232862 526.028646 \nL 715.181035 523.435543 \nL 716.129208 524.442825 \nL 717.077382 525.252631 \nL 718.025555 527.217961 \nL 718.973728 522.565705 \nL 719.921902 526.23379 \nL 720.870075 524.60076 \nL 721.818248 524.701228 \nL 722.766422 525.309496 \nL 724.662768 523.544095 \nL 725.610941 524.3111 \nL 726.559115 527.208157 \nL 727.507288 525.151319 \nL 728.455461 523.974348 \nL 729.403635 525.63622 \nL 730.351808 524.896928 \nL 731.299981 527.03617 \nL 732.248155 524.160858 \nL 733.196328 526.634865 \nL 734.144501 524.485583 \nL 735.092675 525.633432 \nL 736.040848 526.096253 \nL 736.989021 524.546031 \nL 737.937195 525.81587 \nL 738.885368 526.447889 \nL 739.833541 526.520489 \nL 740.781715 525.227893 \nL 741.729888 526.713886 \nL 742.678061 523.687866 \nL 743.626235 527.065931 \nL 744.574408 524.736217 \nL 745.522581 525.031236 \nL 746.470755 523.892546 \nL 747.418928 524.641177 \nL 748.367101 524.690672 \nL 749.315275 525.755912 \nL 750.263448 524.414945 \nL 751.211621 525.234947 \nL 752.159794 526.854434 \nL 753.107968 525.701338 \nL 754.056141 525.021674 \nL 755.004314 525.149463 \nL 755.952488 523.202083 \nL 756.900661 526.20252 \nL 757.848834 524.618097 \nL 758.797008 525.969489 \nL 759.745181 525.264596 \nL 760.693354 526.597305 \nL 761.641528 526.615623 \nL 762.589701 525.385337 \nL 764.486048 524.114294 \nL 765.434221 524.378912 \nL 766.382394 526.324689 \nL 767.330568 525.344039 \nL 768.278741 526.472391 \nL 769.226914 525.251544 \nL 770.175088 529.322499 \nL 771.123261 524.795504 \nL 772.071434 523.764142 \nL 773.019608 526.970084 \nL 773.967781 526.964936 \nL 774.915954 525.080657 \nL 775.864128 524.45959 \nL 776.812301 527.102281 \nL 777.760474 524.659519 \nL 778.708648 525.135622 \nL 779.656821 525.443848 \nL 780.604994 525.339866 \nL 781.553167 523.229684 \nL 782.501341 523.048085 \nL 783.449514 526.425498 \nL 784.397687 525.838167 \nL 785.345861 524.110575 \nL 786.294034 524.870495 \nL 786.294034 524.870495 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p70b95cfae3)\" d=\"M 76.112216 134.315548 \nL 78.008563 241.645777 \nL 79.904909 337.092362 \nL 80.853083 378.875186 \nL 81.801256 415.663313 \nL 82.749429 446.104271 \nL 83.697602 470.088664 \nL 84.645776 486.77372 \nL 85.593949 498.600909 \nL 86.542122 505.798605 \nL 87.490296 509.644235 \nL 88.438469 511.489327 \nL 89.386642 512.19887 \nL 90.334816 512.354986 \nL 92.231162 512.151263 \nL 93.179336 511.893773 \nL 96.023856 511.876307 \nL 96.972029 511.583703 \nL 99.816549 511.920685 \nL 101.712896 511.724599 \nL 104.557416 511.94392 \nL 105.505589 512.29881 \nL 108.350109 512.230538 \nL 111.194629 512.320617 \nL 112.142802 512.175629 \nL 114.039149 512.4922 \nL 114.987322 512.877472 \nL 116.883669 512.511735 \nL 121.624535 513.083611 \nL 122.572709 513.075408 \nL 124.469055 513.390608 \nL 125.417229 513.484611 \nL 126.365402 513.442965 \nL 127.313575 513.626551 \nL 128.261749 513.447951 \nL 131.106269 513.80561 \nL 132.054442 514.002323 \nL 133.002615 513.985135 \nL 133.950789 514.302433 \nL 134.898962 514.401305 \nL 135.847135 514.193745 \nL 136.795309 514.523238 \nL 137.743482 514.617204 \nL 138.691655 514.926617 \nL 139.639828 515.008742 \nL 140.588002 514.937943 \nL 141.536175 514.522083 \nL 142.484348 514.776474 \nL 143.432522 514.830645 \nL 144.380695 514.743372 \nL 145.328868 515.422707 \nL 146.277042 515.22335 \nL 147.225215 515.244381 \nL 148.173388 515.754479 \nL 149.121562 515.850091 \nL 150.069735 515.810531 \nL 151.017908 515.641177 \nL 151.966082 515.861075 \nL 152.914255 515.934886 \nL 153.862428 516.336861 \nL 154.810602 516.413497 \nL 155.758775 516.6823 \nL 156.706948 517.159999 \nL 157.655122 517.108585 \nL 158.603295 517.281746 \nL 159.551468 516.912649 \nL 160.499642 516.963206 \nL 161.447815 517.264453 \nL 162.395988 517.422729 \nL 163.344162 517.809665 \nL 164.292335 517.86016 \nL 165.240508 518.129807 \nL 166.188681 517.914988 \nL 167.136855 518.186101 \nL 169.033201 518.517377 \nL 169.981375 518.781834 \nL 170.929548 518.410483 \nL 171.877721 518.910349 \nL 172.825895 519.032015 \nL 173.774068 519.401888 \nL 175.670415 519.224033 \nL 178.514935 520.443489 \nL 179.463108 519.920934 \nL 180.411281 520.428487 \nL 181.359455 520.340941 \nL 182.307628 520.97827 \nL 183.255801 521.040357 \nL 184.203975 521.404418 \nL 185.152148 521.607278 \nL 186.100321 521.464916 \nL 187.996668 522.007235 \nL 189.893015 521.974984 \nL 190.841188 522.46194 \nL 191.789361 522.749142 \nL 194.633881 522.87382 \nL 195.582054 523.462405 \nL 197.478401 523.467763 \nL 198.426574 523.846752 \nL 199.374748 523.995919 \nL 200.322921 523.606027 \nL 201.271094 524.386674 \nL 202.219268 524.372982 \nL 203.167441 524.753212 \nL 204.115614 524.782018 \nL 205.063788 525.043022 \nL 206.011961 524.713193 \nL 207.908308 525.671614 \nL 208.856481 525.623584 \nL 209.804654 526.084505 \nL 210.752828 526.18732 \nL 211.701001 526.123084 \nL 212.649174 526.268277 \nL 214.545521 526.913498 \nL 215.493694 527.043596 \nL 216.441868 526.962887 \nL 217.390041 527.249107 \nL 218.338214 527.358535 \nL 219.286388 527.216993 \nL 220.234561 527.470738 \nL 221.182734 527.921985 \nL 222.130907 528.109594 \nL 224.027254 528.010704 \nL 228.768121 529.161789 \nL 230.664467 529.247763 \nL 231.612641 529.176126 \nL 232.560814 529.744183 \nL 235.405334 529.958549 \nL 237.301681 530.490498 \nL 238.249854 530.524606 \nL 239.198027 530.239808 \nL 240.146201 530.791702 \nL 241.094374 530.800693 \nL 242.042547 531.053613 \nL 244.887067 531.194733 \nL 246.783414 531.620719 \nL 247.731587 531.441207 \nL 250.576107 532.172173 \nL 251.52428 532.09481 \nL 252.472454 531.649953 \nL 253.420627 532.448117 \nL 257.21332 533.043433 \nL 258.161494 532.937054 \nL 261.006014 533.11905 \nL 261.954187 533.354391 \nL 262.90236 533.179231 \nL 263.850534 533.58261 \nL 264.798707 533.449127 \nL 265.74688 533.77274 \nL 267.643227 533.853909 \nL 268.5914 533.847731 \nL 269.539574 534.175374 \nL 271.43592 534.48948 \nL 274.28044 534.852765 \nL 275.228614 534.686416 \nL 276.176787 534.689571 \nL 278.073133 535.193765 \nL 280.917653 535.015047 \nL 281.865827 535.393147 \nL 282.814 535.309755 \nL 283.762173 535.419301 \nL 284.710347 535.245072 \nL 285.65852 535.515925 \nL 286.606693 535.623433 \nL 287.554867 535.881966 \nL 288.50304 535.886276 \nL 290.399387 535.580887 \nL 291.34756 536.12153 \nL 294.19208 536.073413 \nL 296.088427 536.352561 \nL 297.0366 536.519264 \nL 297.984773 536.201897 \nL 298.932947 536.522424 \nL 299.88112 536.379373 \nL 300.829293 536.654615 \nL 303.673813 536.858456 \nL 304.621987 537.075706 \nL 305.57016 537.134608 \nL 306.518333 536.856494 \nL 307.466506 536.878897 \nL 309.362853 537.391524 \nL 310.311026 537.350405 \nL 311.2592 537.433204 \nL 312.207373 537.146897 \nL 314.10372 537.364112 \nL 315.051893 536.987862 \nL 316.000066 537.381135 \nL 316.94824 537.39026 \nL 317.896413 537.672214 \nL 318.844586 537.616324 \nL 320.740933 537.87576 \nL 321.689106 537.721976 \nL 324.533626 537.877974 \nL 325.4818 538.10582 \nL 326.429973 537.882796 \nL 327.378146 538.239309 \nL 331.17084 538.312638 \nL 334.963533 538.309559 \nL 335.911706 538.605115 \nL 336.859879 538.607015 \nL 337.808053 538.392 \nL 338.756226 538.618086 \nL 340.652573 538.507935 \nL 342.548919 538.774553 \nL 343.497093 538.523393 \nL 344.445266 538.620269 \nL 345.393439 538.863716 \nL 346.341613 538.973109 \nL 348.237959 538.899181 \nL 349.186133 538.415565 \nL 350.134306 539.004532 \nL 351.082479 539.178912 \nL 352.030653 539.059683 \nL 353.926999 539.236945 \nL 355.823346 538.974016 \nL 356.771519 539.043272 \nL 359.616039 538.928548 \nL 362.460559 539.48133 \nL 363.408732 539.195538 \nL 364.356906 539.426153 \nL 365.305079 539.512349 \nL 367.201426 539.466328 \nL 368.149599 539.611292 \nL 369.097772 539.516341 \nL 373.838639 539.639693 \nL 374.786812 538.959526 \nL 375.734986 539.68812 \nL 377.631332 539.436306 \nL 379.527679 539.797222 \nL 380.475852 539.654589 \nL 381.424026 539.726904 \nL 382.372199 539.658411 \nL 383.320372 539.70903 \nL 384.268546 539.571657 \nL 385.216719 539.982099 \nL 387.113066 539.643639 \nL 388.061239 539.949664 \nL 389.009412 540.018095 \nL 390.905759 539.814772 \nL 391.853932 539.906528 \nL 393.750279 539.75506 \nL 395.646625 539.898164 \nL 396.594799 539.833549 \nL 398.491145 540.107615 \nL 399.439319 539.805378 \nL 400.387492 540.044522 \nL 401.335665 540.033249 \nL 402.283839 540.136479 \nL 403.232012 539.866409 \nL 405.128359 540.186297 \nL 407.972879 540.031525 \nL 408.921052 540.244702 \nL 410.817399 539.892392 \nL 411.765572 540.374105 \nL 412.713745 539.987054 \nL 413.661919 540.080347 \nL 414.610092 540.376008 \nL 415.558265 540.003285 \nL 416.506439 540.316839 \nL 418.402785 540.272703 \nL 419.350958 540.440884 \nL 420.299132 540.375291 \nL 421.247305 539.903821 \nL 422.195478 540.410268 \nL 423.143652 540.226149 \nL 424.091825 540.42373 \nL 425.039998 540.478876 \nL 425.988172 540.229998 \nL 426.936345 540.27102 \nL 427.884518 540.439446 \nL 429.780865 540.41246 \nL 430.729038 540.338116 \nL 431.677212 540.086789 \nL 432.625385 540.278751 \nL 433.573558 540.152754 \nL 434.521732 540.483492 \nL 435.469905 540.67597 \nL 437.366252 540.315011 \nL 438.314425 540.292763 \nL 439.262598 540.532425 \nL 440.210772 540.399057 \nL 442.107118 540.599002 \nL 443.055292 540.081486 \nL 444.003465 540.54553 \nL 445.899811 540.69738 \nL 446.847985 540.297634 \nL 447.796158 540.512763 \nL 448.744331 540.415022 \nL 449.692505 540.588064 \nL 450.640678 540.500217 \nL 452.537025 540.739734 \nL 453.485198 540.501931 \nL 454.433371 540.736368 \nL 455.381545 540.645172 \nL 456.329718 540.690935 \nL 457.277891 540.502465 \nL 458.226065 540.81754 \nL 460.122411 540.872058 \nL 461.070585 540.686315 \nL 462.018758 540.715496 \nL 462.966931 540.566282 \nL 463.915105 540.538274 \nL 464.863278 540.331295 \nL 465.811451 540.823278 \nL 466.759625 540.580765 \nL 467.707798 540.476116 \nL 468.655971 540.740314 \nL 470.552318 540.422324 \nL 471.500491 540.774354 \nL 472.448665 540.550348 \nL 475.293184 540.754326 \nL 477.189531 540.702381 \nL 479.085878 540.729473 \nL 480.034051 540.545508 \nL 481.930398 540.79229 \nL 482.878571 540.464492 \nL 483.826744 540.59291 \nL 484.774918 540.60631 \nL 485.723091 540.808922 \nL 488.567611 540.74717 \nL 491.412131 540.733149 \nL 492.360304 540.400203 \nL 493.308478 540.711307 \nL 494.256651 540.644883 \nL 495.204824 540.763631 \nL 496.152998 540.745108 \nL 497.101171 541.027575 \nL 498.049344 540.820446 \nL 498.997518 540.893515 \nL 499.945691 540.777161 \nL 500.893864 540.919603 \nL 501.842037 540.597127 \nL 502.790211 540.939386 \nL 503.738384 540.771364 \nL 505.634731 540.901966 \nL 506.582904 540.952004 \nL 508.479251 540.689267 \nL 512.271944 540.884549 \nL 513.220117 540.585611 \nL 515.116464 540.908253 \nL 516.064637 540.873732 \nL 517.012811 540.990418 \nL 519.857331 540.905732 \nL 520.805504 540.706638 \nL 521.753677 540.908852 \nL 522.701851 540.950132 \nL 523.650024 540.783352 \nL 524.598197 540.819965 \nL 525.546371 540.716508 \nL 526.494544 540.97862 \nL 528.390891 540.900417 \nL 529.339064 540.696737 \nL 530.287237 540.955608 \nL 531.23541 540.986702 \nL 532.183584 540.793283 \nL 533.131757 540.903049 \nL 534.07993 540.693819 \nL 535.976277 540.988735 \nL 538.820797 540.861487 \nL 539.76897 540.758862 \nL 540.717144 541.017826 \nL 545.45801 540.787422 \nL 546.406184 541.011847 \nL 550.198877 540.84207 \nL 551.14705 540.623901 \nL 552.095224 540.888373 \nL 553.99157 540.668854 \nL 554.939744 540.966093 \nL 555.887917 540.843244 \nL 556.83609 540.957204 \nL 557.784263 540.880991 \nL 558.732437 540.653188 \nL 560.628783 540.903437 \nL 562.52513 541.077113 \nL 563.473303 540.62922 \nL 564.421477 540.905499 \nL 565.36965 541.03439 \nL 566.317823 540.598555 \nL 568.21417 541.037522 \nL 571.05869 540.802846 \nL 572.006863 540.98446 \nL 572.955037 540.870636 \nL 573.90321 541.007978 \nL 574.851383 540.895058 \nL 575.799557 541.048876 \nL 576.74773 540.841366 \nL 577.695903 540.981383 \nL 578.644077 540.712189 \nL 579.59225 540.845181 \nL 580.540423 540.836913 \nL 582.43677 541.066539 \nL 584.333117 541.109504 \nL 585.28129 540.982451 \nL 588.12581 540.991734 \nL 590.022156 540.668354 \nL 591.918503 541.14056 \nL 592.866676 540.98709 \nL 595.711196 540.847587 \nL 596.65937 541.011204 \nL 597.607543 541.039571 \nL 598.555716 540.914804 \nL 599.50389 540.638788 \nL 600.452063 541.09915 \nL 601.400236 540.999117 \nL 602.34841 540.769253 \nL 606.141103 541.209034 \nL 607.089276 540.835364 \nL 608.03745 540.850608 \nL 608.985623 541.024396 \nL 609.933796 540.906387 \nL 610.88197 541.173548 \nL 611.830143 541.11069 \nL 612.778316 540.764416 \nL 613.726489 541.110721 \nL 614.674663 540.930206 \nL 617.519183 540.795472 \nL 618.467356 540.56115 \nL 619.415529 541.058622 \nL 621.311876 540.786847 \nL 623.208223 540.958486 \nL 625.104569 540.798301 \nL 628.897263 541.103208 \nL 630.793609 540.718035 \nL 631.741783 541.051611 \nL 632.689956 540.606664 \nL 633.638129 540.97397 \nL 634.586303 540.744835 \nL 635.534476 541.1001 \nL 636.482649 541.112761 \nL 637.430823 540.896647 \nL 638.378996 541.11156 \nL 639.327169 540.890596 \nL 640.275343 541.057156 \nL 645.016209 540.879236 \nL 645.964382 541.059581 \nL 647.860729 540.98487 \nL 648.808902 540.757173 \nL 650.705249 540.853496 \nL 651.653422 540.984081 \nL 652.601596 540.883186 \nL 653.549769 540.904952 \nL 654.497942 540.812234 \nL 656.394289 541.021772 \nL 657.342462 540.713453 \nL 659.238809 540.998316 \nL 661.135156 540.665395 \nL 662.083329 541.069572 \nL 663.031502 540.842359 \nL 664.927849 540.827882 \nL 665.876022 541.047044 \nL 666.824196 540.809561 \nL 668.720542 541.037507 \nL 669.668715 540.54965 \nL 670.616889 541.010599 \nL 673.461409 541.021741 \nL 675.357755 540.82357 \nL 676.305929 540.735887 \nL 677.254102 540.984569 \nL 679.150449 540.903742 \nL 680.098622 540.685616 \nL 681.994969 540.923636 \nL 682.943142 540.687479 \nL 683.891315 541.014998 \nL 685.787662 540.946987 \nL 686.735835 540.724316 \nL 687.684009 540.908982 \nL 688.632182 540.695986 \nL 689.580355 541.005274 \nL 692.424875 541.124174 \nL 693.373049 540.849137 \nL 694.321222 541.047935 \nL 695.269395 540.671235 \nL 696.217569 540.824445 \nL 697.165742 540.73684 \nL 698.113915 541.003178 \nL 701.906608 540.953236 \nL 703.802955 541.050099 \nL 704.751128 540.800114 \nL 705.699302 540.949613 \nL 707.595648 540.981629 \nL 708.543822 540.849472 \nL 710.440168 541.003597 \nL 711.388342 540.688432 \nL 712.336515 540.939837 \nL 714.232862 540.842039 \nL 717.077382 541.015703 \nL 718.025555 540.746279 \nL 718.973728 541.072919 \nL 719.921902 540.936347 \nL 720.870075 541.048991 \nL 721.818248 540.900653 \nL 722.766422 540.617909 \nL 723.714595 540.622306 \nL 724.662768 540.901537 \nL 726.559115 541.025628 \nL 727.507288 540.721696 \nL 728.455461 540.97225 \nL 729.403635 540.635991 \nL 730.351808 540.90077 \nL 735.092675 540.934764 \nL 736.040848 540.935813 \nL 736.989021 540.793957 \nL 737.937195 540.869615 \nL 738.885368 540.746216 \nL 739.833541 541.075465 \nL 741.729888 540.823237 \nL 742.678061 541.056768 \nL 743.626235 540.997382 \nL 744.574408 540.7144 \nL 745.522581 540.97776 \nL 746.470755 541.071099 \nL 749.315275 541.037687 \nL 750.263448 540.544347 \nL 751.211621 540.409492 \nL 752.159794 540.832458 \nL 753.107968 541.074198 \nL 754.056141 541.019742 \nL 755.004314 540.791896 \nL 755.952488 540.836606 \nL 756.900661 540.395739 \nL 757.848834 540.965084 \nL 758.797008 540.982554 \nL 760.693354 540.784926 \nL 761.641528 540.925583 \nL 763.537874 540.921336 \nL 764.486048 540.716927 \nL 766.382394 540.729554 \nL 767.330568 540.896359 \nL 768.278741 540.943041 \nL 769.226914 540.878361 \nL 770.175088 540.683819 \nL 772.071434 540.920942 \nL 773.019608 540.859416 \nL 773.967781 540.95988 \nL 774.915954 540.816152 \nL 775.864128 540.306007 \nL 776.812301 540.719516 \nL 777.760474 540.771116 \nL 778.708648 540.959936 \nL 779.656821 540.81532 \nL 780.604994 540.531885 \nL 781.553167 540.868268 \nL 782.501341 540.7483 \nL 783.449514 540.968335 \nL 785.345861 540.604423 \nL 786.294034 540.834914 \nL 786.294034 540.834914 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_19\">\n    <!-- Model MSE -->\n    <g transform=\"translate(398.503125 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"481.787109\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_20\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_22\"/>\n    <g id=\"text_21\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p70b95cfae3\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "11/11 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}