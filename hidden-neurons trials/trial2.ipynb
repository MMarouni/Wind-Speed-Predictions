{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1)\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1)\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df=pd.read_excel('Pre-Processed-Data.xlsx')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Wind Speed  Pressure  Pressure Grad  Wind Gradient\n",
       "0        2.47    1029.0             -8              3\n",
       "1        7.42    1021.4              0              8\n",
       "2        6.81    1021.8             11              7\n",
       "3        3.94    1033.7             -1              2\n",
       "4        3.33    1033.4            -11              5"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.47</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.42</td>\n",
       "      <td>1021.4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.81</td>\n",
       "      <td>1021.8</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.94</td>\n",
       "      <td>1033.7</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.33</td>\n",
       "      <td>1033.4</td>\n",
       "      <td>-11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df.isnull().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Wind Speed       0\n",
       "Pressure         0\n",
       "Pressure Grad    0\n",
       "Wind Gradient    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "X = df.drop(['Wind Speed'], axis=1)\n",
    "#Assign the Target column as the output \n",
    "Y= df['Wind Speed']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "X_norm=(X-X.min())/(X.max()-X.min())\n",
    "X_norm"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Pressure  Pressure Grad  Wind Gradient\n",
       "0     0.752055       0.449275       0.200000\n",
       "1     0.647945       0.565217       0.533333\n",
       "2     0.653425       0.724638       0.466667\n",
       "3     0.816438       0.550725       0.133333\n",
       "4     0.812329       0.405797       0.333333\n",
       "...        ...            ...            ...\n",
       "1091  0.706849       0.623188       0.066667\n",
       "1092  0.767123       0.594203       0.200000\n",
       "1093  0.795890       0.594203       0.266667\n",
       "1094  0.831507       0.565217       0.200000\n",
       "1095  0.836986       0.594203       0.200000\n",
       "\n",
       "[1096 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Pressure Grad</th>\n",
       "      <th>Wind Gradient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.752055</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.647945</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.653425</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.816438</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.812329</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.706849</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.795890</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.831507</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>0.836986</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows Ã— 3 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_norm, Y, test_size=0.3, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=x_train.shape[1], activation=\"sigmoid\", kernel_initializer='normal'))\n",
    "model.add(Dropout(0.2)) #dropping a few neurons for generalizing the model\n",
    "model.add(Dense(1, activation=\"linear\", kernel_initializer='normal'))\n",
    "adam = Adam(learning_rate=1e-3, decay=1e-3)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=['mse','mae'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print('Fit model...')\n",
    "filepath=\"/home/m-marouni/Documents/CE-901/Heathrow/best_weights\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_mae', patience=100, verbose=1, mode='min')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "log = model.fit(x_train, y_train,\n",
    "          validation_split=0.40, batch_size=30, epochs=1000, shuffle=True, callbacks=callbacks_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fit model...\n",
      "Epoch 1/1000\n",
      "16/16 [==============================] - 1s 16ms/step - loss: 44.1449 - mse: 44.1449 - mae: 6.1066 - val_loss: 38.4099 - val_mse: 38.4099 - val_mae: 5.7026\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 5.70260, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 2/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 43.4802 - mse: 43.4802 - mae: 6.0857 - val_loss: 36.7454 - val_mse: 36.7454 - val_mae: 5.5548\n",
      "\n",
      "Epoch 00002: val_mae improved from 5.70260 to 5.55479, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 3/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 41.8555 - mse: 41.8555 - mae: 5.9684 - val_loss: 35.1227 - val_mse: 35.1227 - val_mae: 5.4068\n",
      "\n",
      "Epoch 00003: val_mae improved from 5.55479 to 5.40680, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 4/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 36.7718 - mse: 36.7718 - mae: 5.5752 - val_loss: 33.5361 - val_mse: 33.5361 - val_mae: 5.2581\n",
      "\n",
      "Epoch 00004: val_mae improved from 5.40680 to 5.25809, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 5/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 37.3813 - mse: 37.3813 - mae: 5.5959 - val_loss: 31.9337 - val_mse: 31.9337 - val_mae: 5.1035\n",
      "\n",
      "Epoch 00005: val_mae improved from 5.25809 to 5.10352, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 6/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 34.0863 - mse: 34.0863 - mae: 5.3010 - val_loss: 30.3559 - val_mse: 30.3559 - val_mae: 4.9466\n",
      "\n",
      "Epoch 00006: val_mae improved from 5.10352 to 4.94661, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 7/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 33.4960 - mse: 33.4960 - mae: 5.2265 - val_loss: 28.7569 - val_mse: 28.7569 - val_mae: 4.7824\n",
      "\n",
      "Epoch 00007: val_mae improved from 4.94661 to 4.78238, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 8/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 32.3752 - mse: 32.3752 - mae: 5.1149 - val_loss: 27.1489 - val_mse: 27.1489 - val_mae: 4.6113\n",
      "\n",
      "Epoch 00008: val_mae improved from 4.78238 to 4.61135, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 9/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 30.6521 - mse: 30.6521 - mae: 4.8903 - val_loss: 25.5454 - val_mse: 25.5454 - val_mae: 4.4349\n",
      "\n",
      "Epoch 00009: val_mae improved from 4.61135 to 4.43494, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 10/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 26.2237 - mse: 26.2237 - mae: 4.5099 - val_loss: 23.9940 - val_mse: 23.9940 - val_mae: 4.2578\n",
      "\n",
      "Epoch 00010: val_mae improved from 4.43494 to 4.25776, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 11/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 26.2560 - mse: 26.2560 - mae: 4.4756 - val_loss: 22.4407 - val_mse: 22.4407 - val_mae: 4.0726\n",
      "\n",
      "Epoch 00011: val_mae improved from 4.25776 to 4.07263, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 12/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 24.6725 - mse: 24.6725 - mae: 4.2722 - val_loss: 20.9322 - val_mse: 20.9322 - val_mae: 3.8846\n",
      "\n",
      "Epoch 00012: val_mae improved from 4.07263 to 3.88458, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 13/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 23.2884 - mse: 23.2884 - mae: 4.1020 - val_loss: 19.4717 - val_mse: 19.4717 - val_mae: 3.6955\n",
      "\n",
      "Epoch 00013: val_mae improved from 3.88458 to 3.69553, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 14/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 20.4739 - mse: 20.4739 - mae: 3.8595 - val_loss: 18.0884 - val_mse: 18.0884 - val_mae: 3.5083\n",
      "\n",
      "Epoch 00014: val_mae improved from 3.69553 to 3.50829, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 15/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 19.1738 - mse: 19.1738 - mae: 3.6253 - val_loss: 16.7824 - val_mse: 16.7824 - val_mae: 3.3243\n",
      "\n",
      "Epoch 00015: val_mae improved from 3.50829 to 3.32432, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 16/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 19.7445 - mse: 19.7445 - mae: 3.7163 - val_loss: 15.5272 - val_mse: 15.5272 - val_mae: 3.1419\n",
      "\n",
      "Epoch 00016: val_mae improved from 3.32432 to 3.14193, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 17/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 19.0060 - mse: 19.0060 - mae: 3.4892 - val_loss: 14.3654 - val_mse: 14.3654 - val_mae: 2.9711\n",
      "\n",
      "Epoch 00017: val_mae improved from 3.14193 to 2.97114, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 18/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 15.4093 - mse: 15.4093 - mae: 3.1893 - val_loss: 13.3155 - val_mse: 13.3155 - val_mae: 2.8185\n",
      "\n",
      "Epoch 00018: val_mae improved from 2.97114 to 2.81855, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 19/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 15.9746 - mse: 15.9746 - mae: 3.0981 - val_loss: 12.3145 - val_mse: 12.3145 - val_mae: 2.6759\n",
      "\n",
      "Epoch 00019: val_mae improved from 2.81855 to 2.67590, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 20/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 13.4488 - mse: 13.4488 - mae: 2.8749 - val_loss: 11.4294 - val_mse: 11.4294 - val_mae: 2.5541\n",
      "\n",
      "Epoch 00020: val_mae improved from 2.67590 to 2.55407, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 21/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 14.7892 - mse: 14.7892 - mae: 2.8487 - val_loss: 10.6111 - val_mse: 10.6111 - val_mae: 2.4436\n",
      "\n",
      "Epoch 00021: val_mae improved from 2.55407 to 2.44360, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 22/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.4035 - mse: 11.4035 - mae: 2.5393 - val_loss: 9.9065 - val_mse: 9.9065 - val_mae: 2.3507\n",
      "\n",
      "Epoch 00022: val_mae improved from 2.44360 to 2.35072, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 23/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.9843 - mse: 12.9843 - mae: 2.7077 - val_loss: 9.2376 - val_mse: 9.2376 - val_mae: 2.2684\n",
      "\n",
      "Epoch 00023: val_mae improved from 2.35072 to 2.26838, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 24/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 10.6204 - mse: 10.6204 - mae: 2.3816 - val_loss: 8.6787 - val_mse: 8.6787 - val_mae: 2.2027\n",
      "\n",
      "Epoch 00024: val_mae improved from 2.26838 to 2.20267, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 25/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.8394 - mse: 9.8394 - mae: 2.2716 - val_loss: 8.1978 - val_mse: 8.1978 - val_mae: 2.1469\n",
      "\n",
      "Epoch 00025: val_mae improved from 2.20267 to 2.14693, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 26/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.8900 - mse: 8.8900 - mae: 2.1956 - val_loss: 7.7755 - val_mse: 7.7755 - val_mae: 2.1011\n",
      "\n",
      "Epoch 00026: val_mae improved from 2.14693 to 2.10115, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 27/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 9.2650 - mse: 9.2650 - mae: 2.2448 - val_loss: 7.4015 - val_mse: 7.4015 - val_mae: 2.0604\n",
      "\n",
      "Epoch 00027: val_mae improved from 2.10115 to 2.06038, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 28/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 10.2303 - mse: 10.2303 - mae: 2.3489 - val_loss: 7.0855 - val_mse: 7.0855 - val_mae: 2.0276\n",
      "\n",
      "Epoch 00028: val_mae improved from 2.06038 to 2.02759, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 29/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.3812 - mse: 7.3812 - mae: 1.9584 - val_loss: 6.8190 - val_mse: 6.8190 - val_mae: 2.0014\n",
      "\n",
      "Epoch 00029: val_mae improved from 2.02759 to 2.00137, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 30/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.5407 - mse: 7.5407 - mae: 2.0550 - val_loss: 6.5993 - val_mse: 6.5993 - val_mae: 1.9792\n",
      "\n",
      "Epoch 00030: val_mae improved from 2.00137 to 1.97918, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 31/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.4899 - mse: 8.4899 - mae: 2.1912 - val_loss: 6.4215 - val_mse: 6.4215 - val_mae: 1.9623\n",
      "\n",
      "Epoch 00031: val_mae improved from 1.97918 to 1.96228, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 32/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.6922 - mse: 7.6922 - mae: 2.0112 - val_loss: 6.2862 - val_mse: 6.2862 - val_mae: 1.9515\n",
      "\n",
      "Epoch 00032: val_mae improved from 1.96228 to 1.95152, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 33/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8303 - mse: 6.8303 - mae: 1.9788 - val_loss: 6.1677 - val_mse: 6.1677 - val_mae: 1.9474\n",
      "\n",
      "Epoch 00033: val_mae improved from 1.95152 to 1.94736, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 34/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.4291 - mse: 7.4291 - mae: 2.0768 - val_loss: 6.0727 - val_mse: 6.0727 - val_mae: 1.9458\n",
      "\n",
      "Epoch 00034: val_mae improved from 1.94736 to 1.94583, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 35/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8876 - mse: 7.8876 - mae: 2.1799 - val_loss: 6.0047 - val_mse: 6.0047 - val_mae: 1.9453\n",
      "\n",
      "Epoch 00035: val_mae improved from 1.94583 to 1.94529, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 36/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5738 - mse: 5.5738 - mae: 1.8557 - val_loss: 5.9503 - val_mse: 5.9503 - val_mae: 1.9448\n",
      "\n",
      "Epoch 00036: val_mae improved from 1.94529 to 1.94478, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 37/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7596 - mse: 6.7596 - mae: 2.0007 - val_loss: 5.9048 - val_mse: 5.9048 - val_mae: 1.9458\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 1.94478\n",
      "Epoch 38/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5697 - mse: 6.5697 - mae: 1.9787 - val_loss: 5.8723 - val_mse: 5.8723 - val_mae: 1.9477\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 1.94478\n",
      "Epoch 39/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6274 - mse: 6.6274 - mae: 2.0419 - val_loss: 5.8481 - val_mse: 5.8481 - val_mae: 1.9511\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 1.94478\n",
      "Epoch 40/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5956 - mse: 6.5956 - mae: 2.0301 - val_loss: 5.8347 - val_mse: 5.8347 - val_mae: 1.9536\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 1.94478\n",
      "Epoch 41/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9695 - mse: 6.9695 - mae: 2.0358 - val_loss: 5.8261 - val_mse: 5.8261 - val_mae: 1.9563\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 1.94478\n",
      "Epoch 42/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2271 - mse: 6.2271 - mae: 1.9389 - val_loss: 5.8218 - val_mse: 5.8218 - val_mae: 1.9579\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 1.94478\n",
      "Epoch 43/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4569 - mse: 6.4569 - mae: 1.9574 - val_loss: 5.8189 - val_mse: 5.8189 - val_mae: 1.9614\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 1.94478\n",
      "Epoch 44/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.2254 - mse: 7.2254 - mae: 2.0902 - val_loss: 5.8188 - val_mse: 5.8188 - val_mae: 1.9646\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 1.94478\n",
      "Epoch 45/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9089 - mse: 6.9089 - mae: 2.0789 - val_loss: 5.8197 - val_mse: 5.8197 - val_mae: 1.9671\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 1.94478\n",
      "Epoch 46/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.6034 - mse: 7.6034 - mae: 2.1660 - val_loss: 5.8210 - val_mse: 5.8210 - val_mae: 1.9692\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 1.94478\n",
      "Epoch 47/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.0878 - mse: 7.0878 - mae: 2.0558 - val_loss: 5.8217 - val_mse: 5.8217 - val_mae: 1.9705\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 1.94478\n",
      "Epoch 48/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6599 - mse: 6.6599 - mae: 2.0137 - val_loss: 5.8230 - val_mse: 5.8230 - val_mae: 1.9720\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 1.94478\n",
      "Epoch 49/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9862 - mse: 6.9862 - mae: 2.0631 - val_loss: 5.8223 - val_mse: 5.8223 - val_mae: 1.9725\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 1.94478\n",
      "Epoch 50/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4859 - mse: 6.4859 - mae: 2.0107 - val_loss: 5.8231 - val_mse: 5.8231 - val_mae: 1.9737\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 1.94478\n",
      "Epoch 51/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8494 - mse: 6.8494 - mae: 2.0703 - val_loss: 5.8225 - val_mse: 5.8225 - val_mae: 1.9741\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 1.94478\n",
      "Epoch 52/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.2015 - mse: 7.2015 - mae: 2.0736 - val_loss: 5.8227 - val_mse: 5.8227 - val_mae: 1.9749\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 1.94478\n",
      "Epoch 53/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8014 - mse: 5.8014 - mae: 1.9102 - val_loss: 5.8239 - val_mse: 5.8239 - val_mae: 1.9760\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 1.94478\n",
      "Epoch 54/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8848 - mse: 6.8848 - mae: 2.0021 - val_loss: 5.8278 - val_mse: 5.8278 - val_mae: 1.9781\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 1.94478\n",
      "Epoch 55/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5348 - mse: 6.5348 - mae: 2.0270 - val_loss: 5.8273 - val_mse: 5.8273 - val_mae: 1.9786\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 1.94478\n",
      "Epoch 56/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.5176 - mse: 7.5176 - mae: 2.0942 - val_loss: 5.8285 - val_mse: 5.8285 - val_mae: 1.9796\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 1.94478\n",
      "Epoch 57/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6688 - mse: 6.6688 - mae: 2.0746 - val_loss: 5.8284 - val_mse: 5.8284 - val_mae: 1.9801\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 1.94478\n",
      "Epoch 58/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3529 - mse: 6.3529 - mae: 1.9988 - val_loss: 5.8289 - val_mse: 5.8289 - val_mae: 1.9808\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 1.94478\n",
      "Epoch 59/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4609 - mse: 6.4609 - mae: 2.0272 - val_loss: 5.8275 - val_mse: 5.8275 - val_mae: 1.9808\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 1.94478\n",
      "Epoch 60/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6776 - mse: 6.6776 - mae: 2.0778 - val_loss: 5.8265 - val_mse: 5.8265 - val_mae: 1.9809\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 1.94478\n",
      "Epoch 61/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.3361 - mse: 7.3361 - mae: 2.0942 - val_loss: 5.8266 - val_mse: 5.8266 - val_mae: 1.9815\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 1.94478\n",
      "Epoch 62/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5113 - mse: 6.5113 - mae: 2.0350 - val_loss: 5.8201 - val_mse: 5.8201 - val_mae: 1.9798\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 1.94478\n",
      "Epoch 63/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7016 - mse: 6.7016 - mae: 2.0347 - val_loss: 5.8192 - val_mse: 5.8192 - val_mae: 1.9800\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 1.94478\n",
      "Epoch 64/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.1485 - mse: 6.1485 - mae: 1.9733 - val_loss: 5.8244 - val_mse: 5.8244 - val_mae: 1.9822\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 1.94478\n",
      "Epoch 65/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7438 - mse: 6.7438 - mae: 2.0666 - val_loss: 5.8173 - val_mse: 5.8173 - val_mae: 1.9804\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 1.94478\n",
      "Epoch 66/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.5603 - mse: 7.5603 - mae: 2.2056 - val_loss: 5.8125 - val_mse: 5.8125 - val_mae: 1.9793\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 1.94478\n",
      "Epoch 67/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1798 - mse: 6.1798 - mae: 1.9300 - val_loss: 5.8074 - val_mse: 5.8074 - val_mae: 1.9781\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 1.94478\n",
      "Epoch 68/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9807 - mse: 5.9807 - mae: 1.9330 - val_loss: 5.8018 - val_mse: 5.8018 - val_mae: 1.9768\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 1.94478\n",
      "Epoch 69/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9567 - mse: 5.9567 - mae: 1.9543 - val_loss: 5.7989 - val_mse: 5.7989 - val_mae: 1.9763\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 1.94478\n",
      "Epoch 70/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3588 - mse: 6.3588 - mae: 1.9833 - val_loss: 5.8031 - val_mse: 5.8031 - val_mae: 1.9782\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 1.94478\n",
      "Epoch 71/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8975 - mse: 6.8975 - mae: 1.9830 - val_loss: 5.7999 - val_mse: 5.7999 - val_mae: 1.9776\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 1.94478\n",
      "Epoch 72/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7532 - mse: 6.7532 - mae: 2.0734 - val_loss: 5.7986 - val_mse: 5.7986 - val_mae: 1.9778\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 1.94478\n",
      "Epoch 73/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3026 - mse: 6.3026 - mae: 2.0027 - val_loss: 5.8002 - val_mse: 5.8002 - val_mae: 1.9788\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 1.94478\n",
      "Epoch 74/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.2253 - mse: 6.2253 - mae: 1.9695 - val_loss: 5.7900 - val_mse: 5.7900 - val_mae: 1.9759\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 1.94478\n",
      "Epoch 75/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6331 - mse: 6.6331 - mae: 2.0340 - val_loss: 5.7917 - val_mse: 5.7917 - val_mae: 1.9771\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 1.94478\n",
      "Epoch 76/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0603 - mse: 6.0603 - mae: 1.8948 - val_loss: 5.7910 - val_mse: 5.7910 - val_mae: 1.9774\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 1.94478\n",
      "Epoch 77/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.1079 - mse: 7.1079 - mae: 2.1315 - val_loss: 5.7799 - val_mse: 5.7799 - val_mae: 1.9743\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 1.94478\n",
      "Epoch 78/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9938 - mse: 5.9938 - mae: 1.9280 - val_loss: 5.7701 - val_mse: 5.7701 - val_mae: 1.9715\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 1.94478\n",
      "Epoch 79/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.0928 - mse: 7.0928 - mae: 2.0688 - val_loss: 5.7662 - val_mse: 5.7662 - val_mae: 1.9708\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 1.94478\n",
      "Epoch 80/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4262 - mse: 6.4262 - mae: 1.9661 - val_loss: 5.7655 - val_mse: 5.7655 - val_mae: 1.9712\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 1.94478\n",
      "Epoch 81/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.2261 - mse: 7.2261 - mae: 2.0464 - val_loss: 5.7627 - val_mse: 5.7627 - val_mae: 1.9708\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 1.94478\n",
      "Epoch 82/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.0330 - mse: 7.0330 - mae: 2.0810 - val_loss: 5.7594 - val_mse: 5.7594 - val_mae: 1.9703\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 1.94478\n",
      "Epoch 83/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0470 - mse: 6.0470 - mae: 1.9686 - val_loss: 5.7555 - val_mse: 5.7555 - val_mae: 1.9696\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 1.94478\n",
      "Epoch 84/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.6701 - mse: 6.6701 - mae: 2.0565 - val_loss: 5.7562 - val_mse: 5.7562 - val_mae: 1.9704\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 1.94478\n",
      "Epoch 85/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.7783 - mse: 6.7783 - mae: 2.0164 - val_loss: 5.7540 - val_mse: 5.7540 - val_mae: 1.9704\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 1.94478\n",
      "Epoch 86/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0745 - mse: 6.0745 - mae: 2.0083 - val_loss: 5.7464 - val_mse: 5.7464 - val_mae: 1.9685\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 1.94478\n",
      "Epoch 87/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3041 - mse: 7.3041 - mae: 2.1189 - val_loss: 5.7443 - val_mse: 5.7443 - val_mae: 1.9684\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 1.94478\n",
      "Epoch 88/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2147 - mse: 6.2147 - mae: 1.9926 - val_loss: 5.7346 - val_mse: 5.7346 - val_mae: 1.9657\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 1.94478\n",
      "Epoch 89/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.9550 - mse: 6.9550 - mae: 2.1285 - val_loss: 5.7326 - val_mse: 5.7326 - val_mae: 1.9658\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 1.94478\n",
      "Epoch 90/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.1688 - mse: 7.1688 - mae: 2.0590 - val_loss: 5.7328 - val_mse: 5.7328 - val_mae: 1.9665\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 1.94478\n",
      "Epoch 91/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9164 - mse: 6.9164 - mae: 2.0528 - val_loss: 5.7308 - val_mse: 5.7308 - val_mae: 1.9665\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 1.94478\n",
      "Epoch 92/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.1034 - mse: 7.1034 - mae: 2.1176 - val_loss: 5.7286 - val_mse: 5.7286 - val_mae: 1.9664\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 1.94478\n",
      "Epoch 93/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.9694 - mse: 6.9694 - mae: 2.0206 - val_loss: 5.7206 - val_mse: 5.7206 - val_mae: 1.9644\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 1.94478\n",
      "Epoch 94/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.2900 - mse: 7.2900 - mae: 2.1032 - val_loss: 5.7141 - val_mse: 5.7141 - val_mae: 1.9628\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 1.94478\n",
      "Epoch 95/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8906 - mse: 5.8906 - mae: 1.9124 - val_loss: 5.7076 - val_mse: 5.7076 - val_mae: 1.9612\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 1.94478\n",
      "Epoch 96/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.7655 - mse: 6.7655 - mae: 2.0700 - val_loss: 5.7058 - val_mse: 5.7058 - val_mae: 1.9614\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 1.94478\n",
      "Epoch 97/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7471 - mse: 5.7471 - mae: 1.9358 - val_loss: 5.7007 - val_mse: 5.7007 - val_mae: 1.9603\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 1.94478\n",
      "Epoch 98/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9584 - mse: 5.9584 - mae: 1.9203 - val_loss: 5.7004 - val_mse: 5.7004 - val_mae: 1.9609\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 1.94478\n",
      "Epoch 99/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.0695 - mse: 7.0695 - mae: 2.0879 - val_loss: 5.6910 - val_mse: 5.6910 - val_mae: 1.9584\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 1.94478\n",
      "Epoch 100/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5011 - mse: 6.5011 - mae: 2.0198 - val_loss: 5.6824 - val_mse: 5.6824 - val_mae: 1.9561\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 1.94478\n",
      "Epoch 101/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9381 - mse: 6.9381 - mae: 2.1222 - val_loss: 5.6836 - val_mse: 5.6836 - val_mae: 1.9573\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 1.94478\n",
      "Epoch 102/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3236 - mse: 6.3236 - mae: 1.9762 - val_loss: 5.6772 - val_mse: 5.6772 - val_mae: 1.9559\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 1.94478\n",
      "Epoch 103/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6576 - mse: 6.6576 - mae: 2.0557 - val_loss: 5.6695 - val_mse: 5.6695 - val_mae: 1.9539\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 1.94478\n",
      "Epoch 104/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5983 - mse: 6.5983 - mae: 2.0168 - val_loss: 5.6645 - val_mse: 5.6645 - val_mae: 1.9530\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 1.94478\n",
      "Epoch 105/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.9721 - mse: 6.9721 - mae: 2.0844 - val_loss: 5.6630 - val_mse: 5.6630 - val_mae: 1.9534\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 1.94478\n",
      "Epoch 106/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7688 - mse: 6.7688 - mae: 2.0598 - val_loss: 5.6673 - val_mse: 5.6673 - val_mae: 1.9557\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 1.94478\n",
      "Epoch 107/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.5382 - mse: 7.5382 - mae: 2.1268 - val_loss: 5.6630 - val_mse: 5.6630 - val_mae: 1.9550\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 1.94478\n",
      "Epoch 108/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1377 - mse: 6.1377 - mae: 1.9352 - val_loss: 5.6557 - val_mse: 5.6557 - val_mae: 1.9533\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 1.94478\n",
      "Epoch 109/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0379 - mse: 6.0379 - mae: 1.9769 - val_loss: 5.6496 - val_mse: 5.6496 - val_mae: 1.9521\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 1.94478\n",
      "Epoch 110/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7968 - mse: 6.7968 - mae: 2.0117 - val_loss: 5.6581 - val_mse: 5.6581 - val_mae: 1.9560\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 1.94478\n",
      "Epoch 111/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5701 - mse: 5.5701 - mae: 1.8668 - val_loss: 5.6485 - val_mse: 5.6485 - val_mae: 1.9535\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 1.94478\n",
      "Epoch 112/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.1174 - mse: 7.1174 - mae: 2.0648 - val_loss: 5.6480 - val_mse: 5.6480 - val_mae: 1.9542\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 1.94478\n",
      "Epoch 113/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3077 - mse: 6.3077 - mae: 1.9939 - val_loss: 5.6377 - val_mse: 5.6377 - val_mae: 1.9515\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 1.94478\n",
      "Epoch 114/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.0009 - mse: 7.0009 - mae: 2.0501 - val_loss: 5.6298 - val_mse: 5.6298 - val_mae: 1.9495\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 1.94478\n",
      "Epoch 115/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2913 - mse: 6.2913 - mae: 2.0387 - val_loss: 5.6111 - val_mse: 5.6111 - val_mae: 1.9437\n",
      "\n",
      "Epoch 00115: val_mae improved from 1.94478 to 1.94370, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 116/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5020 - mse: 6.5020 - mae: 2.0522 - val_loss: 5.6031 - val_mse: 5.6031 - val_mae: 1.9417\n",
      "\n",
      "Epoch 00116: val_mae improved from 1.94370 to 1.94171, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 117/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4347 - mse: 6.4347 - mae: 2.0151 - val_loss: 5.6059 - val_mse: 5.6059 - val_mae: 1.9438\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 1.94171\n",
      "Epoch 118/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1900 - mse: 6.1900 - mae: 1.9792 - val_loss: 5.6028 - val_mse: 5.6028 - val_mae: 1.9436\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 1.94171\n",
      "Epoch 119/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.0564 - mse: 7.0564 - mae: 2.0569 - val_loss: 5.5949 - val_mse: 5.5949 - val_mae: 1.9417\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 1.94171\n",
      "Epoch 120/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5828 - mse: 6.5828 - mae: 1.9580 - val_loss: 5.5967 - val_mse: 5.5967 - val_mae: 1.9434\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 1.94171\n",
      "Epoch 121/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.3569 - mse: 6.3569 - mae: 2.0164 - val_loss: 5.6002 - val_mse: 5.6002 - val_mae: 1.9455\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 1.94171\n",
      "Epoch 122/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.3174 - mse: 7.3174 - mae: 2.0512 - val_loss: 5.5848 - val_mse: 5.5848 - val_mae: 1.9411\n",
      "\n",
      "Epoch 00122: val_mae improved from 1.94171 to 1.94107, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 123/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.5760 - mse: 6.5760 - mae: 2.0377 - val_loss: 5.5755 - val_mse: 5.5755 - val_mae: 1.9387\n",
      "\n",
      "Epoch 00123: val_mae improved from 1.94107 to 1.93868, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 124/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.7473 - mse: 6.7473 - mae: 2.0453 - val_loss: 5.5771 - val_mse: 5.5771 - val_mae: 1.9403\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 1.93868\n",
      "Epoch 125/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0330 - mse: 6.0330 - mae: 1.9199 - val_loss: 5.5827 - val_mse: 5.5827 - val_mae: 1.9433\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 1.93868\n",
      "Epoch 126/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3913 - mse: 6.3913 - mae: 2.0382 - val_loss: 5.5854 - val_mse: 5.5854 - val_mae: 1.9451\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 1.93868\n",
      "Epoch 127/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2401 - mse: 6.2401 - mae: 2.0214 - val_loss: 5.5684 - val_mse: 5.5684 - val_mae: 1.9403\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 1.93868\n",
      "Epoch 128/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0018 - mse: 6.0018 - mae: 1.9407 - val_loss: 5.5608 - val_mse: 5.5608 - val_mae: 1.9386\n",
      "\n",
      "Epoch 00128: val_mae improved from 1.93868 to 1.93863, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 129/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3496 - mse: 6.3496 - mae: 2.0163 - val_loss: 5.5594 - val_mse: 5.5594 - val_mae: 1.9391\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 1.93863\n",
      "Epoch 130/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.0484 - mse: 6.0484 - mae: 1.9511 - val_loss: 5.5498 - val_mse: 5.5498 - val_mae: 1.9367\n",
      "\n",
      "Epoch 00130: val_mae improved from 1.93863 to 1.93666, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 131/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.9181 - mse: 5.9181 - mae: 1.9420 - val_loss: 5.5412 - val_mse: 5.5412 - val_mae: 1.9347\n",
      "\n",
      "Epoch 00131: val_mae improved from 1.93666 to 1.93474, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 132/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 6.7283 - mse: 6.7283 - mae: 2.0451 - val_loss: 5.5458 - val_mse: 5.5458 - val_mae: 1.9373\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 1.93474\n",
      "Epoch 133/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8031 - mse: 6.8031 - mae: 2.0512 - val_loss: 5.5219 - val_mse: 5.5219 - val_mae: 1.9300\n",
      "\n",
      "Epoch 00133: val_mae improved from 1.93474 to 1.93001, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 134/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.7228 - mse: 6.7228 - mae: 1.9751 - val_loss: 5.5125 - val_mse: 5.5125 - val_mae: 1.9276\n",
      "\n",
      "Epoch 00134: val_mae improved from 1.93001 to 1.92763, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 135/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1815 - mse: 6.1815 - mae: 1.9869 - val_loss: 5.4998 - val_mse: 5.4998 - val_mae: 1.9240\n",
      "\n",
      "Epoch 00135: val_mae improved from 1.92763 to 1.92402, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 136/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.6021 - mse: 6.6021 - mae: 1.9524 - val_loss: 5.4936 - val_mse: 5.4936 - val_mae: 1.9228\n",
      "\n",
      "Epoch 00136: val_mae improved from 1.92402 to 1.92280, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 137/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9677 - mse: 5.9677 - mae: 1.9483 - val_loss: 5.4865 - val_mse: 5.4865 - val_mae: 1.9213\n",
      "\n",
      "Epoch 00137: val_mae improved from 1.92280 to 1.92129, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 138/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.4039 - mse: 7.4039 - mae: 2.1068 - val_loss: 5.4861 - val_mse: 5.4861 - val_mae: 1.9224\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 1.92129\n",
      "Epoch 139/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8101 - mse: 6.8101 - mae: 2.0080 - val_loss: 5.4814 - val_mse: 5.4814 - val_mae: 1.9219\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 1.92129\n",
      "Epoch 140/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3304 - mse: 6.3304 - mae: 1.9793 - val_loss: 5.4748 - val_mse: 5.4748 - val_mae: 1.9206\n",
      "\n",
      "Epoch 00140: val_mae improved from 1.92129 to 1.92061, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 141/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9773 - mse: 5.9773 - mae: 1.8662 - val_loss: 5.4762 - val_mse: 5.4762 - val_mae: 1.9224\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 1.92061\n",
      "Epoch 142/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5332 - mse: 6.5332 - mae: 2.0481 - val_loss: 5.4688 - val_mse: 5.4688 - val_mae: 1.9208\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 1.92061\n",
      "Epoch 143/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1137 - mse: 6.1137 - mae: 1.9814 - val_loss: 5.4509 - val_mse: 5.4509 - val_mae: 1.9153\n",
      "\n",
      "Epoch 00143: val_mae improved from 1.92061 to 1.91533, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 144/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5480 - mse: 6.5480 - mae: 1.9807 - val_loss: 5.4456 - val_mse: 5.4456 - val_mae: 1.9147\n",
      "\n",
      "Epoch 00144: val_mae improved from 1.91533 to 1.91467, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 145/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7896 - mse: 5.7896 - mae: 1.9036 - val_loss: 5.4414 - val_mse: 5.4414 - val_mae: 1.9145\n",
      "\n",
      "Epoch 00145: val_mae improved from 1.91467 to 1.91452, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 146/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8515 - mse: 5.8515 - mae: 1.8699 - val_loss: 5.4308 - val_mse: 5.4308 - val_mae: 1.9116\n",
      "\n",
      "Epoch 00146: val_mae improved from 1.91452 to 1.91165, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 147/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.4345 - mse: 6.4345 - mae: 2.0013 - val_loss: 5.4196 - val_mse: 5.4196 - val_mae: 1.9086\n",
      "\n",
      "Epoch 00147: val_mae improved from 1.91165 to 1.90861, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 148/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.0305 - mse: 6.0305 - mae: 1.9380 - val_loss: 5.4116 - val_mse: 5.4116 - val_mae: 1.9069\n",
      "\n",
      "Epoch 00148: val_mae improved from 1.90861 to 1.90689, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 149/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9507 - mse: 5.9507 - mae: 1.9338 - val_loss: 5.4088 - val_mse: 5.4088 - val_mae: 1.9073\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 1.90689\n",
      "Epoch 150/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1727 - mse: 6.1727 - mae: 1.9253 - val_loss: 5.4030 - val_mse: 5.4030 - val_mae: 1.9064\n",
      "\n",
      "Epoch 00150: val_mae improved from 1.90689 to 1.90637, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 151/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.5994 - mse: 6.5994 - mae: 2.0156 - val_loss: 5.3942 - val_mse: 5.3942 - val_mae: 1.9043\n",
      "\n",
      "Epoch 00151: val_mae improved from 1.90637 to 1.90435, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 152/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.8546 - mse: 6.8546 - mae: 2.0773 - val_loss: 5.3917 - val_mse: 5.3917 - val_mae: 1.9049\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 1.90435\n",
      "Epoch 153/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0173 - mse: 6.0173 - mae: 1.9126 - val_loss: 5.3821 - val_mse: 5.3821 - val_mae: 1.9026\n",
      "\n",
      "Epoch 00153: val_mae improved from 1.90435 to 1.90257, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 154/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.1541 - mse: 7.1541 - mae: 2.1043 - val_loss: 5.3714 - val_mse: 5.3714 - val_mae: 1.8999\n",
      "\n",
      "Epoch 00154: val_mae improved from 1.90257 to 1.89986, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 155/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.6884 - mse: 5.6884 - mae: 1.9086 - val_loss: 5.3597 - val_mse: 5.3597 - val_mae: 1.8967\n",
      "\n",
      "Epoch 00155: val_mae improved from 1.89986 to 1.89671, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 156/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.5132 - mse: 6.5132 - mae: 2.0074 - val_loss: 5.3615 - val_mse: 5.3615 - val_mae: 1.8990\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 1.89671\n",
      "Epoch 157/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2993 - mse: 6.2993 - mae: 1.9865 - val_loss: 5.3645 - val_mse: 5.3645 - val_mae: 1.9017\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 1.89671\n",
      "Epoch 158/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0264 - mse: 6.0264 - mae: 1.9347 - val_loss: 5.3613 - val_mse: 5.3613 - val_mae: 1.9018\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 1.89671\n",
      "Epoch 159/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7795 - mse: 5.7795 - mae: 1.9188 - val_loss: 5.3583 - val_mse: 5.3583 - val_mae: 1.9021\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 1.89671\n",
      "Epoch 160/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7638 - mse: 5.7638 - mae: 1.9252 - val_loss: 5.3447 - val_mse: 5.3447 - val_mae: 1.8985\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 1.89671\n",
      "Epoch 161/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.2207 - mse: 7.2207 - mae: 2.1566 - val_loss: 5.3307 - val_mse: 5.3307 - val_mae: 1.8948\n",
      "\n",
      "Epoch 00161: val_mae improved from 1.89671 to 1.89478, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 162/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2120 - mse: 6.2120 - mae: 2.0038 - val_loss: 5.3212 - val_mse: 5.3212 - val_mae: 1.8925\n",
      "\n",
      "Epoch 00162: val_mae improved from 1.89478 to 1.89254, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 163/1000\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.3087 - mse: 7.3087 - mae: 2.0478 - val_loss: 5.3161 - val_mse: 5.3161 - val_mae: 1.8921\n",
      "\n",
      "Epoch 00163: val_mae improved from 1.89254 to 1.89207, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 164/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.3926 - mse: 6.3926 - mae: 1.9615 - val_loss: 5.3138 - val_mse: 5.3138 - val_mae: 1.8927\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 1.89207\n",
      "Epoch 165/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.8515 - mse: 6.8515 - mae: 2.0194 - val_loss: 5.3110 - val_mse: 5.3110 - val_mae: 1.8930\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 1.89207\n",
      "Epoch 166/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1286 - mse: 6.1286 - mae: 1.9942 - val_loss: 5.2964 - val_mse: 5.2964 - val_mae: 1.8892\n",
      "\n",
      "Epoch 00166: val_mae improved from 1.89207 to 1.88923, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 167/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.7105 - mse: 6.7105 - mae: 2.0305 - val_loss: 5.2879 - val_mse: 5.2879 - val_mae: 1.8876\n",
      "\n",
      "Epoch 00167: val_mae improved from 1.88923 to 1.88760, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 168/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9900 - mse: 5.9900 - mae: 1.9351 - val_loss: 5.2785 - val_mse: 5.2785 - val_mae: 1.8857\n",
      "\n",
      "Epoch 00168: val_mae improved from 1.88760 to 1.88570, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 169/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2075 - mse: 6.2075 - mae: 1.9690 - val_loss: 5.2702 - val_mse: 5.2702 - val_mae: 1.8841\n",
      "\n",
      "Epoch 00169: val_mae improved from 1.88570 to 1.88406, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 170/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0674 - mse: 6.0674 - mae: 1.9606 - val_loss: 5.2484 - val_mse: 5.2484 - val_mae: 1.8772\n",
      "\n",
      "Epoch 00170: val_mae improved from 1.88406 to 1.87717, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 171/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0392 - mse: 6.0392 - mae: 1.9680 - val_loss: 5.2313 - val_mse: 5.2313 - val_mae: 1.8719\n",
      "\n",
      "Epoch 00171: val_mae improved from 1.87717 to 1.87188, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 172/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1220 - mse: 6.1220 - mae: 1.9203 - val_loss: 5.2256 - val_mse: 5.2256 - val_mae: 1.8715\n",
      "\n",
      "Epoch 00172: val_mae improved from 1.87188 to 1.87150, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 173/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.2534 - mse: 7.2534 - mae: 2.0059 - val_loss: 5.2253 - val_mse: 5.2253 - val_mae: 1.8735\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 1.87150\n",
      "Epoch 174/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1135 - mse: 6.1135 - mae: 1.9719 - val_loss: 5.2136 - val_mse: 5.2136 - val_mae: 1.8703\n",
      "\n",
      "Epoch 00174: val_mae improved from 1.87150 to 1.87031, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 175/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.6798 - mse: 6.6798 - mae: 2.0068 - val_loss: 5.2063 - val_mse: 5.2063 - val_mae: 1.8693\n",
      "\n",
      "Epoch 00175: val_mae improved from 1.87031 to 1.86934, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 176/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1806 - mse: 6.1806 - mae: 1.9557 - val_loss: 5.1965 - val_mse: 5.1965 - val_mae: 1.8672\n",
      "\n",
      "Epoch 00176: val_mae improved from 1.86934 to 1.86720, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 177/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1343 - mse: 6.1343 - mae: 1.9348 - val_loss: 5.1985 - val_mse: 5.1985 - val_mae: 1.8699\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 1.86720\n",
      "Epoch 178/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1935 - mse: 6.1935 - mae: 1.9568 - val_loss: 5.1971 - val_mse: 5.1971 - val_mae: 1.8710\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 1.86720\n",
      "Epoch 179/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2325 - mse: 6.2325 - mae: 1.9719 - val_loss: 5.1944 - val_mse: 5.1944 - val_mae: 1.8713\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 1.86720\n",
      "Epoch 180/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4987 - mse: 6.4987 - mae: 1.9781 - val_loss: 5.1810 - val_mse: 5.1810 - val_mae: 1.8679\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 1.86720\n",
      "Epoch 181/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2068 - mse: 6.2068 - mae: 1.9089 - val_loss: 5.1674 - val_mse: 5.1674 - val_mae: 1.8642\n",
      "\n",
      "Epoch 00181: val_mae improved from 1.86720 to 1.86421, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 182/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5144 - mse: 5.5144 - mae: 1.8646 - val_loss: 5.1728 - val_mse: 5.1728 - val_mae: 1.8676\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 1.86421\n",
      "Epoch 183/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8429 - mse: 5.8429 - mae: 1.9071 - val_loss: 5.1589 - val_mse: 5.1589 - val_mae: 1.8643\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 1.86421\n",
      "Epoch 184/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0779 - mse: 6.0779 - mae: 1.9116 - val_loss: 5.1550 - val_mse: 5.1550 - val_mae: 1.8642\n",
      "\n",
      "Epoch 00184: val_mae improved from 1.86421 to 1.86417, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 185/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7116 - mse: 5.7116 - mae: 1.9245 - val_loss: 5.1402 - val_mse: 5.1402 - val_mae: 1.8603\n",
      "\n",
      "Epoch 00185: val_mae improved from 1.86417 to 1.86031, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 186/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9008 - mse: 5.9008 - mae: 1.9535 - val_loss: 5.1344 - val_mse: 5.1344 - val_mae: 1.8597\n",
      "\n",
      "Epoch 00186: val_mae improved from 1.86031 to 1.85968, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 187/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7729 - mse: 5.7729 - mae: 1.9044 - val_loss: 5.1405 - val_mse: 5.1405 - val_mae: 1.8628\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 1.85968\n",
      "Epoch 188/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.7083 - mse: 6.7083 - mae: 2.0591 - val_loss: 5.1178 - val_mse: 5.1178 - val_mae: 1.8567\n",
      "\n",
      "Epoch 00188: val_mae improved from 1.85968 to 1.85668, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 189/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2430 - mse: 6.2430 - mae: 1.9318 - val_loss: 5.1030 - val_mse: 5.1030 - val_mae: 1.8529\n",
      "\n",
      "Epoch 00189: val_mae improved from 1.85668 to 1.85289, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 190/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2047 - mse: 6.2047 - mae: 1.9368 - val_loss: 5.0867 - val_mse: 5.0867 - val_mae: 1.8483\n",
      "\n",
      "Epoch 00190: val_mae improved from 1.85289 to 1.84833, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 191/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2701 - mse: 6.2701 - mae: 1.9465 - val_loss: 5.0703 - val_mse: 5.0703 - val_mae: 1.8437\n",
      "\n",
      "Epoch 00191: val_mae improved from 1.84833 to 1.84374, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 192/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6421 - mse: 5.6421 - mae: 1.8876 - val_loss: 5.0495 - val_mse: 5.0495 - val_mae: 1.8368\n",
      "\n",
      "Epoch 00192: val_mae improved from 1.84374 to 1.83681, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 193/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6759 - mse: 5.6759 - mae: 1.8704 - val_loss: 5.0369 - val_mse: 5.0369 - val_mae: 1.8336\n",
      "\n",
      "Epoch 00193: val_mae improved from 1.83681 to 1.83359, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 194/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9010 - mse: 5.9010 - mae: 1.9247 - val_loss: 5.0321 - val_mse: 5.0321 - val_mae: 1.8337\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 1.83359\n",
      "Epoch 195/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2188 - mse: 6.2188 - mae: 1.9860 - val_loss: 5.0325 - val_mse: 5.0325 - val_mae: 1.8360\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 1.83359\n",
      "Epoch 196/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4477 - mse: 5.4477 - mae: 1.8746 - val_loss: 5.0224 - val_mse: 5.0224 - val_mae: 1.8339\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 1.83359\n",
      "Epoch 197/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5757 - mse: 5.5757 - mae: 1.8620 - val_loss: 5.0120 - val_mse: 5.0120 - val_mae: 1.8316\n",
      "\n",
      "Epoch 00197: val_mae improved from 1.83359 to 1.83159, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 198/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1714 - mse: 6.1714 - mae: 1.9302 - val_loss: 5.0171 - val_mse: 5.0171 - val_mae: 1.8356\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 1.83159\n",
      "Epoch 199/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2515 - mse: 6.2515 - mae: 1.9726 - val_loss: 5.0025 - val_mse: 5.0025 - val_mae: 1.8318\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 1.83159\n",
      "Epoch 200/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.5617 - mse: 6.5617 - mae: 2.0029 - val_loss: 4.9863 - val_mse: 4.9863 - val_mae: 1.8273\n",
      "\n",
      "Epoch 00200: val_mae improved from 1.83159 to 1.82731, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 201/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0440 - mse: 6.0440 - mae: 1.9183 - val_loss: 4.9745 - val_mse: 4.9745 - val_mae: 1.8245\n",
      "\n",
      "Epoch 00201: val_mae improved from 1.82731 to 1.82448, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 202/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3543 - mse: 6.3543 - mae: 1.9973 - val_loss: 4.9655 - val_mse: 4.9655 - val_mae: 1.8227\n",
      "\n",
      "Epoch 00202: val_mae improved from 1.82448 to 1.82274, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 203/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9483 - mse: 5.9483 - mae: 1.9185 - val_loss: 4.9656 - val_mse: 4.9656 - val_mae: 1.8246\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 1.82274\n",
      "Epoch 204/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.8130 - mse: 5.8130 - mae: 1.8896 - val_loss: 4.9687 - val_mse: 4.9687 - val_mae: 1.8272\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 1.82274\n",
      "Epoch 205/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.4926 - mse: 6.4926 - mae: 2.0154 - val_loss: 4.9555 - val_mse: 4.9555 - val_mae: 1.8241\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 1.82274\n",
      "Epoch 206/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2349 - mse: 6.2349 - mae: 1.9780 - val_loss: 4.9447 - val_mse: 4.9447 - val_mae: 1.8218\n",
      "\n",
      "Epoch 00206: val_mae improved from 1.82274 to 1.82185, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 207/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9803 - mse: 5.9803 - mae: 1.9153 - val_loss: 4.9197 - val_mse: 4.9197 - val_mae: 1.8140\n",
      "\n",
      "Epoch 00207: val_mae improved from 1.82185 to 1.81402, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 208/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9577 - mse: 5.9577 - mae: 1.8748 - val_loss: 4.9005 - val_mse: 4.9005 - val_mae: 1.8078\n",
      "\n",
      "Epoch 00208: val_mae improved from 1.81402 to 1.80781, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 209/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.7517 - mse: 5.7517 - mae: 1.8793 - val_loss: 4.8878 - val_mse: 4.8878 - val_mae: 1.8045\n",
      "\n",
      "Epoch 00209: val_mae improved from 1.80781 to 1.80445, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 210/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5517 - mse: 5.5517 - mae: 1.8667 - val_loss: 4.8766 - val_mse: 4.8766 - val_mae: 1.8018\n",
      "\n",
      "Epoch 00210: val_mae improved from 1.80445 to 1.80184, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 211/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.8189 - mse: 6.8189 - mae: 2.0329 - val_loss: 4.8816 - val_mse: 4.8816 - val_mae: 1.8063\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 1.80184\n",
      "Epoch 212/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0911 - mse: 6.0911 - mae: 1.9254 - val_loss: 4.8803 - val_mse: 4.8803 - val_mae: 1.8078\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 1.80184\n",
      "Epoch 213/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5058 - mse: 6.5058 - mae: 1.9168 - val_loss: 4.8674 - val_mse: 4.8674 - val_mae: 1.8045\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 1.80184\n",
      "Epoch 214/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8202 - mse: 5.8202 - mae: 1.9280 - val_loss: 4.8471 - val_mse: 4.8471 - val_mae: 1.7979\n",
      "\n",
      "Epoch 00214: val_mae improved from 1.80184 to 1.79793, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 215/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.8551 - mse: 5.8551 - mae: 1.8826 - val_loss: 4.8395 - val_mse: 4.8395 - val_mae: 1.7968\n",
      "\n",
      "Epoch 00215: val_mae improved from 1.79793 to 1.79679, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 216/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1208 - mse: 6.1208 - mae: 1.9418 - val_loss: 4.8307 - val_mse: 4.8307 - val_mae: 1.7952\n",
      "\n",
      "Epoch 00216: val_mae improved from 1.79679 to 1.79520, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 217/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9271 - mse: 5.9271 - mae: 1.8885 - val_loss: 4.8266 - val_mse: 4.8266 - val_mae: 1.7955\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 1.79520\n",
      "Epoch 218/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9982 - mse: 5.9982 - mae: 1.9386 - val_loss: 4.8129 - val_mse: 4.8129 - val_mae: 1.7918\n",
      "\n",
      "Epoch 00218: val_mae improved from 1.79520 to 1.79178, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 219/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5533 - mse: 5.5533 - mae: 1.8592 - val_loss: 4.8011 - val_mse: 4.8011 - val_mae: 1.7888\n",
      "\n",
      "Epoch 00219: val_mae improved from 1.79178 to 1.78878, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 220/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3038 - mse: 5.3038 - mae: 1.8162 - val_loss: 4.7939 - val_mse: 4.7939 - val_mae: 1.7878\n",
      "\n",
      "Epoch 00220: val_mae improved from 1.78878 to 1.78781, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 221/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3060 - mse: 6.3060 - mae: 1.8859 - val_loss: 4.7970 - val_mse: 4.7970 - val_mae: 1.7914\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 1.78781\n",
      "Epoch 222/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6238 - mse: 5.6238 - mae: 1.8476 - val_loss: 4.7870 - val_mse: 4.7870 - val_mae: 1.7895\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 1.78781\n",
      "Epoch 223/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4770 - mse: 6.4770 - mae: 2.0090 - val_loss: 4.7729 - val_mse: 4.7729 - val_mae: 1.7858\n",
      "\n",
      "Epoch 00223: val_mae improved from 1.78781 to 1.78576, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 224/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.4456 - mse: 5.4456 - mae: 1.8083 - val_loss: 4.7592 - val_mse: 4.7592 - val_mae: 1.7821\n",
      "\n",
      "Epoch 00224: val_mae improved from 1.78576 to 1.78205, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 225/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1563 - mse: 6.1563 - mae: 1.9605 - val_loss: 4.7519 - val_mse: 4.7519 - val_mae: 1.7810\n",
      "\n",
      "Epoch 00225: val_mae improved from 1.78205 to 1.78104, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 226/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7953 - mse: 5.7953 - mae: 1.9235 - val_loss: 4.7413 - val_mse: 4.7413 - val_mae: 1.7788\n",
      "\n",
      "Epoch 00226: val_mae improved from 1.78104 to 1.77876, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 227/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3994 - mse: 6.3994 - mae: 1.9432 - val_loss: 4.7454 - val_mse: 4.7454 - val_mae: 1.7823\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 1.77876\n",
      "Epoch 228/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.4811 - mse: 6.4811 - mae: 1.9953 - val_loss: 4.7290 - val_mse: 4.7290 - val_mae: 1.7775\n",
      "\n",
      "Epoch 00228: val_mae improved from 1.77876 to 1.77755, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 229/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1499 - mse: 5.1499 - mae: 1.7918 - val_loss: 4.7135 - val_mse: 4.7135 - val_mae: 1.7734\n",
      "\n",
      "Epoch 00229: val_mae improved from 1.77755 to 1.77345, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 230/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6288 - mse: 5.6288 - mae: 1.8466 - val_loss: 4.6986 - val_mse: 4.6986 - val_mae: 1.7694\n",
      "\n",
      "Epoch 00230: val_mae improved from 1.77345 to 1.76939, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 231/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9793 - mse: 5.9793 - mae: 1.9245 - val_loss: 4.6924 - val_mse: 4.6924 - val_mae: 1.7688\n",
      "\n",
      "Epoch 00231: val_mae improved from 1.76939 to 1.76885, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 232/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8613 - mse: 5.8613 - mae: 1.9032 - val_loss: 4.6768 - val_mse: 4.6768 - val_mae: 1.7646\n",
      "\n",
      "Epoch 00232: val_mae improved from 1.76885 to 1.76458, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 233/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.4683 - mse: 6.4683 - mae: 1.9674 - val_loss: 4.6640 - val_mse: 4.6640 - val_mae: 1.7613\n",
      "\n",
      "Epoch 00233: val_mae improved from 1.76458 to 1.76127, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 234/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1939 - mse: 6.1939 - mae: 1.9221 - val_loss: 4.6509 - val_mse: 4.6509 - val_mae: 1.7577\n",
      "\n",
      "Epoch 00234: val_mae improved from 1.76127 to 1.75766, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 235/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9146 - mse: 5.9146 - mae: 1.8841 - val_loss: 4.6550 - val_mse: 4.6550 - val_mae: 1.7611\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 1.75766\n",
      "Epoch 236/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.4936 - mse: 6.4936 - mae: 1.9936 - val_loss: 4.6393 - val_mse: 4.6393 - val_mae: 1.7566\n",
      "\n",
      "Epoch 00236: val_mae improved from 1.75766 to 1.75656, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 237/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2011 - mse: 5.2011 - mae: 1.7800 - val_loss: 4.6456 - val_mse: 4.6456 - val_mae: 1.7609\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 1.75656\n",
      "Epoch 238/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7788 - mse: 5.7788 - mae: 1.9124 - val_loss: 4.6356 - val_mse: 4.6356 - val_mae: 1.7586\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 1.75656\n",
      "Epoch 239/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4451 - mse: 5.4451 - mae: 1.8254 - val_loss: 4.6233 - val_mse: 4.6233 - val_mae: 1.7557\n",
      "\n",
      "Epoch 00239: val_mae improved from 1.75656 to 1.75566, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 240/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7774 - mse: 5.7774 - mae: 1.9105 - val_loss: 4.6170 - val_mse: 4.6170 - val_mae: 1.7549\n",
      "\n",
      "Epoch 00240: val_mae improved from 1.75566 to 1.75486, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 241/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5257 - mse: 5.5257 - mae: 1.8412 - val_loss: 4.6011 - val_mse: 4.6011 - val_mae: 1.7503\n",
      "\n",
      "Epoch 00241: val_mae improved from 1.75486 to 1.75025, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 242/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6991 - mse: 5.6991 - mae: 1.8608 - val_loss: 4.5810 - val_mse: 4.5810 - val_mae: 1.7436\n",
      "\n",
      "Epoch 00242: val_mae improved from 1.75025 to 1.74360, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 243/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.2321 - mse: 5.2321 - mae: 1.7772 - val_loss: 4.5610 - val_mse: 4.5610 - val_mae: 1.7363\n",
      "\n",
      "Epoch 00243: val_mae improved from 1.74360 to 1.73630, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 244/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7761 - mse: 5.7761 - mae: 1.8439 - val_loss: 4.5579 - val_mse: 4.5579 - val_mae: 1.7373\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 1.73630\n",
      "Epoch 245/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6455 - mse: 5.6455 - mae: 1.7757 - val_loss: 4.5568 - val_mse: 4.5568 - val_mae: 1.7390\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 1.73630\n",
      "Epoch 246/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9115 - mse: 5.9115 - mae: 1.9080 - val_loss: 4.5462 - val_mse: 4.5462 - val_mae: 1.7364\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 1.73630\n",
      "Epoch 247/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2084 - mse: 5.2084 - mae: 1.8004 - val_loss: 4.5388 - val_mse: 4.5388 - val_mae: 1.7352\n",
      "\n",
      "Epoch 00247: val_mae improved from 1.73630 to 1.73515, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 248/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.7233 - mse: 6.7233 - mae: 2.0151 - val_loss: 4.5377 - val_mse: 4.5377 - val_mae: 1.7367\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 1.73515\n",
      "Epoch 249/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4395 - mse: 5.4395 - mae: 1.8231 - val_loss: 4.5237 - val_mse: 4.5237 - val_mae: 1.7326\n",
      "\n",
      "Epoch 00249: val_mae improved from 1.73515 to 1.73261, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 250/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6709 - mse: 5.6709 - mae: 1.8370 - val_loss: 4.5171 - val_mse: 4.5171 - val_mae: 1.7319\n",
      "\n",
      "Epoch 00250: val_mae improved from 1.73261 to 1.73192, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 251/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7528 - mse: 5.7528 - mae: 1.8759 - val_loss: 4.5079 - val_mse: 4.5079 - val_mae: 1.7299\n",
      "\n",
      "Epoch 00251: val_mae improved from 1.73192 to 1.72989, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 252/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3003 - mse: 5.3003 - mae: 1.8490 - val_loss: 4.4939 - val_mse: 4.4939 - val_mae: 1.7258\n",
      "\n",
      "Epoch 00252: val_mae improved from 1.72989 to 1.72578, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 253/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.7016 - mse: 5.7016 - mae: 1.8785 - val_loss: 4.4907 - val_mse: 4.4907 - val_mae: 1.7265\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 1.72578\n",
      "Epoch 254/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2618 - mse: 5.2618 - mae: 1.8029 - val_loss: 4.4921 - val_mse: 4.4921 - val_mae: 1.7290\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 1.72578\n",
      "Epoch 255/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7248 - mse: 5.7248 - mae: 1.8402 - val_loss: 4.4909 - val_mse: 4.4909 - val_mae: 1.7299\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 1.72578\n",
      "Epoch 256/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 6.0221 - mse: 6.0221 - mae: 1.9116 - val_loss: 4.4706 - val_mse: 4.4706 - val_mae: 1.7235\n",
      "\n",
      "Epoch 00256: val_mae improved from 1.72578 to 1.72349, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 257/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6268 - mse: 5.6268 - mae: 1.8573 - val_loss: 4.4583 - val_mse: 4.4583 - val_mae: 1.7202\n",
      "\n",
      "Epoch 00257: val_mae improved from 1.72349 to 1.72025, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 258/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0935 - mse: 6.0935 - mae: 1.9562 - val_loss: 4.4499 - val_mse: 4.4499 - val_mae: 1.7185\n",
      "\n",
      "Epoch 00258: val_mae improved from 1.72025 to 1.71851, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 259/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1011 - mse: 5.1011 - mae: 1.7654 - val_loss: 4.4394 - val_mse: 4.4394 - val_mae: 1.7157\n",
      "\n",
      "Epoch 00259: val_mae improved from 1.71851 to 1.71566, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 260/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2979 - mse: 6.2979 - mae: 1.8907 - val_loss: 4.4407 - val_mse: 4.4407 - val_mae: 1.7178\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 1.71566\n",
      "Epoch 261/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8064 - mse: 4.8064 - mae: 1.6967 - val_loss: 4.4328 - val_mse: 4.4328 - val_mae: 1.7163\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 1.71566\n",
      "Epoch 262/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6906 - mse: 5.6906 - mae: 1.8839 - val_loss: 4.4226 - val_mse: 4.4226 - val_mae: 1.7137\n",
      "\n",
      "Epoch 00262: val_mae improved from 1.71566 to 1.71372, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 263/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6505 - mse: 5.6505 - mae: 1.8245 - val_loss: 4.4102 - val_mse: 4.4102 - val_mae: 1.7101\n",
      "\n",
      "Epoch 00263: val_mae improved from 1.71372 to 1.71015, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 264/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4859 - mse: 5.4859 - mae: 1.8190 - val_loss: 4.3911 - val_mse: 4.3911 - val_mae: 1.7033\n",
      "\n",
      "Epoch 00264: val_mae improved from 1.71015 to 1.70329, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 265/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3787 - mse: 6.3787 - mae: 1.9672 - val_loss: 4.3940 - val_mse: 4.3940 - val_mae: 1.7065\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 1.70329\n",
      "Epoch 266/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2979 - mse: 5.2979 - mae: 1.8552 - val_loss: 4.3940 - val_mse: 4.3940 - val_mae: 1.7080\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 1.70329\n",
      "Epoch 267/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8734 - mse: 5.8734 - mae: 1.8964 - val_loss: 4.3841 - val_mse: 4.3841 - val_mae: 1.7055\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 1.70329\n",
      "Epoch 268/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9934 - mse: 5.9934 - mae: 1.8653 - val_loss: 4.3682 - val_mse: 4.3682 - val_mae: 1.7008\n",
      "\n",
      "Epoch 00268: val_mae improved from 1.70329 to 1.70084, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 269/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3074 - mse: 5.3074 - mae: 1.7741 - val_loss: 4.3620 - val_mse: 4.3620 - val_mae: 1.6999\n",
      "\n",
      "Epoch 00269: val_mae improved from 1.70084 to 1.69989, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 270/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4321 - mse: 5.4321 - mae: 1.8563 - val_loss: 4.3503 - val_mse: 4.3503 - val_mae: 1.6963\n",
      "\n",
      "Epoch 00270: val_mae improved from 1.69989 to 1.69634, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 271/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1705 - mse: 5.1705 - mae: 1.7821 - val_loss: 4.3443 - val_mse: 4.3443 - val_mae: 1.6955\n",
      "\n",
      "Epoch 00271: val_mae improved from 1.69634 to 1.69551, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 272/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7992 - mse: 5.7992 - mae: 1.8967 - val_loss: 4.3398 - val_mse: 4.3398 - val_mae: 1.6953\n",
      "\n",
      "Epoch 00272: val_mae improved from 1.69551 to 1.69532, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 273/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3952 - mse: 5.3952 - mae: 1.8578 - val_loss: 4.3290 - val_mse: 4.3290 - val_mae: 1.6924\n",
      "\n",
      "Epoch 00273: val_mae improved from 1.69532 to 1.69238, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 274/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4297 - mse: 6.4297 - mae: 1.9597 - val_loss: 4.3184 - val_mse: 4.3184 - val_mae: 1.6897\n",
      "\n",
      "Epoch 00274: val_mae improved from 1.69238 to 1.68975, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 275/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4580 - mse: 5.4580 - mae: 1.8312 - val_loss: 4.3059 - val_mse: 4.3059 - val_mae: 1.6858\n",
      "\n",
      "Epoch 00275: val_mae improved from 1.68975 to 1.68585, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 276/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8477 - mse: 5.8477 - mae: 1.9012 - val_loss: 4.2944 - val_mse: 4.2944 - val_mae: 1.6819\n",
      "\n",
      "Epoch 00276: val_mae improved from 1.68585 to 1.68190, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 277/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2541 - mse: 5.2541 - mae: 1.7551 - val_loss: 4.2926 - val_mse: 4.2926 - val_mae: 1.6830\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 1.68190\n",
      "Epoch 278/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0673 - mse: 6.0673 - mae: 1.8618 - val_loss: 4.2832 - val_mse: 4.2832 - val_mae: 1.6802\n",
      "\n",
      "Epoch 00278: val_mae improved from 1.68190 to 1.68019, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 279/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9268 - mse: 4.9268 - mae: 1.7513 - val_loss: 4.2777 - val_mse: 4.2777 - val_mae: 1.6794\n",
      "\n",
      "Epoch 00279: val_mae improved from 1.68019 to 1.67940, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 280/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5895 - mse: 5.5895 - mae: 1.8530 - val_loss: 4.2735 - val_mse: 4.2735 - val_mae: 1.6796\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 1.67940\n",
      "Epoch 281/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3100 - mse: 5.3100 - mae: 1.8374 - val_loss: 4.2651 - val_mse: 4.2651 - val_mae: 1.6775\n",
      "\n",
      "Epoch 00281: val_mae improved from 1.67940 to 1.67747, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 282/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7290 - mse: 5.7290 - mae: 1.9030 - val_loss: 4.2548 - val_mse: 4.2548 - val_mae: 1.6745\n",
      "\n",
      "Epoch 00282: val_mae improved from 1.67747 to 1.67451, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 283/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1038 - mse: 6.1038 - mae: 1.9183 - val_loss: 4.2535 - val_mse: 4.2535 - val_mae: 1.6757\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 1.67451\n",
      "Epoch 284/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0021 - mse: 6.0021 - mae: 1.9165 - val_loss: 4.2447 - val_mse: 4.2447 - val_mae: 1.6734\n",
      "\n",
      "Epoch 00284: val_mae improved from 1.67451 to 1.67335, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 285/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2080 - mse: 5.2080 - mae: 1.7899 - val_loss: 4.2351 - val_mse: 4.2351 - val_mae: 1.6707\n",
      "\n",
      "Epoch 00285: val_mae improved from 1.67335 to 1.67067, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 286/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3970 - mse: 5.3970 - mae: 1.8252 - val_loss: 4.2252 - val_mse: 4.2252 - val_mae: 1.6675\n",
      "\n",
      "Epoch 00286: val_mae improved from 1.67067 to 1.66748, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 287/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3722 - mse: 5.3722 - mae: 1.8144 - val_loss: 4.2205 - val_mse: 4.2205 - val_mae: 1.6673\n",
      "\n",
      "Epoch 00287: val_mae improved from 1.66748 to 1.66730, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 288/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1374 - mse: 5.1374 - mae: 1.7829 - val_loss: 4.2206 - val_mse: 4.2206 - val_mae: 1.6691\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 1.66730\n",
      "Epoch 289/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0475 - mse: 6.0475 - mae: 1.9044 - val_loss: 4.2284 - val_mse: 4.2284 - val_mae: 1.6736\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 1.66730\n",
      "Epoch 290/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8902 - mse: 5.8902 - mae: 1.8824 - val_loss: 4.2159 - val_mse: 4.2159 - val_mae: 1.6699\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 1.66730\n",
      "Epoch 291/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6981 - mse: 5.6981 - mae: 1.8433 - val_loss: 4.2037 - val_mse: 4.2037 - val_mae: 1.6662\n",
      "\n",
      "Epoch 00291: val_mae improved from 1.66730 to 1.66621, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 292/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5021 - mse: 5.5021 - mae: 1.8622 - val_loss: 4.1915 - val_mse: 4.1915 - val_mae: 1.6623\n",
      "\n",
      "Epoch 00292: val_mae improved from 1.66621 to 1.66227, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 293/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0841 - mse: 6.0841 - mae: 1.8998 - val_loss: 4.1780 - val_mse: 4.1780 - val_mae: 1.6572\n",
      "\n",
      "Epoch 00293: val_mae improved from 1.66227 to 1.65723, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 294/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.5081 - mse: 5.5081 - mae: 1.7933 - val_loss: 4.1743 - val_mse: 4.1743 - val_mae: 1.6570\n",
      "\n",
      "Epoch 00294: val_mae improved from 1.65723 to 1.65702, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 295/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2063 - mse: 5.2063 - mae: 1.7864 - val_loss: 4.1752 - val_mse: 4.1752 - val_mae: 1.6589\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 1.65702\n",
      "Epoch 296/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1219 - mse: 5.1219 - mae: 1.7846 - val_loss: 4.1735 - val_mse: 4.1735 - val_mae: 1.6594\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 1.65702\n",
      "Epoch 297/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2737 - mse: 5.2737 - mae: 1.7795 - val_loss: 4.1664 - val_mse: 4.1664 - val_mae: 1.6574\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 1.65702\n",
      "Epoch 298/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2338 - mse: 5.2338 - mae: 1.7797 - val_loss: 4.1605 - val_mse: 4.1605 - val_mae: 1.6562\n",
      "\n",
      "Epoch 00298: val_mae improved from 1.65702 to 1.65623, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 299/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6968 - mse: 5.6968 - mae: 1.8539 - val_loss: 4.1635 - val_mse: 4.1635 - val_mae: 1.6585\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 1.65623\n",
      "Epoch 300/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7554 - mse: 4.7554 - mae: 1.6864 - val_loss: 4.1626 - val_mse: 4.1626 - val_mae: 1.6592\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 1.65623\n",
      "Epoch 301/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6695 - mse: 5.6695 - mae: 1.8516 - val_loss: 4.1451 - val_mse: 4.1451 - val_mae: 1.6531\n",
      "\n",
      "Epoch 00301: val_mae improved from 1.65623 to 1.65310, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 302/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1630 - mse: 5.1630 - mae: 1.7690 - val_loss: 4.1278 - val_mse: 4.1278 - val_mae: 1.6464\n",
      "\n",
      "Epoch 00302: val_mae improved from 1.65310 to 1.64645, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 303/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6456 - mse: 5.6456 - mae: 1.8184 - val_loss: 4.1117 - val_mse: 4.1117 - val_mae: 1.6395\n",
      "\n",
      "Epoch 00303: val_mae improved from 1.64645 to 1.63949, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 304/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8977 - mse: 4.8977 - mae: 1.6882 - val_loss: 4.1105 - val_mse: 4.1105 - val_mae: 1.6407\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 1.63949\n",
      "Epoch 305/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6325 - mse: 5.6325 - mae: 1.8384 - val_loss: 4.1023 - val_mse: 4.1023 - val_mae: 1.6382\n",
      "\n",
      "Epoch 00305: val_mae improved from 1.63949 to 1.63816, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 306/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5166 - mse: 5.5166 - mae: 1.8528 - val_loss: 4.0948 - val_mse: 4.0948 - val_mae: 1.6357\n",
      "\n",
      "Epoch 00306: val_mae improved from 1.63816 to 1.63569, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 307/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2751 - mse: 5.2751 - mae: 1.7998 - val_loss: 4.0928 - val_mse: 4.0928 - val_mae: 1.6363\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 1.63569\n",
      "Epoch 308/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2509 - mse: 6.2509 - mae: 1.8712 - val_loss: 4.0859 - val_mse: 4.0859 - val_mae: 1.6342\n",
      "\n",
      "Epoch 00308: val_mae improved from 1.63569 to 1.63419, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 309/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6392 - mse: 5.6392 - mae: 1.8769 - val_loss: 4.0805 - val_mse: 4.0805 - val_mae: 1.6331\n",
      "\n",
      "Epoch 00309: val_mae improved from 1.63419 to 1.63313, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 310/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8720 - mse: 5.8720 - mae: 1.8956 - val_loss: 4.0764 - val_mse: 4.0764 - val_mae: 1.6327\n",
      "\n",
      "Epoch 00310: val_mae improved from 1.63313 to 1.63269, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 311/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2994 - mse: 5.2994 - mae: 1.7359 - val_loss: 4.0706 - val_mse: 4.0706 - val_mae: 1.6311\n",
      "\n",
      "Epoch 00311: val_mae improved from 1.63269 to 1.63106, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 312/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5455 - mse: 5.5455 - mae: 1.8102 - val_loss: 4.0641 - val_mse: 4.0641 - val_mae: 1.6293\n",
      "\n",
      "Epoch 00312: val_mae improved from 1.63106 to 1.62926, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 313/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2098 - mse: 5.2098 - mae: 1.7562 - val_loss: 4.0613 - val_mse: 4.0613 - val_mae: 1.6296\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 1.62926\n",
      "Epoch 314/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4952 - mse: 5.4952 - mae: 1.8477 - val_loss: 4.0586 - val_mse: 4.0586 - val_mae: 1.6296\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 1.62926\n",
      "Epoch 315/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6630 - mse: 4.6630 - mae: 1.6964 - val_loss: 4.0531 - val_mse: 4.0531 - val_mae: 1.6282\n",
      "\n",
      "Epoch 00315: val_mae improved from 1.62926 to 1.62823, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 316/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3698 - mse: 5.3698 - mae: 1.7785 - val_loss: 4.0746 - val_mse: 4.0746 - val_mae: 1.6394\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 1.62823\n",
      "Epoch 317/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5644 - mse: 5.5644 - mae: 1.8122 - val_loss: 4.0562 - val_mse: 4.0562 - val_mae: 1.6326\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 1.62823\n",
      "Epoch 318/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.8481 - mse: 5.8481 - mae: 1.7813 - val_loss: 4.0457 - val_mse: 4.0457 - val_mae: 1.6291\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 1.62823\n",
      "Epoch 319/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6949 - mse: 5.6949 - mae: 1.8307 - val_loss: 4.0378 - val_mse: 4.0378 - val_mae: 1.6269\n",
      "\n",
      "Epoch 00319: val_mae improved from 1.62823 to 1.62687, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 320/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1876 - mse: 5.1876 - mae: 1.7515 - val_loss: 4.0291 - val_mse: 4.0291 - val_mae: 1.6239\n",
      "\n",
      "Epoch 00320: val_mae improved from 1.62687 to 1.62394, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 321/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9674 - mse: 4.9674 - mae: 1.7597 - val_loss: 4.0186 - val_mse: 4.0186 - val_mae: 1.6199\n",
      "\n",
      "Epoch 00321: val_mae improved from 1.62394 to 1.61990, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 322/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4042 - mse: 5.4042 - mae: 1.7977 - val_loss: 4.0182 - val_mse: 4.0182 - val_mae: 1.6214\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 1.61990\n",
      "Epoch 323/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.7722 - mse: 5.7722 - mae: 1.8325 - val_loss: 4.0136 - val_mse: 4.0136 - val_mae: 1.6204\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 1.61990\n",
      "Epoch 324/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4679 - mse: 5.4679 - mae: 1.8460 - val_loss: 4.0045 - val_mse: 4.0045 - val_mae: 1.6172\n",
      "\n",
      "Epoch 00324: val_mae improved from 1.61990 to 1.61724, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 325/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6620 - mse: 5.6620 - mae: 1.8270 - val_loss: 3.9996 - val_mse: 3.9996 - val_mae: 1.6160\n",
      "\n",
      "Epoch 00325: val_mae improved from 1.61724 to 1.61595, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 326/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8460 - mse: 5.8460 - mae: 1.8445 - val_loss: 3.9992 - val_mse: 3.9992 - val_mae: 1.6173\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 1.61595\n",
      "Epoch 327/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5567 - mse: 4.5567 - mae: 1.6452 - val_loss: 3.9881 - val_mse: 3.9881 - val_mae: 1.6122\n",
      "\n",
      "Epoch 00327: val_mae improved from 1.61595 to 1.61224, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 328/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9499 - mse: 5.9499 - mae: 1.8524 - val_loss: 3.9841 - val_mse: 3.9841 - val_mae: 1.6113\n",
      "\n",
      "Epoch 00328: val_mae improved from 1.61224 to 1.61130, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 329/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0483 - mse: 5.0483 - mae: 1.7223 - val_loss: 3.9815 - val_mse: 3.9815 - val_mae: 1.6112\n",
      "\n",
      "Epoch 00329: val_mae improved from 1.61130 to 1.61116, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 330/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.2349 - mse: 5.2349 - mae: 1.8029 - val_loss: 3.9710 - val_mse: 3.9710 - val_mae: 1.6065\n",
      "\n",
      "Epoch 00330: val_mae improved from 1.61116 to 1.60651, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 331/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9197 - mse: 4.9197 - mae: 1.7228 - val_loss: 3.9626 - val_mse: 3.9626 - val_mae: 1.6023\n",
      "\n",
      "Epoch 00331: val_mae improved from 1.60651 to 1.60233, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 332/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6784 - mse: 5.6784 - mae: 1.8423 - val_loss: 3.9590 - val_mse: 3.9590 - val_mae: 1.6019\n",
      "\n",
      "Epoch 00332: val_mae improved from 1.60233 to 1.60190, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 333/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2013 - mse: 5.2013 - mae: 1.7719 - val_loss: 3.9615 - val_mse: 3.9615 - val_mae: 1.6054\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 1.60190\n",
      "Epoch 334/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1924 - mse: 5.1924 - mae: 1.7724 - val_loss: 3.9668 - val_mse: 3.9668 - val_mae: 1.6095\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 1.60190\n",
      "Epoch 335/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6532 - mse: 4.6532 - mae: 1.6946 - val_loss: 3.9659 - val_mse: 3.9659 - val_mae: 1.6104\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 1.60190\n",
      "Epoch 336/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1081 - mse: 5.1081 - mae: 1.7548 - val_loss: 3.9555 - val_mse: 3.9555 - val_mae: 1.6065\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 1.60190\n",
      "Epoch 337/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1817 - mse: 5.1817 - mae: 1.7675 - val_loss: 3.9486 - val_mse: 3.9486 - val_mae: 1.6040\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 1.60190\n",
      "Epoch 338/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5987 - mse: 5.5987 - mae: 1.8059 - val_loss: 3.9474 - val_mse: 3.9474 - val_mae: 1.6047\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 1.60190\n",
      "Epoch 339/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9837 - mse: 5.9837 - mae: 1.8104 - val_loss: 3.9453 - val_mse: 3.9453 - val_mae: 1.6049\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 1.60190\n",
      "Epoch 340/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3586 - mse: 5.3586 - mae: 1.7723 - val_loss: 3.9342 - val_mse: 3.9342 - val_mae: 1.6005\n",
      "\n",
      "Epoch 00340: val_mae improved from 1.60190 to 1.60054, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 341/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1461 - mse: 5.1461 - mae: 1.7112 - val_loss: 3.9230 - val_mse: 3.9230 - val_mae: 1.5947\n",
      "\n",
      "Epoch 00341: val_mae improved from 1.60054 to 1.59475, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 342/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0192 - mse: 5.0192 - mae: 1.7291 - val_loss: 3.9205 - val_mse: 3.9205 - val_mae: 1.5946\n",
      "\n",
      "Epoch 00342: val_mae improved from 1.59475 to 1.59459, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 343/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7303 - mse: 5.7303 - mae: 1.7753 - val_loss: 3.9192 - val_mse: 3.9192 - val_mae: 1.5947\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 1.59459\n",
      "Epoch 344/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2068 - mse: 5.2068 - mae: 1.8142 - val_loss: 3.9200 - val_mse: 3.9200 - val_mae: 1.5966\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 1.59459\n",
      "Epoch 345/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2835 - mse: 5.2835 - mae: 1.7352 - val_loss: 3.9154 - val_mse: 3.9154 - val_mae: 1.5953\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 1.59459\n",
      "Epoch 346/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0015 - mse: 6.0015 - mae: 1.8769 - val_loss: 3.9128 - val_mse: 3.9128 - val_mae: 1.5950\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 1.59459\n",
      "Epoch 347/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6670 - mse: 5.6670 - mae: 1.8438 - val_loss: 3.9102 - val_mse: 3.9102 - val_mae: 1.5947\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 1.59459\n",
      "Epoch 348/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5794 - mse: 5.5794 - mae: 1.7942 - val_loss: 3.9033 - val_mse: 3.9033 - val_mae: 1.5922\n",
      "\n",
      "Epoch 00348: val_mae improved from 1.59459 to 1.59219, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 349/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4820 - mse: 5.4820 - mae: 1.8261 - val_loss: 3.9006 - val_mse: 3.9006 - val_mae: 1.5921\n",
      "\n",
      "Epoch 00349: val_mae improved from 1.59219 to 1.59208, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 350/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3844 - mse: 5.3844 - mae: 1.7859 - val_loss: 3.9033 - val_mse: 3.9033 - val_mae: 1.5944\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 1.59208\n",
      "Epoch 351/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.2915 - mse: 6.2915 - mae: 1.8964 - val_loss: 3.9017 - val_mse: 3.9017 - val_mae: 1.5944\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 1.59208\n",
      "Epoch 352/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8425 - mse: 4.8425 - mae: 1.7204 - val_loss: 3.8896 - val_mse: 3.8896 - val_mae: 1.5892\n",
      "\n",
      "Epoch 00352: val_mae improved from 1.59208 to 1.58920, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 353/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.7083 - mse: 5.7083 - mae: 1.8109 - val_loss: 3.8897 - val_mse: 3.8897 - val_mae: 1.5900\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 1.58920\n",
      "Epoch 354/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4549 - mse: 5.4549 - mae: 1.8103 - val_loss: 3.8887 - val_mse: 3.8887 - val_mae: 1.5906\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 1.58920\n",
      "Epoch 355/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9985 - mse: 4.9985 - mae: 1.6994 - val_loss: 3.8745 - val_mse: 3.8745 - val_mae: 1.5835\n",
      "\n",
      "Epoch 00355: val_mae improved from 1.58920 to 1.58348, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 356/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6648 - mse: 5.6648 - mae: 1.8565 - val_loss: 3.8730 - val_mse: 3.8730 - val_mae: 1.5840\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 1.58348\n",
      "Epoch 357/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9539 - mse: 4.9539 - mae: 1.7272 - val_loss: 3.8716 - val_mse: 3.8716 - val_mae: 1.5841\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 1.58348\n",
      "Epoch 358/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9510 - mse: 4.9510 - mae: 1.7184 - val_loss: 3.8636 - val_mse: 3.8636 - val_mae: 1.5800\n",
      "\n",
      "Epoch 00358: val_mae improved from 1.58348 to 1.57999, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 359/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7584 - mse: 4.7584 - mae: 1.6763 - val_loss: 3.8664 - val_mse: 3.8664 - val_mae: 1.5833\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 1.57999\n",
      "Epoch 360/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0968 - mse: 5.0968 - mae: 1.7735 - val_loss: 3.8615 - val_mse: 3.8615 - val_mae: 1.5811\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 1.57999\n",
      "Epoch 361/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9778 - mse: 4.9778 - mae: 1.7253 - val_loss: 3.8601 - val_mse: 3.8601 - val_mae: 1.5812\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 1.57999\n",
      "Epoch 362/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4808 - mse: 5.4808 - mae: 1.8060 - val_loss: 3.8535 - val_mse: 3.8535 - val_mae: 1.5784\n",
      "\n",
      "Epoch 00362: val_mae improved from 1.57999 to 1.57842, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 363/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9616 - mse: 5.9616 - mae: 1.8313 - val_loss: 3.8468 - val_mse: 3.8468 - val_mae: 1.5746\n",
      "\n",
      "Epoch 00363: val_mae improved from 1.57842 to 1.57462, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 364/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1465 - mse: 5.1465 - mae: 1.7441 - val_loss: 3.8467 - val_mse: 3.8467 - val_mae: 1.5760\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 1.57462\n",
      "Epoch 365/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1634 - mse: 5.1634 - mae: 1.6979 - val_loss: 3.8552 - val_mse: 3.8552 - val_mae: 1.5819\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 1.57462\n",
      "Epoch 366/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6798 - mse: 5.6798 - mae: 1.8333 - val_loss: 3.8478 - val_mse: 3.8478 - val_mae: 1.5784\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 1.57462\n",
      "Epoch 367/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5849 - mse: 5.5849 - mae: 1.7978 - val_loss: 3.8520 - val_mse: 3.8520 - val_mae: 1.5814\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 1.57462\n",
      "Epoch 368/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8089 - mse: 5.8089 - mae: 1.7642 - val_loss: 3.8404 - val_mse: 3.8404 - val_mae: 1.5757\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 1.57462\n",
      "Epoch 369/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0024 - mse: 5.0024 - mae: 1.7729 - val_loss: 3.8341 - val_mse: 3.8341 - val_mae: 1.5724\n",
      "\n",
      "Epoch 00369: val_mae improved from 1.57462 to 1.57237, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 370/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.3906 - mse: 5.3906 - mae: 1.7695 - val_loss: 3.8322 - val_mse: 3.8322 - val_mae: 1.5724\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 1.57237\n",
      "Epoch 371/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1800 - mse: 5.1800 - mae: 1.7430 - val_loss: 3.8297 - val_mse: 3.8297 - val_mae: 1.5717\n",
      "\n",
      "Epoch 00371: val_mae improved from 1.57237 to 1.57167, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 372/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7188 - mse: 5.7188 - mae: 1.7774 - val_loss: 3.8281 - val_mse: 3.8281 - val_mae: 1.5714\n",
      "\n",
      "Epoch 00372: val_mae improved from 1.57167 to 1.57143, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 373/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0694 - mse: 5.0694 - mae: 1.7159 - val_loss: 3.8258 - val_mse: 3.8258 - val_mae: 1.5707\n",
      "\n",
      "Epoch 00373: val_mae improved from 1.57143 to 1.57073, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 374/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9846 - mse: 4.9846 - mae: 1.7094 - val_loss: 3.8220 - val_mse: 3.8220 - val_mae: 1.5690\n",
      "\n",
      "Epoch 00374: val_mae improved from 1.57073 to 1.56896, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 375/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0025 - mse: 5.0025 - mae: 1.7129 - val_loss: 3.8252 - val_mse: 3.8252 - val_mae: 1.5723\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 1.56896\n",
      "Epoch 376/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3262 - mse: 5.3262 - mae: 1.7843 - val_loss: 3.8265 - val_mse: 3.8265 - val_mae: 1.5736\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 1.56896\n",
      "Epoch 377/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5614 - mse: 5.5614 - mae: 1.7955 - val_loss: 3.8205 - val_mse: 3.8205 - val_mae: 1.5710\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 1.56896\n",
      "Epoch 378/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3493 - mse: 6.3493 - mae: 1.9180 - val_loss: 3.8228 - val_mse: 3.8228 - val_mae: 1.5730\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 1.56896\n",
      "Epoch 379/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6141 - mse: 5.6141 - mae: 1.7561 - val_loss: 3.8191 - val_mse: 3.8191 - val_mae: 1.5719\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 1.56896\n",
      "Epoch 380/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5317 - mse: 5.5317 - mae: 1.8335 - val_loss: 3.8067 - val_mse: 3.8067 - val_mae: 1.5647\n",
      "\n",
      "Epoch 00380: val_mae improved from 1.56896 to 1.56470, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 381/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8687 - mse: 4.8687 - mae: 1.6949 - val_loss: 3.8058 - val_mse: 3.8058 - val_mae: 1.5652\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 1.56470\n",
      "Epoch 382/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5987 - mse: 5.5987 - mae: 1.7657 - val_loss: 3.8049 - val_mse: 3.8049 - val_mae: 1.5656\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 1.56470\n",
      "Epoch 383/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2331 - mse: 5.2331 - mae: 1.7606 - val_loss: 3.8022 - val_mse: 3.8022 - val_mae: 1.5650\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 1.56470\n",
      "Epoch 384/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.8646 - mse: 4.8646 - mae: 1.6996 - val_loss: 3.7960 - val_mse: 3.7960 - val_mae: 1.5618\n",
      "\n",
      "Epoch 00384: val_mae improved from 1.56470 to 1.56176, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 385/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9065 - mse: 4.9065 - mae: 1.7572 - val_loss: 3.7963 - val_mse: 3.7963 - val_mae: 1.5632\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 1.56176\n",
      "Epoch 386/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9178 - mse: 4.9178 - mae: 1.7245 - val_loss: 3.7932 - val_mse: 3.7932 - val_mae: 1.5616\n",
      "\n",
      "Epoch 00386: val_mae improved from 1.56176 to 1.56163, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 387/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3384 - mse: 4.3384 - mae: 1.6274 - val_loss: 3.7899 - val_mse: 3.7899 - val_mae: 1.5601\n",
      "\n",
      "Epoch 00387: val_mae improved from 1.56163 to 1.56006, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 388/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3159 - mse: 5.3159 - mae: 1.7754 - val_loss: 3.7993 - val_mse: 3.7993 - val_mae: 1.5671\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 1.56006\n",
      "Epoch 389/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8004 - mse: 5.8004 - mae: 1.8353 - val_loss: 3.7924 - val_mse: 3.7924 - val_mae: 1.5639\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 1.56006\n",
      "Epoch 390/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6380 - mse: 5.6380 - mae: 1.7878 - val_loss: 3.7896 - val_mse: 3.7896 - val_mae: 1.5632\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 1.56006\n",
      "Epoch 391/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0059 - mse: 5.0059 - mae: 1.7281 - val_loss: 3.7807 - val_mse: 3.7807 - val_mae: 1.5582\n",
      "\n",
      "Epoch 00391: val_mae improved from 1.56006 to 1.55823, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 392/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8489 - mse: 4.8489 - mae: 1.6927 - val_loss: 3.7849 - val_mse: 3.7849 - val_mae: 1.5614\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 1.55823\n",
      "Epoch 393/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1810 - mse: 5.1810 - mae: 1.7551 - val_loss: 3.7796 - val_mse: 3.7796 - val_mae: 1.5587\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 1.55823\n",
      "Epoch 394/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1117 - mse: 5.1117 - mae: 1.7450 - val_loss: 3.7806 - val_mse: 3.7806 - val_mae: 1.5604\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 1.55823\n",
      "Epoch 395/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0581 - mse: 5.0581 - mae: 1.7370 - val_loss: 3.7805 - val_mse: 3.7805 - val_mae: 1.5610\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 1.55823\n",
      "Epoch 396/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1229 - mse: 5.1229 - mae: 1.7205 - val_loss: 3.7791 - val_mse: 3.7791 - val_mae: 1.5608\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 1.55823\n",
      "Epoch 397/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0736 - mse: 5.0736 - mae: 1.7027 - val_loss: 3.7719 - val_mse: 3.7719 - val_mae: 1.5572\n",
      "\n",
      "Epoch 00397: val_mae improved from 1.55823 to 1.55722, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 398/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9037 - mse: 4.9037 - mae: 1.6973 - val_loss: 3.7662 - val_mse: 3.7662 - val_mae: 1.5542\n",
      "\n",
      "Epoch 00398: val_mae improved from 1.55722 to 1.55421, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 399/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7684 - mse: 5.7684 - mae: 1.8237 - val_loss: 3.7668 - val_mse: 3.7668 - val_mae: 1.5555\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 1.55421\n",
      "Epoch 400/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4726 - mse: 5.4726 - mae: 1.6954 - val_loss: 3.7618 - val_mse: 3.7618 - val_mae: 1.5528\n",
      "\n",
      "Epoch 00400: val_mae improved from 1.55421 to 1.55279, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 401/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0391 - mse: 5.0391 - mae: 1.7287 - val_loss: 3.7624 - val_mse: 3.7624 - val_mae: 1.5540\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 1.55279\n",
      "Epoch 402/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1151 - mse: 5.1151 - mae: 1.7147 - val_loss: 3.7594 - val_mse: 3.7594 - val_mae: 1.5527\n",
      "\n",
      "Epoch 00402: val_mae improved from 1.55279 to 1.55266, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 403/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3708 - mse: 5.3708 - mae: 1.7661 - val_loss: 3.7619 - val_mse: 3.7619 - val_mae: 1.5550\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 1.55266\n",
      "Epoch 404/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3159 - mse: 5.3159 - mae: 1.7567 - val_loss: 3.7553 - val_mse: 3.7553 - val_mae: 1.5509\n",
      "\n",
      "Epoch 00404: val_mae improved from 1.55266 to 1.55088, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 405/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8071 - mse: 4.8071 - mae: 1.7068 - val_loss: 3.7498 - val_mse: 3.7498 - val_mae: 1.5471\n",
      "\n",
      "Epoch 00405: val_mae improved from 1.55088 to 1.54711, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 406/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1077 - mse: 5.1077 - mae: 1.7101 - val_loss: 3.7474 - val_mse: 3.7474 - val_mae: 1.5451\n",
      "\n",
      "Epoch 00406: val_mae improved from 1.54711 to 1.54510, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 407/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0955 - mse: 5.0955 - mae: 1.7579 - val_loss: 3.7493 - val_mse: 3.7493 - val_mae: 1.5481\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 1.54510\n",
      "Epoch 408/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1486 - mse: 5.1486 - mae: 1.7751 - val_loss: 3.7511 - val_mse: 3.7511 - val_mae: 1.5503\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 1.54510\n",
      "Epoch 409/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6526 - mse: 5.6526 - mae: 1.8297 - val_loss: 3.7552 - val_mse: 3.7552 - val_mae: 1.5535\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 1.54510\n",
      "Epoch 410/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6373 - mse: 4.6373 - mae: 1.6559 - val_loss: 3.7568 - val_mse: 3.7568 - val_mae: 1.5549\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 1.54510\n",
      "Epoch 411/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5857 - mse: 5.5857 - mae: 1.8288 - val_loss: 3.7561 - val_mse: 3.7561 - val_mae: 1.5549\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 1.54510\n",
      "Epoch 412/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8817 - mse: 4.8817 - mae: 1.6664 - val_loss: 3.7461 - val_mse: 3.7461 - val_mae: 1.5489\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 1.54510\n",
      "Epoch 413/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9943 - mse: 4.9943 - mae: 1.7404 - val_loss: 3.7500 - val_mse: 3.7500 - val_mae: 1.5520\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 1.54510\n",
      "Epoch 414/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9616 - mse: 4.9616 - mae: 1.6889 - val_loss: 3.7542 - val_mse: 3.7542 - val_mae: 1.5547\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 1.54510\n",
      "Epoch 415/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0328 - mse: 5.0328 - mae: 1.7302 - val_loss: 3.7450 - val_mse: 3.7450 - val_mae: 1.5496\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 1.54510\n",
      "Epoch 416/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5201 - mse: 5.5201 - mae: 1.8348 - val_loss: 3.7440 - val_mse: 3.7440 - val_mae: 1.5495\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 1.54510\n",
      "Epoch 417/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3923 - mse: 5.3923 - mae: 1.7534 - val_loss: 3.7513 - val_mse: 3.7513 - val_mae: 1.5540\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 1.54510\n",
      "Epoch 418/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1144 - mse: 5.1144 - mae: 1.7244 - val_loss: 3.7429 - val_mse: 3.7429 - val_mae: 1.5500\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 1.54510\n",
      "Epoch 419/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3260 - mse: 5.3260 - mae: 1.7396 - val_loss: 3.7471 - val_mse: 3.7471 - val_mae: 1.5527\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 1.54510\n",
      "Epoch 420/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0918 - mse: 5.0918 - mae: 1.7152 - val_loss: 3.7407 - val_mse: 3.7407 - val_mae: 1.5499\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 1.54510\n",
      "Epoch 421/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4362 - mse: 5.4362 - mae: 1.7528 - val_loss: 3.7314 - val_mse: 3.7314 - val_mae: 1.5445\n",
      "\n",
      "Epoch 00421: val_mae improved from 1.54510 to 1.54448, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 422/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8533 - mse: 5.8533 - mae: 1.8564 - val_loss: 3.7287 - val_mse: 3.7287 - val_mae: 1.5428\n",
      "\n",
      "Epoch 00422: val_mae improved from 1.54448 to 1.54283, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 423/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9953 - mse: 4.9953 - mae: 1.7241 - val_loss: 3.7280 - val_mse: 3.7280 - val_mae: 1.5430\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 1.54283\n",
      "Epoch 424/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1843 - mse: 5.1843 - mae: 1.7725 - val_loss: 3.7297 - val_mse: 3.7297 - val_mae: 1.5445\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 1.54283\n",
      "Epoch 425/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5627 - mse: 6.5627 - mae: 1.9482 - val_loss: 3.7277 - val_mse: 3.7277 - val_mae: 1.5437\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 1.54283\n",
      "Epoch 426/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0540 - mse: 5.0540 - mae: 1.7458 - val_loss: 3.7230 - val_mse: 3.7230 - val_mae: 1.5408\n",
      "\n",
      "Epoch 00426: val_mae improved from 1.54283 to 1.54077, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 427/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9871 - mse: 4.9871 - mae: 1.7171 - val_loss: 3.7274 - val_mse: 3.7274 - val_mae: 1.5445\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 1.54077\n",
      "Epoch 428/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5079 - mse: 5.5079 - mae: 1.7555 - val_loss: 3.7277 - val_mse: 3.7277 - val_mae: 1.5453\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 1.54077\n",
      "Epoch 429/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4331 - mse: 5.4331 - mae: 1.8088 - val_loss: 3.7305 - val_mse: 3.7305 - val_mae: 1.5471\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 1.54077\n",
      "Epoch 430/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.6707 - mse: 5.6707 - mae: 1.7862 - val_loss: 3.7329 - val_mse: 3.7329 - val_mae: 1.5486\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 1.54077\n",
      "Epoch 431/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7788 - mse: 5.7788 - mae: 1.7650 - val_loss: 3.7266 - val_mse: 3.7266 - val_mae: 1.5457\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 1.54077\n",
      "Epoch 432/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.1570 - mse: 6.1570 - mae: 1.8494 - val_loss: 3.7203 - val_mse: 3.7203 - val_mae: 1.5426\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 1.54077\n",
      "Epoch 433/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7508 - mse: 4.7508 - mae: 1.7288 - val_loss: 3.7153 - val_mse: 3.7153 - val_mae: 1.5395\n",
      "\n",
      "Epoch 00433: val_mae improved from 1.54077 to 1.53952, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 434/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.9712 - mse: 4.9712 - mae: 1.6871 - val_loss: 3.7159 - val_mse: 3.7159 - val_mae: 1.5401\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 1.53952\n",
      "Epoch 435/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5260 - mse: 5.5260 - mae: 1.7811 - val_loss: 3.7195 - val_mse: 3.7195 - val_mae: 1.5427\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 1.53952\n",
      "Epoch 436/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9360 - mse: 4.9360 - mae: 1.6875 - val_loss: 3.7183 - val_mse: 3.7183 - val_mae: 1.5423\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 1.53952\n",
      "Epoch 437/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.2029 - mse: 5.2029 - mae: 1.7635 - val_loss: 3.7161 - val_mse: 3.7161 - val_mae: 1.5413\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 1.53952\n",
      "Epoch 438/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5438 - mse: 5.5438 - mae: 1.7433 - val_loss: 3.7127 - val_mse: 3.7127 - val_mae: 1.5397\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 1.53952\n",
      "Epoch 439/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.6819 - mse: 6.6819 - mae: 1.9980 - val_loss: 3.7093 - val_mse: 3.7093 - val_mae: 1.5378\n",
      "\n",
      "Epoch 00439: val_mae improved from 1.53952 to 1.53779, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 440/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5611 - mse: 5.5611 - mae: 1.8113 - val_loss: 3.7110 - val_mse: 3.7110 - val_mae: 1.5392\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 1.53779\n",
      "Epoch 441/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2084 - mse: 4.2084 - mae: 1.5627 - val_loss: 3.7071 - val_mse: 3.7071 - val_mae: 1.5371\n",
      "\n",
      "Epoch 00441: val_mae improved from 1.53779 to 1.53711, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 442/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0300 - mse: 5.0300 - mae: 1.6976 - val_loss: 3.7046 - val_mse: 3.7046 - val_mae: 1.5358\n",
      "\n",
      "Epoch 00442: val_mae improved from 1.53711 to 1.53582, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 443/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1038 - mse: 5.1038 - mae: 1.7178 - val_loss: 3.7020 - val_mse: 3.7020 - val_mae: 1.5345\n",
      "\n",
      "Epoch 00443: val_mae improved from 1.53582 to 1.53448, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 444/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.0182 - mse: 6.0182 - mae: 1.8085 - val_loss: 3.7022 - val_mse: 3.7022 - val_mae: 1.5352\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 1.53448\n",
      "Epoch 445/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5533 - mse: 4.5533 - mae: 1.6644 - val_loss: 3.6956 - val_mse: 3.6956 - val_mae: 1.5301\n",
      "\n",
      "Epoch 00445: val_mae improved from 1.53448 to 1.53015, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 446/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.1629 - mse: 5.1629 - mae: 1.7575 - val_loss: 3.6967 - val_mse: 3.6967 - val_mae: 1.5316\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 1.53015\n",
      "Epoch 447/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0557 - mse: 6.0557 - mae: 1.8190 - val_loss: 3.6961 - val_mse: 3.6961 - val_mae: 1.5314\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 1.53015\n",
      "Epoch 448/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9040 - mse: 4.9040 - mae: 1.6795 - val_loss: 3.6974 - val_mse: 3.6974 - val_mae: 1.5333\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 1.53015\n",
      "Epoch 449/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8623 - mse: 5.8623 - mae: 1.8216 - val_loss: 3.6957 - val_mse: 3.6957 - val_mae: 1.5326\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 1.53015\n",
      "Epoch 450/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5071 - mse: 5.5071 - mae: 1.8021 - val_loss: 3.6982 - val_mse: 3.6982 - val_mae: 1.5347\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 1.53015\n",
      "Epoch 451/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0674 - mse: 5.0674 - mae: 1.7286 - val_loss: 3.6937 - val_mse: 3.6937 - val_mae: 1.5324\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 1.53015\n",
      "Epoch 452/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3147 - mse: 6.3147 - mae: 1.8462 - val_loss: 3.6954 - val_mse: 3.6954 - val_mae: 1.5338\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 1.53015\n",
      "Epoch 453/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9996 - mse: 4.9996 - mae: 1.7213 - val_loss: 3.6945 - val_mse: 3.6945 - val_mae: 1.5339\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 1.53015\n",
      "Epoch 454/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4300 - mse: 5.4300 - mae: 1.8004 - val_loss: 3.6919 - val_mse: 3.6919 - val_mae: 1.5325\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 1.53015\n",
      "Epoch 455/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8915 - mse: 4.8915 - mae: 1.7173 - val_loss: 3.6933 - val_mse: 3.6933 - val_mae: 1.5336\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 1.53015\n",
      "Epoch 456/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.6069 - mse: 4.6069 - mae: 1.7154 - val_loss: 3.6933 - val_mse: 3.6933 - val_mae: 1.5341\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 1.53015\n",
      "Epoch 457/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4444 - mse: 5.4444 - mae: 1.8106 - val_loss: 3.6921 - val_mse: 3.6921 - val_mae: 1.5335\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 1.53015\n",
      "Epoch 458/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5724 - mse: 5.5724 - mae: 1.7774 - val_loss: 3.6882 - val_mse: 3.6882 - val_mae: 1.5312\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 1.53015\n",
      "Epoch 459/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9565 - mse: 5.9565 - mae: 1.8362 - val_loss: 3.6868 - val_mse: 3.6868 - val_mae: 1.5306\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 1.53015\n",
      "Epoch 460/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7812 - mse: 5.7812 - mae: 1.7580 - val_loss: 3.6898 - val_mse: 3.6898 - val_mae: 1.5327\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 1.53015\n",
      "Epoch 461/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3351 - mse: 5.3351 - mae: 1.7593 - val_loss: 3.6874 - val_mse: 3.6874 - val_mae: 1.5314\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 1.53015\n",
      "Epoch 462/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3460 - mse: 5.3460 - mae: 1.7433 - val_loss: 3.6884 - val_mse: 3.6884 - val_mae: 1.5321\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 1.53015\n",
      "Epoch 463/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1602 - mse: 5.1602 - mae: 1.7168 - val_loss: 3.6841 - val_mse: 3.6841 - val_mae: 1.5299\n",
      "\n",
      "Epoch 00463: val_mae improved from 1.53015 to 1.52994, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 464/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9668 - mse: 4.9668 - mae: 1.7319 - val_loss: 3.6814 - val_mse: 3.6814 - val_mae: 1.5285\n",
      "\n",
      "Epoch 00464: val_mae improved from 1.52994 to 1.52853, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 465/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5268 - mse: 4.5268 - mae: 1.6438 - val_loss: 3.6803 - val_mse: 3.6803 - val_mae: 1.5281\n",
      "\n",
      "Epoch 00465: val_mae improved from 1.52853 to 1.52811, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 466/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.3759 - mse: 5.3759 - mae: 1.7515 - val_loss: 3.6786 - val_mse: 3.6786 - val_mae: 1.5271\n",
      "\n",
      "Epoch 00466: val_mae improved from 1.52811 to 1.52706, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 467/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5070 - mse: 5.5070 - mae: 1.7434 - val_loss: 3.6799 - val_mse: 3.6799 - val_mae: 1.5283\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 1.52706\n",
      "Epoch 468/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3109 - mse: 5.3109 - mae: 1.7867 - val_loss: 3.6810 - val_mse: 3.6810 - val_mae: 1.5292\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 1.52706\n",
      "Epoch 469/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.0976 - mse: 5.0976 - mae: 1.7127 - val_loss: 3.6770 - val_mse: 3.6770 - val_mae: 1.5269\n",
      "\n",
      "Epoch 00469: val_mae improved from 1.52706 to 1.52692, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 470/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.8883 - mse: 5.8883 - mae: 1.8414 - val_loss: 3.6809 - val_mse: 3.6809 - val_mae: 1.5294\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 1.52692\n",
      "Epoch 471/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.8211 - mse: 4.8211 - mae: 1.6410 - val_loss: 3.6813 - val_mse: 3.6813 - val_mae: 1.5299\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 1.52692\n",
      "Epoch 472/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.4973 - mse: 5.4973 - mae: 1.7903 - val_loss: 3.6785 - val_mse: 3.6785 - val_mae: 1.5286\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 1.52692\n",
      "Epoch 473/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1654 - mse: 6.1654 - mae: 1.8583 - val_loss: 3.6793 - val_mse: 3.6793 - val_mae: 1.5292\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 1.52692\n",
      "Epoch 474/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3226 - mse: 5.3226 - mae: 1.7402 - val_loss: 3.6775 - val_mse: 3.6775 - val_mae: 1.5285\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 1.52692\n",
      "Epoch 475/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7981 - mse: 4.7981 - mae: 1.7037 - val_loss: 3.6760 - val_mse: 3.6760 - val_mae: 1.5279\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 1.52692\n",
      "Epoch 476/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8580 - mse: 5.8580 - mae: 1.7541 - val_loss: 3.6710 - val_mse: 3.6710 - val_mae: 1.5249\n",
      "\n",
      "Epoch 00476: val_mae improved from 1.52692 to 1.52490, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 477/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1430 - mse: 5.1430 - mae: 1.7398 - val_loss: 3.6682 - val_mse: 3.6682 - val_mae: 1.5233\n",
      "\n",
      "Epoch 00477: val_mae improved from 1.52490 to 1.52335, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 478/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2408 - mse: 5.2408 - mae: 1.7139 - val_loss: 3.6690 - val_mse: 3.6690 - val_mae: 1.5242\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 1.52335\n",
      "Epoch 479/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6660 - mse: 4.6660 - mae: 1.6178 - val_loss: 3.6720 - val_mse: 3.6720 - val_mae: 1.5264\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 1.52335\n",
      "Epoch 480/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1350 - mse: 5.1350 - mae: 1.6976 - val_loss: 3.6690 - val_mse: 3.6690 - val_mae: 1.5246\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 1.52335\n",
      "Epoch 481/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7402 - mse: 4.7402 - mae: 1.6997 - val_loss: 3.6648 - val_mse: 3.6648 - val_mae: 1.5219\n",
      "\n",
      "Epoch 00481: val_mae improved from 1.52335 to 1.52192, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 482/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1224 - mse: 5.1224 - mae: 1.7144 - val_loss: 3.6676 - val_mse: 3.6676 - val_mae: 1.5243\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 1.52192\n",
      "Epoch 483/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.1997 - mse: 5.1997 - mae: 1.7453 - val_loss: 3.6694 - val_mse: 3.6694 - val_mae: 1.5256\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 1.52192\n",
      "Epoch 484/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1511 - mse: 5.1511 - mae: 1.7234 - val_loss: 3.6712 - val_mse: 3.6712 - val_mae: 1.5266\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 1.52192\n",
      "Epoch 485/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1907 - mse: 5.1907 - mae: 1.7013 - val_loss: 3.6660 - val_mse: 3.6660 - val_mae: 1.5236\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 1.52192\n",
      "Epoch 486/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4041 - mse: 5.4041 - mae: 1.7498 - val_loss: 3.6642 - val_mse: 3.6642 - val_mae: 1.5227\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 1.52192\n",
      "Epoch 487/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4970 - mse: 5.4970 - mae: 1.7821 - val_loss: 3.6630 - val_mse: 3.6630 - val_mae: 1.5221\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 1.52192\n",
      "Epoch 488/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8218 - mse: 5.8218 - mae: 1.7562 - val_loss: 3.6652 - val_mse: 3.6652 - val_mae: 1.5235\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 1.52192\n",
      "Epoch 489/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9193 - mse: 4.9193 - mae: 1.6876 - val_loss: 3.6648 - val_mse: 3.6648 - val_mae: 1.5235\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 1.52192\n",
      "Epoch 490/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5642 - mse: 4.5642 - mae: 1.6218 - val_loss: 3.6658 - val_mse: 3.6658 - val_mae: 1.5243\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 1.52192\n",
      "Epoch 491/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9890 - mse: 4.9890 - mae: 1.7225 - val_loss: 3.6620 - val_mse: 3.6620 - val_mae: 1.5219\n",
      "\n",
      "Epoch 00491: val_mae improved from 1.52192 to 1.52192, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 492/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0981 - mse: 5.0981 - mae: 1.7434 - val_loss: 3.6622 - val_mse: 3.6622 - val_mae: 1.5223\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 1.52192\n",
      "Epoch 493/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9815 - mse: 4.9815 - mae: 1.6671 - val_loss: 3.6614 - val_mse: 3.6614 - val_mae: 1.5220\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 1.52192\n",
      "Epoch 494/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9697 - mse: 4.9697 - mae: 1.7239 - val_loss: 3.6598 - val_mse: 3.6598 - val_mae: 1.5209\n",
      "\n",
      "Epoch 00494: val_mae improved from 1.52192 to 1.52094, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 495/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6141 - mse: 5.6141 - mae: 1.8134 - val_loss: 3.6559 - val_mse: 3.6559 - val_mae: 1.5184\n",
      "\n",
      "Epoch 00495: val_mae improved from 1.52094 to 1.51837, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 496/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1379 - mse: 5.1379 - mae: 1.7426 - val_loss: 3.6601 - val_mse: 3.6601 - val_mae: 1.5216\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 1.51837\n",
      "Epoch 497/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3172 - mse: 5.3172 - mae: 1.7732 - val_loss: 3.6594 - val_mse: 3.6594 - val_mae: 1.5215\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 1.51837\n",
      "Epoch 498/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6106 - mse: 5.6106 - mae: 1.7232 - val_loss: 3.6595 - val_mse: 3.6595 - val_mae: 1.5218\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 1.51837\n",
      "Epoch 499/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9714 - mse: 4.9714 - mae: 1.7533 - val_loss: 3.6624 - val_mse: 3.6624 - val_mae: 1.5238\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 1.51837\n",
      "Epoch 500/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1385 - mse: 5.1385 - mae: 1.7414 - val_loss: 3.6597 - val_mse: 3.6597 - val_mae: 1.5224\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 1.51837\n",
      "Epoch 501/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0111 - mse: 5.0111 - mae: 1.6787 - val_loss: 3.6603 - val_mse: 3.6603 - val_mae: 1.5230\n",
      "\n",
      "Epoch 00501: val_mae did not improve from 1.51837\n",
      "Epoch 502/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2647 - mse: 5.2647 - mae: 1.6895 - val_loss: 3.6644 - val_mse: 3.6644 - val_mae: 1.5255\n",
      "\n",
      "Epoch 00502: val_mae did not improve from 1.51837\n",
      "Epoch 503/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9336 - mse: 4.9336 - mae: 1.6610 - val_loss: 3.6612 - val_mse: 3.6612 - val_mae: 1.5240\n",
      "\n",
      "Epoch 00503: val_mae did not improve from 1.51837\n",
      "Epoch 504/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8504 - mse: 4.8504 - mae: 1.7414 - val_loss: 3.6646 - val_mse: 3.6646 - val_mae: 1.5260\n",
      "\n",
      "Epoch 00504: val_mae did not improve from 1.51837\n",
      "Epoch 505/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1380 - mse: 5.1380 - mae: 1.7134 - val_loss: 3.6669 - val_mse: 3.6669 - val_mae: 1.5271\n",
      "\n",
      "Epoch 00505: val_mae did not improve from 1.51837\n",
      "Epoch 506/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9152 - mse: 4.9152 - mae: 1.7092 - val_loss: 3.6575 - val_mse: 3.6575 - val_mae: 1.5226\n",
      "\n",
      "Epoch 00506: val_mae did not improve from 1.51837\n",
      "Epoch 507/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5188 - mse: 5.5188 - mae: 1.7384 - val_loss: 3.6529 - val_mse: 3.6529 - val_mae: 1.5198\n",
      "\n",
      "Epoch 00507: val_mae did not improve from 1.51837\n",
      "Epoch 508/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5836 - mse: 5.5836 - mae: 1.8119 - val_loss: 3.6511 - val_mse: 3.6511 - val_mae: 1.5187\n",
      "\n",
      "Epoch 00508: val_mae did not improve from 1.51837\n",
      "Epoch 509/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9223 - mse: 4.9223 - mae: 1.6704 - val_loss: 3.6547 - val_mse: 3.6547 - val_mae: 1.5211\n",
      "\n",
      "Epoch 00509: val_mae did not improve from 1.51837\n",
      "Epoch 510/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1647 - mse: 5.1647 - mae: 1.7124 - val_loss: 3.6630 - val_mse: 3.6630 - val_mae: 1.5254\n",
      "\n",
      "Epoch 00510: val_mae did not improve from 1.51837\n",
      "Epoch 511/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4161 - mse: 5.4161 - mae: 1.7562 - val_loss: 3.6673 - val_mse: 3.6673 - val_mae: 1.5276\n",
      "\n",
      "Epoch 00511: val_mae did not improve from 1.51837\n",
      "Epoch 512/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1184 - mse: 5.1184 - mae: 1.7509 - val_loss: 3.6631 - val_mse: 3.6631 - val_mae: 1.5257\n",
      "\n",
      "Epoch 00512: val_mae did not improve from 1.51837\n",
      "Epoch 513/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7216 - mse: 4.7216 - mae: 1.6554 - val_loss: 3.6533 - val_mse: 3.6533 - val_mae: 1.5208\n",
      "\n",
      "Epoch 00513: val_mae did not improve from 1.51837\n",
      "Epoch 514/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3327 - mse: 5.3327 - mae: 1.7719 - val_loss: 3.6506 - val_mse: 3.6506 - val_mae: 1.5194\n",
      "\n",
      "Epoch 00514: val_mae did not improve from 1.51837\n",
      "Epoch 515/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3035 - mse: 5.3035 - mae: 1.7425 - val_loss: 3.6575 - val_mse: 3.6575 - val_mae: 1.5236\n",
      "\n",
      "Epoch 00515: val_mae did not improve from 1.51837\n",
      "Epoch 516/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8402 - mse: 5.8402 - mae: 1.8284 - val_loss: 3.6562 - val_mse: 3.6562 - val_mae: 1.5230\n",
      "\n",
      "Epoch 00516: val_mae did not improve from 1.51837\n",
      "Epoch 517/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5793 - mse: 4.5793 - mae: 1.6473 - val_loss: 3.6513 - val_mse: 3.6513 - val_mae: 1.5205\n",
      "\n",
      "Epoch 00517: val_mae did not improve from 1.51837\n",
      "Epoch 518/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5633 - mse: 5.5633 - mae: 1.7632 - val_loss: 3.6583 - val_mse: 3.6583 - val_mae: 1.5241\n",
      "\n",
      "Epoch 00518: val_mae did not improve from 1.51837\n",
      "Epoch 519/1000\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.1446 - mse: 6.1446 - mae: 1.8235 - val_loss: 3.6498 - val_mse: 3.6498 - val_mae: 1.5198\n",
      "\n",
      "Epoch 00519: val_mae did not improve from 1.51837\n",
      "Epoch 520/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1019 - mse: 5.1019 - mae: 1.7037 - val_loss: 3.6474 - val_mse: 3.6474 - val_mae: 1.5182\n",
      "\n",
      "Epoch 00520: val_mae improved from 1.51837 to 1.51818, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 521/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5958 - mse: 5.5958 - mae: 1.7917 - val_loss: 3.6465 - val_mse: 3.6465 - val_mae: 1.5179\n",
      "\n",
      "Epoch 00521: val_mae improved from 1.51818 to 1.51794, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 522/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5266 - mse: 5.5266 - mae: 1.7276 - val_loss: 3.6457 - val_mse: 3.6457 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00522: val_mae improved from 1.51794 to 1.51770, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 523/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6709 - mse: 4.6709 - mae: 1.6511 - val_loss: 3.6444 - val_mse: 3.6444 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00523: val_mae improved from 1.51770 to 1.51722, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 524/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9589 - mse: 5.9589 - mae: 1.8162 - val_loss: 3.6411 - val_mse: 3.6411 - val_mae: 1.5152\n",
      "\n",
      "Epoch 00524: val_mae improved from 1.51722 to 1.51516, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 525/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7151 - mse: 5.7151 - mae: 1.7465 - val_loss: 3.6425 - val_mse: 3.6425 - val_mae: 1.5163\n",
      "\n",
      "Epoch 00525: val_mae did not improve from 1.51516\n",
      "Epoch 526/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1596 - mse: 5.1596 - mae: 1.7490 - val_loss: 3.6428 - val_mse: 3.6428 - val_mae: 1.5169\n",
      "\n",
      "Epoch 00526: val_mae did not improve from 1.51516\n",
      "Epoch 527/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6016 - mse: 5.6016 - mae: 1.8089 - val_loss: 3.6463 - val_mse: 3.6463 - val_mae: 1.5189\n",
      "\n",
      "Epoch 00527: val_mae did not improve from 1.51516\n",
      "Epoch 528/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7472 - mse: 4.7472 - mae: 1.6542 - val_loss: 3.6431 - val_mse: 3.6431 - val_mae: 1.5172\n",
      "\n",
      "Epoch 00528: val_mae did not improve from 1.51516\n",
      "Epoch 529/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3688 - mse: 5.3688 - mae: 1.6999 - val_loss: 3.6462 - val_mse: 3.6462 - val_mae: 1.5191\n",
      "\n",
      "Epoch 00529: val_mae did not improve from 1.51516\n",
      "Epoch 530/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8882 - mse: 4.8882 - mae: 1.6790 - val_loss: 3.6417 - val_mse: 3.6417 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00530: val_mae did not improve from 1.51516\n",
      "Epoch 531/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0710 - mse: 5.0710 - mae: 1.7269 - val_loss: 3.6478 - val_mse: 3.6478 - val_mae: 1.5202\n",
      "\n",
      "Epoch 00531: val_mae did not improve from 1.51516\n",
      "Epoch 532/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1465 - mse: 5.1465 - mae: 1.6549 - val_loss: 3.6475 - val_mse: 3.6475 - val_mae: 1.5202\n",
      "\n",
      "Epoch 00532: val_mae did not improve from 1.51516\n",
      "Epoch 533/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1806 - mse: 5.1806 - mae: 1.6873 - val_loss: 3.6491 - val_mse: 3.6491 - val_mae: 1.5211\n",
      "\n",
      "Epoch 00533: val_mae did not improve from 1.51516\n",
      "Epoch 534/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4655 - mse: 4.4655 - mae: 1.6328 - val_loss: 3.6438 - val_mse: 3.6438 - val_mae: 1.5186\n",
      "\n",
      "Epoch 00534: val_mae did not improve from 1.51516\n",
      "Epoch 535/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5830 - mse: 5.5830 - mae: 1.8075 - val_loss: 3.6420 - val_mse: 3.6420 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00535: val_mae did not improve from 1.51516\n",
      "Epoch 536/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8476 - mse: 4.8476 - mae: 1.7232 - val_loss: 3.6393 - val_mse: 3.6393 - val_mae: 1.5161\n",
      "\n",
      "Epoch 00536: val_mae did not improve from 1.51516\n",
      "Epoch 537/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7418 - mse: 4.7418 - mae: 1.6529 - val_loss: 3.6388 - val_mse: 3.6388 - val_mae: 1.5159\n",
      "\n",
      "Epoch 00537: val_mae did not improve from 1.51516\n",
      "Epoch 538/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3213 - mse: 5.3213 - mae: 1.7201 - val_loss: 3.6442 - val_mse: 3.6442 - val_mae: 1.5191\n",
      "\n",
      "Epoch 00538: val_mae did not improve from 1.51516\n",
      "Epoch 539/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5837 - mse: 5.5837 - mae: 1.7488 - val_loss: 3.6452 - val_mse: 3.6452 - val_mae: 1.5195\n",
      "\n",
      "Epoch 00539: val_mae did not improve from 1.51516\n",
      "Epoch 540/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0099 - mse: 5.0099 - mae: 1.6657 - val_loss: 3.6428 - val_mse: 3.6428 - val_mae: 1.5184\n",
      "\n",
      "Epoch 00540: val_mae did not improve from 1.51516\n",
      "Epoch 541/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5720 - mse: 5.5720 - mae: 1.8120 - val_loss: 3.6357 - val_mse: 3.6357 - val_mae: 1.5143\n",
      "\n",
      "Epoch 00541: val_mae improved from 1.51516 to 1.51434, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 542/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6388 - mse: 4.6388 - mae: 1.6682 - val_loss: 3.6384 - val_mse: 3.6384 - val_mae: 1.5160\n",
      "\n",
      "Epoch 00542: val_mae did not improve from 1.51434\n",
      "Epoch 543/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8046 - mse: 5.8046 - mae: 1.8401 - val_loss: 3.6427 - val_mse: 3.6427 - val_mae: 1.5185\n",
      "\n",
      "Epoch 00543: val_mae did not improve from 1.51434\n",
      "Epoch 544/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2839 - mse: 5.2839 - mae: 1.7231 - val_loss: 3.6403 - val_mse: 3.6403 - val_mae: 1.5173\n",
      "\n",
      "Epoch 00544: val_mae did not improve from 1.51434\n",
      "Epoch 545/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3913 - mse: 4.3913 - mae: 1.6286 - val_loss: 3.6325 - val_mse: 3.6325 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00545: val_mae improved from 1.51434 to 1.51276, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 546/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0244 - mse: 5.0244 - mae: 1.7586 - val_loss: 3.6331 - val_mse: 3.6331 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00546: val_mae did not improve from 1.51276\n",
      "Epoch 547/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1336 - mse: 5.1336 - mae: 1.7305 - val_loss: 3.6332 - val_mse: 3.6332 - val_mae: 1.5135\n",
      "\n",
      "Epoch 00547: val_mae did not improve from 1.51276\n",
      "Epoch 548/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4220 - mse: 5.4220 - mae: 1.7293 - val_loss: 3.6329 - val_mse: 3.6329 - val_mae: 1.5134\n",
      "\n",
      "Epoch 00548: val_mae did not improve from 1.51276\n",
      "Epoch 549/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7876 - mse: 5.7876 - mae: 1.8160 - val_loss: 3.6382 - val_mse: 3.6382 - val_mae: 1.5166\n",
      "\n",
      "Epoch 00549: val_mae did not improve from 1.51276\n",
      "Epoch 550/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1859 - mse: 5.1859 - mae: 1.7404 - val_loss: 3.6368 - val_mse: 3.6368 - val_mae: 1.5159\n",
      "\n",
      "Epoch 00550: val_mae did not improve from 1.51276\n",
      "Epoch 551/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4874 - mse: 5.4874 - mae: 1.7470 - val_loss: 3.6348 - val_mse: 3.6348 - val_mae: 1.5148\n",
      "\n",
      "Epoch 00551: val_mae did not improve from 1.51276\n",
      "Epoch 552/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6765 - mse: 4.6765 - mae: 1.6606 - val_loss: 3.6331 - val_mse: 3.6331 - val_mae: 1.5140\n",
      "\n",
      "Epoch 00552: val_mae did not improve from 1.51276\n",
      "Epoch 553/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8638 - mse: 5.8638 - mae: 1.7954 - val_loss: 3.6357 - val_mse: 3.6357 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00553: val_mae did not improve from 1.51276\n",
      "Epoch 554/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4990 - mse: 5.4990 - mae: 1.8176 - val_loss: 3.6304 - val_mse: 3.6304 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00554: val_mae improved from 1.51276 to 1.51227, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 555/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4589 - mse: 4.4589 - mae: 1.6733 - val_loss: 3.6315 - val_mse: 3.6315 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00555: val_mae did not improve from 1.51227\n",
      "Epoch 556/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2477 - mse: 5.2477 - mae: 1.7324 - val_loss: 3.6390 - val_mse: 3.6390 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00556: val_mae did not improve from 1.51227\n",
      "Epoch 557/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8974 - mse: 5.8974 - mae: 1.7708 - val_loss: 3.6407 - val_mse: 3.6407 - val_mae: 1.5184\n",
      "\n",
      "Epoch 00557: val_mae did not improve from 1.51227\n",
      "Epoch 558/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8994 - mse: 4.8994 - mae: 1.6422 - val_loss: 3.6416 - val_mse: 3.6416 - val_mae: 1.5189\n",
      "\n",
      "Epoch 00558: val_mae did not improve from 1.51227\n",
      "Epoch 559/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8216 - mse: 5.8216 - mae: 1.8138 - val_loss: 3.6318 - val_mse: 3.6318 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00559: val_mae did not improve from 1.51227\n",
      "Epoch 560/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3096 - mse: 5.3096 - mae: 1.7172 - val_loss: 3.6280 - val_mse: 3.6280 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00560: val_mae improved from 1.51227 to 1.51125, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 561/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2961 - mse: 5.2961 - mae: 1.7101 - val_loss: 3.6330 - val_mse: 3.6330 - val_mae: 1.5142\n",
      "\n",
      "Epoch 00561: val_mae did not improve from 1.51125\n",
      "Epoch 562/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6487 - mse: 5.6487 - mae: 1.7262 - val_loss: 3.6310 - val_mse: 3.6310 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00562: val_mae did not improve from 1.51125\n",
      "Epoch 563/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1892 - mse: 5.1892 - mae: 1.7406 - val_loss: 3.6303 - val_mse: 3.6303 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00563: val_mae did not improve from 1.51125\n",
      "Epoch 564/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6232 - mse: 5.6232 - mae: 1.7855 - val_loss: 3.6312 - val_mse: 3.6312 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00564: val_mae did not improve from 1.51125\n",
      "Epoch 565/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9690 - mse: 4.9690 - mae: 1.6711 - val_loss: 3.6292 - val_mse: 3.6292 - val_mae: 1.5121\n",
      "\n",
      "Epoch 00565: val_mae did not improve from 1.51125\n",
      "Epoch 566/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7614 - mse: 4.7614 - mae: 1.6562 - val_loss: 3.6278 - val_mse: 3.6278 - val_mae: 1.5114\n",
      "\n",
      "Epoch 00566: val_mae did not improve from 1.51125\n",
      "Epoch 567/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7868 - mse: 4.7868 - mae: 1.6482 - val_loss: 3.6320 - val_mse: 3.6320 - val_mae: 1.5140\n",
      "\n",
      "Epoch 00567: val_mae did not improve from 1.51125\n",
      "Epoch 568/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9416 - mse: 4.9416 - mae: 1.7349 - val_loss: 3.6300 - val_mse: 3.6300 - val_mae: 1.5129\n",
      "\n",
      "Epoch 00568: val_mae did not improve from 1.51125\n",
      "Epoch 569/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7591 - mse: 4.7591 - mae: 1.6405 - val_loss: 3.6342 - val_mse: 3.6342 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00569: val_mae did not improve from 1.51125\n",
      "Epoch 570/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3938 - mse: 4.3938 - mae: 1.6400 - val_loss: 3.6331 - val_mse: 3.6331 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00570: val_mae did not improve from 1.51125\n",
      "Epoch 571/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2646 - mse: 5.2646 - mae: 1.6857 - val_loss: 3.6344 - val_mse: 3.6344 - val_mae: 1.5155\n",
      "\n",
      "Epoch 00571: val_mae did not improve from 1.51125\n",
      "Epoch 572/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0529 - mse: 5.0529 - mae: 1.7353 - val_loss: 3.6364 - val_mse: 3.6364 - val_mae: 1.5167\n",
      "\n",
      "Epoch 00572: val_mae did not improve from 1.51125\n",
      "Epoch 573/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4340 - mse: 5.4340 - mae: 1.7698 - val_loss: 3.6389 - val_mse: 3.6389 - val_mae: 1.5180\n",
      "\n",
      "Epoch 00573: val_mae did not improve from 1.51125\n",
      "Epoch 574/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9455 - mse: 4.9455 - mae: 1.6710 - val_loss: 3.6310 - val_mse: 3.6310 - val_mae: 1.5137\n",
      "\n",
      "Epoch 00574: val_mae did not improve from 1.51125\n",
      "Epoch 575/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0880 - mse: 5.0880 - mae: 1.6906 - val_loss: 3.6316 - val_mse: 3.6316 - val_mae: 1.5140\n",
      "\n",
      "Epoch 00575: val_mae did not improve from 1.51125\n",
      "Epoch 576/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7915 - mse: 4.7915 - mae: 1.6616 - val_loss: 3.6340 - val_mse: 3.6340 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00576: val_mae did not improve from 1.51125\n",
      "Epoch 577/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8454 - mse: 4.8454 - mae: 1.7003 - val_loss: 3.6327 - val_mse: 3.6327 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00577: val_mae did not improve from 1.51125\n",
      "Epoch 578/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.8444 - mse: 5.8444 - mae: 1.7553 - val_loss: 3.6301 - val_mse: 3.6301 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00578: val_mae did not improve from 1.51125\n",
      "Epoch 579/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5626 - mse: 5.5626 - mae: 1.7340 - val_loss: 3.6269 - val_mse: 3.6269 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00579: val_mae did not improve from 1.51125\n",
      "Epoch 580/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0953 - mse: 5.0953 - mae: 1.7170 - val_loss: 3.6295 - val_mse: 3.6295 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00580: val_mae did not improve from 1.51125\n",
      "Epoch 581/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9990 - mse: 4.9990 - mae: 1.6941 - val_loss: 3.6290 - val_mse: 3.6290 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00581: val_mae did not improve from 1.51125\n",
      "Epoch 582/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.0712 - mse: 6.0712 - mae: 1.8817 - val_loss: 3.6310 - val_mse: 3.6310 - val_mae: 1.5140\n",
      "\n",
      "Epoch 00582: val_mae did not improve from 1.51125\n",
      "Epoch 583/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4511 - mse: 5.4511 - mae: 1.7612 - val_loss: 3.6294 - val_mse: 3.6294 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00583: val_mae did not improve from 1.51125\n",
      "Epoch 584/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4188 - mse: 5.4188 - mae: 1.7886 - val_loss: 3.6258 - val_mse: 3.6258 - val_mae: 1.5108\n",
      "\n",
      "Epoch 00584: val_mae improved from 1.51125 to 1.51080, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 585/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.2626 - mse: 5.2626 - mae: 1.7687 - val_loss: 3.6282 - val_mse: 3.6282 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00585: val_mae did not improve from 1.51080\n",
      "Epoch 586/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3472 - mse: 5.3472 - mae: 1.7318 - val_loss: 3.6316 - val_mse: 3.6316 - val_mae: 1.5142\n",
      "\n",
      "Epoch 00586: val_mae did not improve from 1.51080\n",
      "Epoch 587/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0829 - mse: 5.0829 - mae: 1.7278 - val_loss: 3.6249 - val_mse: 3.6249 - val_mae: 1.5099\n",
      "\n",
      "Epoch 00587: val_mae improved from 1.51080 to 1.50990, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 588/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.6932 - mse: 4.6932 - mae: 1.6586 - val_loss: 3.6317 - val_mse: 3.6317 - val_mae: 1.5142\n",
      "\n",
      "Epoch 00588: val_mae did not improve from 1.50990\n",
      "Epoch 589/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8592 - mse: 4.8592 - mae: 1.6484 - val_loss: 3.6322 - val_mse: 3.6322 - val_mae: 1.5146\n",
      "\n",
      "Epoch 00589: val_mae did not improve from 1.50990\n",
      "Epoch 590/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3340 - mse: 5.3340 - mae: 1.7078 - val_loss: 3.6370 - val_mse: 3.6370 - val_mae: 1.5173\n",
      "\n",
      "Epoch 00590: val_mae did not improve from 1.50990\n",
      "Epoch 591/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3723 - mse: 5.3723 - mae: 1.7848 - val_loss: 3.6295 - val_mse: 3.6295 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00591: val_mae did not improve from 1.50990\n",
      "Epoch 592/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2509 - mse: 5.2509 - mae: 1.7269 - val_loss: 3.6275 - val_mse: 3.6275 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00592: val_mae did not improve from 1.50990\n",
      "Epoch 593/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7279 - mse: 4.7279 - mae: 1.6563 - val_loss: 3.6298 - val_mse: 3.6298 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00593: val_mae did not improve from 1.50990\n",
      "Epoch 594/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9190 - mse: 4.9190 - mae: 1.6767 - val_loss: 3.6276 - val_mse: 3.6276 - val_mae: 1.5119\n",
      "\n",
      "Epoch 00594: val_mae did not improve from 1.50990\n",
      "Epoch 595/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3431 - mse: 5.3431 - mae: 1.7530 - val_loss: 3.6303 - val_mse: 3.6303 - val_mae: 1.5137\n",
      "\n",
      "Epoch 00595: val_mae did not improve from 1.50990\n",
      "Epoch 596/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6677 - mse: 5.6677 - mae: 1.7466 - val_loss: 3.6250 - val_mse: 3.6250 - val_mae: 1.5106\n",
      "\n",
      "Epoch 00596: val_mae did not improve from 1.50990\n",
      "Epoch 597/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0068 - mse: 5.0068 - mae: 1.7288 - val_loss: 3.6259 - val_mse: 3.6259 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00597: val_mae did not improve from 1.50990\n",
      "Epoch 598/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8028 - mse: 4.8028 - mae: 1.6625 - val_loss: 3.6290 - val_mse: 3.6290 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00598: val_mae did not improve from 1.50990\n",
      "Epoch 599/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2719 - mse: 5.2719 - mae: 1.6873 - val_loss: 3.6283 - val_mse: 3.6283 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00599: val_mae did not improve from 1.50990\n",
      "Epoch 600/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1109 - mse: 5.1109 - mae: 1.7425 - val_loss: 3.6300 - val_mse: 3.6300 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00600: val_mae did not improve from 1.50990\n",
      "Epoch 601/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5371 - mse: 5.5371 - mae: 1.8263 - val_loss: 3.6293 - val_mse: 3.6293 - val_mae: 1.5135\n",
      "\n",
      "Epoch 00601: val_mae did not improve from 1.50990\n",
      "Epoch 602/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9194 - mse: 4.9194 - mae: 1.6928 - val_loss: 3.6254 - val_mse: 3.6254 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00602: val_mae did not improve from 1.50990\n",
      "Epoch 603/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6202 - mse: 4.6202 - mae: 1.6229 - val_loss: 3.6245 - val_mse: 3.6245 - val_mae: 1.5107\n",
      "\n",
      "Epoch 00603: val_mae did not improve from 1.50990\n",
      "Epoch 604/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1487 - mse: 5.1487 - mae: 1.6990 - val_loss: 3.6246 - val_mse: 3.6246 - val_mae: 1.5106\n",
      "\n",
      "Epoch 00604: val_mae did not improve from 1.50990\n",
      "Epoch 605/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2179 - mse: 5.2179 - mae: 1.7587 - val_loss: 3.6290 - val_mse: 3.6290 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00605: val_mae did not improve from 1.50990\n",
      "Epoch 606/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6833 - mse: 5.6833 - mae: 1.8220 - val_loss: 3.6326 - val_mse: 3.6326 - val_mae: 1.5153\n",
      "\n",
      "Epoch 00606: val_mae did not improve from 1.50990\n",
      "Epoch 607/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4238 - mse: 5.4238 - mae: 1.7448 - val_loss: 3.6368 - val_mse: 3.6368 - val_mae: 1.5177\n",
      "\n",
      "Epoch 00607: val_mae did not improve from 1.50990\n",
      "Epoch 608/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8021 - mse: 4.8021 - mae: 1.6970 - val_loss: 3.6360 - val_mse: 3.6360 - val_mae: 1.5174\n",
      "\n",
      "Epoch 00608: val_mae did not improve from 1.50990\n",
      "Epoch 609/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2093 - mse: 5.2093 - mae: 1.7203 - val_loss: 3.6336 - val_mse: 3.6336 - val_mae: 1.5162\n",
      "\n",
      "Epoch 00609: val_mae did not improve from 1.50990\n",
      "Epoch 610/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2249 - mse: 5.2249 - mae: 1.7684 - val_loss: 3.6299 - val_mse: 3.6299 - val_mae: 1.5142\n",
      "\n",
      "Epoch 00610: val_mae did not improve from 1.50990\n",
      "Epoch 611/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6065 - mse: 5.6065 - mae: 1.8317 - val_loss: 3.6281 - val_mse: 3.6281 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00611: val_mae did not improve from 1.50990\n",
      "Epoch 612/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7683 - mse: 5.7683 - mae: 1.8201 - val_loss: 3.6279 - val_mse: 3.6279 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00612: val_mae did not improve from 1.50990\n",
      "Epoch 613/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1947 - mse: 5.1947 - mae: 1.7632 - val_loss: 3.6274 - val_mse: 3.6274 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00613: val_mae did not improve from 1.50990\n",
      "Epoch 614/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4842 - mse: 4.4842 - mae: 1.6150 - val_loss: 3.6324 - val_mse: 3.6324 - val_mae: 1.5159\n",
      "\n",
      "Epoch 00614: val_mae did not improve from 1.50990\n",
      "Epoch 615/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8737 - mse: 4.8737 - mae: 1.7467 - val_loss: 3.6277 - val_mse: 3.6277 - val_mae: 1.5133\n",
      "\n",
      "Epoch 00615: val_mae did not improve from 1.50990\n",
      "Epoch 616/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0062 - mse: 5.0062 - mae: 1.6889 - val_loss: 3.6250 - val_mse: 3.6250 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00616: val_mae did not improve from 1.50990\n",
      "Epoch 617/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0997 - mse: 5.0997 - mae: 1.6963 - val_loss: 3.6284 - val_mse: 3.6284 - val_mae: 1.5138\n",
      "\n",
      "Epoch 00617: val_mae did not improve from 1.50990\n",
      "Epoch 618/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3442 - mse: 6.3442 - mae: 1.8703 - val_loss: 3.6302 - val_mse: 3.6302 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00618: val_mae did not improve from 1.50990\n",
      "Epoch 619/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9309 - mse: 5.9309 - mae: 1.8294 - val_loss: 3.6246 - val_mse: 3.6246 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00619: val_mae did not improve from 1.50990\n",
      "Epoch 620/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.1389 - mse: 6.1389 - mae: 1.7878 - val_loss: 3.6286 - val_mse: 3.6286 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00620: val_mae did not improve from 1.50990\n",
      "Epoch 621/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7581 - mse: 4.7581 - mae: 1.6813 - val_loss: 3.6262 - val_mse: 3.6262 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00621: val_mae did not improve from 1.50990\n",
      "Epoch 622/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.1681 - mse: 5.1681 - mae: 1.7410 - val_loss: 3.6269 - val_mse: 3.6269 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00622: val_mae did not improve from 1.50990\n",
      "Epoch 623/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5757 - mse: 5.5757 - mae: 1.7422 - val_loss: 3.6241 - val_mse: 3.6241 - val_mae: 1.5112\n",
      "\n",
      "Epoch 00623: val_mae did not improve from 1.50990\n",
      "Epoch 624/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4520 - mse: 5.4520 - mae: 1.7826 - val_loss: 3.6318 - val_mse: 3.6318 - val_mae: 1.5156\n",
      "\n",
      "Epoch 00624: val_mae did not improve from 1.50990\n",
      "Epoch 625/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2430 - mse: 5.2430 - mae: 1.7464 - val_loss: 3.6307 - val_mse: 3.6307 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00625: val_mae did not improve from 1.50990\n",
      "Epoch 626/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2624 - mse: 5.2624 - mae: 1.7526 - val_loss: 3.6247 - val_mse: 3.6247 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00626: val_mae did not improve from 1.50990\n",
      "Epoch 627/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4335 - mse: 5.4335 - mae: 1.7577 - val_loss: 3.6269 - val_mse: 3.6269 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00627: val_mae did not improve from 1.50990\n",
      "Epoch 628/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6792 - mse: 4.6792 - mae: 1.6424 - val_loss: 3.6218 - val_mse: 3.6218 - val_mae: 1.5101\n",
      "\n",
      "Epoch 00628: val_mae did not improve from 1.50990\n",
      "Epoch 629/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7090 - mse: 5.7090 - mae: 1.8366 - val_loss: 3.6232 - val_mse: 3.6232 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00629: val_mae did not improve from 1.50990\n",
      "Epoch 630/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2193 - mse: 5.2193 - mae: 1.6990 - val_loss: 3.6213 - val_mse: 3.6213 - val_mae: 1.5098\n",
      "\n",
      "Epoch 00630: val_mae improved from 1.50990 to 1.50983, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 631/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5553 - mse: 4.5553 - mae: 1.6305 - val_loss: 3.6213 - val_mse: 3.6213 - val_mae: 1.5098\n",
      "\n",
      "Epoch 00631: val_mae improved from 1.50983 to 1.50980, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 632/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.9800 - mse: 4.9800 - mae: 1.6751 - val_loss: 3.6214 - val_mse: 3.6214 - val_mae: 1.5099\n",
      "\n",
      "Epoch 00632: val_mae did not improve from 1.50980\n",
      "Epoch 633/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0481 - mse: 5.0481 - mae: 1.6681 - val_loss: 3.6241 - val_mse: 3.6241 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00633: val_mae did not improve from 1.50980\n",
      "Epoch 634/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8190 - mse: 4.8190 - mae: 1.6670 - val_loss: 3.6264 - val_mse: 3.6264 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00634: val_mae did not improve from 1.50980\n",
      "Epoch 635/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4962 - mse: 5.4962 - mae: 1.7631 - val_loss: 3.6249 - val_mse: 3.6249 - val_mae: 1.5121\n",
      "\n",
      "Epoch 00635: val_mae did not improve from 1.50980\n",
      "Epoch 636/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8524 - mse: 4.8524 - mae: 1.6884 - val_loss: 3.6221 - val_mse: 3.6221 - val_mae: 1.5106\n",
      "\n",
      "Epoch 00636: val_mae did not improve from 1.50980\n",
      "Epoch 637/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4905 - mse: 4.4905 - mae: 1.6161 - val_loss: 3.6193 - val_mse: 3.6193 - val_mae: 1.5089\n",
      "\n",
      "Epoch 00637: val_mae improved from 1.50980 to 1.50893, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 638/1000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.4938 - mse: 5.4938 - mae: 1.7235 - val_loss: 3.6224 - val_mse: 3.6224 - val_mae: 1.5109\n",
      "\n",
      "Epoch 00638: val_mae did not improve from 1.50893\n",
      "Epoch 639/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8829 - mse: 4.8829 - mae: 1.6736 - val_loss: 3.6257 - val_mse: 3.6257 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00639: val_mae did not improve from 1.50893\n",
      "Epoch 640/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9454 - mse: 4.9454 - mae: 1.7098 - val_loss: 3.6291 - val_mse: 3.6291 - val_mae: 1.5148\n",
      "\n",
      "Epoch 00640: val_mae did not improve from 1.50893\n",
      "Epoch 641/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0921 - mse: 5.0921 - mae: 1.7543 - val_loss: 3.6310 - val_mse: 3.6310 - val_mae: 1.5159\n",
      "\n",
      "Epoch 00641: val_mae did not improve from 1.50893\n",
      "Epoch 642/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2193 - mse: 5.2193 - mae: 1.7710 - val_loss: 3.6234 - val_mse: 3.6234 - val_mae: 1.5117\n",
      "\n",
      "Epoch 00642: val_mae did not improve from 1.50893\n",
      "Epoch 643/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9072 - mse: 5.9072 - mae: 1.8186 - val_loss: 3.6209 - val_mse: 3.6209 - val_mae: 1.5102\n",
      "\n",
      "Epoch 00643: val_mae did not improve from 1.50893\n",
      "Epoch 644/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1522 - mse: 5.1522 - mae: 1.6821 - val_loss: 3.6208 - val_mse: 3.6208 - val_mae: 1.5101\n",
      "\n",
      "Epoch 00644: val_mae did not improve from 1.50893\n",
      "Epoch 645/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5703 - mse: 4.5703 - mae: 1.6146 - val_loss: 3.6221 - val_mse: 3.6221 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00645: val_mae did not improve from 1.50893\n",
      "Epoch 646/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9761 - mse: 4.9761 - mae: 1.6972 - val_loss: 3.6203 - val_mse: 3.6203 - val_mae: 1.5098\n",
      "\n",
      "Epoch 00646: val_mae did not improve from 1.50893\n",
      "Epoch 647/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6005 - mse: 5.6005 - mae: 1.7611 - val_loss: 3.6257 - val_mse: 3.6257 - val_mae: 1.5130\n",
      "\n",
      "Epoch 00647: val_mae did not improve from 1.50893\n",
      "Epoch 648/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9976 - mse: 5.9976 - mae: 1.8524 - val_loss: 3.6213 - val_mse: 3.6213 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00648: val_mae did not improve from 1.50893\n",
      "Epoch 649/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7177 - mse: 5.7177 - mae: 1.7632 - val_loss: 3.6194 - val_mse: 3.6194 - val_mae: 1.5093\n",
      "\n",
      "Epoch 00649: val_mae did not improve from 1.50893\n",
      "Epoch 650/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0898 - mse: 5.0898 - mae: 1.7004 - val_loss: 3.6165 - val_mse: 3.6165 - val_mae: 1.5059\n",
      "\n",
      "Epoch 00650: val_mae improved from 1.50893 to 1.50593, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 651/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5642 - mse: 4.5642 - mae: 1.6268 - val_loss: 3.6169 - val_mse: 3.6169 - val_mae: 1.5052\n",
      "\n",
      "Epoch 00651: val_mae improved from 1.50593 to 1.50522, saving model to /home/m-marouni/Documents/CE-901/Heathrow/best_weights\n",
      "INFO:tensorflow:Assets written to: /home/m-marouni/Documents/CE-901/Heathrow/best_weights/assets\n",
      "Epoch 652/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.9032 - mse: 5.9032 - mae: 1.7765 - val_loss: 3.6183 - val_mse: 3.6183 - val_mae: 1.5076\n",
      "\n",
      "Epoch 00652: val_mae did not improve from 1.50522\n",
      "Epoch 653/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2205 - mse: 5.2205 - mae: 1.7497 - val_loss: 3.6173 - val_mse: 3.6173 - val_mae: 1.5067\n",
      "\n",
      "Epoch 00653: val_mae did not improve from 1.50522\n",
      "Epoch 654/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6356 - mse: 4.6356 - mae: 1.6490 - val_loss: 3.6229 - val_mse: 3.6229 - val_mae: 1.5113\n",
      "\n",
      "Epoch 00654: val_mae did not improve from 1.50522\n",
      "Epoch 655/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3977 - mse: 5.3977 - mae: 1.7304 - val_loss: 3.6239 - val_mse: 3.6239 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00655: val_mae did not improve from 1.50522\n",
      "Epoch 656/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6331 - mse: 4.6331 - mae: 1.6304 - val_loss: 3.6225 - val_mse: 3.6225 - val_mae: 1.5112\n",
      "\n",
      "Epoch 00656: val_mae did not improve from 1.50522\n",
      "Epoch 657/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.7223 - mse: 5.7223 - mae: 1.7937 - val_loss: 3.6186 - val_mse: 3.6186 - val_mae: 1.5087\n",
      "\n",
      "Epoch 00657: val_mae did not improve from 1.50522\n",
      "Epoch 658/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4687 - mse: 5.4687 - mae: 1.8037 - val_loss: 3.6198 - val_mse: 3.6198 - val_mae: 1.5095\n",
      "\n",
      "Epoch 00658: val_mae did not improve from 1.50522\n",
      "Epoch 659/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0983 - mse: 5.0983 - mae: 1.7330 - val_loss: 3.6197 - val_mse: 3.6197 - val_mae: 1.5095\n",
      "\n",
      "Epoch 00659: val_mae did not improve from 1.50522\n",
      "Epoch 660/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.2496 - mse: 5.2496 - mae: 1.7493 - val_loss: 3.6230 - val_mse: 3.6230 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00660: val_mae did not improve from 1.50522\n",
      "Epoch 661/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3984 - mse: 5.3984 - mae: 1.7525 - val_loss: 3.6237 - val_mse: 3.6237 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00661: val_mae did not improve from 1.50522\n",
      "Epoch 662/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.9198 - mse: 5.9198 - mae: 1.8556 - val_loss: 3.6227 - val_mse: 3.6227 - val_mae: 1.5115\n",
      "\n",
      "Epoch 00662: val_mae did not improve from 1.50522\n",
      "Epoch 663/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7039 - mse: 4.7039 - mae: 1.6171 - val_loss: 3.6220 - val_mse: 3.6220 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00663: val_mae did not improve from 1.50522\n",
      "Epoch 664/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8815 - mse: 4.8815 - mae: 1.6870 - val_loss: 3.6239 - val_mse: 3.6239 - val_mae: 1.5122\n",
      "\n",
      "Epoch 00664: val_mae did not improve from 1.50522\n",
      "Epoch 665/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8527 - mse: 4.8527 - mae: 1.6588 - val_loss: 3.6199 - val_mse: 3.6199 - val_mae: 1.5101\n",
      "\n",
      "Epoch 00665: val_mae did not improve from 1.50522\n",
      "Epoch 666/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.7039 - mse: 5.7039 - mae: 1.7956 - val_loss: 3.6173 - val_mse: 3.6173 - val_mae: 1.5084\n",
      "\n",
      "Epoch 00666: val_mae did not improve from 1.50522\n",
      "Epoch 667/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9218 - mse: 4.9218 - mae: 1.6794 - val_loss: 3.6168 - val_mse: 3.6168 - val_mae: 1.5080\n",
      "\n",
      "Epoch 00667: val_mae did not improve from 1.50522\n",
      "Epoch 668/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8456 - mse: 4.8456 - mae: 1.6916 - val_loss: 3.6180 - val_mse: 3.6180 - val_mae: 1.5091\n",
      "\n",
      "Epoch 00668: val_mae did not improve from 1.50522\n",
      "Epoch 669/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3866 - mse: 5.3866 - mae: 1.7210 - val_loss: 3.6170 - val_mse: 3.6170 - val_mae: 1.5084\n",
      "\n",
      "Epoch 00669: val_mae did not improve from 1.50522\n",
      "Epoch 670/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9757 - mse: 4.9757 - mae: 1.7074 - val_loss: 3.6158 - val_mse: 3.6158 - val_mae: 1.5073\n",
      "\n",
      "Epoch 00670: val_mae did not improve from 1.50522\n",
      "Epoch 671/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1808 - mse: 5.1808 - mae: 1.7036 - val_loss: 3.6180 - val_mse: 3.6180 - val_mae: 1.5091\n",
      "\n",
      "Epoch 00671: val_mae did not improve from 1.50522\n",
      "Epoch 672/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7526 - mse: 4.7526 - mae: 1.6552 - val_loss: 3.6177 - val_mse: 3.6177 - val_mae: 1.5090\n",
      "\n",
      "Epoch 00672: val_mae did not improve from 1.50522\n",
      "Epoch 673/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8691 - mse: 4.8691 - mae: 1.6711 - val_loss: 3.6186 - val_mse: 3.6186 - val_mae: 1.5095\n",
      "\n",
      "Epoch 00673: val_mae did not improve from 1.50522\n",
      "Epoch 674/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4627 - mse: 4.4627 - mae: 1.6409 - val_loss: 3.6211 - val_mse: 3.6211 - val_mae: 1.5109\n",
      "\n",
      "Epoch 00674: val_mae did not improve from 1.50522\n",
      "Epoch 675/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9024 - mse: 4.9024 - mae: 1.7087 - val_loss: 3.6213 - val_mse: 3.6213 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00675: val_mae did not improve from 1.50522\n",
      "Epoch 676/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1934 - mse: 5.1934 - mae: 1.7405 - val_loss: 3.6174 - val_mse: 3.6174 - val_mae: 1.5085\n",
      "\n",
      "Epoch 00676: val_mae did not improve from 1.50522\n",
      "Epoch 677/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9237 - mse: 4.9237 - mae: 1.6462 - val_loss: 3.6162 - val_mse: 3.6162 - val_mae: 1.5073\n",
      "\n",
      "Epoch 00677: val_mae did not improve from 1.50522\n",
      "Epoch 678/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6228 - mse: 5.6228 - mae: 1.8108 - val_loss: 3.6202 - val_mse: 3.6202 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00678: val_mae did not improve from 1.50522\n",
      "Epoch 679/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2924 - mse: 5.2924 - mae: 1.7425 - val_loss: 3.6195 - val_mse: 3.6195 - val_mae: 1.5099\n",
      "\n",
      "Epoch 00679: val_mae did not improve from 1.50522\n",
      "Epoch 680/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4804 - mse: 5.4804 - mae: 1.7467 - val_loss: 3.6235 - val_mse: 3.6235 - val_mae: 1.5123\n",
      "\n",
      "Epoch 00680: val_mae did not improve from 1.50522\n",
      "Epoch 681/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1387 - mse: 5.1387 - mae: 1.7473 - val_loss: 3.6198 - val_mse: 3.6198 - val_mae: 1.5103\n",
      "\n",
      "Epoch 00681: val_mae did not improve from 1.50522\n",
      "Epoch 682/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7823 - mse: 4.7823 - mae: 1.7229 - val_loss: 3.6182 - val_mse: 3.6182 - val_mae: 1.5094\n",
      "\n",
      "Epoch 00682: val_mae did not improve from 1.50522\n",
      "Epoch 683/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3946 - mse: 5.3946 - mae: 1.7731 - val_loss: 3.6201 - val_mse: 3.6201 - val_mae: 1.5106\n",
      "\n",
      "Epoch 00683: val_mae did not improve from 1.50522\n",
      "Epoch 684/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9538 - mse: 4.9538 - mae: 1.6809 - val_loss: 3.6247 - val_mse: 3.6247 - val_mae: 1.5131\n",
      "\n",
      "Epoch 00684: val_mae did not improve from 1.50522\n",
      "Epoch 685/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9009 - mse: 4.9009 - mae: 1.6718 - val_loss: 3.6246 - val_mse: 3.6246 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00685: val_mae did not improve from 1.50522\n",
      "Epoch 686/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6407 - mse: 5.6407 - mae: 1.7656 - val_loss: 3.6280 - val_mse: 3.6280 - val_mae: 1.5151\n",
      "\n",
      "Epoch 00686: val_mae did not improve from 1.50522\n",
      "Epoch 687/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5140 - mse: 4.5140 - mae: 1.6380 - val_loss: 3.6224 - val_mse: 3.6224 - val_mae: 1.5121\n",
      "\n",
      "Epoch 00687: val_mae did not improve from 1.50522\n",
      "Epoch 688/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7968 - mse: 4.7968 - mae: 1.6734 - val_loss: 3.6179 - val_mse: 3.6179 - val_mae: 1.5097\n",
      "\n",
      "Epoch 00688: val_mae did not improve from 1.50522\n",
      "Epoch 689/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5786 - mse: 4.5786 - mae: 1.6072 - val_loss: 3.6148 - val_mse: 3.6148 - val_mae: 1.5076\n",
      "\n",
      "Epoch 00689: val_mae did not improve from 1.50522\n",
      "Epoch 690/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5069 - mse: 5.5069 - mae: 1.7182 - val_loss: 3.6197 - val_mse: 3.6197 - val_mae: 1.5108\n",
      "\n",
      "Epoch 00690: val_mae did not improve from 1.50522\n",
      "Epoch 691/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7088 - mse: 4.7088 - mae: 1.7196 - val_loss: 3.6187 - val_mse: 3.6187 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00691: val_mae did not improve from 1.50522\n",
      "Epoch 692/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3222 - mse: 5.3222 - mae: 1.7645 - val_loss: 3.6201 - val_mse: 3.6201 - val_mae: 1.5112\n",
      "\n",
      "Epoch 00692: val_mae did not improve from 1.50522\n",
      "Epoch 693/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.5111 - mse: 5.5111 - mae: 1.7375 - val_loss: 3.6173 - val_mse: 3.6173 - val_mae: 1.5096\n",
      "\n",
      "Epoch 00693: val_mae did not improve from 1.50522\n",
      "Epoch 694/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8301 - mse: 4.8301 - mae: 1.6696 - val_loss: 3.6179 - val_mse: 3.6179 - val_mae: 1.5101\n",
      "\n",
      "Epoch 00694: val_mae did not improve from 1.50522\n",
      "Epoch 695/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3525 - mse: 4.3525 - mae: 1.6059 - val_loss: 3.6174 - val_mse: 3.6174 - val_mae: 1.5100\n",
      "\n",
      "Epoch 00695: val_mae did not improve from 1.50522\n",
      "Epoch 696/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.8761 - mse: 5.8761 - mae: 1.7991 - val_loss: 3.6180 - val_mse: 3.6180 - val_mae: 1.5102\n",
      "\n",
      "Epoch 00696: val_mae did not improve from 1.50522\n",
      "Epoch 697/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1727 - mse: 5.1727 - mae: 1.7432 - val_loss: 3.6157 - val_mse: 3.6157 - val_mae: 1.5088\n",
      "\n",
      "Epoch 00697: val_mae did not improve from 1.50522\n",
      "Epoch 698/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2503 - mse: 5.2503 - mae: 1.7575 - val_loss: 3.6145 - val_mse: 3.6145 - val_mae: 1.5080\n",
      "\n",
      "Epoch 00698: val_mae did not improve from 1.50522\n",
      "Epoch 699/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0068 - mse: 5.0068 - mae: 1.7271 - val_loss: 3.6146 - val_mse: 3.6146 - val_mae: 1.5080\n",
      "\n",
      "Epoch 00699: val_mae did not improve from 1.50522\n",
      "Epoch 700/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9159 - mse: 4.9159 - mae: 1.6936 - val_loss: 3.6229 - val_mse: 3.6229 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00700: val_mae did not improve from 1.50522\n",
      "Epoch 701/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3199 - mse: 5.3199 - mae: 1.7338 - val_loss: 3.6227 - val_mse: 3.6227 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00701: val_mae did not improve from 1.50522\n",
      "Epoch 702/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1252 - mse: 5.1252 - mae: 1.7141 - val_loss: 3.6227 - val_mse: 3.6227 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00702: val_mae did not improve from 1.50522\n",
      "Epoch 703/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7801 - mse: 4.7801 - mae: 1.6368 - val_loss: 3.6193 - val_mse: 3.6193 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00703: val_mae did not improve from 1.50522\n",
      "Epoch 704/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9571 - mse: 4.9571 - mae: 1.7234 - val_loss: 3.6155 - val_mse: 3.6155 - val_mae: 1.5088\n",
      "\n",
      "Epoch 00704: val_mae did not improve from 1.50522\n",
      "Epoch 705/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0140 - mse: 5.0140 - mae: 1.6803 - val_loss: 3.6144 - val_mse: 3.6144 - val_mae: 1.5079\n",
      "\n",
      "Epoch 00705: val_mae did not improve from 1.50522\n",
      "Epoch 706/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3814 - mse: 5.3814 - mae: 1.8094 - val_loss: 3.6181 - val_mse: 3.6181 - val_mae: 1.5103\n",
      "\n",
      "Epoch 00706: val_mae did not improve from 1.50522\n",
      "Epoch 707/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9213 - mse: 5.9213 - mae: 1.8761 - val_loss: 3.6181 - val_mse: 3.6181 - val_mae: 1.5102\n",
      "\n",
      "Epoch 00707: val_mae did not improve from 1.50522\n",
      "Epoch 708/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7215 - mse: 4.7215 - mae: 1.6102 - val_loss: 3.6177 - val_mse: 3.6177 - val_mae: 1.5098\n",
      "\n",
      "Epoch 00708: val_mae did not improve from 1.50522\n",
      "Epoch 709/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9097 - mse: 4.9097 - mae: 1.7069 - val_loss: 3.6167 - val_mse: 3.6167 - val_mae: 1.5092\n",
      "\n",
      "Epoch 00709: val_mae did not improve from 1.50522\n",
      "Epoch 710/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7909 - mse: 4.7909 - mae: 1.6497 - val_loss: 3.6149 - val_mse: 3.6149 - val_mae: 1.5078\n",
      "\n",
      "Epoch 00710: val_mae did not improve from 1.50522\n",
      "Epoch 711/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.1875 - mse: 6.1875 - mae: 1.8958 - val_loss: 3.6147 - val_mse: 3.6147 - val_mae: 1.5073\n",
      "\n",
      "Epoch 00711: val_mae did not improve from 1.50522\n",
      "Epoch 712/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.7990 - mse: 4.7990 - mae: 1.7006 - val_loss: 3.6178 - val_mse: 3.6178 - val_mae: 1.5097\n",
      "\n",
      "Epoch 00712: val_mae did not improve from 1.50522\n",
      "Epoch 713/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4892 - mse: 5.4892 - mae: 1.7363 - val_loss: 3.6202 - val_mse: 3.6202 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00713: val_mae did not improve from 1.50522\n",
      "Epoch 714/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.5109 - mse: 5.5109 - mae: 1.7793 - val_loss: 3.6190 - val_mse: 3.6190 - val_mae: 1.5106\n",
      "\n",
      "Epoch 00714: val_mae did not improve from 1.50522\n",
      "Epoch 715/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0172 - mse: 5.0172 - mae: 1.7328 - val_loss: 3.6167 - val_mse: 3.6167 - val_mae: 1.5093\n",
      "\n",
      "Epoch 00715: val_mae did not improve from 1.50522\n",
      "Epoch 716/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0466 - mse: 5.0466 - mae: 1.7199 - val_loss: 3.6160 - val_mse: 3.6160 - val_mae: 1.5089\n",
      "\n",
      "Epoch 00716: val_mae did not improve from 1.50522\n",
      "Epoch 717/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.3454 - mse: 5.3454 - mae: 1.7675 - val_loss: 3.6195 - val_mse: 3.6195 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00717: val_mae did not improve from 1.50522\n",
      "Epoch 718/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0310 - mse: 5.0310 - mae: 1.7321 - val_loss: 3.6184 - val_mse: 3.6184 - val_mae: 1.5105\n",
      "\n",
      "Epoch 00718: val_mae did not improve from 1.50522\n",
      "Epoch 719/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.0899 - mse: 6.0899 - mae: 1.8125 - val_loss: 3.6247 - val_mse: 3.6247 - val_mae: 1.5138\n",
      "\n",
      "Epoch 00719: val_mae did not improve from 1.50522\n",
      "Epoch 720/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3656 - mse: 5.3656 - mae: 1.7637 - val_loss: 3.6193 - val_mse: 3.6193 - val_mae: 1.5109\n",
      "\n",
      "Epoch 00720: val_mae did not improve from 1.50522\n",
      "Epoch 721/1000\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1723 - mse: 5.1723 - mae: 1.6958 - val_loss: 3.6183 - val_mse: 3.6183 - val_mae: 1.5103\n",
      "\n",
      "Epoch 00721: val_mae did not improve from 1.50522\n",
      "Epoch 722/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.2694 - mse: 5.2694 - mae: 1.7660 - val_loss: 3.6175 - val_mse: 3.6175 - val_mae: 1.5100\n",
      "\n",
      "Epoch 00722: val_mae did not improve from 1.50522\n",
      "Epoch 723/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6084 - mse: 4.6084 - mae: 1.6271 - val_loss: 3.6187 - val_mse: 3.6187 - val_mae: 1.5107\n",
      "\n",
      "Epoch 00723: val_mae did not improve from 1.50522\n",
      "Epoch 724/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0816 - mse: 5.0816 - mae: 1.6790 - val_loss: 3.6205 - val_mse: 3.6205 - val_mae: 1.5118\n",
      "\n",
      "Epoch 00724: val_mae did not improve from 1.50522\n",
      "Epoch 725/1000\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.5631 - mse: 4.5631 - mae: 1.6182 - val_loss: 3.6221 - val_mse: 3.6221 - val_mae: 1.5126\n",
      "\n",
      "Epoch 00725: val_mae did not improve from 1.50522\n",
      "Epoch 726/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6524 - mse: 4.6524 - mae: 1.6589 - val_loss: 3.6183 - val_mse: 3.6183 - val_mae: 1.5106\n",
      "\n",
      "Epoch 00726: val_mae did not improve from 1.50522\n",
      "Epoch 727/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.8933 - mse: 4.8933 - mae: 1.6893 - val_loss: 3.6209 - val_mse: 3.6209 - val_mae: 1.5120\n",
      "\n",
      "Epoch 00727: val_mae did not improve from 1.50522\n",
      "Epoch 728/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5815 - mse: 4.5815 - mae: 1.6442 - val_loss: 3.6218 - val_mse: 3.6218 - val_mae: 1.5125\n",
      "\n",
      "Epoch 00728: val_mae did not improve from 1.50522\n",
      "Epoch 729/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.4343 - mse: 5.4343 - mae: 1.6737 - val_loss: 3.6256 - val_mse: 3.6256 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00729: val_mae did not improve from 1.50522\n",
      "Epoch 730/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.9535 - mse: 4.9535 - mae: 1.6968 - val_loss: 3.6217 - val_mse: 3.6217 - val_mae: 1.5124\n",
      "\n",
      "Epoch 00730: val_mae did not improve from 1.50522\n",
      "Epoch 731/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6090 - mse: 4.6090 - mae: 1.6377 - val_loss: 3.6186 - val_mse: 3.6186 - val_mae: 1.5108\n",
      "\n",
      "Epoch 00731: val_mae did not improve from 1.50522\n",
      "Epoch 732/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0048 - mse: 5.0048 - mae: 1.7075 - val_loss: 3.6187 - val_mse: 3.6187 - val_mae: 1.5108\n",
      "\n",
      "Epoch 00732: val_mae did not improve from 1.50522\n",
      "Epoch 733/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6609 - mse: 5.6609 - mae: 1.7333 - val_loss: 3.6177 - val_mse: 3.6177 - val_mae: 1.5103\n",
      "\n",
      "Epoch 00733: val_mae did not improve from 1.50522\n",
      "Epoch 734/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5331 - mse: 4.5331 - mae: 1.6686 - val_loss: 3.6150 - val_mse: 3.6150 - val_mae: 1.5088\n",
      "\n",
      "Epoch 00734: val_mae did not improve from 1.50522\n",
      "Epoch 735/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0606 - mse: 5.0606 - mae: 1.7121 - val_loss: 3.6181 - val_mse: 3.6181 - val_mae: 1.5104\n",
      "\n",
      "Epoch 00735: val_mae did not improve from 1.50522\n",
      "Epoch 736/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.3060 - mse: 5.3060 - mae: 1.7258 - val_loss: 3.6226 - val_mse: 3.6226 - val_mae: 1.5127\n",
      "\n",
      "Epoch 00736: val_mae did not improve from 1.50522\n",
      "Epoch 737/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1497 - mse: 5.1497 - mae: 1.7478 - val_loss: 3.6173 - val_mse: 3.6173 - val_mae: 1.5100\n",
      "\n",
      "Epoch 00737: val_mae did not improve from 1.50522\n",
      "Epoch 738/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.6292 - mse: 5.6292 - mae: 1.7557 - val_loss: 3.6194 - val_mse: 3.6194 - val_mae: 1.5111\n",
      "\n",
      "Epoch 00738: val_mae did not improve from 1.50522\n",
      "Epoch 739/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.6513 - mse: 4.6513 - mae: 1.6536 - val_loss: 3.6258 - val_mse: 3.6258 - val_mae: 1.5145\n",
      "\n",
      "Epoch 00739: val_mae did not improve from 1.50522\n",
      "Epoch 740/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.0235 - mse: 5.0235 - mae: 1.7094 - val_loss: 3.6255 - val_mse: 3.6255 - val_mae: 1.5142\n",
      "\n",
      "Epoch 00740: val_mae did not improve from 1.50522\n",
      "Epoch 741/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1599 - mse: 5.1599 - mae: 1.6981 - val_loss: 3.6237 - val_mse: 3.6237 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00741: val_mae did not improve from 1.50522\n",
      "Epoch 742/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.9726 - mse: 4.9726 - mae: 1.7216 - val_loss: 3.6174 - val_mse: 3.6174 - val_mae: 1.5098\n",
      "\n",
      "Epoch 00742: val_mae did not improve from 1.50522\n",
      "Epoch 743/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.4498 - mse: 5.4498 - mae: 1.7068 - val_loss: 3.6182 - val_mse: 3.6182 - val_mae: 1.5102\n",
      "\n",
      "Epoch 00743: val_mae did not improve from 1.50522\n",
      "Epoch 744/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.9088 - mse: 5.9088 - mae: 1.8070 - val_loss: 3.6240 - val_mse: 3.6240 - val_mae: 1.5132\n",
      "\n",
      "Epoch 00744: val_mae did not improve from 1.50522\n",
      "Epoch 745/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.1776 - mse: 5.1776 - mae: 1.7127 - val_loss: 3.6178 - val_mse: 3.6178 - val_mae: 1.5099\n",
      "\n",
      "Epoch 00745: val_mae did not improve from 1.50522\n",
      "Epoch 746/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6430 - mse: 5.6430 - mae: 1.7142 - val_loss: 3.6207 - val_mse: 3.6207 - val_mae: 1.5116\n",
      "\n",
      "Epoch 00746: val_mae did not improve from 1.50522\n",
      "Epoch 747/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0758 - mse: 5.0758 - mae: 1.6639 - val_loss: 3.6191 - val_mse: 3.6191 - val_mae: 1.5106\n",
      "\n",
      "Epoch 00747: val_mae did not improve from 1.50522\n",
      "Epoch 748/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1807 - mse: 5.1807 - mae: 1.6940 - val_loss: 3.6200 - val_mse: 3.6200 - val_mae: 1.5110\n",
      "\n",
      "Epoch 00748: val_mae did not improve from 1.50522\n",
      "Epoch 749/1000\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 5.0887 - mse: 5.0887 - mae: 1.7052 - val_loss: 3.6234 - val_mse: 3.6234 - val_mae: 1.5128\n",
      "\n",
      "Epoch 00749: val_mae did not improve from 1.50522\n",
      "Epoch 750/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.7401 - mse: 4.7401 - mae: 1.6538 - val_loss: 3.6269 - val_mse: 3.6269 - val_mae: 1.5147\n",
      "\n",
      "Epoch 00750: val_mae did not improve from 1.50522\n",
      "Epoch 751/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.1515 - mse: 5.1515 - mae: 1.6714 - val_loss: 3.6250 - val_mse: 3.6250 - val_mae: 1.5139\n",
      "\n",
      "Epoch 00751: val_mae did not improve from 1.50522\n",
      "Epoch 00751: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def show_info(model, X, y, log, weights = None):\n",
    "    '''\n",
    "    Show metrics about the evaluation model and plots about loss, rmse and rmspe\n",
    "    '''\n",
    "    if (log != None):\n",
    "        # summarize history for loss\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['loss'])\n",
    "        plt.plot(log.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MAE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mae'])\n",
    "        plt.plot(log.history['val_mae'])\n",
    "        plt.title('Model MAE')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "        # summarize history for MSE\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['mse'])\n",
    "        plt.plot(log.history['val_mse'])\n",
    "        plt.title('Model MSE')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    if (weights != None):\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    predictions = model.predict(X, verbose=1)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    mae= mean_absolute_error(y, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "show_info(model, x_test, y_test, log, weights='/home/m-marouni/Documents/CE-901/Heathrow/best_weights')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-13T20:12:55.262408</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc8428a475f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#mc8428a475f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.803125\" xlink:href=\"#mc8428a475f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(161.259375 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"265.494034\" xlink:href=\"#mc8428a475f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(255.950284 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.184943\" xlink:href=\"#mc8428a475f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(350.641193 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"454.875852\" xlink:href=\"#mc8428a475f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(445.332102 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"549.566761\" xlink:href=\"#mc8428a475f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(540.023011 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"644.25767\" xlink:href=\"#mc8428a475f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(634.71392 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"738.94858\" xlink:href=\"#mc8428a475f\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(729.40483 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m782e6476cc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"524.143262\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 527.94248)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"462.561441\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 466.36066)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"400.97962\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 404.778839)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"339.3978\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 343.197019)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"277.815979\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 281.615198)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"216.234159\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 220.033377)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"154.652338\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 158.451557)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"93.070517\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 96.869736)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m782e6476cc\" y=\"31.488697\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 45 -->\n      <g transform=\"translate(20.878125 35.287916)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 303.775937)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pfd48828d2c)\" d=\"M 76.112216 47.027216 \nL 78.952943 111.58406 \nL 84.634398 236.163281 \nL 85.581307 255.874613 \nL 87.475125 298.599656 \nL 88.422034 315.230135 \nL 91.262761 369.538369 \nL 94.103489 409.45586 \nL 95.997307 433.318087 \nL 96.944216 441.959848 \nL 97.891125 453.038968 \nL 99.784943 463.880577 \nL 100.731852 471.022164 \nL 101.678761 471.731035 \nL 102.62567 480.489878 \nL 103.57258 485.255983 \nL 104.519489 487.768956 \nL 105.466398 492.934336 \nL 106.413307 491.825027 \nL 107.360216 493.815018 \nL 108.307125 496.868891 \nL 109.254034 498.678032 \nL 111.147852 501.537248 \nL 112.094761 500.128691 \nL 113.04167 498.288236 \nL 113.98858 501.007254 \nL 114.935489 500.719558 \nL 115.882398 502.366419 \nL 116.829307 499.567547 \nL 117.776216 499.828187 \nL 118.723125 499.877766 \nL 119.670034 500.338859 \nL 120.616943 502.290183 \nL 121.563852 502.290412 \nL 122.510761 504.137727 \nL 123.45767 504.454288 \nL 124.40458 505.398169 \nL 125.351489 506.00579 \nL 126.298398 501.950835 \nL 127.245307 503.677027 \nL 128.192216 502.828334 \nL 129.139125 498.852692 \nL 130.086034 506.098712 \nL 131.032943 506.536742 \nL 132.926761 503.40473 \nL 133.87367 502.267067 \nL 134.82058 500.770646 \nL 135.767489 503.077257 \nL 136.714398 503.423124 \nL 137.661307 503.108941 \nL 138.608216 506.951756 \nL 139.555125 502.242013 \nL 140.502034 504.800636 \nL 141.448943 504.913014 \nL 143.342761 499.955129 \nL 144.28967 504.356081 \nL 145.23658 501.657513 \nL 146.183489 504.115932 \nL 147.130398 506.137614 \nL 148.077307 502.265 \nL 149.024216 502.505712 \nL 149.971125 499.839439 \nL 150.918034 508.00741 \nL 151.864943 501.552764 \nL 152.811852 501.864903 \nL 153.758761 507.037465 \nL 154.70567 505.890652 \nL 155.65258 507.739594 \nL 156.599489 506.256005 \nL 157.546398 505.528523 \nL 158.493307 503.790914 \nL 159.440216 503.81507 \nL 160.387125 499.836456 \nL 161.334034 501.844336 \nL 162.280943 503.583296 \nL 163.227852 502.918013 \nL 164.174761 503.956442 \nL 165.12167 503.889291 \nL 166.06858 504.688053 \nL 167.015489 504.750887 \nL 167.962398 505.470364 \nL 168.909307 503.04138 \nL 169.856216 504.77496 \nL 170.803125 506.185501 \nL 171.750034 505.019995 \nL 172.696943 503.550977 \nL 173.643852 505.445052 \nL 174.590761 505.081232 \nL 175.53767 507.918976 \nL 176.48458 502.583381 \nL 177.431489 509.657595 \nL 178.378398 503.754009 \nL 179.325307 506.880488 \nL 180.272216 504.391401 \nL 181.219125 504.886874 \nL 182.166034 505.252896 \nL 183.112943 507.543768 \nL 184.059852 508.914203 \nL 185.006761 504.425922 \nL 185.95367 506.929404 \nL 186.90058 503.796141 \nL 187.847489 506.62944 \nL 188.794398 505.471944 \nL 189.741307 505.489704 \nL 190.688216 502.56319 \nL 191.635125 500.450403 \nL 192.582034 503.222218 \nL 193.528943 505.20411 \nL 194.475852 511.111467 \nL 195.422761 503.184091 \nL 196.36967 507.998583 \nL 197.31658 503.005273 \nL 198.263489 503.917916 \nL 199.210398 509.035038 \nL 200.157307 503.324177 \nL 201.104216 502.400934 \nL 202.051125 503.668347 \nL 202.998034 507.299438 \nL 203.944943 507.142643 \nL 204.891852 506.478723 \nL 205.838761 503.632546 \nL 206.78567 507.519337 \nL 207.73258 509.125563 \nL 208.679489 510.190073 \nL 209.626398 506.369153 \nL 210.573307 506.625746 \nL 211.520216 506.474413 \nL 212.467125 507.082164 \nL 213.414034 506.092263 \nL 214.360943 508.556467 \nL 215.307852 509.885569 \nL 216.254761 509.336817 \nL 217.20167 503.444484 \nL 218.14858 503.123054 \nL 219.095489 503.922003 \nL 220.042398 508.387886 \nL 220.989307 504.600353 \nL 221.936216 506.504676 \nL 222.883125 507.243745 \nL 223.830034 504.340236 \nL 224.776943 509.78351 \nL 225.723852 508.114261 \nL 226.670761 509.884089 \nL 227.61767 506.572167 \nL 228.56458 510.709778 \nL 229.511489 506.571909 \nL 230.458398 508.58279 \nL 231.405307 508.800181 \nL 232.352216 510.559021 \nL 234.246034 506.394095 \nL 235.192943 505.870256 \nL 236.139852 504.740844 \nL 237.086761 505.541027 \nL 238.03367 508.040134 \nL 238.98058 504.729157 \nL 239.927489 508.82204 \nL 240.874398 506.584195 \nL 241.821307 511.224009 \nL 242.768216 505.725665 \nL 243.715125 511.655984 \nL 244.662034 507.063905 \nL 245.608943 509.302455 \nL 247.502761 508.634272 \nL 248.44967 509.900022 \nL 249.39658 508.535066 \nL 250.343489 509.514208 \nL 251.290398 511.244899 \nL 252.237307 510.2651 \nL 253.184216 508.767052 \nL 254.131125 510.642187 \nL 255.078034 511.792388 \nL 256.024943 510.696188 \nL 256.971852 508.745569 \nL 257.918761 511.436121 \nL 258.86567 511.868178 \nL 259.81258 510.320205 \nL 260.759489 513.437669 \nL 261.706398 510.188417 \nL 262.653307 510.354415 \nL 263.600216 507.94595 \nL 264.547125 510.524094 \nL 265.494034 506.68949 \nL 267.387852 515.33249 \nL 268.334761 510.359236 \nL 269.28167 509.837247 \nL 270.22858 508.279977 \nL 271.175489 511.709639 \nL 272.122398 511.910316 \nL 273.069307 508.861829 \nL 274.016216 512.171267 \nL 274.963125 510.778215 \nL 275.910034 513.054709 \nL 276.856943 512.224739 \nL 277.803852 516.760492 \nL 278.750761 513.353998 \nL 279.69767 511.123488 \nL 280.64458 516.502425 \nL 281.591489 512.079315 \nL 282.538398 513.206119 \nL 283.485307 513.538302 \nL 284.432216 514.400097 \nL 285.379125 514.425151 \nL 286.326034 508.66225 \nL 287.272943 516.556808 \nL 288.219852 516.097359 \nL 289.166761 511.61165 \nL 290.11367 513.779167 \nL 291.06058 511.412406 \nL 292.007489 512.842639 \nL 292.954398 516.006276 \nL 293.901307 514.400479 \nL 294.848216 512.566777 \nL 295.795125 509.903035 \nL 296.742034 516.340127 \nL 297.688943 512.001323 \nL 298.635852 513.31512 \nL 299.582761 520.287292 \nL 300.52967 517.963573 \nL 301.47658 514.763377 \nL 302.423489 515.763038 \nL 303.370398 513.398298 \nL 304.317307 515.045082 \nL 305.264216 515.665167 \nL 306.211125 512.495304 \nL 307.158034 514.2665 \nL 308.104943 515.139336 \nL 309.051852 517.912937 \nL 309.998761 511.75015 \nL 310.94567 513.625033 \nL 311.89258 514.156054 \nL 312.839489 515.35886 \nL 314.733307 514.019738 \nL 315.680216 517.674368 \nL 316.627125 512.111287 \nL 317.574034 517.707438 \nL 318.520943 515.217834 \nL 319.467852 514.826387 \nL 320.414761 517.996379 \nL 321.36167 517.255413 \nL 322.30858 520.725756 \nL 323.255489 520.031832 \nL 324.202398 515.512735 \nL 325.149307 515.627868 \nL 326.096216 516.759769 \nL 327.043125 516.624006 \nL 327.990034 515.722592 \nL 328.936943 517.188761 \nL 329.883852 519.500699 \nL 330.830761 516.583113 \nL 331.77767 516.86833 \nL 332.72458 514.81709 \nL 333.671489 519.157304 \nL 334.618398 513.290888 \nL 335.565307 517.356808 \nL 336.512216 517.682355 \nL 337.459125 518.614578 \nL 338.406034 514.490587 \nL 339.352943 517.740831 \nL 340.299852 514.500688 \nL 341.246761 517.284583 \nL 342.19367 514.444038 \nL 343.14058 515.840267 \nL 344.087489 512.791286 \nL 345.034398 518.466728 \nL 345.981307 517.914417 \nL 346.928216 517.661195 \nL 347.875125 521.35297 \nL 348.822034 520.354618 \nL 349.768943 518.315313 \nL 350.715852 520.799438 \nL 351.662761 515.994789 \nL 352.60967 515.008336 \nL 353.55658 519.960676 \nL 354.503489 520.431988 \nL 355.450398 518.167674 \nL 356.397307 518.74579 \nL 357.344216 517.133814 \nL 358.291125 517.581382 \nL 359.238034 522.296745 \nL 362.078761 516.264249 \nL 363.02567 519.107678 \nL 363.97258 517.180862 \nL 364.919489 517.252265 \nL 365.866398 518.653134 \nL 366.813307 518.543639 \nL 367.760216 518.157502 \nL 368.707125 517.423301 \nL 369.654034 519.884399 \nL 370.600943 516.985441 \nL 371.547852 522.101806 \nL 372.494761 520.935742 \nL 373.44167 516.738357 \nL 375.335489 520.248636 \nL 376.282398 518.613897 \nL 377.229307 519.171482 \nL 378.176216 517.938872 \nL 379.123125 518.574413 \nL 380.070034 517.915145 \nL 381.016943 519.710673 \nL 381.963852 516.518851 \nL 382.910761 518.84866 \nL 383.85767 516.570909 \nL 384.80458 523.548777 \nL 385.751489 519.304127 \nL 386.698398 518.165877 \nL 387.645307 518.043726 \nL 388.592216 519.138922 \nL 389.539125 515.911535 \nL 390.486034 521.653592 \nL 391.432943 520.915316 \nL 392.379852 519.081239 \nL 393.326761 520.133363 \nL 394.27367 518.067429 \nL 395.22058 517.474965 \nL 396.167489 514.64633 \nL 397.114398 521.323207 \nL 398.061307 519.505262 \nL 399.008216 520.002791 \nL 399.955125 520.78058 \nL 400.902034 519.911714 \nL 401.848943 521.844121 \nL 402.795852 519.064378 \nL 403.742761 522.297485 \nL 404.68967 518.058596 \nL 405.63658 520.732387 \nL 406.583489 521.742156 \nL 407.530398 518.144817 \nL 408.477307 521.684513 \nL 409.424216 519.520326 \nL 410.371125 521.256532 \nL 412.264943 519.002107 \nL 413.211852 523.689192 \nL 414.158761 521.46297 \nL 415.10567 518.571001 \nL 416.05258 521.436172 \nL 416.999489 520.917577 \nL 417.946398 521.432848 \nL 418.893307 520.698988 \nL 419.840216 521.425184 \nL 420.787125 520.777174 \nL 421.734034 521.650656 \nL 422.680943 522.161892 \nL 423.627852 522.402158 \nL 424.574761 522.163542 \nL 425.52167 522.056597 \nL 427.415489 519.679957 \nL 428.362398 523.369824 \nL 429.309307 520.65961 \nL 430.256216 521.256273 \nL 431.203125 518.296367 \nL 432.150034 518.115769 \nL 433.096943 519.508504 \nL 434.043852 522.761544 \nL 434.990761 520.150118 \nL 435.93767 520.536608 \nL 436.88458 520.18767 \nL 437.831489 518.434139 \nL 438.778398 523.363023 \nL 439.725307 518.838424 \nL 440.672216 518.756479 \nL 441.619125 522.102194 \nL 442.566034 522.827896 \nL 443.512943 518.18208 \nL 444.459852 522.700032 \nL 445.406761 521.191965 \nL 446.35367 522.45882 \nL 447.30058 519.070474 \nL 448.247489 521.593759 \nL 449.194398 525.597462 \nL 450.141307 519.844305 \nL 451.088216 518.987272 \nL 452.035125 525.016785 \nL 452.982034 519.779626 \nL 453.928943 521.852443 \nL 454.875852 519.73558 \nL 455.822761 522.437037 \nL 456.76967 518.996751 \nL 457.71658 519.128762 \nL 458.663489 522.935353 \nL 459.610398 518.865028 \nL 460.557307 516.659989 \nL 461.504216 520.680482 \nL 462.451125 521.012401 \nL 463.398034 523.156685 \nL 464.344943 516.857694 \nL 465.291852 523.367269 \nL 466.238761 523.392153 \nL 467.18567 521.854011 \nL 468.13258 518.192082 \nL 469.079489 519.985037 \nL 470.026398 522.944767 \nL 470.973307 521.921455 \nL 471.920216 520.644934 \nL 472.867125 521.082036 \nL 473.814034 521.357404 \nL 474.760943 520.803531 \nL 475.707852 519.487567 \nL 476.654761 519.199654 \nL 477.60167 518.48296 \nL 478.54858 522.723406 \nL 479.495489 523.020475 \nL 481.389307 522.794832 \nL 482.336216 520.90209 \nL 484.230034 518.563871 \nL 485.176943 518.644929 \nL 486.123852 520.432611 \nL 487.070761 521.594535 \nL 488.01767 519.440825 \nL 488.96458 522.007476 \nL 489.911489 519.155043 \nL 490.858398 519.441488 \nL 491.805307 518.781392 \nL 492.752216 523.40216 \nL 493.699125 520.190718 \nL 494.646034 522.522699 \nL 495.592943 522.250508 \nL 496.539852 522.513485 \nL 497.486761 523.755844 \nL 498.43367 516.018956 \nL 499.38058 522.973257 \nL 500.327489 521.800474 \nL 501.274398 521.36027 \nL 502.221307 522.068724 \nL 503.168216 519.345736 \nL 504.115125 521.705756 \nL 505.062034 520.249999 \nL 506.008943 524.021405 \nL 506.955852 522.581111 \nL 507.902761 519.543824 \nL 508.84967 518.648694 \nL 509.79658 518.835147 \nL 510.743489 520.935848 \nL 511.690398 520.577877 \nL 512.637307 522.497505 \nL 513.584216 521.610832 \nL 514.531125 523.71178 \nL 515.478034 524.779626 \nL 516.424943 522.114903 \nL 517.371852 525.126602 \nL 518.318761 518.085312 \nL 519.26567 521.689899 \nL 520.21258 520.365196 \nL 521.159489 523.921518 \nL 522.106398 521.040391 \nL 523.053307 522.153265 \nL 524.000216 519.853413 \nL 524.947125 525.227528 \nL 525.894034 521.851192 \nL 526.840943 520.24207 \nL 527.787852 523.921395 \nL 528.734761 522.705822 \nL 529.68167 519.274287 \nL 530.62858 523.613655 \nL 531.575489 523.237784 \nL 532.522398 521.946509 \nL 533.469307 519.844739 \nL 534.416216 521.700816 \nL 535.363125 521.130417 \nL 536.310034 519.592093 \nL 538.203852 522.658205 \nL 539.150761 523.234747 \nL 540.09767 521.267302 \nL 541.04458 522.169292 \nL 541.991489 524.417033 \nL 543.885307 520.759103 \nL 544.832216 520.658676 \nL 545.779125 523.375867 \nL 546.726034 521.753103 \nL 547.672943 523.351072 \nL 548.619852 520.863335 \nL 549.566761 521.389359 \nL 550.51367 520.712114 \nL 551.46058 522.715424 \nL 552.407489 522.458667 \nL 553.354398 520.059687 \nL 554.301307 521.77263 \nL 555.248216 520.709101 \nL 556.195125 520.994342 \nL 557.142034 518.85152 \nL 558.088943 523.336078 \nL 559.035852 524.631258 \nL 559.982761 522.03213 \nL 560.92967 524.261554 \nL 561.87658 520.401895 \nL 562.823489 519.70292 \nL 563.770398 520.266513 \nL 564.717307 525.072343 \nL 565.664216 521.655172 \nL 566.611125 519.982259 \nL 567.558034 523.746929 \nL 568.504943 521.827278 \nL 569.451852 520.409395 \nL 570.398761 523.16937 \nL 571.34567 519.480731 \nL 572.29258 521.635568 \nL 573.239489 519.290226 \nL 575.133307 521.549959 \nL 577.027125 522.207671 \nL 577.974034 520.785501 \nL 578.920943 522.554061 \nL 579.867852 525.657859 \nL 580.814761 524.239184 \nL 581.76167 524.116064 \nL 582.70858 522.492301 \nL 583.655489 521.630553 \nL 584.602398 521.712122 \nL 585.549307 523.283122 \nL 586.496216 522.699532 \nL 587.443125 520.159268 \nL 588.390034 523.708232 \nL 589.336943 520.915304 \nL 590.283852 522.597285 \nL 591.230761 523.804072 \nL 592.17767 521.780247 \nL 593.12458 522.611733 \nL 594.071489 523.095337 \nL 595.018398 516.811439 \nL 595.965307 522.937038 \nL 596.912216 520.414698 \nL 597.859125 521.43939 \nL 598.806034 521.701415 \nL 599.752943 519.718002 \nL 600.699852 523.158505 \nL 601.646761 520.830899 \nL 602.59367 522.567052 \nL 603.54058 522.366134 \nL 604.487489 517.261186 \nL 605.434398 522.634349 \nL 606.381307 520.235281 \nL 607.328216 520.994677 \nL 608.275125 519.997675 \nL 609.222034 524.164022 \nL 610.168943 522.969809 \nL 611.115852 523.140188 \nL 612.062761 522.993054 \nL 613.00967 519.991949 \nL 613.95658 519.562552 \nL 614.903489 522.707414 \nL 615.850398 524.415876 \nL 616.797307 523.920843 \nL 617.744216 519.694998 \nL 618.691125 520.985527 \nL 619.638034 520.754434 \nL 620.584943 524.217548 \nL 621.531852 523.070529 \nL 622.478761 521.123798 \nL 623.42567 521.420855 \nL 624.37258 519.388785 \nL 625.319489 522.823321 \nL 626.266398 520.56905 \nL 627.213307 523.08039 \nL 628.160216 520.360873 \nL 629.107125 523.076338 \nL 630.054034 522.324742 \nL 631.000943 521.840539 \nL 631.947852 521.922354 \nL 632.894761 521.796462 \nL 633.84167 522.074116 \nL 634.78858 518.91224 \nL 635.735489 520.697337 \nL 638.576216 522.540835 \nL 639.523125 519.788389 \nL 640.470034 519.83334 \nL 641.416943 522.625299 \nL 642.363852 524.082136 \nL 643.310761 522.59012 \nL 644.25767 520.305404 \nL 645.20458 524.997592 \nL 646.151489 524.263139 \nL 647.098398 522.111972 \nL 648.045307 523.059259 \nL 648.992216 523.169147 \nL 649.939125 525.340446 \nL 650.886034 524.055984 \nL 651.832943 521.43757 \nL 652.779852 523.803597 \nL 653.726761 521.031224 \nL 654.67367 521.39549 \nL 655.62058 520.955205 \nL 656.567489 524.689653 \nL 657.514398 521.910914 \nL 658.461307 521.268583 \nL 659.408216 525.197606 \nL 660.355125 523.364674 \nL 661.302034 519.739509 \nL 662.248943 519.46357 \nL 663.195852 520.732322 \nL 664.142761 522.35807 \nL 666.03658 524.509983 \nL 667.930398 523.06932 \nL 668.877307 522.631794 \nL 669.824216 524.044456 \nL 670.771125 519.335606 \nL 671.718034 522.815393 \nL 673.611852 521.949035 \nL 674.558761 523.07363 \nL 675.50567 522.135523 \nL 676.45258 521.49762 \nL 677.399489 524.005331 \nL 678.346398 524.55201 \nL 679.293307 524.714031 \nL 680.240216 523.458111 \nL 681.187125 524.04953 \nL 683.080943 520.589699 \nL 684.027852 521.8465 \nL 684.974761 523.409789 \nL 685.92167 522.198186 \nL 686.86858 522.127218 \nL 687.815489 523.110665 \nL 688.762398 518.805788 \nL 689.709307 521.318397 \nL 690.656216 521.83463 \nL 691.603125 526.183918 \nL 692.550034 521.173066 \nL 693.496943 521.324575 \nL 694.443852 523.231241 \nL 695.390761 519.961034 \nL 696.33767 522.882843 \nL 697.28458 521.570696 \nL 698.231489 523.959898 \nL 699.178398 521.191225 \nL 700.125307 521.509877 \nL 701.072216 524.126066 \nL 702.019125 518.452944 \nL 702.966034 518.441034 \nL 703.912943 522.390724 \nL 704.859852 521.643515 \nL 705.806761 519.41755 \nL 706.75367 524.58276 \nL 707.70058 521.661738 \nL 708.647489 524.040832 \nL 709.594398 521.229933 \nL 710.541307 522.008304 \nL 711.488216 521.768648 \nL 712.435125 526.589783 \nL 713.382034 524.760756 \nL 715.275852 520.353914 \nL 716.222761 523.448556 \nL 717.16967 520.135559 \nL 718.11658 524.608853 \nL 719.063489 525.062564 \nL 720.010398 521.660117 \nL 720.957307 522.863856 \nL 721.904216 520.987588 \nL 722.851125 521.806758 \nL 723.798034 523.842041 \nL 724.744943 521.137024 \nL 725.691852 523.363693 \nL 726.638761 522.102652 \nL 727.58567 525.650994 \nL 728.53258 525.396944 \nL 729.479489 521.565017 \nL 730.426398 524.740994 \nL 731.373307 521.554258 \nL 732.320216 520.77476 \nL 733.267125 525.290961 \nL 734.214034 517.461669 \nL 735.160943 523.131913 \nL 736.107852 521.058315 \nL 737.054761 522.11273 \nL 738.00167 521.373032 \nL 738.94858 523.407892 \nL 739.895489 522.739732 \nL 740.842398 524.091862 \nL 741.789307 523.266667 \nL 742.736216 523.704826 \nL 743.683125 521.129377 \nL 744.630034 521.750266 \nL 745.576943 526.064111 \nL 746.523852 521.967934 \nL 747.470761 525.257856 \nL 748.41767 518.76449 \nL 749.36458 522.254778 \nL 750.311489 523.991641 \nL 751.258398 522.921969 \nL 752.205307 523.770773 \nL 753.152216 526.316293 \nL 754.099125 525.389062 \nL 755.046034 522.386589 \nL 755.992943 521.930065 \nL 756.939852 519.526698 \nL 757.886761 521.678916 \nL 758.83367 519.405664 \nL 759.78058 520.110617 \nL 760.727489 523.324015 \nL 761.674398 522.046789 \nL 762.621307 526.046187 \nL 763.568216 524.124363 \nL 764.515125 520.029689 \nL 765.462034 523.666687 \nL 766.408943 522.900462 \nL 767.355852 525.200783 \nL 768.302761 523.907341 \nL 769.24967 522.132657 \nL 770.19658 521.519966 \nL 771.143489 521.279371 \nL 772.090398 520.737449 \nL 773.037307 523.503949 \nL 773.984216 520.520181 \nL 774.931125 522.485741 \nL 775.878034 521.423927 \nL 776.824943 520.775471 \nL 777.771852 522.171447 \nL 778.718761 521.459252 \nL 779.66567 522.594237 \nL 780.61258 522.054494 \nL 781.559489 521.929425 \nL 782.506398 523.524305 \nL 783.453307 521.121514 \nL 784.400216 525.346683 \nL 785.347125 523.387319 \nL 786.294034 522.663044 \nL 786.294034 522.663044 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pfd48828d2c)\" d=\"M 76.112216 112.654807 \nL 79.899852 192.417975 \nL 87.475125 345.904762 \nL 89.368943 379.027213 \nL 91.262761 408.795311 \nL 93.15658 434.055805 \nL 95.050398 455.034955 \nL 96.944216 471.95202 \nL 97.891125 478.835247 \nL 98.838034 484.757609 \nL 100.731852 494.565187 \nL 101.678761 498.4571 \nL 102.62567 501.739428 \nL 103.57258 504.445854 \nL 104.519489 506.635735 \nL 106.413307 509.760858 \nL 107.360216 510.931409 \nL 108.307125 511.768574 \nL 110.200943 513.000015 \nL 112.094761 513.697575 \nL 113.98858 513.969014 \nL 116.829307 514.058435 \nL 132.926761 513.96276 \nL 134.82058 514.053097 \nL 135.767489 513.989634 \nL 139.555125 514.268092 \nL 143.342761 514.307064 \nL 144.28967 514.288042 \nL 145.23658 514.412941 \nL 147.130398 514.401266 \nL 149.971125 514.706733 \nL 155.65258 514.857185 \nL 159.440216 515.12089 \nL 162.280943 515.169253 \nL 165.12167 515.428706 \nL 167.962398 515.517011 \nL 169.856216 515.738032 \nL 171.750034 515.802422 \nL 174.590761 515.977728 \nL 176.48458 515.978075 \nL 178.378398 516.142839 \nL 179.325307 516.038348 \nL 182.166034 516.288933 \nL 183.112943 516.386834 \nL 184.059852 516.61704 \nL 185.006761 516.715353 \nL 186.90058 516.718829 \nL 187.847489 516.815856 \nL 189.741307 516.750584 \nL 191.635125 517.055012 \nL 194.475852 516.932879 \nL 195.422761 517.142606 \nL 199.210398 517.478007 \nL 200.157307 517.420553 \nL 201.104216 517.714815 \nL 204.891852 518.151271 \nL 206.78567 518.214575 \nL 214.360943 518.975808 \nL 218.14858 519.288129 \nL 220.042398 519.436954 \nL 221.936216 519.713221 \nL 225.723852 519.730112 \nL 228.56458 520.187323 \nL 233.299125 520.596799 \nL 235.192943 520.815806 \nL 237.086761 521.2942 \nL 249.39658 522.234598 \nL 251.290398 522.488026 \nL 252.237307 522.412676 \nL 254.131125 522.874398 \nL 257.918761 523.688505 \nL 263.600216 524.112916 \nL 265.494034 524.457274 \nL 266.440943 524.56846 \nL 268.334761 524.528812 \nL 270.22858 524.824148 \nL 272.122398 525.369247 \nL 274.016216 525.663203 \nL 275.910034 525.617154 \nL 279.69767 526.228664 \nL 280.64458 526.27836 \nL 283.485307 526.681846 \nL 284.432216 526.644036 \nL 293.901307 527.931558 \nL 296.742034 528.443135 \nL 297.688943 528.392522 \nL 298.635852 528.585946 \nL 299.582761 528.507889 \nL 303.370398 529.056835 \nL 305.264216 529.550241 \nL 308.104943 529.732589 \nL 315.680216 530.398963 \nL 316.627125 530.414009 \nL 318.520943 530.814853 \nL 326.096216 531.606443 \nL 327.043125 531.607107 \nL 338.406034 532.97107 \nL 344.087489 533.445248 \nL 346.928216 533.744337 \nL 349.768943 533.801046 \nL 353.55658 534.312617 \nL 355.450398 534.323341 \nL 357.344216 534.482637 \nL 359.238034 534.457325 \nL 362.078761 535.084075 \nL 363.97258 535.199554 \nL 367.760216 535.468239 \nL 373.44167 535.806078 \nL 374.38858 535.540787 \nL 376.282398 535.896368 \nL 381.963852 536.404602 \nL 390.486034 536.933965 \nL 392.379852 536.879949 \nL 394.27367 537.093221 \nL 396.167489 537.133236 \nL 399.008216 537.438709 \nL 403.742761 537.566212 \nL 405.63658 537.684081 \nL 407.530398 537.670747 \nL 408.477307 537.818873 \nL 410.371125 537.830819 \nL 411.318034 538.00473 \nL 416.999489 538.182676 \nL 419.840216 538.34796 \nL 420.787125 538.243634 \nL 421.734034 538.334173 \nL 422.680943 538.282938 \nL 424.574761 538.50306 \nL 437.831489 538.895763 \nL 439.725307 538.96793 \nL 441.619125 539.047334 \nL 442.566034 538.931327 \nL 445.406761 539.160173 \nL 446.35367 539.109378 \nL 448.247489 539.162073 \nL 451.088216 539.269033 \nL 453.928943 539.392769 \nL 457.71658 539.473387 \nL 459.610398 539.570794 \nL 464.344943 539.463961 \nL 465.291852 539.586381 \nL 467.18567 539.486433 \nL 469.079489 539.613206 \nL 470.026398 539.522977 \nL 470.973307 539.626381 \nL 471.920216 539.57416 \nL 475.707852 539.810271 \nL 477.60167 539.81316 \nL 478.54858 539.871099 \nL 482.336216 539.748967 \nL 486.123852 539.958879 \nL 488.01767 539.929038 \nL 500.327489 540.206891 \nL 503.168216 540.210917 \nL 549.566761 540.644085 \nL 552.407489 540.590136 \nL 553.354398 540.56165 \nL 555.248216 540.734586 \nL 557.142034 540.712137 \nL 559.035852 540.556672 \nL 562.823489 540.678039 \nL 577.027125 540.87264 \nL 578.920943 540.800597 \nL 580.814761 540.846151 \nL 584.602398 540.842169 \nL 586.496216 540.859576 \nL 587.443125 540.94656 \nL 590.283852 540.890045 \nL 592.17767 540.978617 \nL 622.478761 541.015763 \nL 625.319489 541.02908 \nL 627.213307 541.024528 \nL 629.107125 541.039005 \nL 630.054034 540.99736 \nL 631.000943 541.079569 \nL 633.84167 540.931049 \nL 635.735489 541.047723 \nL 638.576216 541.013135 \nL 640.470034 541.067547 \nL 644.25767 541.024719 \nL 647.098398 541.082905 \nL 650.886034 540.942305 \nL 655.62058 541.048492 \nL 656.567489 540.987153 \nL 658.461307 541.078444 \nL 660.355125 541.014433 \nL 661.302034 541.083222 \nL 663.195852 541.064003 \nL 668.877307 541.054398 \nL 670.771125 541.100805 \nL 674.558761 541.089952 \nL 676.45258 541.079889 \nL 679.293307 541.11034 \nL 682.134034 541.004226 \nL 684.027852 541.128264 \nL 723.798034 541.083724 \nL 724.744943 541.04094 \nL 727.58567 541.203455 \nL 729.479489 541.155967 \nL 734.214034 541.164656 \nL 737.054761 541.206653 \nL 738.94858 541.106763 \nL 769.24967 541.168006 \nL 771.143489 541.163234 \nL 772.090398 541.107715 \nL 773.984216 541.14746 \nL 775.878034 541.072709 \nL 783.453307 541.140104 \nL 786.294034 541.078312 \nL 786.294034 541.078312 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Model Loss -->\n    <g transform=\"translate(398.119687 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"385.994141\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"447.175781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"499.275391\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_21\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_22\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pfd48828d2c\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz0AAAJcCAYAAADTt8o+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAACBIklEQVR4nOzdd3ib5dn+8fOWZMmWvEecvUN2CCQEwibsUUahFCijfdvSvm0pXbTQ0l1a2rellF+hjEKhQKGUXfYKKySE7EH23na8LduSJd2/Px5ZsTMgTqxh5/s5Dh+yH63bsgk6fV339RhrrQAAAACgp3KlewEAAAAAkEyEHgAAAAA9GqEHAAAAQI9G6AEAAADQoxF6AAAAAPRohB4AAAAAPRqhBwCQEYwxg40x1hjj2Y/bftEY834q1gUA6P4IPQCATjPGrDfGhI0xpbsdnx8PLoPTtLROhScAwKGB0AMAOFDrJF3e9oUxZrwkf/qWAwDA3hF6AAAH6mFJV7f7+hpJ/2x/A2NMgTHmn8aYSmPMBmPMzcYYV/w6tzHmj8aYncaYtZLO3ct97zfGbDPGbDHG/MYY4z6YBRtj+hpjnjfGVBtjVhtjvtruuinGmDnGmHpjzA5jzG3x49nGmEeMMVXGmFpjzEfGmPKDWQcAILUIPQCAAzVLUr4xZnQ8jFwm6ZHdbvP/JBVIGirpJDkh6Uvx674q6TxJR0iaLOmS3e77oKSIpOHx25wh6SsHuebHJW2W1Df+fL81xkyLX/cXSX+x1uZLGibpifjxa+LfwwBJJZK+Lqn5INcBAEghQg8A4GC0VXtOl7RM0pa2K9oFoZustQ3W2vWS/iTpqvhNLpV0u7V2k7W2WtLv2t23XNI5kr5jrQ1aaysk/Tn+eAfEGDNA0nGSfmStbbHWLpD0d+2qVrVKGm6MKbXWNlprZ7U7XiJpuLU2aq2da62tP9B1AABSj9ADADgYD0u6QtIXtVtrm6RSSVmSNrQ7tkFSv/jnfSVt2u26NoPi990WbymrlXSPpF4Hsda+kqqttQ37WM+XJR0maXm8he28+PGHJb0q6XFjzFZjzB+MMVkHsQ4AQIoRegAAB8xau0HOQINzJD2929U75VRJBrU7NlC7qkHb5LSMtb+uzSZJIUml1trC+Ee+tXbsQSx3q6RiY0ze3tZjrV1lrb1cTrD6vaQnjTEBa22rtfaX1toxko6V05J3tQAA3QahBwBwsL4saZq1Ntj+oLU2KmdfzC3GmDxjzCBJ39OufT9PSPq2Maa/MaZI0o3t7rtN0muS/mSMyTfGuIwxw4wxJ3ViXb74EIJsY0y2nHDzgaTfxY9NiK/9EUkyxlxpjCmz1sYk1cYfI2aMOcUYMz7erlcvJ8jFOrEOAECaEXoAAAfFWrvGWjtnH1dfJykoaa2k9yX9S9ID8evuk9M2tlDSPO1ZKbpaklfSx5JqJD0pqU8nltYoZ+BA28c0OSO2B8up+jwj6efW2jfitz9L0lJjTKOcoQaXWWubJfWOP3e9nH1L78hpeQMAdBPGWpvuNQAAAABA0lDpAQAAANCjEXoAAAAA9GiEHgAAAAA9GqEHAAAAQI/mSfcC9kdpaakdPHhwupcBAAAAIEPNnTt3p7W2bG/XdYvQM3jwYM2Zs69pqAAAAAAOdcaYDfu6jvY2AAAAAD0aoQcAAABAj0boAQAAANCjdYs9PXvT2tqqzZs3q6WlJd1LSars7Gz1799fWVlZ6V4KAAAA0C0lPfQYY9yS5kjaYq09zxjzoKSTJNXFb/JFa+2Czj7u5s2blZeXp8GDB8sY02XrzSTWWlVVVWnz5s0aMmRIupcDAAAAdEupaG+7XtKy3Y7dYK2dGP9YcCAP2tLSopKSkh4beCTJGKOSkpIeX80CAAAAkimpoccY01/SuZL+nqTHT8bDZpRD4XsEAAAAkinZlZ7bJf1QUmy347cYYxYZY/5sjPHt7Y7GmGuNMXOMMXMqKyuTvEwAAAAAPVXSQo8x5jxJFdbaubtddZOkUZKOklQs6Ud7u7+19l5r7WRr7eSysr2eWDWtamtrddddd3X6fuecc45qa2u7fkEAAAAA9iqZlZ7jJJ1vjFkv6XFJ04wxj1hrt1lHSNI/JE1J4hqSZl+hJxKJfOL9XnrpJRUWFiZpVQAAAAB2l7TQY629yVrb31o7WNJlkt6y1l5pjOkjScbZrHKhpCXJWkMy3XjjjVqzZo0mTpyoo446SieccILOP/98jRkzRpJ04YUXatKkSRo7dqzuvffexP0GDx6snTt3av369Ro9erS++tWvauzYsTrjjDPU3Nycrm8HAAAA6LHScZ6eR40xZZKMpAWSvn6wD/jL/y7Vx1vrD/ZhOhjTN18//8zYfV5/6623asmSJVqwYIHefvttnXvuuVqyZElitPQDDzyg4uJiNTc366ijjtLFF1+skpKSDo+xatUqPfbYY7rvvvt06aWX6qmnntKVV17Zpd8HAAAAcKhLSeix1r4t6e3459NS8ZypNmXKlA7n0rnjjjv0zDPPSJI2bdqkVatW7RF6hgwZookTJ0qSJk2apPXr16dquQAAAMAhIx2Vni73SRWZVAkEAonP3377bb3xxhuaOXOm/H6/Tj755L2ea8fn2zW4zu12094GAAAAJEEqTk7aI+Xl5amhoWGv19XV1amoqEh+v1/Lly/XrFmzUrw6AAAAAG16RKUnHUpKSnTcccdp3LhxysnJUXl5eeK6s846S3fffbdGjx6tkSNH6phjjknjSgEAAIBDm7HWpnsNn2ry5Ml2zpw5HY4tW7ZMo0ePTtOKUutQ+l4BAACAA2GMmWutnby362hvAwAAANCjEXoAAAAA9GiEHgAAAAA9GqEHAAAAQI9G6AEAAADQoxF6OqGxpVUrtjeopTWa7qUAAAAA2E+Enk6wkkKRqKIxq9raWt11110H9Di33367mpqaunZxAAAAAPaK0NMJLmMkSTFL6AEAAAC6C0+6F9CdJEJPzOrGG2/UmjVrNHHiRJ1++unq1auXnnjiCYVCIV100UX65S9/qWAwqEsvvVSbN29WNBrVT3/6U+3YsUNbt27VKaecotLSUk2fPj3N3xUAAADQs/WM0PPyjdL2xV37mL3HS2ff2uGQK14Xi1np1ltv1ZIlS7RgwQK99tprevLJJzV79mxZa3X++efr3XffVWVlpfr27asXX3xRklRXV6eCggLddtttmj59ukpLS7t2zQAAAAD2QHtbJ7Rvb2vvtdde02uvvaYjjjhCRx55pJYvX65Vq1Zp/Pjxev311/WjH/1I7733ngoKCtKxbAAAAOCQ1jMqPbtVZJJlX6HHWqubbrpJX/va1/a4z7x58/TSSy/p5ptv1qmnnqqf/exnKVkrAAAAAAeVnk5wOZlHMSvl5eWpoaFBknTmmWfqgQceUGNjoyRpy5Ytqqio0NatW+X3+3XllVfqhhtu0Lx58yR1vC8AAACA5OoZlZ4UMcbIZYxi1qqkpETHHXecxo0bp7PPPltXXHGFpk6dKknKzc3VI488otWrV+uGG26Qy+VSVlaW/va3v0mSrr32Wp111lnq27cvgwwAAACAJDN2t1atTDR58mQ7Z86cDseWLVum0aNHp3wtH2+tV0GOR/2K/Cl7znR9rwAAAEB3YYyZa62dvLfraG/rJJdx2tsAAAAAdA+Enk5yucwegwwAAAAAZK5uHXrS0Zrn7OlJ3fN1h/ZDAAAAIJN129CTnZ2tqqqqlIcCl5FiKUo91lpVVVUpOzs7Jc8HAAAA9ETddnpb//79tXnzZlVWVqb0easaQ4rGrMJVqQki2dnZ6t+/f0qeCwAAAOiJum3oycrK0pAhQ1L+vNc9Nl9LttRp+g+OSPlzAwAAAOi8btveli7+LLeawpF0LwMAAADAfiL0dFKO162mUDTdywAAAACwnwg9neT3utXUGmWqGgAAANBNEHo6KeDzOIMMorF0LwUAAADAfiD0dFJOlluS1BymxQ0AAADoDgg9neT3OqEnSOgBAAAAugVCTyf5fc6U72YmuAEAAADdAqGnk/zx9rYmKj0AAABAt0Do6aREextjqwEAAIBugdDTSTnx0NPcSnsbAAAA0B0QejopEN/TQ3sbAAAA0D0Qejophz09AAAAQLdC6Omktj09TSHa2wAAAIDugNDTSX5vvL2tlUoPAAAA0B0QejopO8slY6Rm2tsAAACAboHQ00nGGPmz3OzpAQAAALoJQs8ByPF61BRmTw8AAADQHRB6DkDAR6UHAAAA6C4IPQcgh/Y2AAAAoNsg9BwAv9dNexsAAADQTRB6DoDf66HSAwAAAHQThJ4D4Pe6GVkNAAAAdBOEngPgtLcRegAAAIDugNBzABhZDQAAAHQfhJ4DQKUHAAAA6D4IPQcg4HWruTUqa226lwIAAADgUxB6DkCO1yNrpZbWWLqXAgAAAOBTEHoOgN/rliQF2dcDAAAAZLykhx5jjNsYM98Y80L86yHGmA+NMauNMf82xniTvYaulhMPPYytBgAAADJfKio910ta1u7r30v6s7V2uKQaSV9OwRq6VMDrkSSGGQAAAADdQFJDjzGmv6RzJf09/rWRNE3Sk/GbPCTpwmSuIRna2tsYWw0AAABkvmRXem6X9ENJbTv+SyTVWmvb0sJmSf32dkdjzLXGmDnGmDmVlZVJXmbn5CRCD5UeAAAAINMlLfQYY86TVGGtnXsg97fW3mutnWytnVxWVtbFqzs4tLcBAAAA3YcniY99nKTzjTHnSMqWlC/pL5IKjTGeeLWnv6QtSVxDUuTQ3gYAAAB0G0mr9Fhrb7LW9rfWDpZ0maS3rLVfkDRd0iXxm10j6blkrSFZ/LS3AQAAAN1GOs7T8yNJ3zPGrJazx+f+NKzhoBB6AAAAgO4jme1tCdbatyW9Hf98raQpqXjeZPHH9/Q0094GAAAAZLx0VHq6Pa/HJY/LUOkBAAAAugFCzwHK8boJPQAAAEA3QOg5QH6vm+ltAAAAQDdA6DlAAa+HSg8AAADQDRB6DlCO161mQg8AAACQ8Qg9B8jvdStIexsAAACQ8Qg9B8jv9VDpAQAAALoBQs8B8jO9DQAAAOgWCD0HiJHVAAAAQPdA6DlAjKwGAAAAugdCzwFiZDUAAADQPRB6DlCO161QJKZozKZ7KQAAAAA+AaGnM5prpU0fSaFG+b1uSaLFDQAAAMhwhJ7O2PShdP9p0s4V8ns9ksTYagAAACDDEXo6wxtwLsNBBXxOpSdI6AEAAAAyGqGnM9qFnpwsp9JDexsAAACQ2Qg9neHNdS7bVXqY4AYAAABkNkJPZyQqPY2JPT3BEJUeAAAAIJMRejqjXXtb2/Q2BhkAAAAAmY3Q0xlt7W2hRgXaKj2EHgAAACCjEXo6w+WWPDlOe5uP8/QAAAAA3QGhp7O8gQ7tbQwyAAAAADIboaez4qEn2+OWMVITgwwAAACAjEbo6SxvrhRulMtl5M9ys6cHAAAAyHCEns6KV3okKcfrob0NAAAAyHCEns7yBqRwoyQp4HMzyAAAAADIcISezvLlJio9fq9HwRCVHgAAACCTEXo6K76nR5L8Xio9AAAAQKYj9HRWuz09Tuih0gMAAABkMkJPZ7ULPQGvh0oPAAAAkOEIPZ3lzZUiLVI0Ir/XzZ4eAAAAIMMRejrLG3Auw43y+9xqbiX0AAAAAJmM0NNZidATVMDrUTBEexsAAACQyQg9neXNdS7DQeV43QpFYorGbHrXBAAAAGCfCD2dlQg9jQp4PZLEMAMAAAAggxF6Oqtde5vf55YkxlYDAAAAGYzQ01ntQ4/XCT3s6wEAAAAyF6Gns9q1t/kT7W1UegAAAIBMRejprHYjqwOEHgAAACDjEXo6q117W05bexuDDAAAAICMRejprHYjqwPxQQbNVHoAAACAjEXo6Sy3R/Jkd2hvY5ABAAAAkLkIPQfCG+jQ3saeHgAAACBzEXoORDz0MMgAAAAAyHyEngPhzZVCDcrOcskYqYlBBgAAAEDGIvQciHilxxgjf5ZbwRCVHgAAACBTEXoORDz0SJLf51FzK5UeAAAAIFMReg6ENzcRegJeKj0AAABAJiP0HAhvrhRulCTleD3s6QEAAAAyGKHnQLRrbwt43UxvAwAAADIYoedAeAOJSo/f51GQ0AMAAABkLELPgfDmSpEWKRpxKj0h2tsAAACATEXoORDegHPZGlQO7W0AAABARkta6DHGZBtjZhtjFhpjlhpjfhk//qAxZp0xZkH8Y2Ky1pA0vlznMhxUgEEGAAAAQEbzJPGxQ5KmWWsbjTFZkt43xrwcv+4Ga+2TSXzu5PLuCj1+n5s9PQAAAEAGS1qlxzoa419mxT9ssp4vpdra28KN8md5FI7EFInG0rsmAAAAAHuV1D09xhi3MWaBpApJr1trP4xfdYsxZpEx5s/GGN8+7nutMWaOMWZOZWVlMpfZeW2hJ9SogM8tSWpqpdoDAAAAZKKkhh5rbdRaO1FSf0lTjDHjJN0kaZSkoyQVS/rRPu57r7V2srV2cllZWTKX2XmJSk9Qfq/TIdgUIvQAAAAAmSgl09ustbWSpks6y1q7Ld76FpL0D0lTUrGGLpXY09Movzde6WGYAQAAAJCRkjm9rcwYUxj/PEfS6ZKWG2P6xI8ZSRdKWpKsNSRNh0pPW+ih0gMAAABkomROb+sj6SFjjFtOuHrCWvuCMeYtY0yZJCNpgaSvJ3ENydFuelugwHkJg5ygFAAAAMhISQs91tpFko7Yy/FpyXrOlGlX6cnxMsgAAAAAyGQp2dPT47izJLdPCjcowCADAAAAIKMReg6UN9BhT0+QQQYAAABARiL0HChvbofQ08wgAwAAACAjEXoOlDcghRsV8MUHGVDpAQAAADISoedAxdvbfB6XXIY9PQAAAECmIvQcKJ/T3maMkd/r4Tw9AAAAQIYi9Byo+J4eSfJ73WqivQ0AAADISISeA+UNSKEGSVLA51GQSg8AAACQkQg9Byq+p0eScrLcaqbSAwAAAGQkQs+Bahd6Aj63ggwyAAAAADISoedAeXOlSLMUi8YHGVDpAQAAADIRoedAeQPOZfwEpUxvAwAAADIToedAeXOdy3CQkdUAAABABiP0HKhE6Gl09vTQ3gYAAABkJELPgUq0tzUqh/Y2AAAAIGMReg5Uuz09Aa9H4UhMrdFYetcEAAAAYA+EngPVYU+PW5Ko9gAAAAAZiNBzoNq1t/m9HklSM6EHAAAAyDiEngPl21XpCficSg/DDAAAAIDMQ+g5UG2VntCuSk9TiEoPAAAAkGkIPQcqq+PJSSWpiUoPAAAAkHEIPQfK45Xc3vieHgYZAAAAAJmK0HMwvIH4nh6nvY09PQAAAEDmIfQcDG+uFA4qJ4tKDwAAAJCpCD0HwxuQwo2JSk9TiEoPAAAAkGkIPQfDm9thT0+QSg8AAACQcQg9ByO+p8fnccllODkpAAAAkIkIPQcjvqfHGKOA18MgAwAAACADEXoORnxPjyT5fW5OTgoAAABkIELPwYi3t0mS3+tRUyuhBwAAAMg0hJ6D0SH0uJneBgAAAGQgQs/B8OZKrU1SLMqeHgAAACBDEXoOhi/XuQwHleN1M70NAAAAyECEnoPhbQs9jQr43JynBwAAAMhAhJ6D0RZ6Qo3OIAP29AAAAAAZh9BzMHy7Kj1+L5UeAAAAIBMReg6GN+Bchp1KD3t6AAAAgMxD6DkY7drbAl63wtGYwpFYetcEAAAAoANCz8Hw5TmX8eltkqj2AAAAABmG0HMwEu1tDQr4PJKkplaGGQAAAACZhNBzMDpMb3MqPcEQlR4AAAAgkxB6Dkai0hOU3xuv9ISp9AAAAACZhNBzMFxuKcvvnJw0XulpYk8PAAAAkFEIPQfLm+uMrPZR6QEAAAAyEaHnYPly2dMDAAAAZDBCz8HyBuInJ2VkNQAAAJCJCD0Hy5snhYMKxAcZBGlvAwAAADIKoedgeQNSqCFxclIGGQAAAACZhdBzsHzOIAOfxyW3yzDIAAAAAMgwhJ6D5c2VwkEZY+T3uhlkAAAAAGQYQs/B8jrT2yTJ73VT6QEAAAAyDKHnYMXb22StAl4Pe3oAAACADJO00GOMyTbGzDbGLDTGLDXG/DJ+fIgx5kNjzGpjzL+NMd5krSElvLmSrNTaJL/PTegBAAAAMkwyKz0hSdOstYdLmijpLGPMMZJ+L+nP1trhkmokfTmJa0g+b8C5DDXKn+VRMER7GwAAAJBJkhZ6rKMx/mVW/MNKmibpyfjxhyRdmKw1pIQvz7kMN8rvc6u5lUoPAAAAkEmSuqfHGOM2xiyQVCHpdUlrJNVaa9vKIZsl9dvHfa81xswxxsyprKxM5jIPjjfXuQw3KuCl0gMAAABkmqSGHmtt1Fo7UVJ/SVMkjerEfe+11k621k4uKytL1hIPXrv2thwve3oAAACATJOS6W3W2lpJ0yVNlVRojPHEr+ovaUsq1pA0ifa2oAKEHgAAACDjJHN6W5kxpjD+eY6k0yUtkxN+Lonf7BpJzyVrDSmRaG9rkN/n4Tw9AAAAQIbxfPpNDlgfSQ8ZY9xywtUT1toXjDEfS3rcGPMbSfMl3Z/ENSRfh+ltbrVGrcKRmLweToEEAAAAZIKkhR5r7SJJR+zl+Fo5+3t6Bl9bpScov895OZvDUUIPAAAAkCF4Z36wOkxvc0uSgrS4AQAAABmD0HOw3FmS2yeFneltktjXAwAAAGQQQk9X8OVKIec8PZKY4AYAAABkEEJPV/AGpHCj/L54e1uI0AMAAABkCkJPV/Dmxc/T01bpob0NAAAAyBSEnq7gy5VCDfIn9vRQ6QEAAAAyBaGnKyTa26j0AAAAAJmG0NMVvLnx9jb29AAAAACZhtDTFXx5UmjXyOrmVkIPAAAAkCkIPV3BG5DCDfK6XfK4jIIh2tsAAACATEHo6Qrx9jYjye91M8gAAAAAyCCEnq7gDUixiBQJye/1MMgAAAAAyCCEnq7gy3Muw0H5fW4FqfQAAAAAGYPQ0xW8uc5luEEBr0dN7OkBAAAAMgahpyt4A85lqFF+L5UeAAAAIJMQerqCr63SE1TAx54eAAAAIJMQerqCt21PT4MzvY2TkwIAAAAZg9DTFdq1twW8HgWp9AAAAAAZg9DTFXZrbwtS6QEAAAAyBqGnKyTa2xoV8LkVDEdkrU3vmgAAAABIIvR0jbb2tnCj/F6PrJVaWmPpXRMAAAAASYSeruHxSS6PFGpUrs8tSWrkXD0AAABARiD0dAVjnGpPvNIjibHVAAAAQIYg9HQVb158kIFT6WGYAQAAAJAZCD1dxZcrhRoU8FHpAQAAADIJoaer7Nbexp4eAAAAIDMQerqKN7dDe1tTmPY2AAAAIBMQerqKL08KNSoQr/QEqfQAAAAAGYHQ01W8ASm8a08PoQcAAADIDISerhJvb/N749PbaG8DAAAAMgKhp6v4cqVQo3welzwuw/Q2AAAAIEMQerqKN1eKhmRiEfm9bs7TAwAAAGQIQk9X8eY6l+FGBXwe9vQAAAAAGYLQ01V88dATioce2tsAAACAjEDo6SregHMZH2bAeXoAAACAzEDo6SrePOcy3EjoAQAAADIIoaertFV6Qg3yez1qJvQAAAAAGYHQ01Xa9vSEg8rxutnTAwAAAGQIQk9XaT+9zeum0gMAAABkCEJPV2kLPfH2Nvb0AAAAAJmB0NNVdmtva6K9DQAAAMgIhJ6ukuWXZBLtba1Rq9ZoLN2rAgAAAA55hJ6uYozT4hYOKsfrkSRa3AAAAIAMQOjpSr7c+J4etyTR4gYAAABkAEJPV/LmJk5OKlHpAQAAADIBoacreQNSOCh/W3tbiNADAAAApBuhpyv58qRQI+1tAAAAQAYh9HQlb0AKt9vT00qlBwAAAEg3Qk9Xik9vo70NAAAAyByEnq7ky6W9DQAAAMgwhJ6ulKj0OKGnmfY2AAAAIO0IPV3Jmyu1BuXPcl7WIO1tAAAAQNoRerqSL1eSlG2bZIzUTHsbAAAAkHZJCz3GmAHGmOnGmI+NMUuNMdfHj//CGLPFGLMg/nFOstaQct6AJMmEm+TPcnNyUgAAACADeJL42BFJ37fWzjPG5Emaa4x5PX7dn621f0zic6eHN8+5DDcqx+tRkNADAAAApF3SQo+1dpukbfHPG4wxyyT1S9bzZYR4e5tCDQr43LS3AQAAABkgJXt6jDGDJR0h6cP4oW8ZYxYZYx4wxhTt4z7XGmPmGGPmVFZWpmKZBy/e3qZwUDm0twEAAAAZIemhxxiTK+kpSd+x1tZL+pukYZImyqkE/Wlv97PW3mutnWytnVxWVpbsZXYNb7zSE3bO1UPoAQAAANIvqaHHGJMlJ/A8aq19WpKstTustVFrbUzSfZKmJHMNKdUWekKNCvg8nJwUAAAAyADJnN5mJN0vaZm19rZ2x/u0u9lFkpYkaw0p59tV6aG9DQAAAMgMyZzedpykqyQtNsYsiB/7saTLjTETJVlJ6yV9LYlrSC3a2wAAAICMk8zpbe9LMnu56qVkPWfatRtk4Pd5CD0AAABABkjJ9LZDhsstZfmlUEP85KTs6QEAAADSjdDT1by5ifa25taorLXpXhEAAABwSCP0dDVvINHeZq3U0hpL94oAAACAQxqhp6v5cqWQU+mRpCAtbgAAAEBa7VfoMcZcb4zJN477jTHzjDFnJHtx3ZI3LzGyWpKaGWYAAAAApNX+Vnr+x1pbL+kMSUVyRlHfmrRVdWfegBR2Tk4qiQluAAAAQJrtb+hpGz19jqSHrbVLtfdx1Ii3t+XQ3gYAAABkhP0NPXONMa/JCT2vGmPyJLFDf2/aprfR3gYAAABkhP09OemXJU2UtNZa22SMKZb0paStqjvz5krhIO1tAAAAQIbY30rPVEkrrLW1xpgrJd0sqS55y+rGfLnxQQbOS8sJSgEAAID02t/Q8zdJTcaYwyV9X9IaSf9M2qq6M29AsjEFXK2SqPQAAAAA6ba/oSdirbWSLpD0V2vtnZLykresbsybK0nyq1kSoQcAAABIt/3d09NgjLlJzqjqE4wxLklZyVtWN+ZzsmCOjYeeEO1tAAAAQDrtb6Xn85JCcs7Xs11Sf0n/l7RVdWfegCQpK9Ikr9ulplYqPQAAAEA67VfoiQedRyUVGGPOk9RirWVPz97E29sUds7VQ6UHAAAASK/9Cj3GmEslzZb0OUmXSvrQGHNJMhfWbcXb2xRqVK7PowZCDwAAAJBW+7un5yeSjrLWVkiSMaZM0huSnkzWwrqteHubwo3Kyy5VQwuhBwAAAEin/d3T42oLPHFVnbjvoaVde1t+dpYaCT0AAABAWu1vpecVY8yrkh6Lf/15SS8lZ0ndnK8t9ASVm+1RRUNLetcDAAAAHOL2K/RYa28wxlws6bj4oXuttc8kb1ndWFulJ9SovGyP1lRS6QEAAADSaX8rPbLWPiXpqSSupWdwZ0lunxRuUK7PQ3sbAAAAkGafGHqMMQ2S7N6ukmSttflJWVV35w1I4aDysrMYZAAAAACk2SeGHmttXqoW0qP4cp32tiKPwtGYWlqjys5yp3tVAAAAwCGJCWzJ4M2Lj6x2MmUj5+oBAAAA0obQkwzegBR2Tk4qiRY3AAAAII0IPcnQ1t6WnSVJDDMAAAAA0ojQkwze3A7tbQ0trWleEAAAAHDoIvQkgzfXOTlpvL2tnkoPAAAAkDaEnmTw5UqhBuW3tbcxyAAAAABIG0JPMrRVemhvAwAAANKO0JMM3oAUa1WuOyqJQQYAAABAOhF6ksHnnNPVG22Sz+NSA+1tAAAAQNoQepLBG3Au4+fqYU8PAAAAkD6EnmTw5jqX4UblZnsUJPQAAAAAaUPoSQZfPPSEGhXwetjTAwAAAKQRoScZ2ld6aG8DAAAA0orQkwy7t7eFCT0AAABAuhB6kqF9e5uP9jYAAAAgnQg9yZCo9ATj7W3R9K4HAAAAOIQRepIhEXoalOtzqzHUmt71AAAAAIcwQk8yeHySyyOFgwr4PGppjSkSjaV7VQAAAMAhidCTDMY4JygNOdPbJCkYpsUNAAAASAdCT7J48xIjqyUxthoAAABIE0JPsvhypbAzvU2SgoQeAAAAIC0IPcnS1t6W7YSeBsZWAwAAAGlB6EkWb26H9jYqPQAAAEB6EHqSxZubOE+PROgBAAAA0oXQkyy+XCnUkAg9DYQeAAAAIC0IPckSr/QwyAAAAABIL0JPsngD8eltbklSI4MMAAAAgLQg9CSLL0+KtMhnrLxulxrDhB4AAAAgHQg9yeLNdS7Dzthq2tsAAACA9Eha6DHGDDDGTDfGfGyMWWqMuT5+vNgY87oxZlX8sihZa0grb8C5jLe40d4GAAAApEcyKz0RSd+31o6RdIykbxpjxki6UdKb1toRkt6Mf93z+OKVnlCjAl6PGkPR9K4HAAAAOEQlLfRYa7dZa+fFP2+QtExSP0kXSHoofrOHJF2YrDWklTfPuQwHlUd7GwAAAJA2KdnTY4wZLOkISR9KKrfWbotftV1S+T7uc60xZo4xZk5lZWUqltm1Eu1tDQr4PGok9AAAAABpkfTQY4zJlfSUpO9Ya+vbX2ettZLs3u5nrb3XWjvZWju5rKws2cvseu3b23xUegAAAIB0SWroMcZkyQk8j1prn44f3mGM6RO/vo+kimSuIW0S09uCyvN51EDoAQAAANIimdPbjKT7JS2z1t7W7qrnJV0T//waSc8law1plQg9DVR6AAAAgDTyJPGxj5N0laTFxpgF8WM/lnSrpCeMMV+WtEHSpUlcQ/r4dlV6cn0eNYWjisas3C6T3nUBAAAAh5ikhR5r7fuS9vUO/9RkPW/GyPJLMlKoUbk+52UOhiPKz85K77oAAACAQ0xKprcdkoxxWtzCziADSbS4AQAAAGlA6EkmX54UqlduthN6GlsIPQAAAECqEXqSKTtfaqlXrs8tSZyrBwAAAEgDQk8y+fKkUINyfc4+nmAomuYFAQAAAIceQk8y+fKlUL0CiUpPa5oXBAAAABx6CD3JlJ0fr/TE9/RQ6QEAAABSjtCTTL68+J6etkEGVHoAAACAVCP0JFOiva3tPD1UegAAAIBUI/QkU3aB1Noknysmj8swvQ0AAABIA0JPMvnyJEkm1KDcbA/n6QEAAADSgNCTTL585zJUr4DXoyCVHgAAACDlCD3JFK/0KNSgvGwP7W0AAABAGhB6kik7XulpcYYZEHoAAACA1CP0JFP79jYf7W0AAABAOhB6kikRehqU5/OogdADAAAApByhJ5kS7W11CvjcVHoAAACANCD0JFO79rZcX5aCIU5OCgAAAKQaoSeZPD7JlSWFGpTrc6sxFFEsZtO9KgAAAOCQQuhJJmOcFrf49DZJamql2gMAAACkEqEn2Xz5TqUn2wk9jS3s6wEAAABSidCTbL68+J6eeOhhmAEAAACQUoSeZMsukFp2hR4muAEAAACpRehJNl+eFGpI7Omh0gMAAACkFqEn2Xz5UqiO9jYAAAAgTQg9yRaf3tYWehoYZAAAAACkFKEn2eLtbYU5Tuipa25N84IAAACAQwuhJ9l8+ZKNKs/dKmOkuqZwulcEAAAAHFIIPcmWnS9JcocbVJCTpZomKj0AAABAKhF6ks3nhB6nxS1LtbS3AQAAAClF6Em2ROipV4Hfq1ra2wAAAICUIvQkW7y9TS11KszJYpABAAAAkGKEnmTz5TmXoQYV+bNUy54eAAAAIKUIPcnWrr2t0O9VDe1tAAAAQEoRepIt0d5Wr4KcLDW0RBSJxtK7JgAAAOAQQuhJNm+eJBOv9GRJkupbIuldEwAAAHAIIfQkm8vlVHta6lTk90oSE9wAAACAFCL0pEJ2gdRcq4J4pYdz9QAAAACpQ+hJheyCxMhqiUoPAAAAkEqEnlTILnRCT6K9jUoPAAAAkCqEnlTILpBaalXU1t5G6AEAAABShtCTCvFKT152loxhTw8AAACQSoSeVIjv6XG7jPKzs9jTAwAAAKQQoScVcgqlcKMUjajQn0V7GwAAAJBChJ5UyC5wLuMT3GhvAwAAAFKH0JMKidBTq0K/V3W0twEAAAApQ+hJhfaVHj+VHgAAACCVCD2pkF3oXLbUqjAnSzVBKj0AAABAqhB6UqFdpafA71V9S0TRmE3vmgAAAIBDBKEnFdqFnrYTlNbT4gYAAACkBKEnFXIKncv4nh6JE5QCAAAAqULoSYUsv+TySM21KszxSpJqmOAGAAAApAShJxWMcVrcWupUEK/01HGCUgAAACAlCD2pEg89RX6n0lPbTKUHAAAASIWkhR5jzAPGmApjzJJ2x35hjNlijFkQ/zgnWc+fcbILEyOrJamWSg8AAACQEsms9Dwo6ay9HP+ztXZi/OOlJD5/ZolXevIJPQAAAEBKJS30WGvflVSdrMfvduKhx+0yys/2qJZBBgAAAEBKpGNPz7eMMYvi7W9F+7qRMeZaY8wcY8ycysrKVK4vOXIKpeZaSVKh38vIagAAACBFUh16/iZpmKSJkrZJ+tO+bmitvddaO9laO7msrCxFy0uieKVHkor8WbS3AQAAACmS0tBjrd1hrY1aa2OS7pM0JZXPn1bZBVI0JLW2qIBKDwAAAJAyKQ09xpg+7b68SNKSfd22x8kucC5b6lSYk8WeHgAAACBFPMl6YGPMY5JOllRqjNks6eeSTjbGTJRkJa2X9LVkPX/GyS50LltqVUh7GwAAAJAySQs91trL93L4/mQ9X8ZLhJ46FfoLVd/SqmjMyu0yaV0WAAAA0NOlY3rboWm39jZrpYYWqj0AAABAshF6UiWn0LlsdtrbJE5QCgAAAKQCoSdVcuKnJGquVpHfK0mqZpgBAAAAkHSEnlRp29PTVK2igBN6mOAGAAAAJB+hJ1XcHmdfT3O1iuOVnqpGQg8AAACQbISeVMopjld6nD09NVR6AAAAgKQj9KSSv0RqrlauzyOv26XqIIMMAAAAgGQj9KSSv1hqqpIxRkWBLFUHQ+leEQAAANDjEXpSKadYaqqRJBX5vVR6AAAAgBQg9KSSv1hqrpYkleR62dMDAAAApAChJ5VyiqVwoxQJq8jvVU2Q0AMAAAAkG6EnlfzFzmVztYoDXlURegAAAICkI/SkUlvoaXJCT11zqyLRWHrXBAAAAPRwhJ5UymkLPVUqDjgnKK1tZpgBAAAAkEyEnlRq195W5HdCTzUtbgAAAEBSEXpSKWdXe1tJgNADAAAApAKhJ5XaV3rioYcJbgAAAEByEXpSKStHyvInBhlIYoIbAAAAkGSEnlTLKZaaaxJ7eqj0AAAAAMlF6Ek1f5HUVCWvx6U8n0fVTYQeAAAAIJkIPamWUyw1VUuSigJeKj0AAABAkhF6Us1fIjXvCj3s6QEAAACSi9CTav5dlZ6SgFc1tLcBAAAASUXoSbWcYqmlVorFVOT3qibYmu4VAQAAAD0aoSfV/MWSjUkttSoOZKkqGEr3igAAAIAejdCTajnxE5Q2Vas44FNLa0zN4Wh61wQAAAD0YISeVPOXOJfN1SrJdc7VU9lAtQcAAABIFkJPqgXioSe4U30KsiVJ2+tb0rggAAAAoGcj9KRaoJdz2bgjEXq21TWncUEAAABAz0boSbVAmXMZrFTvghxJ0vY6Kj0AAABAshB6Us3jlbILpcYK5fo8yvN5tI3QAwAAACQNoScdcntJwQpJUu+CbNrbAAAAgCQi9KRDoJfUWCnJCT20twEAAADJQ+hJh9yyRKWnT0E27W0AAABAEhF60qFDpSdHlY0htUZjaV4UAAAA0DMRetIht0wK1UmtLSrL88laqToYTveqAAAAgB6J0JMOueXOZbBCpQGvJKmqkdADAAAAJAOhJx0SJyitVEmuT5JUFQylcUEAAABAz0XoSYfcthOUVqgkl0oPAAAAkEyEnnRIVHoqVBpwKj07G6n0AAAAAMlA6EmHwK5KT36ORx6XURWDDAAAAICkIPSkQ1a25CuQGitljFFJrldVVHoAAACApCD0pEu7E5QWB3zs6QEAAACShNCTLoFeUqMTekpzvdpJexsAAACQFISedMktS4SekgDtbQAAAECyEHrSJdAr0d5Wkuu0t1lr07woAAAAoOch9KRLbrnUUidFQhpQlKPm1qgqGqj2AAAAAF2N0JMuiROUVmpk73xJ0vLtDWlcEAAAANAzEXrSJa+Pc1m/TSN750mSVmyvT+OCAAAAgJ6J0JMu+X2dy/otKg54VZbn04rtjeldEwAAANADEXrSJb+fc1m/RZI0qneeVuyg0gMAAAB0NUJPuuQUSVl+qc4JPSPL87RqR6OiMSa4AQAAAF0paaHHGPOAMabCGLOk3bFiY8zrxphV8cuiZD1/xjPGaXGLV3oO652nUCSmDVXBNC8MAAAA6FmSWel5UNJZux27UdKb1toRkt6Mf33oyu/Xob1NklYwwQ0AAADoUkkLPdbadyVV73b4AkkPxT9/SNKFyXr+bqGgf6K9bUSvPBnD2GoAAACgq6V6T0+5tXZb/PPtksr3dUNjzLXGmDnGmDmVlZWpWV2q5feVGrdL0YhyvG4NLglQ6QEAAAC6WNoGGVhrraR97tq31t5rrZ1srZ1cVlaWwpWlUH4/ycac4CPpsPJcrdxB6AEAAAC6UqpDzw5jTB9Jil9WpPj5M0tibPVWSdLQslxtqmlighsAAADQhVIdep6XdE3882skPZfi588sBfHQU7dZkjSw2K/WqNX2+pY0LgoAAADoWZI5svoxSTMljTTGbDbGfFnSrZJON8asknRa/OtD124nKB1Y7JckbaxqSteKAAAAgB7Hk6wHttZevo+rTk3Wc3Y72QVSViDR3tYWejZVN2nqsJJ0rgwAAADoMdI2yAByTlBa0C/R3tanIFtul9HGaio9AAAAQFch9KRbuxOUetwu9S3MJvQAAAAAXYjQk275/RLtbZLT4rZwc62qGkNpXBQAAADQcxB60q2gn9SwXYq2SpKuOmawttW16IYnF6V5YQAAAEDPQOhJt8KBkqxUu1GSdNa43jpvfB9OUgoAAAB0EUJPuhUPdS5r1iUOlRdka0d9i2KcpBQAAAA4aISedCsa4lxW7wo9fQqy1Rq1qgqG07QoAAAAoOcg9KRbXm/JkyPVrE8cKs/PliRtr2tJ06IAAACAnoPQk27GSEWDpeq1iUN9CuKhp57QAwAAABwsQk8mKB7Sob2td1voqWtO14oAAACAHoPQkwmKhzqDDGIxSVJpwCePy2gb7W0AAADAQSP0ZIKS4VKkRarfLElyuYzK87PZ0wMAAAB0AUJPJig9zLmsXJk41K8wRxurm9K0IAAAAKDnIPRkgrKRzuXOFYlDY/rm6+Nt9Ypyrh4AAADgoBB6MkGgVMoplip3hZ5x/QrUFI5q3c7GNC4MAAAA6P4IPZmibKS0c1d727h++ZKkxVvq0rUiAAAAoEcg9GSK0sOkyuWSddrZhpflKjvLpcWb69O8MAAAAKB7I/Rkit7jpeYaqX6LJMnjdunIgUV6a/kOWcu+HgAAAOBAEXoyRZ/DncttixKHPntkf62vatLcDTVpWhQAAADQ/RF6MkX5WElG2rYwcejscb3l97r13IKt6VsXAAAA0M0RejKFN+Ds69m+q9IT8Hk0oX+BFjHMAAAAADhghJ5M0mdCh0qPJI3uk68V2zlfDwAAAHCgCD2ZpM/hziCD4M7EodF98tXSGtO6ncE0LgwAAADovgg9maT3BOeyXbVnTB/nfD2n3faO/jFjXTpWBQAAAHRrhJ5M0iceetrt6xneKzfx+V/eXKVgKJLqVQEAAADdGqEnk+QUSYWDOlR6srPc+uPnDtevLxyn2qZWPT1vcxoXCAAAAHQ/hJ5M0+dwacu8DocumdRfVx0zSCN65erlJdvTtDAAAACgeyL0ZJqBU6XaDVLdlj2uOn1MuT5cV63apnAaFgYAAAB0T4SeTDPoWOdy48w9rjpjbG9FY5ZqDwAAANAJhJ5M03u85M2T1r+/x1WH9y/Q+H4FunP6aoUjMUlSZUNId729WpFoLNUrBQAAALoFQk+mcbmlQVOlde/scZUxRj84c6Q21zTrgfj46ltfXq4/vLJC76yslCTVNbdq/sYatbRG9+vpFm6q1eAbX9Saysau+x4AAACADELoyUTDT5eq10pVa/a46sQRpTprbG/d9vpKvbJkm56Z70xze3r+FtU1tepX//1YF931gUb99BVN++Pbmr6i4hOfqu3cP++v2tnheCxmFYrsOzg1hSO65G8f7HE/SXpk1gaNvPnlTw1e4UhMU3/3pu57d+0n3i5Zlm+v1+qKhrQ8NwAAAFKH0JOJRpzuXK56fY+rjDH69YXj5Pe69fVH5ik/J0tnje2tFxdt0zG/e1OvLNmm0lyfvn/6YTJGuv6x+Wpoad3nU22ta5EkNe52/p8fP7NYp9/2rlr30Tb38uLtmrOhRi8u3qZtdc2y1iauu/nZJQpFYlq2rV7zNtbss/Vu8ZZabatr0S0vLdvvylRXCUdiuuaB2frfR+Z9+o0BAADQrRF6MlHxEKlkhLTy5b1eXZbn068uGCe3y+g3F47TTeeM0hePHSwrq2A4qtsuPVzXnTpCt3/+CNW3RPTNf83XzDVVifs3hiI6/6/va/qKCi3bWi9J2lzTlLj+vVWVevyjTdpY3aQ3Pt6ROP7M/M36YLVT2XliziZJ0itLtunYW9/Scwu2qiYY1rcfm5+4/Y+fWaLP3vWBrrz/w71+H7PX1SQ+b3u8ztpc07RHYNsf/124VTvqQ1pV0agV2/dd7WloadWz87ck9lABAACg+yH0ZKox50vr3pUa996edv7hfbXgZ6frvAl9NagkoF+cP1Y/OGOkRpbnaeqwEknS+P4FumRSf324tkrf/feCRDXlo/XVWrS5Tl/6x0dqiAeGjdVO6NlU3aRvPzZfw8oC6leYoz++tkLff2Kh/jlzvb7774W64u8fav3OoD5cV63igFc1Ta2yVvrXhxv19Pwten7h1sQal21zAtWstdWqa+5YbQpHYpq+okJDywKaMqRYt72+Uif933S9vHhbh9u9vaJC//vI3EToqAmGtaPeqU5Za3XhnR/oz6+vVCgS7VBt2p21VrHYrusf/XCDBhTnyGWk5xc648HvnL5aczdU66v/nKPnF25VcziqK++fre/8e4H+8MryxH1bWqOqiK/h00RjVg+8v07XPz4/8XoAAAAgtQg9mWr85yQbk5Y+u8+b5GVndfj6KycM1avfPVFZ7l0/1j9+7nD940tHaXt9i3745CJtrmnS3PW7KizHDy/VGWPKtanaaVG78elFisSs/n7NUfr5Z8YoGIrqqXmb9fPnlybu89PnlshlpBvOHClJ8rpdmr2+Wo/P3qh+hTma99PT5TLObf1etyRpebs3/C2tUZ1629uava5aRw8p1k1nj1J9c6sqG0L65r/m6chfv67/zNmkmmBYd7y5Si8v2a5/fbhBzeGoLrxrhi68c4ZCkai21rVoZ2NIi7fUaeTNr+iPr63Qs/O3aENVMPFcjaGIXlu6XQ99sF7H3vqWWlqj2l7Xonkba/X5yQN08sheenz2JlU1hvR/r67Qj55arNc/3qFvPzZft7+5Ugs31WrigEL9/f11iWEPt7y4TGf/5T1FY/sOWW3mbazRr174WC8v3q5L/vaBNtc0aW38cdZUNurVpZ0bP94UdkKqtVbBA6hwAQAAHIo86V4A9qHXaKl8vDT/n9KUr0rGHPBDHTusVF88drD+9eFGzV5XrXA0pgn9C/T7iydoWFmubn9jpd5aXqHnF27VjNVV+tUFYzWkNKAhpQGdPqZcj83epB8/szjxeO+t2qmTR5bpnPF9dPc7a/Sd00boh08u0qqKRl0yqb+KA16V52drW12LzhnfR0/O3axl2+p19FCnAvXAjHXaVN2sb586QpdPGaA+BTmaedOpynK79MD76zRjzU7d8OSixPNlZ7n0lzdXacnWem2ocipSl907S6W5PknSvA1OiLtz+q7BD+eO76PfXTxe/569Sbe8tExF/izVNLXqP3M2aWm8pe+scX00ZUiJLr1npm5/Y5UkaXXFril2/5q1UZL024vG65w73tP7q3ZqYLFfLyzaqpqmVi3bVq9x/Qo+8bVfucNpnbvj8on6+iPz9MV/fKTVFY164brjdd7/c8aSr77lbHncn/73h8dnb9SPn1msb00boQFFOfrF80v1/o+mqSjgVSQaUyRmlZ3l/tTHAQAAONQQejLZlK9I/71e2vCBNPi4g3qoX5w/VpdNGaAv3PehqoNhnX94X43uky9JGljsVyRmdfMzSzSuX76+cPSgxP2MMTpmaLEkyetx6Z6rJmnBxlqdN6GPCnKy9M4Np0iSnp63Re+t2qkjBhZKkvoW5mhbXYuOHlKs1z/eoV/892N9tL5GU4YU64+vrtBpo8v1vdMPSzxPeX62JOkHZ47UtyMj9M7KSj0xZ5M+Wl+tu6+cpKvvn60n527WF48drNUVjXp/9a6pcZHdKi7XTB2kh2Zu0Ji++Vq6tU6SVNPktNf99DmnYjWuX76G98qVJA3vlasn525O3L8gJ0uluV6tqQyqb0G2xvTNV/+iHH2wZqeGlAYSjzVrbZXG9SvQR+urtW5nUH6vW4NLAvrnzPW6+Mj+OnpoiVZXNCony61TR5crJ8udCFW3vLgs8Xzrq4Ia3itP2+ta5HEbNYWiKvBnqSDHqeRFY1a3vb5C97yzVsUBn+54c5XK8nwKhqN6f/VOje9XoIdmrte8DTV67lvHd/I349PVt7Qq2+OW10NhGAAAdE+Enkw2/lLpjV9K79wqDXr+oKo9kjSqd75e/e6JenDGen1ucv/E8Wmje+mw8lytrQwmBiS0N6Q0oLI8n/oWZOuUkb10ysheezz2rRdP0C0vfqxzxvWR5ISeuRtqNLQsoOwsl+qapRcXb9OLi7fp1FG9dPtlE/e5Tq/HpdPHlOv0MeVqjcaU5XbplovGac76Gv3k3NFyGaMXFm3V9Y8v2OO+10wdpF9eME6rKhr1rw83dmhBO3JgoeZtrNWjXzlakwYVJY6P7J3XocIztm++igNO6BlRnidJOm5Yqf49Z5M+WF2l4oBXOVluzV5Xra+cMFTXPzY/MQWvzcJNdXr1uydqdUWjhvUKKMvt0vj+BZq9rlqSNHPtrsESK7Y3anivPH3xH7NVlufTe6t2qnd+tmb9+FRJ0stLtunO6Wt0/uF99bPPjNG0P76tyoaQJOm6+OCIkeV5WlXRoLvfWSNrpf89edg+X9/OsNbqnL+8p7PH9dZPzh3TJY8JAACQaoSeTOb1S6f8WHrpB9LSp6VxFx/0Q5bm+vSD+F6cNr3ysvXK9SeqpimsknjLWHvGGP3+4vHye/f969KvMEd3fWFS4uu+BU7lZnBJQL+/eIKWbWvQwzPXqzVmddulE5Xr279fvbb9SZ+bPECfmzwgcfy00eUdbjegOEffnjZC00Y5gezqqYP19UfmSpK+dNxgDS0N6NKjBqiuqVW94lWlNsPLcjt8PaZPvopzvXph0TaNiFeDLpjYV++v3qlRvfP00/PG6J531+rZ+Vs0d0ONtta16PIpA/X5owbo6XmbtWRLneZtrNWaykatrmjUMfG2vokDCjV7XbVuu/Rwbapu1sLNtZq+okK/f2W55m2s0fLtDYl2uO31LZq1tkpHDynWve+u1ZDSgP78+Ylyu4zOn9hXj8zaqOwsl1panQEPK+L3u/Xl5fHXq7+aQlGV5nm1fmeTBpb4E695W5CUnFDT3Brd68/2xqcWqaElos01zXp/9a6Q1tIa1X3vrtVVUwep0O/dj5/iniobQioJeOVyHVyQBwAA2B+Enkw3+X+k+Q9Lr/5EGnGG5MtLytO4XGavgafNtFHl+7xubz43eYBKc30qDnh18sheOnlkL33mcKcKVODP+pR7f7qAz6N7r5okK+lrD8/V8LLcDqHozLHlGlCco03VzTp7XB9NGeK06PXK33PPS1ubW7/CHF09dZDOGNtb6+PDEEaUO9cdO7xUM26clrjPN04epqfnbdbFf/tAknTlMQM1tm+BJg4oVEVDi4679S39+oWPta2uJfH400b10nMLtuiEEWUqy3Ne61E/fVkbq5t0//vOSWLbd+pddu8snTm2XIs21+mWi3ZV4L5+0jBle9yKWqt/zFi/19fnrNvf087GkEb0ytXqykaN6ZOvf331GC3dUqevPTxXd115pE4YUaaHPlivP762Um/fcHJij5Qkbaxq0uMf7RojvmJ7vYKhiAI+j/746gr9/f11spKmDivRdf+ar2e/eZx6F2TvZSV7qm0K64Q/vKVbLhyviyf1//Q7AAAAHCSa9DOdyy2de5vUsF165UbpE8YyZ5LhvXL11ROHyrRryetf5Ff/In+XPccZY3vrtNHlyvN59hgoYIzRs984Tj89b0yHVrZ9rVWS+hXl6GsnDdOQ0oCmDi3RV08YojPG9N7rfQYU+zvsSRrVOz/xea+8bH3/jJF6e0Wl3C6j44aXSpKOGVqiD398WiLwSNKQUue5C3J27eEp9Gdpxo3TdMrIMr26dIeKA15dfOSucNC/yK+bzxujb50yXPdfM1mFu4XIQSV+9S/K0ZXHDNSqikb1yvNpxfYG/ey5Jbrx6cVqCEX0h1dWqDkc1V+nr1ZjKKKXFm/T0q11+u/CrXp58Tadcfs7HR4zZqVFm+t05/TVemjmeknO6PMn52zW9voWvb5sxz5PQru75dsb1NIaS1Sn9uW211fqyw9+tF+PCQAA8Emo9HQH/SdLJ3xfeu+PUn5/6eQbD3p/T0/hdhm9+O0TVJq3Z5tVSa5PXz5+yKc+xpDSgFzGqfS0yc5yf+oelmtPHKq65lZlZ7n32Ad17QlDlZft0ZTBxYl9QXtz39WTVNkQUiRmZa30/f8s0Oje+epXmKOffWasZqx5V/9z3OC9TmUryfXp1NHlGt+vQO+v3qnSXJ92Nob04rdPSLSyHTW4WOP7FejhWRv0jxnr5XEZXT5lgB6bvUmX3TtTOxvDKvRn6T9zNutn8SEPfQqy1dIa02eP7KdFm+t0WHmuXlq8Xc8v3KrHZm/UtFG9VOjP0n8XbpU33ib302eXOOPFrz8hUTF6Ys4mNYejuubYwR3WvSq+f2pLTbMkZ3R3n4JsvbZ0hz5zeF+5XUbRmNUdbzoT9cKRWKeGKFhrFYrEmGQHAAASCD3dxSk/keq3OkMNti2QzvuzlN833avKCANLDq56lJ3l1nXTRiRa4PaXMUY/PGvUXq9zuUyHKXj7snv16+H/OVp+n/NmfUhpQDN+NE0lgU/eN/O5yQPUvyhHNcFWbaxu6rBf6oKJ/SRJ100boZU7GnTl0YN05tje2tkY1usf79AXjx2sYWWBxFQ7SdpW16JfXTBWV08drKZwRB6XS8u3vatn5zsncf3OaSNU09Sqp+dtUWs0qrxsjxpaIqpsCOn6x+frrLG99dkj++uH8bHjTjB06doTneEKq+MVni21zfpgzU5dcd+HGloW0NrKoHY2hvSVE4bqgzW7pvP9+JnF8nlc+s2F4zpUDttYa/X+6p2aOrREHrdLzy90hly88b2TElW8xOs7a4N8HpcubdcKmUyxmGXfEgAAGYDQ0124XNKFd0m9x0lv/lq682jp8MulCZdK5WOlrJxPfwzs03fbtaql0+DSQIev27fC7cv5h/fV+Yf3VU0wrPA+WsyKA149+pVjEl/fcdkRem9VpU4dXS6XkYb3ylNlY0i/eH6pqoPhxIS+tgEHEwcWau28LXK7jA4rz5PX7dKdVxypjdVNmjqsRHdOX63NNc2asbpKM1ZXqTEUTTzXba+vlCRVNYb1lROGauWOeKWntlnzN9ZKktZWOnuo/u/VFfrskf318MwNifu3jRMf169Al08ZmDj+0uJtmjy4SKt3NOqq+2frhjNH6punDNdbyyskSXdNX63bPj+xw+tw99trlJftOejQ8/aKCmVnuRNDKvbm9Y936Kv/nKN3bjhZg0oC+7wdAABIPkJPd2KMNPWb0sizpTd/Jc39hzT7Hue6QC8n+GTlSK4sycYkWefStl3ufszueRtZSUbyBiTjco7lFEn+YucyK8c5btzOfiNrpUCp5M5yjueWS3l9nEuPT3J797ykNS8pij6lItRejtetM8bu2q80dZjz5n3+xhot3FSrAcUdq2dHDCzS0/O2aHhZbqJt7NwJfRLX33f1ZC3ZUqcf/Gehlm9v0F/fWrXHc9773lr9d+HWxHjvyoaQ5qyvTlz/9ZOG6e531uiON1fptXgV6sEP1ieu//dHm3T5lIFqjca0akejvvHoPJ0ysiwRFO9+Z40umdRfDS0RSdLT87coErP69YXjdNtrK2TlBC2v29Vhgl1nWGtljNGv/vuxfFluvXz9CYnXbWhZrpZvq9fhAwo1fXmF/vzGyvh1tT0m9Gypbe7QBgoAQHdB6OmOiodKn3tQaq6R1kyXqlZLdZukSEiKtEjRiBMsjJFk4iHFFT/manes/dftjsWiUmuTc2lczvM0VkiVy53niEUlG42HJUmhuv1fu8vjhKecIim7UMrKdo65vVJOsZRbJgXKnHDlyXbCV35/Kbct1PnjgYzglAw/O2/MXlvIjhhQKMk5h9G+jOtXoMevPUYTf/W6guGoPntkPz09z2mJe/27JyoUiekrD82RJB0ztFiz1lZr+opKnTu+jy6Y2Fenji7X0/M268EP1qvIn6XvnXGYnl2wRbVNrRrRK1cbq5sUi1mddfu7WhOvDE1fUSmtqNSo3nnaWN2kz909Uw0trTptdLnG9MnTHW+t1pvLdigY3lV5CkdjuvmZJXp5yTb978nDE+c0stbq7ZWVKvJ7dXj/AhljEhPr2nzt4bmqaQonpvs1tLTKSvrc3TM1dViJ3lu1M3E+qDbrdgb3+nq1RmO66enFGtErV187qWvOq5RMryzZpq8/Mk8Pf3mKThhRlu7lAADQKYSe7iynSBr32XSvYlcQikWccNSwVQpWSpGwFA21uwxJ4aAToto+IiEp1iRFw9L2JVKwwvn8kxi3lF3ghCLJGePtL5HCjU4oyi1zKl8F/aS+R0h5fZ2qlNvr3DYScgJUTmHSX5ruZm+BR5JG9c7TxAGFOn3MJ48uL/R7NbjEr/VVTfrcpAF6c1mFYjGrYWW5crmMZtw4TeFITAs21WrW2lmSpEmDihJVp7F981WxolI/OHOk8rOzNKQ0oMWb63TO+D76y5ur9MGaqkTgOXVUL4WjMb23aqe+eOxgDSzx64r7PpQkjemTp++dMVKtMatXl2zXeRP66I63VifW+e85zjjuJ+duSoSeeRtr9KV/ONPirj91hDbXNOuFRVv1zDeO04DiHL27cqde+3hHh+93/sZaWUmRmNV7q3bGH6e2w21WVex9St0tLy5LtO59UuipagzpnzM36BunDJPP8+nDGZrDUfk8rn3uJWoOR7V8e71G9s7TjvqQhuzWUnn1A7N17LASfX23NbWNMF+xveGQDT21TWH5PG7leDNrSMb3nlig8yb06fSpBQDgUELowcHztNt3kp0vlQ4/8MeyVgrV76patdRL9VucENXa7ISmUL3UXBsPR9a5TVOVlNfbuU3lSmn9+06o+iSBXk7wCTc5la2Cfk4VLdDLqSzlxEddD5gilQx3KkyHKI/bpWe/edx+3faIgUWqagxr0qAiTRlSLLcxiTfgbpdRjtetYWXOa1kS8HZok7v5vDEa33+rLjvK2btz6qheGlDk18jezgS8Bz9YJ5eR7r1qsqYMLVaez6OlW+s1pk9+h+Lf0PgJZ3901ij96KxRCoYiuuvtNXK7jEIRZ9/TNVMH6aGZG/TDJxfqumkjNG9DrSRp6tAS/SU+OS4ny60bn16kkeV5+k88oLT3wqKtez1B60VH9NOxw0r00uJtWrWjUY2hiIKhiMrbnRj39XiAchkniOR43QpFovr5c0t1WHmerjl2sNwuozveXKWHZm5QNGa1viqoUb3z9I2Th8vlMlqypU7WSs8u2KLyfJ++fPxQnfzH6frMhL66+by9Tx+85901uv2NVfJ6XApHYlr723MSP59tdc16d2Wl3l1Z2SH0tLRGtWCT8/psj7cnHmoi0ZjO/+sMTR5cpNsunaiW1qjO+ct7+vE5o3Xap/wxIJkaWpyhIm5jCD1p8t6qSl3/+AJN//7Jn3geuifnbtao3nl7nGIBhw5rrd5eUakTRpTKcwAt1jg4hB5kFmOcKk57vccd2GM110rbFjqBSXICUajBaakLNTjhKNwgeXOdqlH1WmnHx1LwHamlds/HyymSCvrH9ytlS758J2jl9Ylfxj/a9jMdom46e5S+fPwQeT0u3fWFI/d6aqle+dmae/NpKvJ7O1QkhpXldjj/0bemjZAkLdnitFC+saxCRw0u6vAms/0biNNG99Ibyyr2mOgX8Hk0eXCRWqNWczc4Yfh/jh+ih2Zu0BNzNsvtMqpviahfYY4e/J+j9PaKSvXOz9bKHQ264clFWrKlTgGvW784f6xueHKRjJFOHFGmJ+Y4QSgny62WSFSXTxmoRZtr9aXjBmtC/0Kt2xnU9BWVGvfzV+X3uvXhj0/V2sqglm6t15baZh01uEgfra/Rsu31emdFpVZsb9ArS7dLknJ9Hk0b3StRYfnr9NXKchu9sGibWlpjao3GdM+7ayU5YXJ4Wa5OGFGmHfUhPfjBel01dZBiVqoOhnTjU4s1vn+BFm6qTQy7CMfDX0VDSO+srNB/5mzu0IrXtn+pvqVVl/ztA9U2tUqS1lc17fNn/8+Z61Xf3KpvTRuhu99Zo+FluXsNBNZaLd1af8Bv/sKRmP7y5kqt2N6oX14wtsM+o621zdpY3aSjhxSrrrlVAZ9HWW6X/jFjnUaW5+nY+HmzOuv1j3doY3WTWlqjstZqbWVQa3cG9cGaKp02ply1TWH9+fWV+u7ph+01CLdZvr1euT7PHucss9Zq4eY6ed0ujdlHG+nepgFuqXVGv2+o3vfPpU04EtPmmiYNLPbv8w1XazSmHz65SF89Yeg+14GOPlxbrepgWMu21+9zuEk4EtNNTy/SGWN7684rjkzxCnuutn+n9ldjKKL/LtyqIr9XZ43b+3n4kmnJlnp96cGP9NcrjtB5Ezo3gbc1GpPHZTr1/aIjQg96rpxCaehJB3bfSNipFEXD0qYPpdqNUt1mp+rUWBGvQtVJjTuctr49nrvY2ZsUKJMCJZK/1Bn4ECiTioY4VSWXR4n9VNmFzu16gF752eoVr2h80rCAktz9D4btQ8wn/Y/iT5dO1LPzt2hi/8I9rrvj8iMUizlVC7/PrV552fr5Z8boj6+u0Mw1VYrErCYOKJTP49aZ8Xa7UX3y9LuXl6s6GNbtlx2h08eU6y9vrpLLGP3ji0fp5SXb9c1/zdPVUwfp9DHlGt0nv8MeoKOHluiut9doQHGONlU3a9baav3u5WWJaXWXTh6gj9bX6LEPNyYqSaeNLtfcDdWas6FaG6ubFI7GdMKIUr23aqduOHOkZqyu0l+nr5bLOCe1rWtuVTRmtbqyUR+sqZLk/Ep949F5Wrq1XpLkdbu0qqJRxjjF1Owsl1pandDzjxnrdM+7a9WvMEdVwV2tpdvrW3TbaytV19yqlTsadfeVk/TM/M1auaNR766s1DFDSxLnT3puwRYt2lynN5bt0La6Fl0yaYD++OoKjetXoPL8bA0s8auuqVU/+M9C/ewzY7R2Z1Dffmy+Hr/2mE+cgNdeVWNI+TlZynK79MrS7bpzulO5Cz4R0aNfOVoul5G1Vsfe+pYk6apjBunhWRt0wohS/f2ayfrtS8t07LBSHTu8VC2tUX3viQX6xsnDNa5fgV5bul0PzVyvO684Urk+j1zGqCUS1VNzN+vRDzfqgon9NGO1075Y0RDSpupmrd3pTCFcF798cu5mPTRzgxpaIntMDZSktZWNevCD9frnzA06YmChnvlGx8rpAzPW69cvfCxJ+stlE3Xa6HLNWL1TZXk+PTxrgz7eWi+P2+iF607ocL+2811tbBdGQ5Go/v3RJl06eUBi8EhNMKyrH5itxVvqNKF/gZ75xnF7nF/MWWdQz8zfohmrd2r2T0771J/Le6sq1RqN7bXKtLGqSf2LcrpsbPvcDdW6+v7Z+u91xyequZLzZjAYinxi2Jy5pkpj+uTrqXmbdd6EPol/o7pC2+/CmsrGxO9zOBJTJBZLTMBcXdGo1qjViu2ffFLmuqZWba9vSVS3W1qjao3GlJe9ZwXppcXbdNTg4k+d8nnPO2v0/uqdeuhLUxI/i9ZoTKFIrMMpDrrCtrpmfbS+RqePLt9nG+iybfXaXteiU0b1Oqjn+u1Ly/Thumo9+41j9zsIPD1v1znp3vvhKZKcPxzs779DB6ttP+jybQ06b8L+3y8Wsxrxk5f15eOH6Kf7qOI3hSNaXdGoklyflm2t16mjexGQdkPoAfbG45Xy4v8TL/yE8caxmNNa17BNatguNW53Lhu2ORWmYJVUsUwK7oy32+2l7NEmp1gqGykVDoyHpVKn1S5Q5pyTqXDAnlWwQ0R+u//hX3Rkv33eriAna4+Tobbplbfnm5wvHTdERtIv/uu82bx6asdzK/k8bl11zCA9+qHz5lly9t+0RmJyuYzOndBHU4acpvwcz17325x0WJnW/PYctUZjmvDL1/SL55cm/jIvSWeM7a1bXlqm/8zdLK/HpTsuO0JTh5XoW/+ap4/W16iqMaSzxvbWD84cqb+9vUZXHjNIVxw9SEu21GloWUDFfq+O+/1b2lEfUjRm9e+PNqo016fLjhqgv05frd752RrZO08/OGOkCv1ZeuTDDbrnnbW67+rJWrq1Xre+vFz3vLtWQ0oDevn6E/S3t9doc02znpq3Wbe8uEwvLNomSTpueInOGtdb8zfW6NWlO3T1A7M1qMSvE0aUaunWetU2tXaoEv3yv0sViVkt2lyri+6aoeNHlGpLTbNWVTTq9Y93aNk2J4z9d+FWHTO0RNGYVX1zqwr9WWqN2j1ORrtkS50+d/dM/e/Jw3TZlAF6adE2leb6dP2pw/XT55bqkQ83aMHG2g5vep+I79uasXqn5qyvUWvUauHmWllrNXNtlV5avF1ul0vfOmW4rn14riTp588v1XMLtuqzR/bTuyt3Ktfn1vqqJm2evrpD+Jy9vlrb4j/HtspX2/j1FxZv043njFKR36vtdS0aUOxXTTCsy++bpZpga+K2kWgsUW2ZsXqnfvvSMp0+plxbapr1x9dWaOGmOj0wY50GFvu1sV0V5+FZG+R1G30+3gLa9vu0vb5FLa1RZWe59fyCrfrZc0sT0wqvmzZCf52+Wsu31+vyKQP12OyNuv/9tfrM4X3Vp6DjNL5N8eeqaAjp0Q836KIj+snv9ejZ+Vv0y/8u1bs/PEWrKhq1aFOtvnjcEP38uaUKx0NPazSm8+54X2eP761jhpbo8vtm6X+O2/ubNGutNlQ1ye917xFAmsIRZbldsla6/vH5umrqII3oladfPP+xguGo3li2Q9fGQ8+m6iZ95aE5Wl8V1M3njdFVxwzSzc8uVp+CHH3zlOGavrxCudkeXX7frMR+w6fnb9Z/vnZs4k15+5/FvvzptRU6YUTZXs/n1vZHjLkbatSnIFunjOyl7/57gdbtDOql+ITHtt/5dTuDuuPNVRpQnKMn527WT88bo1G98xP3v/hvH0iSZt40TX0KcnTjU4v07IKtmnnTNJXl+hLrnLexRt94dJ7+57ghunhSPw0ry9WTczfrc5P7d/i3yFqrRz/cqI3VTXp+4VZdeEQ/WWt13h3va8WOBv3gjMP0rWkjFInG9OAH63XJpP6fGB4/SWVDSGfc9q4aQhGdf3hf/eWyiXt90/1/r67QrLVVWvCzM/brxNPWWq2vatpj/+Gby3ZoTWVQCzbV6oiBRYnjkWhMVz8wW188dnCHKaWStLxd6Jy9rlrf/89CSerQ5ru7WMzq8Y826fjhpbrjrVX68TmjVdyJaaltojGb+O91X3s992VrnXO/+99ft8/Qc9trK3X/jHUaVOz8nl9x9ED99qLxiet/8fxSTehfoM8e2X+v928KR+QyRh9vq9eRA4tkrVUkZg9o0mmmIvQAB8PlcgYn5JZJfT7lzzaxqBOEqtc6oSgxStxKTTulyhXSzpXSxplSY6UUad7zMUpHSgOPcUJQTrEzStxf7Axy8Jc4x7wHd7LWTHXVMYNU6M/qEIC6womHOZvyvW7XXqtI1586Ql87aWjiL+ZXHdMxGH3aX1ndLiO3y61hZblatq1eI3rlalWF85fhgpws3XzuGP3mxY/1+ckDEu0W4/sVJAYj/O/JwzSsLFd//Nzhicds/1fJv15xpNbtDOqHTy7Syh2NOmNMub520lAt3lKnLx8/JPH9SdL3Tx+pM8aUa9KgYh0ztER/eGW5YtYJZ9lZbn339MO0o75FT83brBcWbdOE/gUa0StPXzpucIfvtW9BtsKRmB6ZtXGP7zc7y6WXlzgtejErxeI97B6XUZE/y3mzE98f9OiHGzVzTZWys9xaXdGoQSV+NbRE9OD/HJV4I1gdDOtrD89Vc2tUT8zZlDjv0+VTBuqzR/bXr19Ypl/+92NFY84fFNqqaqFITKeO6qU3l1fo7nfWSJJqm1r1/ScWak08oL26dHuHiYTPLdgqSYmpgzsbpcP7F2jh5rrEcy7b1qCbn12ceL5N1U0KR2L6YM3OxG1fWbJdc9bX6PmFWzVlsNNmVx0M65lvHKc1lY26/vEFWr69QeP6FagmGNa3/jVPw8oC+vPnJ2rehhpd/cBsPTBjnSRpY3WTvnjsYJ01rrcuu3eWfvbcEnndLp01to8K/FnaXLPr34l5G2q0obpJ97/v3Peed5z2x8mDi/Xq0u06cUSZfn3BWL27slK/fWm5/jlzg578+rEK+NyJSkL7gPWTZ5ZobWVQG6qa9MYyZw/avI21uuaB2YnHXRt/LSsbQnp/daVW7GjQhuqgnp2/RdY6lcSLjuin/kU5enjmBl01dZAK/V7d/sYq/eXNVSoJeDXjxmlyu4y+8+8FWr8zqLWVQZ03oY9OGdVLLy/ZrtnrqjtUIVdsb0x8ftfbq7Wxukmjeufp9y8v16Bif+L38oiBhfrSgx8lqhnrq5pU5M/S0q31+u6/F+iPlx6ux2dv1N3vrNFz3zq+Q5tkXZNzsueRvfO0qqJB/++t1Zq7oUbVwbCumzZC507oo0g0JpcxicD/9LwtenreFl121AC9uNj5g8Fn75qh4oBPg+LV6mjMJn6HJempuZv1k3PHxP972HV+sjeXVejKYwbp2fjv5NTfvaUzx5br7isn6bcvLdN97zk/4//M3aQHZqzT4QMKtXBTrTwuo8umDFQsZmXlhK2N1U3Kchvd/OwSNYWjGtUnTyt2NMjncenxjzbpm6cM19Pztug3Ly5TZWNIN509Wrtraxtdsb1B76ysVFM4oqOHlOh/Tx6mR2Zt0PEjSrVsW70aQhGdO6GPnl+4VaePKddnDu/4b6q1Vos216kpHNX8jTU6ut2/ZXM31GhYWWCP0NVWBb31s+N1WfxcbbVN4cRQmwdmrNdt/Qr08MwNWrG9QeP7FySq3sN65ao016c/vLJcRw4s0qodDTpyYKFWVzTqn7N2vd7LtzeoX1GO3l7hnOftzLG95fO4ZIzRG8t26MfPLE78AWLK4GJdMqm/KhpCWrCpRv+cuUG3fnaCFm2p1eaa5g77IdfvDKo416s3Pt6h7z2xUAOKnd+xN5ZV6Np/ztE1xw5WZUNIF0zs2yEgPjFnk9ZWBvWtacOV6/No/c5d/13WNbeqIKfj/wdbWqN6ct5mWbvrDzGPz96oG84YqaKAV/UtrXpo5npN6Ncx9ESiMb24eJv+/dEmzV5XrXPGOz+7/3f5EXpgxjpV1If0589PVE6WW+P7FyR+hr97eblOOqxMxx1gu3C6GLu3hvtkP6kx6yU1SIpKilhrJ3/S7SdPnmznzJmTiqUBmSMcdFrpgpVOW131WmnDTGnLnE8e0uDJ2RWGcuKBqKC/U0GKRaVeo6X+R/XYcHQgFm6q1dCywF5bSLrK0/M26+/vrdO9V09SOBJTUzia2NPS9u9w2//02sZDHz+8VI985ehPfexYzGroj1+SJP3rK0fv976V43//ljbXNOsvl03UBRP7JdYy4RevqSEU0SvfOSERPiRpdUWDvvzQHN171WSV5Hq1aHOt7n5nreZuqFFhTpbqW1r192uO0jUPzNbI8jxtrWvWUYOLFYlZXTixr95fvTMRKK6bNlz3vLtWw8ucceRj+uRr7c5GuYxRbVOrPnN4XxkjfbS+WtvqWnTC8FK9GT/xbFmeT//44lEa169AV93/od5btVOXTOqvG88epcKcLI39+asKRWJ6+MtT9I1H5yXO3dRers+jxlBEpbleNYWdCkl1uzfWfq9bTeGoHv7yFF11v/Mm/6OfnKbqYFjf+feCxF/uJekPF0/QD59apD997nDd9fbqxJux08eUa0NVUJGY1c3njta0UeXaVN2kE/4wXb++YKyumjpY//fqct319hq9fP2u1/rWl5fr3nfXaFTvfH28rV5Pfn2qRvfJ17hfvJrYI1eW59OVRw/SOysrNH9T7R5754aWBhKB5JSRZZq+olK/v3i8Pn/UQM1ZX62/vLlK763aKZeRjh5Son999Wj98r8fa9baqg5/Dd9d+8c9eWSZ3l7h7Jm884oj9Zc3VyoYimpHfYv8Xrf+cMnhuvnZxepX5NcRAwr14AfrNap3nu65apLO+PO7snLawAaV+OV2mUTFpM3oPvlavzOo5taojhhYqCMHFmnBplrVNoX1h0sm6M7pa7RyR4NG9c7TGWN664dPLVK/wpwO1dQ22VkuhSIx/eQc5w39b15cpuwsl6Ixq9ao1ecm9df/xf+wsHRrna55YLZ2NoY1dWiJRvXJ0z9mrE88Vq88n27//ER95Z9zdMHEvnps9qY9ns/rdnU4WXSRP0tWSuyN+5/jhujxjzZqXN8CPfH1qWqNxjT5N2/o1FG9NCf+5v+Wi8br2FvfSoz4z3Ib3Xv1ZH3pHx/JZZyTWe/+mg0q8euw8jy9uWyHXMYoEg/nT359qv702krNXFul/GyPWlpjuvHsUfrVCx/rmW8cq9+9vFyz11Xrwol99YMzR+q211bqO6cdlmgt/uo/5zgnZfa41RBy/nvK9Xk0pk++Zq+v1mmje6nQ79Uby3boo5+cpovumqGqxrDe+N5JiZbfYCiiFTsa9Nm7nGrWqaN6qa65VcPKctUai+npeVt0/uF9dcflRyS+n7qmVp34f9PV0NKqLLdLv794gl5esk0fb6vXpupmTRxQqAWbanXBxL56efH2vZ6ge1TvvA6/05dPGajNNU2JPyxJUn62R70LshMnz5akb08bru+dMVJX3DcrEaIk6YqjB2pCvwLd+PRi5WV71NASUZ+CbLmM0ba6Zl09dbCGlQV0+ZSBmvSbNzR5UJGmr6hQ7BPebj/wxcmaOrQ0MdBm5M2vSJLOHd9H2VluLdlSpxU7nO/h7iuP1Fnjdg0AagxFdNPTi/XfhVs1sjxP63YGdecXjtRX/zlHg0v8umRSf43vX6hrHpgtl5Hm/+wMFcT/vf7CfR9q8Za6ROtze/nZHjWFo4rEnOr7tJG9tGBTrcb1y9cbyyp05thy3XPVZM1cU6XJg4sypiJkjJm7r1yRztAz2Vq789NuKxF6gD1EI07waaqSmqudy6bqdl9X7/q6aadUt8UZG97G5ZHKxznhp/9kqfd4Z9+Rv9g50SzSqjoY1hX3zdJvPzteR7Zr2/gkd729WuV52bp40t5bF/bmC3+fpRmrq/TeD0/pcELarbXNysv27FcI3FTdpNUVjVq2vV7rdwb1h0sO15rKxnahwpf4q+Q/ZqzTL//7sQpysrTgZ6dLcoJeW3tRNGZV1RjSn99YqZcWb5fLOG98v3jsYOX6PLri7x9q8qAiPfm/xyae/8EZ6/SL/36sJ742NdF6dMGdM7Roc60W/fwMPThjvf70+spEFaZtH8stF47T715errrmVk3oX6DsLLdmr6vWWWN767zD+2jl9ga9vGS7XvvuiTr1tncUi1m9fYOzB+CVJdv19UfmdngdyvN9evP7J+uON1fp3nfX6rTR5br3qkl7tMxYa3XcrW+puiksr9ul+hbnL+PtN7dba1XZGFIwFNVzC7bo29NGyOUyOv22d7SqolGHledq5Y5de7Tat8A9/OUpemXJdl03bYQ2VAV16yvLNX9jrdwuo1k3nZqo1llr9fl7Zmnp1joFw1H9+oKx+ml8r8PoPvn67UXj9M+ZG/TM/C3Kz/bo1NHlen7hVkVjVoeV52ptpRPmyvN92lG/69+W+66erP5FOSrPz1ZxwKtn52/Rd/69IPG4y7bVa0BxjrbVtujV756oz909MxE2jZFm/Mip+pzwh+kKR2K6+dzRGtMnXxMGFCrX59H/e3OV/vT6Sk0dWqKZa503oj85Z7SOHV6ic+94X5J05thyvbrUqUx97cShuufdtTp3Qh9997TDNKQ0ILfLaO6GGj23YItW7WhU74JsPbdgiz76yWkK+Dw648/vqjUa0/kT++qed9bKZZxx/NXBsPJ8HjWEIokQ1fYWqiTgVVUwrH9fe4zyc7Lkdhl98YHZqmgIqTw/W1tqm/WNk4fprrfXaMqQYj3xtan69Qsf6/7312lU77xEW9HdV07S7HXVeuTDDfrRWaP06xc+1vPfOk5VwXBipH5Znk/v3nCKNlY36czb3038DIr8WappapXHZXTV1EHKyXIrErPKiVdxrbX690ebdMuLy3TSyDLdctF4HXXLGxpZnqfF8WExw3vl6qjBRXps9iaVBLw6e3xvffvUETru1rfUGnW+2b994UjVt7TqR08tTvzcS3N98rqNDh9QqL9dOUlzN1Trc3fP1Mkje+neqybpiTmbdevLy1Qf/wNEfrZH9S3Ovw87G3f9/nhcRnN/erryfB69sWyH3llZqX/N3qgHvzRF33p0nhpCERUHvInfmSW/PFM/e26Jnlvg/G4eO6xEH6ypUu/8bG2v3zVpcmjZroD488+MUZHfq1tfXq5bLhqnr/5zTodAMm1UL721vEKTBxXplovG68zb3+0Qpsf0yVdJrjcRmtp+rrt79pvH6cI7ZyS+LsvzqbJh1/c6pDSgMX3yE1XBgpws3XT2KD3y4QYt2VKvPgXZamiJKBiOyFopy21UkONVaa5XkZjVl44brC8cPUh3vb1af3hlhb520lB965Th2lbXouFluYk/hEnSt04Zrr9O33Xahj997nCt2xnUX6ev1l8um6jPTOirLz34kd5ZWanDynO1rbZFD3/laH28tV5zN9Ro9voq7WwI67Dy3ETluyzPp0sn99ed09foJ+eM1ldPHLrHa5AOnxR6aG8DuiO3Z1db3f6IxZyKkTHS1gXShhnSlrnSwsekj+7reFtfvjMEIqdYKhrkhKPioc5EulhEyu8nlY7YNdIbXa444NUr3zmxU/f5xsmdHxU/rm+BttQ0q39Rx30dfQtz9nGPPQ0o9mtAsb/DpuRh8f0Wu29tnxBvj/jisYM7tHK07VNwu4x65Wfrd5+dkOhFb7tdS2tUQ0sDunq3PVuXHz1QQ8pyO+y1OHNsuUoCXuVlZ+m6U0do8uBi9S7IVigSVZ+CHOVkueX1uPT6xzv05vIKjeiVJ6/HpdnrqnXSyDKdN6Gv7Hir755+mIwx+s0F4zq8KTp9TLk+N6m/zhrXWz96apHqWyK65cLxyvV59L8nDVOvPJ+uPGbQXvcIGGP0jy9N0WOzNypmrfoX5ejzkwfucZteedlSnvSd03ZNMzxtTLnK87P192smKxKz+nhrvS69Z6ZG98nTVccM0omHlWlk77zEeZR6F2RrbN98zd9Yq6+cMKRDK6YxRo985Wg1t0Z1/O/fSgQeyXkDf8TAIm2sbtIz87fopnNG6/IpA7Wltlmz11XrtNHlejm6Xet2BvWLz4zVAzPW6aP1NfrWKcP3OJfXBRP7KhyJ6Yk5m/SnSw/X1x+Zp2Xb6nXF0QM1rCxXXzh6oN5fvVMDi/1yG5P43Xv6f4+VMYqPo9/1Oh4/olR/en1lIvBI0pQhxTqsPC9RXfnScUMSoeeHZ41SZWNIlxzZX8N77Rp+MGlQkSYNcv4NW7KlTs/M36I/vLJC66qC2ljdpEe+fLSOHVaiTdVNchmj608dobP/8p4+f9QAZzLl22v0ndNGaEhpQDPXVOnrJw3Tu6sqNWVIcWK935w2XDXBsK46ZrBCkah65Wfr8ikDEz+HIwcW6X6t0/LtDTp2WImuOmaQThlVpjF98vXIrA36/cvLlZ3l0ug++WptV8G44cyRyvG6NbJ3nv72hSN1+IBC3fHmKl15zCD9a/ZGfeX4IR0GPbT/mV82ZaDOn9hXLmOUneXWdacM159eX6lBJX6dM76P/vb2Gq2uaNSJh5XJWqtHZm3Uqh3OEIZ+hTmqbQrrlFG9VBEPui4jffe0w/SneMved07vFX99i/XrC8fpJ88s0el/flfrdgYTgUSS/vXVY1TZENKJh5Xp7nfWqDw/WyN65eqCO2folhc/VlHAm2jPvPjI/jrpsDL95NzR+vMbK/XoV47Wx9satKm6Sbk+j44ZWpKoIP/orFH629tr9JnD+yoYimhnMKQ/vLJCF07sp7eWV2jBploNLgnolFG9dOERTnW7rWLmcRnl52Tp3qsm6TcvLtMTczbpgffXyedx6a9XHKHL75ulCf0KNXt9tVxGOmNMuS48op/OHtdbM1bv1PLtDfrM4X0T5197pF37XJ+CbF0yqb/+31urdcHEvorGrH563hiV52frpI826dZXliscienGp50g2SvPp+umjdCPn9kVLLM9bn339BH6yTNL5HYZ/eSZJRpQ5NerS3doQv+CRFti2x+rrps2XP8vfn66xz/aqJHleVpd2ahozGlPC4Yi+szhfRNV/ksnD9DMtVW6/5qj1CvfJ5/HrYkDCnXF0QNV19SqqLXyuJ0/vkSiVpUNId05fY0umdRfV+22HzZTpavSs05S267ue6y19+7lNtdKulaSBg4cOGnDhg273wTAwYpFpcrlzkdTdbx6VL2relS91vnYm/Lx0vBpzmXhQGd/kjtLKhlGIOomwpGYQpFoUtv6drdgU60m9CvosoleB+Nvb6/R719ZrhvPHiWPy+g3Ly7TM984tsOm6E9jrVXMaq+T0FJh/sYa9SvM2ec0sk3VTXrkww367mmHJfal7e6lxdv0jUfnJaYBTuhfoOe/dbxaWqN6ZNYGXXnMIGVnufWz55bonzM36N/XHqOAz6M1lY26YGI/1QTDao3F9josZHfPL9yqnz23RC99+4ROhev2fvXfj/XQzPX67mkjNGtttR780lHyuF06/6/va01Fo+b/7Ayt2xlUXrZnv57DWquRP31F4UhMpbk+XTFlgL53xsg9brdkS52GlAaUneXW9OUVOuGw0v06YfC+1DU7e8y+c9qIPca33/baCj08a4NuOme0Lp3sDNO55cWPlZPl3uvaDlRrNKbfv7xc50/sq52NIf3Pg05XzVvfP0mDSwI67bZ3tHZnUMcPL9VvLxqvysYWTRpULGutTvq/tzWoxK+ff2asTrvtHUnSyt+c3WE4wR9fXaG73l6t7552mL55ynB9sKZKS7fW7fWEzNZa/TL+s7XWqbj0K8zRdacOT/xu7W1E9eqKBp1227vyuIyW/PLMDr/n4UhM9767Rl84epAaQxH98bUV+t1nxyem6knSDf9ZqLkbanTdqcPlMkYXTOynJ+du1g/iQw4+P3mAfn/JBEVjVjPXVOnK+52TYN9z1aTEpM+1lY3aWtui40eUanNNk47//XRJTtjZ2RjS548aoFNG9tKXH5qjH58zStee2PH7t9bq7fg50i6dPEA5WW61RmM6/c/vdvxebzlb97y7VqeO7qUr7vtQg0r8mr+xVjecOVLfPGXPP3yt2xnUKX98W5L0ndNG6OIj++udlZW6+dklysv26OXrT+gwPr8xFPnUiX6NoYjWVDTqgngVa/mvz9rnvy3pkIntbf2stVuMMb0kvS7pOmvtu/u6Pe1tQBqFGqW6TU6Vx7ik2k3SjsXSmunSxlmSje55n9xyqfQwJwzl9nIqRcVD4/uNiqSCAbTRIe2WbKnTef/vff3rq0dreFmuHp61QdefOuKQPGng3A01Gljs1+1vrNSlkwfo8AGFe9xm5poq3f/+Wv3tykkH1b/f2XOr7O3+2+pa9gg0ryzZpp2NYV15TOf/6vy9Jxbo6Xlb9Op3TkyMi06n3ff5pUIoEk1USdqqtS8u2qa/Tl+tB790VIeTK0vOSHK/z62SgFfXPTZfFx3RT6eO3nN0eUNLa6f+sLIpPjDh+BGl+/VmOhazOvxXr6lfYU6nK+SSM7UsHIl1GKCwfHu9zrr9PUlOAGxfOXtx0TZ9uK5KPz5n9D7Xd9pt72h1RaO+e9phOnlkmQaXBCQjXX3/h7rlovH7dY6ytu9rcElAA4pzdObY3omqjORMY3vwg/WSpLd/cLIGl+55AnVrrYbc5LS5LfjZ6Sr0exWKRPX9Jxbq4kn9dcrIAxsbHonGdMGdM3TF0QP1haMzq8qTcaGnwwKM+YWkRmvtH/d1G0IPkKEiIacSVLdFcrmd8xftXOlMoqtc4Uypa6yQYq0d72fczgjuoiFS0WCpeIjzedulb8/WDCAZNtc07XGiUBx6gqGIdtS37LUtDJnvr2+tUqHfe0CBd28i0ZiG/+RlTR1aoseuPabT999W16xgKKphZYGDCq6PfrhBZbm+PUZvS855ob756Dz98KyRez1PVpun4qdE2H2SXk+VUaHHGBOQ5LLWNsQ/f13Sr6y1r+zrPoQeoBuLRqT6zU44irbGx3avk2rW7brcfRpdbrnUa4xUPtb53JfntMxl+Z32ucKBVIoAAEmzo75FxQFvxkwlw/7JtEEG5ZKeiSdfj6R/fVLgAdDNuT1ONado8L5v01wr1azfFYSqVks7lkqz7+s4da6NyyMVDnKmzw0+3hnJnVMoZRc6J3P1BpyhDQAAHIDd2/nQ/aU89Fhr10o6/FNvCODQkVMo5UyU+k7seDwakVqbnKEK4aAUbpSq1kjVa5w2utVvSIse3/PxPDnOZLtAmRToJQVKnb1Fvjwpr4/Ub5Kzx8iVOZsvAQBA8jCyGkDmcnskd76UvesEmRrYrr86FnMCULDSqRY11zift/+o2yxtne983n7ognE7rXP5fZwglNcn/nlfJzB5cpyv8/tLno5nCAcAAN0LoQdA9+VyOecMKh3x6beNxZxBCzXrnXMU1ax3Bi3Ub3WqR+vfk1rq9nJHI+X1dibOFfR3BjAUxD/aPm8fygAAQMYh9AA4NLhcktcvlY9xPvYmHJQatjtVodZmJxDVbXLGdNdtlLYtkJa/IEXDHe/nK5DyyiV/qdNKFyiNt9aVOYEpr69zzO11BjD48pyTvQIAgJQg9ABAG2/AmQ5XsueJ8xJiMSlY4bTN1W50QlHdZmc0d3Cns9dowwznJK/a13RMI+X3jQ94GCJlFziDF3z5UqDEab2TpF6jpT4TpSw21AIAcDAIPQDQGS5XvHrTW+q/16mYjljUGcBQv9WpHjXtdCpE0YjUXC3VbHCm1a1+wxnQYGPO0IY9ni/LCT+BUicc5RTt+sgudI5l5zuBKbvACW4Vy5zhEGWjCUwAAIjQAwDJ4XI7E+NyO3HG60jYCUTWSrGItH2RtPkjaftiZ1BD7cZdAxvaD2XYF+N2WuzaT7Jr+zy70AlL2QXxj0Ln0pfP4AYAQI9D6AGATOHxOhWkNoUDpFHn7nk7a6VQgxN+QvVSS70zhCFU7xwvGuJUj3YsdYY1BCud9rudq5zLvZ37qMM6cpx2u2jYqSjl95WyAs76csudSXe55U4Vye1z9ie5vc6eKV+Bczsbi4erAid8ef1d+1oBANAJhB4A6G6MiVdpPmVq3NgL9zxmrROI2oJS+49QvdRS63xurRNkmqqc4NTaLIUapaq1UsNWpxLVGYEyKafYCUgen+TJdi6zcpzqki/PuV0k5FznznIm4+UUtbuPb1fIart/h2Px8MWJaQEAuyH0AMChxBgnYPjypIJ+B/YYsZgTjiIhp2oUCTuX4SYnMLVNt2vc4UzEi7U6I8JDDc59Ii3OZXONs+eppd4JXDbmhJm2x919St7+8mQ7YSrL73yEg5Ks5C+R/MVOoDPGCUtZ2fHbtbt9Vs6uSXvuLKfy5ct1KlaxVinaGt+f1eo8po06odCb69y/tckJpNFIxxZCT7bzvG2VtEiL1NrifK+enPhUP1oLASAZCD0AgM5xuZzwkEyxmDP8oaUuHpTaAlbLrq8Tx3Y/3uyEidZmqTXoBBFjnIl6TVVOeLExJ4S1tsRv3+yEttagc126uH3OMApZZx2urHgA88Qv28KYN35dVrvjHsm4dj2WcTtfG5ezx8yYvRxzxY+ZvRxru51rL8fafb7XY+7d7tv+WNvtzF6OuZzvO9oaD5jx4Ogvda6Lxve9tdQ733NW9q7KoMvjPJ7L4zyvy7P3Y4mv3ZKM8/OPtsZfU/duP5D46+Lm7RLQ3fFfMQAg87hcnR8E0RWsdYJToqLT6rwpDsUn7CVCRpbz5jlY6XyeleNUlMJNzuehBueNdahhV8tgpCX++M3ONPO29j53lvOcbXuywsF4eDFOG2FbVSnWrsLUdhkJOfdpO9Y2Jt3GQ5ONOZUoa52Jgh2OxZxwucex+OU+R64fgrL8TsiMtTo/E0+8QtghZLZvqzT7cVz7OL6fj2PM3q+PRZzgHG5yfjdikV2VRht1vjZu52spfiz+M3d54uHZ3e4PCO1Cobtd0G6rxHoDu/67ycrZFVwTv4u7fTtur/M81jpXtr9sH4Sjofhzh+PV6U9o57Xtn8R++vH9vk8nHi/a6lz64q3HxuUca/9HlLafUSJ0y/nvvbW53QOZdrc1He+3++Pscf0nfb2P66Rd1eq2Pzy0/RFpr3/MMM6xWHTXvzsn/mDv+08zDKEHAIA2xsTHfO/nqO8DbRHsDtoHp70Fpg5BKrqP2+5+bD/ua1zxyk2W8wa8pU4KVkmyzpvFnCLnTXws0rHC1/aGPhZ/Ex+LtDsW2+3r+G1kd1WKovFA0/4NZts6Q/XOdW3Vo0goXhVse+Pb/o1xhxex42N12fF2YaHtzWvbm2uXx3kj7fU7YcG4nfVHWnZVumzU+R6kjm9sYxGnumaj8VDu7/j6tIXvSMgJL9KukO7xOkFLdteb5IR2a4yG273OpuOltbt+H3x5u/4oEGpw/vDQ3r7CYeIxdzu8x312u8E+H+8Trmt/3OVxrgvVOye0trFdAdL55uMXtl3QtE5ozMrZ9f23af+7Ze2ev2ud+foTbxt/vbICu/7byMpx/hvb479hu+sPMcbt/I74cp0KdTdA6AEAAHtq+6u73M4bTwDoxlyffhMAAAAA6L4IPQAAAAB6NEIPAAAAgB6N0AMAAACgRyP0AAAAAOjRCD0AAAAAejRCDwAAAIAejdADAAAAoEcj9AAAAADo0Qg9AAAAAHo0Qg8AAACAHo3QAwAAAKBHI/QAAAAA6NEIPQAAAAB6NEIPAAAAgB6N0AMAAACgRyP0AAAAAOjRCD0AAAAAejRCDwAAAIAejdADAAAAoEcj9AAAAADo0Qg9AAAAAHo0Qg8AAACAHs1Ya9O9hk9ljKmUtCHd64grlbQz3Ys4RPHapw+vffrw2qcHr3v68NqnD699evC6d51B1tqyvV3RLUJPJjHGzLHWTk73Og5FvPbpw2ufPrz26cHrnj689unDa58evO6pQXsbAAAAgB6N0AMAAACgRyP0dN696V7AIYzXPn147dOH1z49eN3Th9c+fXjt04PXPQXY0wMAAACgR6PSAwAAAKBHI/QAAAAA6NEIPZ1gjDnLGLPCGLPaGHNjutfT0xhjHjDGVBhjlrQ7VmyMed0Ysyp+WRQ/bowxd8R/FouMMUemb+XdmzFmgDFmujHmY2PMUmPM9fHjvPZJZozJNsbMNsYsjL/2v4wfH2KM+TD+Gv/bGOONH/fFv14dv35wWr+Bbs4Y4zbGzDfGvBD/mtc9BYwx640xi40xC4wxc+LH+PcmBYwxhcaYJ40xy40xy4wxU3ntk88YMzL++972UW+M+Q6vfWoRevaTMcYt6U5JZ0saI+lyY8yY9K6qx3lQ0lm7HbtR0pvW2hGS3ox/LTk/hxHxj2sl/S1Fa+yJIpK+b60dI+kYSd+M/27z2idfSNI0a+3hkiZKOssYc4yk30v6s7V2uKQaSV+O3/7Lkmrix/8cvx0O3PWSlrX7mtc9dU6x1k5sd24S/r1Jjb9IesVaO0rS4XJ+/3ntk8xauyL++z5R0iRJTZKeEa99ShF69t8USauttWuttWFJj0u6IM1r6lGste9Kqt7t8AWSHop//pCkC9sd/6d1zJJUaIzpk5KF9jDW2m3W2nnxzxvk/E+wn3jtky7+GjbGv8yKf1hJ0yQ9GT+++2vf9jN5UtKpxhiTmtX2LMaY/pLOlfT3+NdGvO7pxL83SWaMKZB0oqT7JclaG7bW1orXPtVOlbTGWrtBvPYpRejZf/0kbWr39eb4MSRXubV2W/zz7ZLK45/z80iCeNvOEZI+FK99SsRbrBZIqpD0uqQ1kmqttZH4Tdq/vonXPn59naSSlC6457hd0g8lxeJfl4jXPVWspNeMMXONMdfGj/HvTfINkVQp6R/xts6/G2MC4rVPtcskPRb/nNc+hQg96DasM1+dGetJYozJlfSUpO9Ya+vbX8drnzzW2mi85aG/nIryqPSuqOczxpwnqcJaOzfdazlEHW+tPVJOC883jTEntr+Sf2+SxiPpSEl/s9YeISmoXe1Uknjtky2+T/B8Sf/Z/Tpe++Qj9Oy/LZIGtPu6f/wYkmtHW0k3flkRP87PowsZY7LkBJ5HrbVPxw/z2qdQvM1kuqSp/7+9uwm1qgrjMP78UyxT0SybFCVWRAVmBBJZIElBEdHAPtXEcZMGQRiFIDitSUEOCqwsMsmSRpGG5KBUzLKskfRhlEKGZJGEvQ32Mm+OVDjn2L7Pb3L3Xnvdxdrvvaxz3rPW2oduKcPEdmlsfP+Nfbs+HfhluD3thQXAfUm+pVuqfAfdXgfjPgRV9WP7eYhuX8N8HG+G4QBwoKo+becb6ZIgYz88dwO7q+pgOzf2Q2TSc/p2Ate0p/tMopue3DziPo0Hm4Hl7Xg58N6Y8sfaE05uAY6MmSLWGWh7E14Gvq6q58ZcMvYDlmRWkhnteDJwJ92eqo+Axa3aqbE/8TdZDGwtv2H6jFXVyqq6vKpm043lW6tqCcZ94JJMSTLtxDFwF/AljjcDV1U/Az8kubYVLQL2YeyH6RFOLm0DYz9Ucdw+fUnuoVsHPgF4parWjLZH/ZLkTWAhcAlwEFgFvAtsAK4AvgMerKrD7Y36C3RPe/sDWFFVu0bQ7f+9JLcBHwN7Obm/4Wm6fT3GfoCSzKXbvDqB7kOoDVW1OskcuhmImcBnwNKqOpbkAuA1un1Xh4GHq2r/aHrfD0kWAk9W1b3GffBajDe104nAG1W1JsnFON4MXJJ5dA/vmATsB1bQxh6M/UC1JP97YE5VHWll/t8PkUmPJEmSpF5zeZskSZKkXjPpkSRJktRrJj2SJEmSes2kR5IkSVKvmfRIkiRJ6jWTHklSLyVZmOT9UfdDkjR6Jj2SJEmSes2kR5I0UkmWJtmRZE+StUkmJDma5PkkXyXZkmRWqzsvySdJvkiyKclFrfzqJB8m+TzJ7iRXteanJtmY5Jsk69uX/kmSxhmTHknSyCS5DngIWFBV84DjwBJgCrCrqm4AtgGr2q+8CjxVVXOBvWPK1wMvVtWNwK3AT638JuAJ4HpgDrBgwLckSToHTRx1ByRJ49oi4GZgZ5uEmQwcAv4G3mp1XgfeSTIdmFFV21r5OuDtJNOAy6pqE0BV/QnQ2ttRVQfa+R5gNrB94HclSTqnmPRIkkYpwLqqWvmfwuTZU+rVWbZ/bMzxcXzdk6RxyeVtkqRR2gIsTnIpQJKZSa6ke31a3Oo8CmyvqiPAr0lub+XLgG1V9RtwIMn9rY3zk1w4zJuQJJ3b/MRLkjQyVbUvyTPAB0nOA/4CHgd+B+a3a4fo9v0ALAdeaknNfmBFK18GrE2yurXxwBBvQ5J0jkvV2a4YkCRpMJIcraqpo+6HJKkfXN4mSZIkqdec6ZEkSZLUa870SJIkSeo1kx5JkiRJvWbSI0mSJKnXTHokSZIk9ZpJjyRJkqRe+wf1/PtwhcxCmwAAAABJRU5ErkJggg=="
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 822.640625 603.474375\" width=\"822.640625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-13T20:12:55.558902</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 822.640625 603.474375 \nL 822.640625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \nL 815.440625 22.318125 \nL 34.240625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"ma0ab965aee\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.749716\" xlink:href=\"#ma0ab965aee\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(66.568466 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.440625\" xlink:href=\"#ma0ab965aee\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(154.896875 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"259.131534\" xlink:href=\"#ma0ab965aee\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(249.587784 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"353.822443\" xlink:href=\"#ma0ab965aee\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(344.278693 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"448.513352\" xlink:href=\"#ma0ab965aee\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(438.969602 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"543.204261\" xlink:href=\"#ma0ab965aee\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(533.660511 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"637.89517\" xlink:href=\"#ma0ab965aee\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(628.35142 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"732.58608\" xlink:href=\"#ma0ab965aee\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(723.04233 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- epoch -->\n     <g transform=\"translate(409.6125 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m48cf528102\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m48cf528102\" y=\"488.186707\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2 -->\n      <g transform=\"translate(20.878125 491.985926)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m48cf528102\" y=\"381.023281\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 3 -->\n      <g transform=\"translate(20.878125 384.8225)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m48cf528102\" y=\"273.859855\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 4 -->\n      <g transform=\"translate(20.878125 277.659073)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m48cf528102\" y=\"166.696428\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 5 -->\n      <g transform=\"translate(20.878125 170.495647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m48cf528102\" y=\"59.533002\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 6 -->\n      <g transform=\"translate(20.878125 63.332221)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- MAE -->\n     <g transform=\"translate(14.798438 305.011875)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"154.6875\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p3aa3c31ee4)\" d=\"M 69.749716 47.027216 \nL 75.43117 145.515071 \nL 78.271898 198.691105 \nL 80.165716 240.321513 \nL 81.112625 261.248957 \nL 82.059534 279.164647 \nL 84.900261 340.694888 \nL 85.84717 356.394904 \nL 86.79408 373.954865 \nL 87.740989 388.18807 \nL 88.687898 405.676262 \nL 89.634807 418.536167 \nL 90.581716 427.137798 \nL 91.528625 444.677754 \nL 93.422443 453.71304 \nL 94.369352 461.269379 \nL 95.316261 461.531366 \nL 96.26317 473.325285 \nL 97.21008 473.255943 \nL 99.103898 484.284008 \nL 100.050807 481.567891 \nL 100.997716 482.114144 \nL 101.944625 482.526109 \nL 102.891534 484.36707 \nL 103.838443 487.673898 \nL 104.785352 488.17102 \nL 105.732261 483.427325 \nL 106.67917 480.70551 \nL 107.62608 485.570004 \nL 109.519898 487.436899 \nL 110.466807 481.10148 \nL 111.413716 479.23387 \nL 112.360625 479.880892 \nL 113.307534 482.595016 \nL 114.254443 481.013819 \nL 115.201352 486.072772 \nL 116.148261 488.509426 \nL 117.09517 485.618599 \nL 118.04208 487.754635 \nL 118.988989 488.336416 \nL 119.935898 484.178462 \nL 120.882807 480.819743 \nL 121.829716 485.143042 \nL 122.776625 476.186065 \nL 123.723534 489.713688 \nL 124.670443 487.331097 \nL 125.617352 486.172697 \nL 126.564261 482.705519 \nL 127.51117 482.256303 \nL 128.45808 480.299422 \nL 129.404989 481.928704 \nL 130.351898 484.555168 \nL 131.298807 483.289126 \nL 132.245716 488.808652 \nL 133.192625 483.313884 \nL 134.139534 489.537139 \nL 135.086443 487.816312 \nL 136.033352 482.327791 \nL 136.980261 479.282133 \nL 137.92717 484.628061 \nL 138.87408 480.988729 \nL 139.820989 486.648408 \nL 140.767898 490.199018 \nL 141.714807 480.991131 \nL 142.661716 485.045902 \nL 143.608625 478.821165 \nL 144.555534 490.387793 \nL 145.502443 480.848895 \nL 146.449352 482.8789 \nL 147.396261 487.714471 \nL 148.34317 488.752736 \nL 149.29008 489.564082 \nL 150.236989 484.680183 \nL 151.183898 486.019194 \nL 152.130807 485.941165 \nL 153.077716 482.118718 \nL 154.024625 484.211728 \nL 154.971534 482.944537 \nL 155.918443 480.688187 \nL 156.865352 487.458233 \nL 157.812261 485.399255 \nL 158.75917 485.624067 \nL 159.70608 484.662068 \nL 160.652989 486.930451 \nL 161.599898 487.894622 \nL 162.546807 483.547843 \nL 163.493716 486.460796 \nL 164.440625 484.188631 \nL 165.387534 486.446437 \nL 166.334443 483.179211 \nL 167.281352 488.772154 \nL 168.228261 490.096832 \nL 169.17517 490.669798 \nL 170.12208 482.525802 \nL 171.068989 493.224096 \nL 172.015898 484.467609 \nL 172.962807 488.562327 \nL 173.909716 484.077285 \nL 174.856625 486.713815 \nL 175.803534 486.429702 \nL 177.697352 488.805164 \nL 178.644261 486.543833 \nL 179.59117 489.743977 \nL 180.53808 483.963512 \nL 181.484989 489.852321 \nL 182.431898 488.141254 \nL 183.378807 487.12394 \nL 184.325716 486.321729 \nL 185.272625 483.118888 \nL 186.219534 483.926133 \nL 187.166443 486.71172 \nL 188.113352 492.951532 \nL 189.060261 483.391504 \nL 190.00717 489.147442 \nL 190.95408 485.056224 \nL 191.900989 486.069808 \nL 192.847898 491.644508 \nL 193.794807 487.006488 \nL 194.741716 483.151822 \nL 195.688625 488.764758 \nL 196.635534 487.313774 \nL 197.582443 493.638258 \nL 198.529352 488.312846 \nL 199.476261 484.174144 \nL 201.37008 494.158566 \nL 202.316989 495.788167 \nL 203.263898 487.118114 \nL 204.210807 487.764114 \nL 205.157716 487.746383 \nL 206.104625 488.653974 \nL 207.051534 490.980509 \nL 208.945352 494.388386 \nL 209.892261 493.008967 \nL 210.83917 487.601669 \nL 211.78608 484.095962 \nL 212.732989 484.26316 \nL 213.679898 493.504939 \nL 214.626807 485.450508 \nL 215.573716 489.20162 \nL 216.520625 488.160416 \nL 217.467534 484.976432 \nL 218.414443 491.247555 \nL 219.361352 488.902445 \nL 220.308261 492.261867 \nL 221.25517 485.736154 \nL 222.20208 493.844202 \nL 223.148989 490.87532 \nL 224.095898 494.205015 \nL 225.042807 493.151701 \nL 225.989716 492.73593 \nL 226.936625 489.950189 \nL 227.883534 492.456083 \nL 228.830443 488.123727 \nL 229.777352 484.442672 \nL 230.724261 486.831216 \nL 231.67117 494.5058 \nL 232.61808 490.484473 \nL 233.564989 491.92935 \nL 234.511898 491.72904 \nL 235.458807 498.598091 \nL 236.405716 489.088856 \nL 237.352625 496.672099 \nL 238.299534 488.321827 \nL 239.246443 491.927242 \nL 240.193352 493.905483 \nL 241.140261 490.736113 \nL 242.08717 496.151382 \nL 243.03408 490.185669 \nL 243.980989 492.516662 \nL 244.927898 495.426651 \nL 245.874807 491.19031 \nL 246.821716 489.00168 \nL 247.768625 494.280655 \nL 248.715534 496.723173 \nL 249.662443 495.216057 \nL 250.609352 491.109369 \nL 251.556261 495.390217 \nL 252.50317 496.678333 \nL 253.45008 491.947758 \nL 254.396989 498.889741 \nL 255.343898 493.838044 \nL 256.290807 497.489321 \nL 257.237716 490.020157 \nL 258.184625 494.157748 \nL 259.131534 492.818136 \nL 260.078443 495.038295 \nL 261.025352 499.816507 \nL 261.972261 494.667478 \nL 262.91917 492.802053 \nL 263.86608 491.996558 \nL 264.812989 495.7862 \nL 265.759898 496.714397 \nL 266.706807 491.336148 \nL 267.653716 496.222142 \nL 268.600625 494.997326 \nL 269.547534 499.730992 \nL 270.494443 499.072933 \nL 271.441352 502.027953 \nL 272.388261 500.540485 \nL 273.33517 496.285353 \nL 274.28208 504.699984 \nL 275.228989 497.435922 \nL 276.175898 498.500899 \nL 277.122807 500.25733 \nL 278.069716 500.835457 \nL 279.016625 499.92181 \nL 279.963534 491.148933 \nL 280.910443 504.424392 \nL 281.857352 503.434914 \nL 282.804261 497.856356 \nL 283.75117 500.587803 \nL 284.69808 493.900194 \nL 285.644989 500.177065 \nL 286.591898 502.889222 \nL 287.538807 500.138792 \nL 288.485716 495.963145 \nL 289.432625 493.899772 \nL 290.379534 503.066653 \nL 291.326443 498.153576 \nL 292.273352 497.571182 \nL 293.220261 509.876965 \nL 294.16717 505.527222 \nL 295.11408 503.594996 \nL 296.060989 501.95322 \nL 297.007898 499.095953 \nL 297.954807 503.552583 \nL 298.901716 504.878577 \nL 299.848625 503.121981 \nL 300.795534 503.337046 \nL 301.742443 502.985839 \nL 302.689352 508.218926 \nL 303.636261 494.960879 \nL 304.58317 503.155259 \nL 305.53008 504.562271 \nL 306.476989 503.33481 \nL 307.423898 499.554865 \nL 308.370807 500.268457 \nL 309.317716 507.421978 \nL 310.264625 501.475184 \nL 311.211534 506.438837 \nL 312.158443 505.178391 \nL 313.105352 502.789195 \nL 314.052261 509.870245 \nL 314.99917 506.294889 \nL 315.94608 511.30471 \nL 316.892989 507.070222 \nL 317.839898 501.294701 \nL 318.786807 505.833384 \nL 319.733716 503.662486 \nL 320.680625 502.453689 \nL 321.627534 502.71307 \nL 322.574443 506.229967 \nL 323.521352 510.303607 \nL 324.468261 502.645836 \nL 325.41517 505.296009 \nL 326.36208 502.434692 \nL 327.308989 505.509758 \nL 328.255898 500.714466 \nL 329.202807 504.873493 \nL 330.149716 506.919785 \nL 331.096625 511.878085 \nL 332.043534 506.873464 \nL 332.990443 506.789826 \nL 333.937352 501.274836 \nL 334.884261 505.295421 \nL 335.83117 499.708866 \nL 336.77808 505.193976 \nL 337.724989 500.331066 \nL 338.671898 506.382142 \nL 339.618807 507.546955 \nL 340.565716 506.113128 \nL 341.512625 514.277373 \nL 343.406443 509.730705 \nL 344.353352 512.460607 \nL 345.300261 502.332966 \nL 346.24717 503.625451 \nL 347.19408 511.884792 \nL 348.140989 512.259556 \nL 349.087898 507.038489 \nL 350.034807 510.515121 \nL 350.981716 505.85735 \nL 351.928625 506.91465 \nL 352.875534 514.954505 \nL 353.822443 510.856772 \nL 354.769352 508.241768 \nL 355.716261 507.21141 \nL 356.66317 509.790849 \nL 357.61008 507.448601 \nL 358.556989 507.402382 \nL 359.503898 508.552874 \nL 360.450807 511.255003 \nL 361.397716 505.547994 \nL 363.291534 512.25709 \nL 364.238443 508.46253 \nL 365.185352 516.002914 \nL 367.07917 506.491814 \nL 368.02608 511.371625 \nL 368.972989 512.339999 \nL 369.919898 509.600248 \nL 370.866807 510.563857 \nL 371.813716 507.76469 \nL 372.760625 507.218538 \nL 373.707534 506.944837 \nL 374.654443 512.726132 \nL 375.601352 505.352793 \nL 376.548261 508.143644 \nL 377.49517 507.77261 \nL 378.44208 515.820143 \nL 379.388989 511.075618 \nL 380.335898 511.935508 \nL 381.282807 509.817421 \nL 382.229716 510.928669 \nL 383.176625 505.816611 \nL 384.123534 513.669506 \nL 385.070443 513.344654 \nL 386.017352 507.134058 \nL 386.964261 511.827356 \nL 387.91117 507.549574 \nL 388.85808 510.420944 \nL 389.804989 505.357316 \nL 390.751898 514.803864 \nL 391.698807 511.934154 \nL 392.645716 513.032078 \nL 393.592625 517.481439 \nL 394.539534 508.746976 \nL 395.486443 516.442063 \nL 396.433352 509.79868 \nL 397.380261 515.428325 \nL 398.32717 509.876262 \nL 399.27408 509.128101 \nL 400.220989 516.31227 \nL 401.167898 508.713033 \nL 402.114807 513.47147 \nL 403.061716 510.274532 \nL 404.008625 512.884784 \nL 404.955534 513.910044 \nL 405.902443 506.815849 \nL 406.849352 517.296957 \nL 407.796261 513.508491 \nL 408.74317 512.888169 \nL 409.69008 510.867056 \nL 410.636989 513.068882 \nL 411.583898 513.969818 \nL 412.530807 511.615727 \nL 413.477716 518.947446 \nL 414.424625 516.636931 \nL 415.371534 514.637279 \nL 416.318443 513.662492 \nL 417.265352 517.600501 \nL 418.212261 513.216343 \nL 419.15917 515.642994 \nL 420.10608 513.749528 \nL 421.052989 514.762141 \nL 421.999898 518.529414 \nL 422.946807 513.593317 \nL 423.893716 512.825419 \nL 424.840625 511.686028 \nL 425.787534 512.822353 \nL 426.734443 510.877237 \nL 427.681352 518.534881 \nL 428.628261 512.68906 \nL 429.57517 514.885993 \nL 430.52208 515.040084 \nL 431.468989 510.337869 \nL 432.415898 517.149063 \nL 433.362807 509.535134 \nL 434.309716 511.40834 \nL 435.256625 516.362565 \nL 436.203534 516.95956 \nL 437.150443 512.376267 \nL 438.097352 515.879828 \nL 439.044261 515.139817 \nL 439.99117 516.194997 \nL 440.93808 511.530059 \nL 441.884989 515.213656 \nL 442.831898 524.32273 \nL 443.778807 512.36896 \nL 444.725716 512.80015 \nL 445.672625 520.003966 \nL 446.619534 513.951179 \nL 447.566443 519.178646 \nL 448.513352 511.614782 \nL 449.460261 517.425715 \nL 451.35408 512.629286 \nL 452.300989 515.535839 \nL 453.247898 514.038317 \nL 454.194807 507.533567 \nL 455.141716 513.18088 \nL 456.088625 514.985037 \nL 457.035534 518.266762 \nL 457.982443 506.58401 \nL 458.929352 516.319718 \nL 459.876261 517.87175 \nL 460.82317 515.901404 \nL 462.716989 509.595291 \nL 463.663898 519.26342 \nL 464.610807 514.654219 \nL 465.557716 516.960672 \nL 466.504625 516.867326 \nL 467.451534 516.081006 \nL 468.398443 512.355635 \nL 469.345352 512.582875 \nL 470.292261 513.212344 \nL 471.23917 511.024583 \nL 472.18608 516.543495 \nL 474.079898 517.635134 \nL 475.026807 515.872342 \nL 475.973716 515.931553 \nL 476.920625 513.805086 \nL 477.867534 513.72827 \nL 478.814443 510.838874 \nL 479.761352 513.223509 \nL 480.708261 516.355245 \nL 481.65517 515.826224 \nL 482.60208 518.048465 \nL 483.548989 511.036974 \nL 484.495898 510.623426 \nL 485.442807 512.83813 \nL 486.389716 522.475138 \nL 487.336625 514.7463 \nL 488.283534 517.891807 \nL 489.230443 516.504494 \nL 490.177352 516.311261 \nL 491.124261 518.102924 \nL 492.07117 510.395778 \nL 493.01808 519.77697 \nL 493.964989 515.151634 \nL 494.911898 515.221819 \nL 495.858807 513.77403 \nL 496.805716 515.622133 \nL 497.752625 514.515356 \nL 498.699534 514.25669 \nL 499.646443 518.643928 \nL 500.593352 516.316537 \nL 501.540261 512.141912 \nL 502.48717 514.035902 \nL 503.43408 512.615757 \nL 504.380989 518.529733 \nL 505.327898 517.922441 \nL 506.274807 517.915504 \nL 507.221716 515.63814 \nL 508.168625 517.474337 \nL 509.115534 518.108047 \nL 510.062443 517.206562 \nL 511.009352 521.157474 \nL 511.956261 511.17189 \nL 512.90317 520.186647 \nL 513.85008 517.56804 \nL 514.796989 520.651282 \nL 515.743898 514.615179 \nL 516.690807 515.131514 \nL 517.637716 516.077621 \nL 518.584625 519.676214 \nL 519.531534 518.174119 \nL 520.478443 513.787239 \nL 521.425352 520.196867 \nL 522.372261 517.525653 \nL 523.31917 511.90527 \nL 524.26608 519.542001 \nL 525.212989 519.589268 \nL 526.159898 517.187783 \nL 527.106807 515.670345 \nL 528.053716 519.215884 \nL 529.000625 517.084933 \nL 529.947534 514.125786 \nL 530.894443 518.477241 \nL 531.841352 519.691966 \nL 532.788261 519.765255 \nL 533.73517 515.028587 \nL 534.68208 517.575207 \nL 535.628989 520.839341 \nL 536.575898 518.90092 \nL 537.522807 514.610605 \nL 538.469716 514.87083 \nL 539.416625 517.814659 \nL 540.363534 517.332267 \nL 541.310443 517.723587 \nL 542.257352 514.462353 \nL 543.204261 520.541674 \nL 544.15117 515.888195 \nL 545.09808 520.347866 \nL 546.044989 515.609639 \nL 546.991898 514.414038 \nL 547.938807 516.957938 \nL 548.885716 517.84698 \nL 550.779534 516.473974 \nL 551.726443 521.382529 \nL 552.673352 519.987806 \nL 553.620261 517.582399 \nL 554.56717 521.667868 \nL 555.51408 515.833391 \nL 556.460989 516.211285 \nL 557.407898 513.520947 \nL 558.354807 522.148178 \nL 559.301716 516.61598 \nL 560.248625 517.066563 \nL 561.195534 521.038527 \nL 562.142443 518.702104 \nL 563.089352 514.49926 \nL 564.036261 517.50087 \nL 564.98317 513.805929 \nL 565.93008 517.038662 \nL 566.876989 512.551628 \nL 567.823898 513.785246 \nL 568.770807 517.558625 \nL 569.717716 518.902121 \nL 570.664625 517.61808 \nL 571.611534 515.737554 \nL 572.558443 522.033869 \nL 573.505352 522.278354 \nL 574.452261 518.890892 \nL 575.39917 517.914329 \nL 576.34608 516.557458 \nL 577.292989 520.092164 \nL 578.239898 518.139077 \nL 579.186807 520.72728 \nL 580.133716 520.046635 \nL 581.080625 513.133562 \nL 582.027534 519.600025 \nL 582.974443 515.049933 \nL 583.921352 521.320366 \nL 584.868261 518.371708 \nL 585.81517 517.573189 \nL 586.76208 518.718622 \nL 587.708989 519.635245 \nL 588.655898 511.091817 \nL 589.602807 516.034429 \nL 590.549716 515.7879 \nL 591.496625 518.485941 \nL 592.443534 518.684041 \nL 593.390443 513.075615 \nL 594.337352 516.740394 \nL 595.284261 514.546297 \nL 596.23117 519.170674 \nL 597.17808 518.104317 \nL 598.124989 512.55012 \nL 599.071898 518.2174 \nL 600.018807 515.710216 \nL 600.965716 517.03437 \nL 601.912625 515.33526 \nL 602.859534 521.055952 \nL 603.806443 522.134841 \nL 604.753352 521.464697 \nL 605.700261 518.898978 \nL 606.64717 513.582854 \nL 607.59408 514.87143 \nL 608.540989 516.84198 \nL 609.487898 521.581217 \nL 610.434807 519.310163 \nL 611.381716 515.240841 \nL 612.328625 516.018729 \nL 613.275534 516.952943 \nL 614.222443 522.918567 \nL 615.169352 517.012359 \nL 616.116261 519.386071 \nL 617.06317 519.503856 \nL 618.01008 513.860286 \nL 618.956989 520.654157 \nL 619.903898 513.312423 \nL 620.850807 517.537649 \nL 621.797716 514.758321 \nL 622.744625 519.511329 \nL 623.691534 516.637327 \nL 624.638443 519.472212 \nL 625.585352 515.917373 \nL 626.532261 519.719751 \nL 627.47917 520.566585 \nL 628.42608 513.901791 \nL 629.372989 515.011456 \nL 630.319898 515.766131 \nL 631.266807 518.61687 \nL 632.213716 518.224771 \nL 633.160625 515.661032 \nL 634.107534 514.035212 \nL 635.054443 519.480312 \nL 636.001352 520.393 \nL 636.948261 518.534728 \nL 637.89517 512.90461 \nL 638.84208 520.506211 \nL 639.788989 521.471084 \nL 640.735898 520.019117 \nL 641.682807 519.387643 \nL 642.629716 518.111688 \nL 643.576625 521.390807 \nL 644.523534 519.083894 \nL 645.470443 515.820195 \nL 647.364261 517.296178 \nL 648.31117 516.760668 \nL 649.25808 515.272651 \nL 650.204989 521.035448 \nL 651.151898 516.806274 \nL 652.098807 516.568853 \nL 653.045716 522.861617 \nL 653.992625 519.704332 \nL 654.939534 514.549746 \nL 655.886443 515.267771 \nL 656.833352 514.313347 \nL 657.780261 516.786346 \nL 658.72717 521.487538 \nL 660.620989 520.750939 \nL 661.567898 518.559831 \nL 662.514807 516.88926 \nL 663.461716 520.523891 \nL 664.408625 513.01243 \nL 665.355534 520.712678 \nL 666.302443 519.640138 \nL 667.249352 518.742805 \nL 668.196261 520.642493 \nL 669.14317 517.140261 \nL 670.09008 517.559737 \nL 671.036989 520.758719 \nL 671.983898 521.59868 \nL 672.930807 522.132733 \nL 673.877716 520.108976 \nL 674.824625 521.453621 \nL 675.771534 517.024776 \nL 676.718443 514.804285 \nL 677.665352 518.223992 \nL 678.612261 521.061305 \nL 679.55917 517.165057 \nL 680.50608 518.388379 \nL 681.452989 520.295349 \nL 682.399898 513.146732 \nL 683.346807 515.879662 \nL 684.293716 520.946637 \nL 685.240625 523.177246 \nL 686.187534 518.537181 \nL 687.134443 515.844237 \nL 688.081352 518.693954 \nL 689.028261 514.662395 \nL 689.97517 518.812237 \nL 690.92208 517.410258 \nL 691.868989 521.347602 \nL 692.815898 518.923186 \nL 693.762807 518.128065 \nL 694.709716 519.167084 \nL 695.656625 510.920033 \nL 696.603534 517.499133 \nL 697.550443 519.077558 \nL 698.497352 519.23764 \nL 699.444261 513.787405 \nL 700.39117 519.661344 \nL 701.33808 516.988853 \nL 702.284989 519.965041 \nL 703.231898 517.980771 \nL 704.178807 515.030912 \nL 706.072625 525.387401 \nL 707.019534 519.969129 \nL 707.966443 516.835005 \nL 708.913352 515.449276 \nL 709.860261 521.964488 \nL 710.80717 513.981277 \nL 711.75408 522.470897 \nL 712.700989 524.59205 \nL 713.647898 516.67675 \nL 714.594807 516.136743 \nL 715.541716 513.817183 \nL 716.488625 518.563982 \nL 717.435534 520.147134 \nL 719.329352 518.250206 \nL 720.276261 519.146287 \nL 721.22317 524.117106 \nL 722.17008 521.496455 \nL 723.116989 513.74102 \nL 724.063898 521.53676 \nL 725.010807 518.800215 \nL 725.957716 518.049998 \nL 726.904625 524.442316 \nL 727.851534 513.340489 \nL 728.798443 519.447173 \nL 729.745352 513.835567 \nL 730.692261 518.187967 \nL 731.63917 519.897654 \nL 732.58608 519.890066 \nL 733.532989 519.365146 \nL 734.479898 522.006313 \nL 735.426807 520.456031 \nL 736.373716 521.602282 \nL 737.320625 514.866972 \nL 739.214443 525.626151 \nL 740.161352 517.678045 \nL 741.108261 523.151044 \nL 742.05517 512.656701 \nL 743.00208 519.460281 \nL 743.948989 521.672519 \nL 744.895898 520.573049 \nL 745.842807 519.141445 \nL 746.789716 522.925747 \nL 747.736625 519.499321 \nL 748.683534 519.593114 \nL 749.630443 516.703411 \nL 750.577352 512.966837 \nL 751.524261 515.728816 \nL 752.47117 512.428836 \nL 753.41808 516.045377 \nL 754.364989 521.496966 \nL 755.311898 520.917741 \nL 756.258807 524.498347 \nL 757.205716 520.946842 \nL 758.152625 513.489022 \nL 759.099534 520.421833 \nL 760.046443 521.17412 \nL 760.993352 521.243692 \nL 761.940261 521.889781 \nL 763.83408 515.358447 \nL 764.780989 517.136914 \nL 765.727898 516.328481 \nL 766.674807 518.806909 \nL 767.621716 514.283326 \nL 768.568625 518.907716 \nL 769.515534 518.460864 \nL 770.462443 518.624523 \nL 771.409352 519.253276 \nL 772.356261 518.299913 \nL 773.30317 517.977232 \nL 774.25008 521.281492 \nL 775.196989 516.644621 \nL 776.143898 520.936405 \nL 777.090807 519.555185 \nL 778.037716 523.772925 \nL 778.984625 522.462006 \nL 779.931534 520.893737 \nL 779.931534 520.893737 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p3aa3c31ee4)\" d=\"M 69.749716 91.402945 \nL 74.484261 172.417529 \nL 77.324989 227.250158 \nL 80.165716 286.229179 \nL 83.953352 365.813156 \nL 85.84717 400.468482 \nL 86.79408 415.755307 \nL 88.687898 440.649171 \nL 90.581716 459.425735 \nL 91.528625 466.467579 \nL 92.475534 472.440676 \nL 94.369352 481.716437 \nL 95.316261 485.229681 \nL 96.26317 488.039566 \nL 97.21008 490.417392 \nL 98.156989 492.228397 \nL 99.103898 493.382262 \nL 100.050807 493.828271 \nL 101.944625 494.049404 \nL 102.891534 494.104055 \nL 104.785352 493.788299 \nL 106.67917 493.160132 \nL 107.62608 492.868789 \nL 108.572989 492.695676 \nL 110.466807 491.985342 \nL 112.360625 491.484707 \nL 116.148261 491.000591 \nL 118.04208 490.873442 \nL 123.723534 490.248278 \nL 128.45808 490.333244 \nL 129.404989 490.093958 \nL 131.298807 490.40136 \nL 134.139534 490.727426 \nL 135.086443 490.522261 \nL 136.980261 490.569873 \nL 137.92717 490.454171 \nL 138.87408 490.765099 \nL 140.767898 490.607099 \nL 142.661716 491.236632 \nL 143.608625 491.3188 \nL 145.502443 491.311774 \nL 147.396261 491.449296 \nL 149.29008 491.361928 \nL 150.236989 491.564921 \nL 151.183898 491.568051 \nL 152.130807 491.861413 \nL 155.918443 491.783474 \nL 158.75917 492.341352 \nL 159.70608 492.326291 \nL 160.652989 492.44258 \nL 161.599898 492.381772 \nL 163.493716 492.892116 \nL 164.440625 492.757596 \nL 167.281352 493.225731 \nL 168.228261 493.185772 \nL 169.17517 492.932982 \nL 171.068989 493.187062 \nL 172.015898 493.320585 \nL 172.962807 492.90079 \nL 173.909716 493.175143 \nL 174.856625 493.094981 \nL 176.750443 493.5949 \nL 177.697352 494.220204 \nL 178.644261 494.433775 \nL 179.59117 494.213319 \nL 180.53808 494.232047 \nL 181.484989 494.429355 \nL 183.378807 494.023599 \nL 184.325716 494.502223 \nL 185.272625 494.75781 \nL 186.219534 494.58448 \nL 187.166443 494.264929 \nL 188.113352 494.072604 \nL 189.060261 494.586894 \nL 190.00717 494.76375 \nL 190.95408 494.717326 \nL 192.847898 495.180198 \nL 193.794807 494.902677 \nL 194.741716 495.687246 \nL 195.688625 495.942641 \nL 196.635534 496.329004 \nL 198.529352 496.622047 \nL 199.476261 496.499306 \nL 201.37008 496.69397 \nL 202.316989 496.50167 \nL 203.263898 496.670017 \nL 204.210807 497.260497 \nL 206.104625 497.347239 \nL 208.945352 498.164435 \nL 209.892261 498.126033 \nL 210.83917 498.220555 \nL 211.78608 498.437383 \nL 212.732989 498.37628 \nL 214.626807 498.918076 \nL 215.573716 499.256035 \nL 217.467534 498.72087 \nL 219.361352 498.682967 \nL 221.25517 499.462081 \nL 222.20208 499.702542 \nL 223.148989 499.753195 \nL 225.042807 499.650165 \nL 225.989716 500.056739 \nL 228.830443 500.611232 \nL 229.777352 501.349927 \nL 230.724261 501.916198 \nL 231.67117 501.957257 \nL 232.61808 501.747889 \nL 233.564989 502.084661 \nL 234.511898 502.188699 \nL 235.458807 502.418353 \nL 236.405716 502.127891 \nL 238.299534 501.974644 \nL 240.193352 502.738594 \nL 241.140261 502.375353 \nL 242.08717 502.730431 \nL 243.03408 502.742439 \nL 243.980989 503.155872 \nL 244.927898 503.223375 \nL 245.874807 502.88502 \nL 246.821716 503.545468 \nL 248.715534 504.440016 \nL 249.662443 504.932053 \nL 250.609352 505.67504 \nL 251.556261 506.019719 \nL 252.50317 506.006778 \nL 253.45008 505.75768 \nL 255.343898 506.234451 \nL 256.290807 505.808856 \nL 259.131534 506.996013 \nL 260.078443 507.182692 \nL 261.972261 506.706509 \nL 263.86608 507.277903 \nL 264.812989 508.117328 \nL 265.759898 508.782618 \nL 267.653716 509.422562 \nL 268.600625 508.939736 \nL 269.547534 508.784215 \nL 270.494443 509.135179 \nL 271.441352 509.840888 \nL 274.28208 510.100704 \nL 276.175898 510.821411 \nL 277.122807 510.925616 \nL 278.069716 510.545665 \nL 279.016625 510.748262 \nL 280.910443 511.542617 \nL 281.857352 511.65174 \nL 282.804261 511.895957 \nL 283.75117 511.517284 \nL 285.644989 512.464925 \nL 286.591898 512.899564 \nL 287.538807 512.957843 \nL 289.432625 513.76938 \nL 290.379534 514.156369 \nL 291.326443 513.791685 \nL 292.273352 514.274141 \nL 293.220261 513.805022 \nL 296.060989 514.456694 \nL 297.007898 514.950417 \nL 298.901716 516.44578 \nL 300.795534 516.159138 \nL 301.742443 516.433529 \nL 302.689352 516.568457 \nL 303.636261 516.407175 \nL 304.58317 516.841265 \nL 305.53008 516.915091 \nL 306.476989 517.13303 \nL 307.423898 517.573495 \nL 308.370807 517.498813 \nL 309.317716 517.231755 \nL 310.264625 517.13059 \nL 311.211534 517.818032 \nL 312.158443 518.166186 \nL 313.105352 518.352648 \nL 314.052261 518.657673 \nL 314.99917 518.423165 \nL 316.892989 518.865789 \nL 317.839898 519.248473 \nL 318.786807 519.983552 \nL 319.733716 519.636357 \nL 320.680625 519.481155 \nL 321.627534 519.742912 \nL 322.574443 520.246025 \nL 323.521352 520.34793 \nL 324.468261 520.728276 \nL 326.36208 520.83708 \nL 329.202807 521.852236 \nL 330.149716 522.275748 \nL 331.096625 522.157913 \nL 332.043534 522.458122 \nL 334.884261 522.749709 \nL 335.83117 523.067471 \nL 336.77808 522.943606 \nL 338.671898 523.478541 \nL 339.618807 523.821214 \nL 340.565716 523.839572 \nL 341.512625 523.649814 \nL 342.459534 523.162031 \nL 345.300261 524.379464 \nL 346.24717 524.919164 \nL 347.19408 524.941149 \nL 348.140989 524.740482 \nL 349.087898 524.684745 \nL 350.981716 525.026358 \nL 351.928625 524.779918 \nL 352.875534 524.710154 \nL 354.769352 526.074613 \nL 355.716261 526.819733 \nL 356.66317 526.685239 \nL 358.556989 527.227839 \nL 359.503898 527.157079 \nL 360.450807 527.388675 \nL 365.185352 527.884966 \nL 366.132261 527.878911 \nL 367.07917 528.026997 \nL 368.02608 526.830924 \nL 368.972989 527.563448 \nL 369.919898 527.93498 \nL 371.813716 528.486573 \nL 372.760625 528.919003 \nL 373.707534 528.75514 \nL 374.654443 528.865489 \nL 375.601352 529.204483 \nL 376.548261 529.342758 \nL 377.49517 529.201366 \nL 378.44208 529.740236 \nL 380.335898 529.855644 \nL 382.229716 530.802518 \nL 383.176625 530.848048 \nL 385.070443 530.030456 \nL 386.017352 529.934452 \nL 386.964261 530.350632 \nL 387.91117 530.619428 \nL 389.804989 530.52363 \nL 390.751898 530.99469 \nL 391.698807 531.614847 \nL 393.592625 531.617964 \nL 394.539534 531.413183 \nL 396.433352 531.591635 \nL 397.380261 531.624057 \nL 398.32717 531.889149 \nL 399.27408 531.900838 \nL 400.220989 531.649224 \nL 401.167898 531.652315 \nL 402.114807 532.209747 \nL 404.008625 532.061597 \nL 404.955534 532.821932 \nL 406.849352 532.755911 \nL 407.796261 533.196389 \nL 408.74317 532.84669 \nL 409.69008 533.081134 \nL 410.636989 533.068832 \nL 412.530807 533.771565 \nL 413.477716 533.628423 \nL 414.424625 532.994457 \nL 415.371534 533.367815 \nL 416.318443 533.049069 \nL 417.265352 533.655314 \nL 418.212261 534.012806 \nL 421.052989 534.114264 \nL 421.999898 534.188409 \nL 422.946807 534.37873 \nL 423.893716 534.015629 \nL 424.840625 533.884367 \nL 425.787534 534.16217 \nL 426.734443 533.940871 \nL 427.681352 534.06526 \nL 428.628261 534.835406 \nL 430.52208 534.73672 \nL 431.468989 534.804018 \nL 432.415898 535.150332 \nL 433.362807 534.999525 \nL 435.256625 535.3324 \nL 436.203534 534.576944 \nL 437.150443 534.92147 \nL 438.097352 534.999307 \nL 439.044261 535.528187 \nL 439.99117 535.191403 \nL 440.93808 535.478314 \nL 441.884989 535.298329 \nL 443.778807 535.248814 \nL 445.672625 535.958573 \nL 446.619534 535.823683 \nL 447.566443 536.1109 \nL 448.513352 535.985157 \nL 449.460261 536.124736 \nL 450.40717 535.87454 \nL 452.300989 536.719611 \nL 453.247898 536.935442 \nL 456.088625 536.040434 \nL 457.035534 535.887302 \nL 457.982443 535.888234 \nL 458.929352 536.525407 \nL 460.82317 535.910182 \nL 461.77008 536.448081 \nL 462.716989 536.462746 \nL 463.663898 535.983535 \nL 464.610807 536.406038 \nL 465.557716 536.121989 \nL 466.504625 536.421675 \nL 467.451534 537.001986 \nL 468.398443 537.178842 \nL 469.345352 537.155119 \nL 470.292261 536.998857 \nL 471.23917 537.083669 \nL 472.18608 537.398978 \nL 473.132989 537.001744 \nL 474.079898 536.919077 \nL 475.973716 536.559145 \nL 478.814443 537.533447 \nL 479.761352 537.474274 \nL 480.708261 537.194121 \nL 482.60208 537.344149 \nL 484.495898 537.719104 \nL 485.442807 537.565448 \nL 487.336625 537.929545 \nL 488.283534 538.073454 \nL 489.230443 537.99637 \nL 490.177352 538.537731 \nL 491.124261 538.385595 \nL 492.07117 538.406788 \nL 493.01808 538.199005 \nL 493.964989 538.275131 \nL 494.911898 538.046933 \nL 495.858807 538.298253 \nL 496.805716 538.143677 \nL 497.752625 538.137047 \nL 498.699534 538.284022 \nL 500.593352 538.11151 \nL 501.540261 538.178029 \nL 502.48717 538.422105 \nL 503.43408 538.486031 \nL 504.380989 538.268705 \nL 505.327898 538.402062 \nL 506.274807 538.327239 \nL 508.168625 538.710664 \nL 510.062443 538.868753 \nL 511.956261 538.643916 \nL 512.90317 538.883125 \nL 513.85008 538.61346 \nL 514.796989 538.565414 \nL 515.743898 538.699589 \nL 516.690807 538.634322 \nL 518.584625 538.782625 \nL 519.531534 539.100464 \nL 520.478443 539.266525 \nL 521.425352 539.174162 \nL 522.372261 538.935681 \nL 524.26608 539.419185 \nL 526.159898 539.021541 \nL 527.106807 538.915267 \nL 528.053716 539.235392 \nL 529.947534 539.400904 \nL 530.894443 539.24574 \nL 532.788261 539.164402 \nL 533.73517 539.419823 \nL 535.628989 539.409041 \nL 536.575898 539.524794 \nL 537.522807 539.799544 \nL 538.469716 539.456462 \nL 540.363534 539.430886 \nL 541.310443 539.216345 \nL 542.257352 539.367459 \nL 543.204261 539.299446 \nL 544.15117 539.033396 \nL 545.09808 539.194564 \nL 546.991898 538.85907 \nL 547.938807 539.343736 \nL 548.885716 539.647267 \nL 549.832625 539.763902 \nL 550.779534 539.508698 \nL 551.726443 539.043667 \nL 552.673352 538.815917 \nL 553.620261 539.010401 \nL 554.56717 539.537224 \nL 555.51408 539.687725 \nL 556.460989 539.240017 \nL 557.407898 539.304211 \nL 558.354807 539.575779 \nL 559.301716 539.182108 \nL 560.248625 539.650512 \nL 561.195534 539.820609 \nL 564.036261 539.923332 \nL 564.98317 540.143686 \nL 567.823898 539.73917 \nL 568.770807 539.92636 \nL 569.717716 539.718053 \nL 570.664625 539.974968 \nL 571.611534 539.604293 \nL 573.505352 539.507855 \nL 574.452261 539.774901 \nL 577.292989 540.061263 \nL 578.239898 539.722601 \nL 579.186807 539.676764 \nL 580.133716 539.798713 \nL 581.080625 540.231309 \nL 582.974443 539.782566 \nL 583.921352 539.916306 \nL 584.868261 540.400717 \nL 587.708989 540.333534 \nL 588.655898 539.992559 \nL 591.496625 540.271039 \nL 592.443534 540.107316 \nL 593.390443 540.453937 \nL 594.337352 540.369456 \nL 595.284261 539.901142 \nL 597.17808 539.742248 \nL 598.124989 540.339116 \nL 599.071898 540.562485 \nL 600.018807 540.242525 \nL 601.912625 540.417746 \nL 602.859534 540.361357 \nL 604.753352 540.551831 \nL 605.700261 540.263502 \nL 606.64717 540.382474 \nL 607.59408 540.108006 \nL 608.540989 540.192563 \nL 611.381716 539.841203 \nL 612.328625 540.304381 \nL 615.169352 540.192716 \nL 617.06317 540.537804 \nL 618.01008 540.375665 \nL 618.956989 540.407398 \nL 619.903898 540.266057 \nL 620.850807 540.364972 \nL 621.797716 540.610876 \nL 623.691534 540.242436 \nL 624.638443 540.707416 \nL 625.585352 540.242551 \nL 626.532261 540.200841 \nL 627.47917 539.910902 \nL 628.42608 540.367579 \nL 629.372989 540.507336 \nL 630.319898 540.350626 \nL 631.266807 540.489298 \nL 632.213716 540.298671 \nL 633.160625 540.636694 \nL 634.107534 540.591369 \nL 635.054443 540.392528 \nL 636.001352 540.4139 \nL 636.948261 540.284031 \nL 637.89517 540.32035 \nL 638.84208 540.581634 \nL 640.735898 540.628199 \nL 643.576625 539.875107 \nL 645.470443 540.034959 \nL 647.364261 540.339436 \nL 649.25808 540.369252 \nL 650.204989 540.069783 \nL 652.098807 540.521222 \nL 653.045716 540.294685 \nL 653.992625 540.18813 \nL 654.939534 540.534367 \nL 655.886443 540.280441 \nL 656.833352 540.434455 \nL 657.780261 540.396041 \nL 658.72717 540.571606 \nL 659.67408 540.093455 \nL 660.620989 540.146152 \nL 661.567898 540.521733 \nL 662.514807 540.365062 \nL 663.461716 540.681048 \nL 664.408625 540.584751 \nL 665.355534 540.714851 \nL 667.249352 540.704248 \nL 669.14317 540.37495 \nL 671.036989 540.630818 \nL 671.983898 540.811148 \nL 674.824625 540.185243 \nL 675.771534 540.068889 \nL 676.718443 540.513583 \nL 677.665352 540.674239 \nL 680.50608 540.723576 \nL 681.452989 540.375882 \nL 682.399898 540.655371 \nL 683.346807 540.773258 \nL 684.293716 541.133075 \nL 685.240625 541.209034 \nL 686.187534 540.952693 \nL 687.134443 541.046154 \nL 688.081352 540.555165 \nL 689.028261 540.486768 \nL 689.97517 540.571248 \nL 690.92208 540.839661 \nL 694.709716 540.477391 \nL 696.603534 540.577649 \nL 697.550443 540.462534 \nL 699.444261 540.865415 \nL 700.39117 540.906103 \nL 701.33808 540.788664 \nL 703.231898 540.988399 \nL 704.178807 540.794898 \nL 706.072625 540.751847 \nL 707.019534 540.599059 \nL 707.966443 540.583806 \nL 708.913352 540.858875 \nL 709.860261 540.982267 \nL 710.80717 540.656304 \nL 711.75408 540.705027 \nL 712.700989 540.446195 \nL 713.647898 540.669832 \nL 714.594807 540.760687 \nL 715.541716 540.631852 \nL 716.488625 540.360207 \nL 717.435534 540.355379 \nL 718.382443 540.15535 \nL 720.276261 540.733898 \nL 721.22317 540.956654 \nL 722.17008 540.609369 \nL 723.116989 540.651245 \nL 724.063898 540.571708 \nL 725.010807 540.736083 \nL 727.851534 540.678647 \nL 729.745352 540.915276 \nL 730.692261 540.906487 \nL 731.63917 540.405226 \nL 733.532989 540.41155 \nL 736.373716 540.923605 \nL 737.320625 540.669027 \nL 740.161352 540.779006 \nL 742.05517 540.985244 \nL 743.00208 540.72502 \nL 743.948989 540.584368 \nL 746.789716 540.81742 \nL 747.736625 540.589414 \nL 748.683534 540.645866 \nL 749.630443 540.293127 \nL 750.577352 540.597488 \nL 752.47117 540.692597 \nL 755.311898 540.415152 \nL 756.258807 540.63442 \nL 759.099534 540.213871 \nL 760.993352 540.61209 \nL 762.88717 540.661017 \nL 763.83408 540.825724 \nL 765.727898 540.402837 \nL 766.674807 540.692763 \nL 767.621716 540.584151 \nL 768.568625 540.219249 \nL 770.462443 540.355672 \nL 771.409352 540.721455 \nL 772.356261 540.679477 \nL 773.30317 540.351418 \nL 774.25008 540.706355 \nL 775.196989 540.529692 \nL 776.143898 540.634599 \nL 777.090807 540.585773 \nL 778.984625 540.190187 \nL 779.931534 540.280888 \nL 779.931534 540.280888 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 565.918125 \nL 34.240625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 815.440625 565.918125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 565.918125 \nL 815.440625 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 22.318125 \nL 815.440625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Model MAE -->\n    <g transform=\"translate(391.845313 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"486.71875\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 41.240625 59.674375 \nL 96.515625 59.674375 \nQ 98.515625 59.674375 98.515625 57.674375 \nL 98.515625 29.318125 \nQ 98.515625 27.318125 96.515625 27.318125 \nL 41.240625 27.318125 \nQ 39.240625 27.318125 39.240625 29.318125 \nL 39.240625 57.674375 \nQ 39.240625 59.674375 41.240625 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 43.240625 35.416562 \nL 63.240625 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_17\">\n     <!-- train -->\n     <g transform=\"translate(71.240625 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 43.240625 50.094687 \nL 63.240625 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_18\">\n     <!-- test -->\n     <g transform=\"translate(71.240625 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3aa3c31ee4\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"34.240625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"603.474375pt\" version=\"1.1\" viewBox=\"0 0 829.003125 603.474375\" width=\"829.003125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-13T20:12:55.863068</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 603.474375 \nL 829.003125 603.474375 \nL 829.003125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \nL 821.803125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m723e005109\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.112216\" xlink:href=\"#m723e005109\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(72.930966 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.803125\" xlink:href=\"#m723e005109\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(161.259375 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"265.494034\" xlink:href=\"#m723e005109\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(255.950284 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.184943\" xlink:href=\"#m723e005109\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(350.641193 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"454.875852\" xlink:href=\"#m723e005109\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(445.332102 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"549.566761\" xlink:href=\"#m723e005109\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(540.023011 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"644.25767\" xlink:href=\"#m723e005109\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 600 -->\n      <g transform=\"translate(634.71392 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"738.94858\" xlink:href=\"#m723e005109\" y=\"565.918125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 700 -->\n      <g transform=\"translate(729.40483 580.516562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- epoch -->\n     <g transform=\"translate(415.975 594.194687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mdec3e6f01b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"524.143262\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 527.94248)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"462.561441\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 466.36066)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"400.97962\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 404.778839)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"339.3978\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 343.197019)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"277.815979\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 281.615198)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"216.234159\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 220.033377)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"154.652338\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 158.451557)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"93.070517\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 96.869736)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mdec3e6f01b\" y=\"31.488697\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 45 -->\n      <g transform=\"translate(20.878125 35.287916)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- MSE -->\n     <g transform=\"translate(14.798438 304.765781)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p8fb15c2b4c)\" d=\"M 76.112216 47.027216 \nL 78.952943 111.58406 \nL 84.634398 236.163234 \nL 85.581307 255.874613 \nL 87.475125 298.599656 \nL 88.422034 315.230135 \nL 91.262761 369.538369 \nL 94.103489 409.45586 \nL 95.997307 433.318087 \nL 96.944216 441.959848 \nL 97.891125 453.038968 \nL 99.784943 463.880577 \nL 100.731852 471.022152 \nL 101.678761 471.731035 \nL 102.62567 480.489878 \nL 103.57258 485.255983 \nL 104.519489 487.768956 \nL 105.466398 492.934336 \nL 106.413307 491.825027 \nL 107.360216 493.815018 \nL 108.307125 496.868891 \nL 109.254034 498.678032 \nL 111.147852 501.537248 \nL 112.094761 500.128691 \nL 113.04167 498.288236 \nL 113.98858 501.007254 \nL 114.935489 500.719558 \nL 115.882398 502.366419 \nL 116.829307 499.567547 \nL 117.776216 499.828187 \nL 118.723125 499.877766 \nL 119.670034 500.338859 \nL 120.616943 502.290183 \nL 121.563852 502.290412 \nL 122.510761 504.137727 \nL 123.45767 504.454288 \nL 124.40458 505.398169 \nL 125.351489 506.00579 \nL 126.298398 501.950835 \nL 127.245307 503.677027 \nL 128.192216 502.828334 \nL 129.139125 498.852692 \nL 130.086034 506.098712 \nL 131.032943 506.536742 \nL 132.926761 503.40473 \nL 133.87367 502.267067 \nL 134.82058 500.770646 \nL 135.767489 503.077257 \nL 136.714398 503.423124 \nL 137.661307 503.108941 \nL 138.608216 506.951756 \nL 139.555125 502.242013 \nL 140.502034 504.800636 \nL 141.448943 504.913014 \nL 143.342761 499.955129 \nL 144.28967 504.356081 \nL 145.23658 501.657513 \nL 146.183489 504.115932 \nL 147.130398 506.137614 \nL 148.077307 502.265 \nL 149.024216 502.505712 \nL 149.971125 499.839439 \nL 150.918034 508.00741 \nL 151.864943 501.552764 \nL 152.811852 501.864903 \nL 153.758761 507.037465 \nL 154.70567 505.890652 \nL 155.65258 507.739594 \nL 156.599489 506.256005 \nL 157.546398 505.528523 \nL 158.493307 503.790914 \nL 159.440216 503.81507 \nL 160.387125 499.836456 \nL 161.334034 501.844336 \nL 162.280943 503.583296 \nL 163.227852 502.918013 \nL 164.174761 503.956442 \nL 165.12167 503.889291 \nL 166.06858 504.688053 \nL 167.015489 504.750887 \nL 167.962398 505.470364 \nL 168.909307 503.04138 \nL 169.856216 504.77496 \nL 170.803125 506.185501 \nL 171.750034 505.019995 \nL 172.696943 503.550977 \nL 173.643852 505.445052 \nL 174.590761 505.081232 \nL 175.53767 507.918976 \nL 176.48458 502.583381 \nL 177.431489 509.657595 \nL 178.378398 503.754009 \nL 179.325307 506.880488 \nL 180.272216 504.391401 \nL 181.219125 504.886874 \nL 182.166034 505.252896 \nL 183.112943 507.543768 \nL 184.059852 508.914203 \nL 185.006761 504.425922 \nL 185.95367 506.929404 \nL 186.90058 503.796141 \nL 187.847489 506.62944 \nL 188.794398 505.471944 \nL 189.741307 505.489704 \nL 190.688216 502.56319 \nL 191.635125 500.450403 \nL 192.582034 503.222218 \nL 193.528943 505.20411 \nL 194.475852 511.111467 \nL 195.422761 503.184091 \nL 196.36967 507.998583 \nL 197.31658 503.005273 \nL 198.263489 503.917916 \nL 199.210398 509.035038 \nL 200.157307 503.324177 \nL 201.104216 502.40094 \nL 202.051125 503.668347 \nL 202.998034 507.299438 \nL 203.944943 507.142643 \nL 204.891852 506.478723 \nL 205.838761 503.632546 \nL 206.78567 507.519337 \nL 207.73258 509.125563 \nL 208.679489 510.190073 \nL 209.626398 506.369153 \nL 210.573307 506.625746 \nL 211.520216 506.474413 \nL 212.467125 507.082164 \nL 213.414034 506.092263 \nL 214.360943 508.556467 \nL 215.307852 509.885569 \nL 216.254761 509.336817 \nL 217.20167 503.444484 \nL 218.14858 503.123054 \nL 219.095489 503.922003 \nL 220.042398 508.387886 \nL 220.989307 504.600353 \nL 221.936216 506.504676 \nL 222.883125 507.243745 \nL 223.830034 504.340236 \nL 224.776943 509.78351 \nL 225.723852 508.114261 \nL 226.670761 509.884089 \nL 227.61767 506.572167 \nL 228.56458 510.709778 \nL 229.511489 506.571909 \nL 230.458398 508.58279 \nL 231.405307 508.800181 \nL 232.352216 510.559021 \nL 234.246034 506.394095 \nL 235.192943 505.870256 \nL 236.139852 504.740844 \nL 237.086761 505.541027 \nL 238.03367 508.040134 \nL 238.98058 504.729157 \nL 239.927489 508.82204 \nL 240.874398 506.584195 \nL 241.821307 511.224009 \nL 242.768216 505.725665 \nL 243.715125 511.655984 \nL 244.662034 507.063887 \nL 245.608943 509.302455 \nL 247.502761 508.634272 \nL 248.44967 509.900022 \nL 249.39658 508.535066 \nL 250.343489 509.514208 \nL 251.290398 511.244899 \nL 252.237307 510.2651 \nL 253.184216 508.767052 \nL 254.131125 510.642187 \nL 255.078034 511.792388 \nL 256.024943 510.696188 \nL 256.971852 508.745569 \nL 257.918761 511.436121 \nL 258.86567 511.868178 \nL 259.81258 510.320205 \nL 260.759489 513.437669 \nL 261.706398 510.188417 \nL 262.653307 510.354409 \nL 263.600216 507.94595 \nL 264.547125 510.524094 \nL 265.494034 506.68949 \nL 267.387852 515.332496 \nL 268.334761 510.359236 \nL 269.28167 509.837247 \nL 270.22858 508.279977 \nL 271.175489 511.709639 \nL 272.122398 511.910316 \nL 273.069307 508.861829 \nL 274.016216 512.171267 \nL 274.963125 510.778215 \nL 275.910034 513.054709 \nL 276.856943 512.224739 \nL 277.803852 516.760492 \nL 278.750761 513.353998 \nL 279.69767 511.123488 \nL 280.64458 516.502425 \nL 281.591489 512.079315 \nL 282.538398 513.206119 \nL 283.485307 513.538302 \nL 284.432216 514.400097 \nL 285.379125 514.425151 \nL 286.326034 508.66225 \nL 287.272943 516.556808 \nL 288.219852 516.097359 \nL 289.166761 511.61165 \nL 290.11367 513.779167 \nL 291.06058 511.412406 \nL 292.007489 512.842639 \nL 292.954398 516.006276 \nL 293.901307 514.400479 \nL 294.848216 512.566777 \nL 295.795125 509.903035 \nL 296.742034 516.340133 \nL 297.688943 512.001323 \nL 298.635852 513.31512 \nL 299.582761 520.287292 \nL 300.52967 517.963573 \nL 301.47658 514.763377 \nL 302.423489 515.763038 \nL 303.370398 513.398298 \nL 304.317307 515.045082 \nL 305.264216 515.665167 \nL 306.211125 512.495304 \nL 307.158034 514.2665 \nL 308.104943 515.139336 \nL 309.051852 517.912937 \nL 309.998761 511.75015 \nL 310.94567 513.625033 \nL 311.89258 514.156054 \nL 312.839489 515.35886 \nL 314.733307 514.019738 \nL 315.680216 517.674368 \nL 316.627125 512.111287 \nL 317.574034 517.707438 \nL 318.520943 515.217834 \nL 319.467852 514.826387 \nL 320.414761 517.996379 \nL 321.36167 517.255413 \nL 322.30858 520.725756 \nL 323.255489 520.031832 \nL 324.202398 515.512735 \nL 325.149307 515.627868 \nL 326.096216 516.759775 \nL 327.043125 516.624006 \nL 327.990034 515.722592 \nL 328.936943 517.188761 \nL 329.883852 519.500699 \nL 330.830761 516.583113 \nL 331.77767 516.86833 \nL 332.72458 514.81709 \nL 333.671489 519.157304 \nL 334.618398 513.290888 \nL 335.565307 517.356808 \nL 336.512216 517.682355 \nL 337.459125 518.614578 \nL 338.406034 514.490587 \nL 339.352943 517.740831 \nL 340.299852 514.500688 \nL 341.246761 517.284583 \nL 342.19367 514.444038 \nL 343.14058 515.840267 \nL 344.087489 512.791286 \nL 345.034398 518.466728 \nL 345.981307 517.914417 \nL 346.928216 517.661195 \nL 347.875125 521.35297 \nL 348.822034 520.354618 \nL 349.768943 518.315313 \nL 350.715852 520.799438 \nL 351.662761 515.994789 \nL 352.60967 515.008336 \nL 353.55658 519.960676 \nL 354.503489 520.431988 \nL 355.450398 518.167674 \nL 356.397307 518.74579 \nL 357.344216 517.133814 \nL 358.291125 517.581382 \nL 359.238034 522.296745 \nL 362.078761 516.264249 \nL 363.02567 519.107678 \nL 363.97258 517.180862 \nL 364.919489 517.252265 \nL 365.866398 518.653134 \nL 366.813307 518.543657 \nL 367.760216 518.157502 \nL 368.707125 517.423301 \nL 369.654034 519.884399 \nL 370.600943 516.985441 \nL 371.547852 522.101806 \nL 372.494761 520.935742 \nL 373.44167 516.738357 \nL 375.335489 520.248636 \nL 376.282398 518.613897 \nL 377.229307 519.171482 \nL 378.176216 517.938872 \nL 379.123125 518.574413 \nL 380.070034 517.915145 \nL 381.016943 519.710673 \nL 381.963852 516.518851 \nL 382.910761 518.84866 \nL 383.85767 516.570909 \nL 384.80458 523.548777 \nL 385.751489 519.304127 \nL 386.698398 518.165877 \nL 387.645307 518.043726 \nL 388.592216 519.138922 \nL 389.539125 515.911535 \nL 390.486034 521.653592 \nL 391.432943 520.915316 \nL 392.379852 519.081239 \nL 393.326761 520.133363 \nL 394.27367 518.067429 \nL 395.22058 517.474965 \nL 396.167489 514.64633 \nL 397.114398 521.323207 \nL 398.061307 519.505262 \nL 399.008216 520.002791 \nL 399.955125 520.78058 \nL 400.902034 519.911714 \nL 401.848943 521.844121 \nL 402.795852 519.064378 \nL 403.742761 522.297485 \nL 404.68967 518.058596 \nL 405.63658 520.732387 \nL 406.583489 521.742156 \nL 407.530398 518.144817 \nL 408.477307 521.684513 \nL 409.424216 519.520326 \nL 410.371125 521.256532 \nL 412.264943 519.002107 \nL 413.211852 523.689192 \nL 414.158761 521.46297 \nL 415.10567 518.571001 \nL 416.05258 521.436172 \nL 416.999489 520.917577 \nL 417.946398 521.432848 \nL 418.893307 520.698988 \nL 419.840216 521.425184 \nL 420.787125 520.777174 \nL 421.734034 521.650656 \nL 422.680943 522.161892 \nL 423.627852 522.402158 \nL 424.574761 522.163542 \nL 425.52167 522.056597 \nL 427.415489 519.679957 \nL 428.362398 523.369824 \nL 429.309307 520.65961 \nL 430.256216 521.256273 \nL 431.203125 518.296367 \nL 432.150034 518.115769 \nL 433.096943 519.508504 \nL 434.043852 522.761544 \nL 434.990761 520.150118 \nL 435.93767 520.536608 \nL 436.88458 520.18767 \nL 437.831489 518.434139 \nL 438.778398 523.363023 \nL 439.725307 518.838424 \nL 440.672216 518.756479 \nL 441.619125 522.102194 \nL 442.566034 522.827896 \nL 443.512943 518.18208 \nL 444.459852 522.700032 \nL 445.406761 521.191965 \nL 446.35367 522.45882 \nL 447.30058 519.070474 \nL 448.247489 521.593759 \nL 449.194398 525.597462 \nL 450.141307 519.844305 \nL 451.088216 518.987272 \nL 452.035125 525.016785 \nL 452.982034 519.779626 \nL 453.928943 521.852443 \nL 454.875852 519.73558 \nL 455.822761 522.437037 \nL 456.76967 518.996751 \nL 457.71658 519.128762 \nL 458.663489 522.935353 \nL 459.610398 518.865028 \nL 460.557307 516.659989 \nL 461.504216 520.680482 \nL 462.451125 521.012401 \nL 463.398034 523.156685 \nL 464.344943 516.857694 \nL 465.291852 523.367269 \nL 466.238761 523.392153 \nL 467.18567 521.854011 \nL 468.13258 518.192082 \nL 469.079489 519.985037 \nL 470.026398 522.944767 \nL 470.973307 521.921455 \nL 471.920216 520.644934 \nL 472.867125 521.082036 \nL 473.814034 521.357404 \nL 474.760943 520.803531 \nL 475.707852 519.487567 \nL 476.654761 519.199654 \nL 477.60167 518.48296 \nL 478.54858 522.723406 \nL 479.495489 523.020475 \nL 481.389307 522.794832 \nL 482.336216 520.90209 \nL 484.230034 518.563871 \nL 485.176943 518.644929 \nL 486.123852 520.432611 \nL 487.070761 521.594535 \nL 488.01767 519.440825 \nL 488.96458 522.007476 \nL 489.911489 519.155043 \nL 490.858398 519.441488 \nL 491.805307 518.781392 \nL 492.752216 523.40216 \nL 493.699125 520.190718 \nL 494.646034 522.522699 \nL 495.592943 522.250508 \nL 496.539852 522.513485 \nL 497.486761 523.755844 \nL 498.43367 516.018956 \nL 499.38058 522.973257 \nL 500.327489 521.800474 \nL 501.274398 521.36027 \nL 502.221307 522.068724 \nL 503.168216 519.345736 \nL 504.115125 521.705756 \nL 505.062034 520.249999 \nL 506.008943 524.021405 \nL 506.955852 522.581111 \nL 507.902761 519.543824 \nL 508.84967 518.648694 \nL 509.79658 518.835147 \nL 510.743489 520.935848 \nL 511.690398 520.577877 \nL 512.637307 522.497505 \nL 513.584216 521.610832 \nL 514.531125 523.71178 \nL 515.478034 524.779626 \nL 516.424943 522.114903 \nL 517.371852 525.126602 \nL 518.318761 518.085312 \nL 519.26567 521.689899 \nL 520.21258 520.365196 \nL 521.159489 523.921518 \nL 522.106398 521.040391 \nL 523.053307 522.153265 \nL 524.000216 519.853413 \nL 524.947125 525.227528 \nL 525.894034 521.851192 \nL 526.840943 520.24207 \nL 527.787852 523.921395 \nL 528.734761 522.705822 \nL 529.68167 519.274287 \nL 530.62858 523.613655 \nL 531.575489 523.237784 \nL 532.522398 521.946509 \nL 533.469307 519.844739 \nL 534.416216 521.700816 \nL 535.363125 521.130417 \nL 536.310034 519.592093 \nL 538.203852 522.658205 \nL 539.150761 523.234747 \nL 540.09767 521.267302 \nL 541.04458 522.169292 \nL 541.991489 524.417033 \nL 543.885307 520.759103 \nL 544.832216 520.658676 \nL 545.779125 523.375867 \nL 546.726034 521.753103 \nL 547.672943 523.351072 \nL 548.619852 520.863335 \nL 549.566761 521.389359 \nL 550.51367 520.712114 \nL 551.46058 522.715424 \nL 552.407489 522.458667 \nL 553.354398 520.059687 \nL 554.301307 521.77263 \nL 555.248216 520.709101 \nL 556.195125 520.994342 \nL 557.142034 518.85152 \nL 558.088943 523.336078 \nL 559.035852 524.631258 \nL 559.982761 522.03213 \nL 560.92967 524.261554 \nL 561.87658 520.401895 \nL 562.823489 519.70292 \nL 563.770398 520.266513 \nL 564.717307 525.072343 \nL 565.664216 521.655172 \nL 566.611125 519.982259 \nL 567.558034 523.746929 \nL 568.504943 521.827278 \nL 569.451852 520.409395 \nL 570.398761 523.16937 \nL 571.34567 519.480731 \nL 572.29258 521.635568 \nL 573.239489 519.290226 \nL 575.133307 521.549959 \nL 577.027125 522.207671 \nL 577.974034 520.785501 \nL 578.920943 522.554061 \nL 579.867852 525.657859 \nL 580.814761 524.239184 \nL 581.76167 524.116064 \nL 582.70858 522.492301 \nL 583.655489 521.630553 \nL 584.602398 521.712122 \nL 585.549307 523.283122 \nL 586.496216 522.699532 \nL 587.443125 520.159268 \nL 588.390034 523.708232 \nL 589.336943 520.915304 \nL 590.283852 522.597285 \nL 591.230761 523.804072 \nL 592.17767 521.780247 \nL 593.12458 522.611733 \nL 594.071489 523.095337 \nL 595.018398 516.811439 \nL 595.965307 522.937044 \nL 596.912216 520.414698 \nL 597.859125 521.43939 \nL 598.806034 521.701415 \nL 599.752943 519.718002 \nL 600.699852 523.158505 \nL 601.646761 520.830899 \nL 602.59367 522.567052 \nL 603.54058 522.366134 \nL 604.487489 517.261186 \nL 605.434398 522.634349 \nL 606.381307 520.235281 \nL 607.328216 520.994677 \nL 608.275125 519.997675 \nL 609.222034 524.164022 \nL 610.168943 522.969809 \nL 611.115852 523.140188 \nL 612.062761 522.993054 \nL 613.00967 519.991949 \nL 613.95658 519.562552 \nL 614.903489 522.707414 \nL 615.850398 524.415876 \nL 616.797307 523.920843 \nL 617.744216 519.694992 \nL 618.691125 520.985527 \nL 619.638034 520.754434 \nL 620.584943 524.217548 \nL 621.531852 523.070529 \nL 622.478761 521.123798 \nL 623.42567 521.420855 \nL 624.37258 519.388785 \nL 625.319489 522.823321 \nL 626.266398 520.56905 \nL 627.213307 523.08039 \nL 628.160216 520.360873 \nL 629.107125 523.076338 \nL 630.054034 522.324742 \nL 631.000943 521.840539 \nL 631.947852 521.922354 \nL 632.894761 521.796462 \nL 633.84167 522.074116 \nL 634.78858 518.91224 \nL 635.735489 520.697337 \nL 638.576216 522.540835 \nL 639.523125 519.788389 \nL 640.470034 519.83334 \nL 641.416943 522.625299 \nL 642.363852 524.082136 \nL 643.310761 522.59012 \nL 644.25767 520.305404 \nL 645.20458 524.997592 \nL 646.151489 524.263139 \nL 647.098398 522.111972 \nL 648.045307 523.059259 \nL 648.992216 523.169147 \nL 649.939125 525.340446 \nL 650.886034 524.055984 \nL 651.832943 521.43757 \nL 652.779852 523.803597 \nL 653.726761 521.031224 \nL 654.67367 521.39549 \nL 655.62058 520.955205 \nL 656.567489 524.689653 \nL 657.514398 521.910914 \nL 658.461307 521.268583 \nL 659.408216 525.197606 \nL 660.355125 523.364674 \nL 661.302034 519.739509 \nL 662.248943 519.46357 \nL 663.195852 520.732322 \nL 664.142761 522.35807 \nL 666.03658 524.509983 \nL 667.930398 523.06932 \nL 668.877307 522.631794 \nL 669.824216 524.044456 \nL 670.771125 519.3356 \nL 671.718034 522.815393 \nL 673.611852 521.949035 \nL 674.558761 523.073624 \nL 675.50567 522.135523 \nL 676.45258 521.49762 \nL 677.399489 524.005331 \nL 678.346398 524.55201 \nL 679.293307 524.714031 \nL 680.240216 523.458111 \nL 681.187125 524.04953 \nL 683.080943 520.589699 \nL 684.027852 521.8465 \nL 684.974761 523.409789 \nL 685.92167 522.198186 \nL 686.86858 522.127218 \nL 687.815489 523.110653 \nL 688.762398 518.805788 \nL 689.709307 521.318397 \nL 690.656216 521.83463 \nL 691.603125 526.183918 \nL 692.550034 521.173066 \nL 693.496943 521.324575 \nL 694.443852 523.231241 \nL 695.390761 519.961034 \nL 696.33767 522.882843 \nL 697.28458 521.570696 \nL 698.231489 523.959898 \nL 699.178398 521.191225 \nL 700.125307 521.509877 \nL 701.072216 524.126066 \nL 702.019125 518.452944 \nL 702.966034 518.441028 \nL 703.912943 522.390724 \nL 704.859852 521.643515 \nL 705.806761 519.41755 \nL 706.75367 524.582772 \nL 707.70058 521.661738 \nL 708.647489 524.040832 \nL 709.594398 521.229933 \nL 710.541307 522.008304 \nL 711.488216 521.768648 \nL 712.435125 526.589783 \nL 713.382034 524.760756 \nL 715.275852 520.353914 \nL 716.222761 523.448556 \nL 717.16967 520.135559 \nL 718.11658 524.608853 \nL 719.063489 525.062564 \nL 720.010398 521.660117 \nL 720.957307 522.863856 \nL 721.904216 520.987588 \nL 722.851125 521.806758 \nL 723.798034 523.842041 \nL 724.744943 521.137024 \nL 725.691852 523.363693 \nL 726.638761 522.102652 \nL 727.58567 525.650994 \nL 728.53258 525.396944 \nL 729.479489 521.565017 \nL 730.426398 524.740994 \nL 731.373307 521.554258 \nL 732.320216 520.77476 \nL 733.267125 525.290961 \nL 734.214034 517.461669 \nL 735.160943 523.131913 \nL 736.107852 521.058309 \nL 737.054761 522.11273 \nL 738.00167 521.373032 \nL 738.94858 523.407892 \nL 739.895489 522.739732 \nL 740.842398 524.091862 \nL 741.789307 523.266667 \nL 742.736216 523.704826 \nL 743.683125 521.129377 \nL 744.630034 521.750266 \nL 745.576943 526.064111 \nL 746.523852 521.967934 \nL 747.470761 525.257856 \nL 748.41767 518.76449 \nL 749.36458 522.254778 \nL 750.311489 523.991635 \nL 751.258398 522.921969 \nL 752.205307 523.770773 \nL 753.152216 526.316287 \nL 754.099125 525.389062 \nL 755.046034 522.386589 \nL 755.992943 521.930065 \nL 756.939852 519.526698 \nL 757.886761 521.678916 \nL 758.83367 519.405664 \nL 759.78058 520.110617 \nL 760.727489 523.324015 \nL 761.674398 522.046789 \nL 762.621307 526.046187 \nL 763.568216 524.124363 \nL 764.515125 520.029689 \nL 765.462034 523.666687 \nL 766.408943 522.900462 \nL 767.355852 525.200783 \nL 768.302761 523.907341 \nL 769.24967 522.132657 \nL 770.19658 521.519966 \nL 771.143489 521.279371 \nL 772.090398 520.737449 \nL 773.037307 523.503949 \nL 773.984216 520.520181 \nL 774.931125 522.485741 \nL 775.878034 521.423927 \nL 776.824943 520.775471 \nL 777.771852 522.171447 \nL 778.718761 521.459252 \nL 779.66567 522.594237 \nL 780.61258 522.054494 \nL 781.559489 521.929425 \nL 782.506398 523.524305 \nL 783.453307 521.121514 \nL 784.400216 525.346683 \nL 785.347125 523.387319 \nL 786.294034 522.663044 \nL 786.294034 522.663044 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p8fb15c2b4c)\" d=\"M 76.112216 112.654807 \nL 79.899852 192.417975 \nL 87.475125 345.904762 \nL 89.368943 379.027213 \nL 91.262761 408.795311 \nL 93.15658 434.055805 \nL 95.050398 455.034955 \nL 96.944216 471.95202 \nL 97.891125 478.835247 \nL 98.838034 484.757609 \nL 100.731852 494.565187 \nL 101.678761 498.4571 \nL 102.62567 501.739428 \nL 103.57258 504.445854 \nL 104.519489 506.635735 \nL 106.413307 509.760864 \nL 107.360216 510.931409 \nL 108.307125 511.768574 \nL 110.200943 513.000015 \nL 112.094761 513.697575 \nL 113.98858 513.969014 \nL 116.829307 514.058435 \nL 132.926761 513.96276 \nL 134.82058 514.053097 \nL 135.767489 513.989628 \nL 139.555125 514.268092 \nL 143.342761 514.307064 \nL 144.28967 514.288042 \nL 145.23658 514.412941 \nL 147.130398 514.401266 \nL 149.971125 514.706733 \nL 155.65258 514.857185 \nL 159.440216 515.12089 \nL 162.280943 515.169253 \nL 165.12167 515.428706 \nL 167.962398 515.517005 \nL 169.856216 515.738032 \nL 171.750034 515.802422 \nL 174.590761 515.977728 \nL 176.48458 515.978075 \nL 178.378398 516.142839 \nL 179.325307 516.038348 \nL 182.166034 516.288933 \nL 183.112943 516.386834 \nL 184.059852 516.61704 \nL 185.006761 516.715353 \nL 186.90058 516.718829 \nL 187.847489 516.815856 \nL 189.741307 516.750584 \nL 191.635125 517.055012 \nL 194.475852 516.932879 \nL 195.422761 517.142606 \nL 199.210398 517.478007 \nL 200.157307 517.420553 \nL 201.104216 517.714815 \nL 204.891852 518.151271 \nL 206.78567 518.214575 \nL 214.360943 518.975808 \nL 218.14858 519.288129 \nL 220.042398 519.436954 \nL 221.936216 519.713221 \nL 225.723852 519.730112 \nL 228.56458 520.187323 \nL 233.299125 520.596799 \nL 235.192943 520.815806 \nL 237.086761 521.2942 \nL 249.39658 522.234598 \nL 251.290398 522.488026 \nL 252.237307 522.412671 \nL 254.131125 522.874398 \nL 257.918761 523.688505 \nL 263.600216 524.112916 \nL 265.494034 524.457274 \nL 266.440943 524.56846 \nL 268.334761 524.528812 \nL 270.22858 524.824148 \nL 272.122398 525.369247 \nL 274.016216 525.663203 \nL 275.910034 525.617154 \nL 279.69767 526.228664 \nL 280.64458 526.27836 \nL 283.485307 526.681846 \nL 284.432216 526.644036 \nL 293.901307 527.931558 \nL 296.742034 528.443135 \nL 297.688943 528.392522 \nL 298.635852 528.585946 \nL 299.582761 528.507889 \nL 303.370398 529.056835 \nL 305.264216 529.550241 \nL 308.104943 529.732589 \nL 315.680216 530.398963 \nL 316.627125 530.414009 \nL 318.520943 530.814853 \nL 326.096216 531.606443 \nL 327.043125 531.607107 \nL 338.406034 532.97107 \nL 344.087489 533.445248 \nL 346.928216 533.744337 \nL 349.768943 533.801046 \nL 353.55658 534.312617 \nL 355.450398 534.323341 \nL 357.344216 534.482637 \nL 359.238034 534.457325 \nL 362.078761 535.084075 \nL 363.97258 535.199554 \nL 367.760216 535.468239 \nL 373.44167 535.806078 \nL 374.38858 535.540787 \nL 376.282398 535.896368 \nL 381.963852 536.404602 \nL 390.486034 536.933965 \nL 392.379852 536.879949 \nL 394.27367 537.093221 \nL 396.167489 537.133236 \nL 399.008216 537.438709 \nL 403.742761 537.566212 \nL 405.63658 537.684081 \nL 407.530398 537.670747 \nL 408.477307 537.818873 \nL 410.371125 537.830819 \nL 411.318034 538.00473 \nL 416.999489 538.182676 \nL 419.840216 538.34796 \nL 420.787125 538.243634 \nL 421.734034 538.334173 \nL 422.680943 538.282938 \nL 424.574761 538.50306 \nL 437.831489 538.895763 \nL 439.725307 538.96793 \nL 441.619125 539.047334 \nL 442.566034 538.931327 \nL 445.406761 539.160173 \nL 446.35367 539.109378 \nL 448.247489 539.162073 \nL 451.088216 539.269033 \nL 453.928943 539.392769 \nL 457.71658 539.473387 \nL 459.610398 539.570794 \nL 464.344943 539.463961 \nL 465.291852 539.586381 \nL 467.18567 539.486433 \nL 469.079489 539.613206 \nL 470.026398 539.522977 \nL 470.973307 539.626381 \nL 471.920216 539.57416 \nL 475.707852 539.810271 \nL 477.60167 539.81316 \nL 478.54858 539.871099 \nL 482.336216 539.748967 \nL 486.123852 539.958879 \nL 488.01767 539.929038 \nL 500.327489 540.206891 \nL 503.168216 540.210917 \nL 549.566761 540.644085 \nL 552.407489 540.590136 \nL 553.354398 540.56165 \nL 555.248216 540.734586 \nL 557.142034 540.712137 \nL 559.035852 540.556672 \nL 562.823489 540.678039 \nL 577.027125 540.87264 \nL 578.920943 540.800597 \nL 580.814761 540.846151 \nL 584.602398 540.842169 \nL 586.496216 540.859576 \nL 587.443125 540.94656 \nL 590.283852 540.890045 \nL 592.17767 540.978617 \nL 622.478761 541.015763 \nL 625.319489 541.02908 \nL 627.213307 541.024528 \nL 629.107125 541.039005 \nL 630.054034 540.99736 \nL 631.000943 541.079569 \nL 633.84167 540.931049 \nL 635.735489 541.047723 \nL 638.576216 541.013135 \nL 640.470034 541.067547 \nL 644.25767 541.024719 \nL 647.098398 541.082905 \nL 650.886034 540.942305 \nL 655.62058 541.048492 \nL 656.567489 540.987153 \nL 658.461307 541.078444 \nL 660.355125 541.014433 \nL 661.302034 541.083222 \nL 663.195852 541.064003 \nL 668.877307 541.054398 \nL 670.771125 541.100799 \nL 674.558761 541.089952 \nL 676.45258 541.079889 \nL 679.293307 541.11034 \nL 682.134034 541.004226 \nL 684.027852 541.128264 \nL 723.798034 541.083724 \nL 724.744943 541.04094 \nL 727.58567 541.203455 \nL 729.479489 541.155967 \nL 734.214034 541.164656 \nL 737.054761 541.206653 \nL 738.94858 541.106763 \nL 769.24967 541.168006 \nL 771.143489 541.163234 \nL 772.090398 541.107715 \nL 773.984216 541.14746 \nL 775.878034 541.072709 \nL 783.453307 541.140104 \nL 786.294034 541.078312 \nL 786.294034 541.078312 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 565.918125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 821.803125 565.918125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 565.918125 \nL 821.803125 565.918125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 821.803125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Model MSE -->\n    <g transform=\"translate(398.503125 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"418.310547\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"481.787109\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 59.674375 \nL 102.878125 59.674375 \nQ 104.878125 59.674375 104.878125 57.674375 \nL 104.878125 29.318125 \nQ 104.878125 27.318125 102.878125 27.318125 \nL 47.603125 27.318125 \nQ 45.603125 27.318125 45.603125 29.318125 \nL 45.603125 57.674375 \nQ 45.603125 59.674375 47.603125 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 35.416562 \nL 69.603125 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_21\">\n     <!-- train -->\n     <g transform=\"translate(77.603125 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 50.094687 \nL 69.603125 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_22\">\n     <!-- test -->\n     <g transform=\"translate(77.603125 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8fb15c2b4c\">\n   <rect height=\"543.6\" width=\"781.2\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "11/11 [==============================] - 0s 799us/step\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}